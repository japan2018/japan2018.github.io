<!doctype html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  
  <meta name="generator" content="Hugo 0.72.0" />

  
  <meta name="description" content="Some description">
  

  
  <link rel="apple-touch-icon" sizes="180x180" href="https://memotut.com/apple-touch-icon.png">

  
  <link rel="icon" type="image/png" sizes="32x32" href="https://memotut.com/favicon-32x32.png">

  
  <link rel="icon" type="image/png" sizes="16x16" href="https://memotut.com/favicon-16x16.png">

  
  <link rel="manifest" href="https://memotut.com/site.webmanifest">

  
  <link rel="mask-icon" href="https://memotut.com/safari-pinned-tab.svg" color="#5bbad5">

  <meta name="msapplication-TileColor" content="#da532c">

  <meta name="theme-color" content="#ffffff">

  
  <link rel="stylesheet" href="https://memotut.com/css/bootstrap.min.css" />

  
  <title>[Python] Why Deep Metric Learning based on the Softmax function works | Memo Tut</title>
  

  <style>
body {
  min-width: 300px;
}

.custom-navbar {
  margin-bottom: 1em;
  height: 60px;
}

.custom-navbar a {
  display: inline-block; 
  padding: 18px 0;
  margin-right: 1em; 
  font-weight: bold; 
}

.custom-navbar a:hover,
.custom-navbar a:focus {
  text-decoration: none; 
}

@media print {
  .custom-navbar {
    display: none;
  }
}

article {
  padding-bottom: 1em;
}

img {
  max-width: 100%;
}


body {
  background-color: #fff;
}



body {
  color: #212529;
}



a {
  color: #007bff;
}



a:hover,
a:focus {
  color: #0056b3;
}



.custom-navbar {
  background-color: #212529;
}



.custom-navbar a {
  color: rgba(255, 255, 255, 0.75);
}



.custom-navbar a:hover,
.custom-navbar a:focus {
  color: rgba(255, 255, 255, 1);
}



.container {
  max-width: 800px;
}



pre {
  display: block;
  padding: 9.5px;
  word-break: break-all;
  word-wrap: break-word;
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 4px;
}

pre code {
  padding: 0;
  font-size: inherit;
  color: inherit; 
  white-space: pre-wrap;
  background-color: transparent;
  border: none;
  border-radius: 0;
}

code {
  padding: 2px 4px;
  color: inherit; 
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 4px;
  font-size: .9em;
}



blockquote,
.blockquote {
  padding: 10px 20px;
  margin: 0 0 20px;
  font-size: 1em;
  border-left: 5px solid #6c757d;
}

</style>
</head>

<body>
  <nav class="custom-navbar">
  <div class="container">
    
    <a href="/">Posts</a>
    
    <a href="/tags/">Tags</a>
    
    <a href="/about/">About</a>
    
    <a href="/index.xml">RSS</a>
    
  </div>
</nav>
  
  <div class="container">
    <article>
      <h1>[Python] Why Deep Metric Learning based on the Softmax function works</h1>
<p>
  <small class="text-secondary">
  
  
  Feb 6, 2020
  </small>
  

<small><code><a href="https://memotut.com/tags/python">Python</a></code></small>


<small><code><a href="https://memotut.com/tags/machine-learning"> Machine Learning</a></code></small>


<small><code><a href="https://memotut.com/tags/deep-learning"> Deep Learning</a></code></small>


<small><code><a href="https://memotut.com/tags/paper-reading"> Paper Reading</a></code></small>


<small><code><a href="https://memotut.com/tags/metric-learning"> Metric Learning</a></code></small>

</p>
<pre><code>#Introduction
</code></pre>
<p>In distance learning (metric learning) using deep learning, similarities (and dissimilarities) between images are used by using Contrastive Loss and <a href="https://qiita.com/tancoro/items/35d0925de74f21bfff14">Triplet Loss</a>.) Is widely used, but it is also known that it is difficult to combine and select learning data, and learning itself is difficult in many cases. For that reason, various improvements and ideas have been proposed so far.
However, recently, metric learning, which can learn based on the Softmax function as if it is a general class classification task without the difficulty of selecting learning data like this, has been attracting attention. ArcFace is a typical method and is explained in detail on <a href="https://qiita.com/yu4u/items/078054dfb5592cbb80cc">here</a>.
Reason why Metric Learning based on Softmax function works, and is there room for further improvement? I will introduce a little about these.</p>
<h1 id="center-loss">Center Loss</h1>
<p>Center Loss<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup> is probably the ancestor that introduced the element of Metric Learning into the classification. About Center Loss, it was explained in detail in <a href="https://cpp-learning.com/center-loss/">here</a>, so I will briefly introduce only the main points.
Center Loss learns class classification and class center position on the feature space at the same time by setting a penalty on the distance between each feature vector and the corresponding class center. This will reduce intra-class variation and was announced at ECCV2016 as a more effective class separation method.</p>
<p>In this paper, the FC layer, which is one layer before the final layer of the CNN, is improved to two dimensions, and the values are plotted as they are on the two-dimensional plane to visualize the features. The figure below visualizes the intermediate features when classifying MNIST with the normal Softmax function.</p>
<img width="90%" alt="Screenshot 2020-02-03 17.45.57.png" src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/146511/2ba98ba7-1845-8092-58e4-0b9b26f5ec64.png">
<p>The loss function is the normal Softmax Cross Entropy Loss plus the term of Center Loss that represents the distance between each feature vector and the center vector of its class.
<img width="70%" alt="Screenshot 2020-02-03 18.09.05.png" src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/146511/7a3aefd5-a2c7-5db3-f230-633720df8901.png"></p>
<p>Since it is not realistic to calculate the class center $C_{y_i}$ exactly each time, the following calculation is performed for each mini-batch and the class center $C_{y_i}$ is updated sequentially. I will.</p>
<img width="60%" alt="Screenshot 2020-02-03 19.02.18.png" src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/146511/7b43dc9c-5971-ab31-4709-b9edf1d6be78.png">
<img width="100%" alt="Screenshot 2020-02-03 19.02.32.png" src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/146511/d8186bbd-b8d3-b2e6-f7eb-b1b5ccc55dfd.png">
<img width="40%" alt="Screenshot 2020-02-03 19.06.13.png" src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/146511/fa1dd667-4b6d-4408-7cf8-38f40e79acdd.png">
<p>The following is a concrete update example centered on the class. If the data belonging to class A is $A1$, $A2$, and $A3$ in the mini-batch, the center position $C_A$ of class A is updated as shown below.</p>
<img width="90%" alt="Screenshot 2020-02-03 19.22.49.png" src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/146511/0372e982-e656-8011-d2dd-46994d29d657.png">
<p>Finally, I will post a diagram that visualizes the results of Center Loss. It can be seen that by increasing the value of hyperparameter $\lambda$, the within-class variation is also reduced.</p>
<img width="80%" alt="Screenshot 2020-02-03 19.32.22.png" src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/146511/d770393e-bbfe-d7bc-2f06-495e62fdac7b.png">
<h1 id="sphereface">Sphereface</h1>
<p>We found that the feature space when learning using the Softmax function has a unique angle distribution, and argued that the margin based on the angle is more suitable than the margin based on the Euclidean distance like Center Loss. Proposed <sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup> Angular Softmax (A-Softmax) Loss and adopted by CVPR2017.</p>
<p>I would like to look at it in order. First, the normal Softmax Cross Entropy Loss is as follows.
<img width="40%" alt="Screenshot 2020-02-03 22.30.08.png" src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/146511/916a1613-bee3-305d-3779-799f4cbae40f.png">
Then, this is the dot product formula $\boldsymbol{a}\cdot\boldsymbol{b}=||\boldsymbol{a}||,||\boldsymbol{b}|| Rewrite the inner product part of $\boldsymbol{W^T_j}$ and $\boldsymbol{x_i}$ using \cos\theta$.
<img width="60%" alt="Screenshot 2020-02-03 22.49.50.png" src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/146511/afb5236d-ee72-fb64-918c-49965689b2f7.png">
Furthermore, the weight parameters are normalized so that $||\boldsymbol{W_j}|| = 1$ and the bias $b_j$ is also set to $0$.
Then, it becomes as follows, and this is defined as Modified Softmax Loss.
<img width="70%" alt="Screenshot 2020-02-03 23.03.35.png" src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/146511/8e3f73d3-0959-48f3-b276-3252545b4002.png">
Next, let&rsquo;s check what the decision boundary in this $L_{modified}$ is. A decision boundary is a boundary value at which two classes have exactly the same probability.</p>
<p>If the angle between the weight $W_1$ of the class 1 and the feature amount $x_i$ is $\cos\theta_1$, and the angle between the weight $W_2$ of the class 2 and the feature amount $x_i$ is $\cos\theta_2$, The two probabilities are exactly the same when the following conditions are met.</p>
<pre><code class="language-math" data-lang="math">\frac{e^{\|\boldsymbol{x_i}\|\cos\theta_1}}{\sum_{j} e^{\|\boldsymbol{x_i}\|\cos(\theta_{j,i}) }} = \frac{e^{\|\boldsymbol{x_i}\|\cos\theta_2}}{\sum_{j} e^{\|\boldsymbol{x_i}\|\cos(\theta_{j, i})}} \\
\|\boldsymbol{x_i}\|\cos\theta_1 = \|\boldsymbol{x_i}\|\cos\theta_2 \\
\theta_1 = \theta_2
</code></pre><p>Therefore, the probabilities of class 1 and class 2 are exactly the same when there is $x_i$ in the direction of the straight line that bisects the angle of weights $W_1$ and $W_2$.
<img width="30%" alt="Screenshot 2020-02-03 23.42.12.png" src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/146511/f0c1edc0-11ae-f42f-4e76-f21cc7f35418.png">
From this result, it can be seen that the feature $x_i$ learned by Softmax Loss essentially has a distribution along the angle of the final weight $Wj$ immediately before applying softmax.
<img width="90%" alt="Screenshot 2020-02-03 23.54.56.png" src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/146511/a9bba142-4b8a-0108-08b7-8d8b4622456a.png">
This suggests that combining the Euclidean distance-based margin such as Center Loss with the Softmax function does not have a good affinity, and **providing a margin along the angle is the most effective. Target **.</p>
<h2 id="angular-softmax-a-softmax-loss">Angular Softmax (A-Softmax) Loss</h2>
<p>Angular Softmax (A-Softmax) Loss is defined as follows.
<img width="80%" alt="Screenshot 2020-02-04 0.29.05.png" src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/146511/074ebbf4-a762-d0e8-5459-a89ae70a980e.png">For $L_{modified}$, a penalty of coefficient $m$ is given only to the angle $\theta_{y_i,i}$ formed by the weight of own class $W_{y_i}$. It is expected to bring the effect closer to the own class weight $W_{y_i}$ than the position.
<img width="205" alt="Screenshot 2020-02-04 0.39.42.png" src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/146511/ee10fb46-84ce-d647-34d6-932e8396d988.png"></p>
<h2 id="cosface-and-arcface">CosFace and ArcFace</h2>
<p>▽ Several methods of taking a margin along the angle as claimed by Sphereface have also been proposed after Sphereface. Examples include CosFace<sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup> and ArcFace<sup id="fnref:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup>.
<img width="90%" alt="Screenshot 2020-02-04 0.59.41.png" src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/146511/960d1884-1391-e490-b96f-3851ac54912a.png"></p>
<p>I will omit the details, but in brief, CosFace can also normalize $x_i$, so the angle between two vectors $\cos\theta$ is expressed as a similarity. It is now possible. Also, hyperparameter $s$ is introduced to prevent Softmax from failing because the value of $\cos\theta$ is too small.
After that, ArcFace has changed the location of the margin. Since the margin is directly specified for the angle, the separation boundary in Softmax is linear and constant over the entire interval.</p>
<h1 id="uniform-loss">Uniform Loss</h1>
<p>By adding a new term called Uniform Loss to Cosine Based Softmax Loss such as Sphereface, we propose a method that imposes an equal distribution constraint that evenly distributes class centers to maximize the use of feature space [^Uniformface ]it was done. Adopted at CVPR2019. Uniform Loss thinks the center of a class on the hypersphere, which is a feature space, like a charge with repulsion between classes, and works to minimize its potential energy. Many of the existing methods so far aim to increase the interclass distance and to reduce the intraclass variation, but Uniform Loss also expects to maximize the minimum interclass distance. I can do it.</p>
<p>In the case of <strong>Center Loss</strong>, the distance between the element in the class and the center of the class is simply minimized, and the relationship between the classes is not described in the first place.
<img width="80%" alt="Screenshot 2020-02-04 13.03.15.png" src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/146511/6d89b9a8-c7fd-8765-d5d0-2b9f17e7167e.png"></p>
<p>In the case of <strong>Sphereface</strong>, <strong>Cosface</strong>, and <strong>Arcface</strong>, discriminative boundaries with margins are learned on the hypersphere, resulting in small intra-class variation and less inter-class distance. Learning progresses as it grows. However, there is nothing that explicitly constrains the overall distribution of the feature space, so it may be locally located in the feature space and unbalanced.
<img width="70%" alt="Screenshot 2020-02-04 13.08.08.png" src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/146511/c5074e62-0f75-60da-1b3d-5ca5851b2498.png"></p>
<p>Uniform Loss is motivated by the fact that when the electric charges are arranged on the surface of the sphere, the potential energy is minimized when they are evenly distributed. Express the energy function as Loss.
<img width="80%" alt="Screenshot 2020-02-04 13.15.19.png" src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/146511/55d3afc6-e580-be9b-9aec-336a730f7611.png"></p>
<p>A repulsive force $F$ that is inversely proportional to the square of the distance is defined between two classes.
<img width="30%" alt="Screenshot 2020-02-04 13.17.52.png" src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/146511/79736825-0fb1-5672-9b68-eedc38b874a1.png"></p>
<p>The distance function $d$ follows the Coulomb&rsquo;s law and uses the Euclidean distance. Also, add 1 to prevent the repulsive force $F$ from becoming too large.
<img width="40%" alt="Screenshot 2020-02-04 13.21.18.png" src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/146511/101a733e-b6fb-4ee4-e559-c6feca3c3d85.png"></p>
<p>We formulate Uniform Loss as the average value of all class-centered pairwise energies.
<img width="50%" alt="Screenshot 2020-02-04 13.25.32.png" src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/146511/62032b84-f16e-daf1-0eff-6036eedd5bb5.png"></p>
<p>The part of the Loss (Cosine Based Softmax Loss) function of Sphereface is as follows.
<img width="70%" alt="Screenshot 2020-02-04 13.30.10.png" src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/146511/01a75d29-b53a-3973-e40a-f82fd2cc3856.png"></p>
<p>The sum of these two Loss is the final Loss function.
<img width="20%" alt="Screenshot 2020-02-04 13.31.25.png" src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/146511/e326acd4-389a-2cc5-f81a-5e62317a2df1.png"></p>
<p>Finally, the class-centric update method, which does exactly the same as Center Loss.</p>
<p>As an aside, by arranging N points on a 3D sphere and arranging them so that the minimum value of the spherical distance between points becomes maximum, what is the length of the spherical distance? The problem is called the Tammes problem, and it seems to be one of the mathematical unsolved problems for which a complete answer has not yet been found.</p>
<p>#P2SGrad
As a joint research between Sense Time Research and The Chinese University of Hong Kong, two papers, AdaCos<sup id="fnref:5"><a href="#fn:5" class="footnote-ref" role="doc-noteref">5</a></sup> and P2SGrad<sup id="fnref:6"><a href="#fn:6" class="footnote-ref" role="doc-noteref">6</a></sup>, were simultaneously published by the same Author on May 7, 2019. Both papers have been accepted by CVPR2019 (AdaCos has adopted Oral). In addition to the proposed method, I found it particularly interesting to mention the essential difference in Cosine Based Softmax Loss such as Sphereface, Cosface, and Arcface, and the process of gradient optimization. AdaCos has already been explained in <a href="https://qiita.com/ninhydrin/items/797a308c864e9238c71e">here</a>, so I would like to take a closer look at P2SGrad.</p>
<p>Now, for P2SGrad, I think it is worth mentioning that the model can be optimized without formulating a specific loss function. Regardless of metric learning, it is a common practice to define a loss function in general deep learning and perform learning while updating weights and bias so that the loss function becomes smaller. However, in P2SGrad, the optimum gradient direction and magnitude for updating the weight vector in the feature space are directly obtained, and backpropagation is performed to optimize the parameters. Furthermore, the fact that you don&rsquo;t have to define a loss function means you don&rsquo;t have to adjust hyperparameters like $m$ and $s$ that were in the loss functions of ArcFace and CosFace.</p>
<h2 id="cosine-based-softmax-loss-backpropagation">Cosine Based Softmax Loss Backpropagation</h2>
<p>First, check the slope of Cosine Based Softmax Loss without the angular margin.
The next $f_{i,j}$ is the logits for the class $j$ of the feature $\vec{x_i}$.</p>
<pre><code class="language-math" data-lang="math">f_{i,j} = s \cdot \frac{\langle \vec{x_i}, \vec{W_j} \rangle}{\|\vec{x_i}\|_2\|\vec{W_j}\|_2 } = s \cdot \langle \boldsymbol{\hat{x_i}}, \boldsymbol{\hat{W_j}} \rangle = s \cdot \cos\theta_{i,j} \tag{1}
</code></pre><p>$\boldsymbol{\hat{x_i}}$ and $\boldsymbol{\hat{W_j}}$ represent the normalized vectors of $\vec{x_i}$ and $\vec{W_j}$, respectively.</p>
<p>If the number of classes is $C$, the probability $P_{i,j}$ that the $i$th feature is the class $j$ can be written by the Softmax function as follows.</p>
<pre><code class="language-math" data-lang="math">P_{i,j} = Softmax(f_{i,j}) = \frac{e^{f_{i,j}}}\\sum^{C}_{k=1}e^{f_{i ,k}}} \tag{2}
</code></pre><p>Cross Entropy Loss when the correct answer class is $y_i$ is as follows.</p>
<pre><code class="language-math" data-lang="math">L_{CE}(\vec{x_i}) =-\log{P_{i, y_i}} = -\log{\frac{e^{f_{i,y_i}}}{\sum^{C}_ {k=1}e^{f_{i,k}}}} \tag{3}
```For this $L_{CE}(\vec{x_i})$, find the gradient of $\vec{x_i}$ and $\vec{W_j}$.

```math
\frac{\partial L_{CE}(\vec{x_i})}{\partial \vec{x_i}} = \sum_{j=1}^{C}(P_{i,j}-\mathbb{1 }(y_i=j)) \nabla f(\cos\theta_{i,j}) \cdot \frac{\partial \cos\theta_{i,j}}{\partial \vec{x_i}} \tag{ Four}
</code></pre><pre><code class="language-math" data-lang="math">\frac{\partial L_{CE}(\vec{x_i})}{\partial \vec{W_j}} = (P_{i,j}-\mathbb{1}(y_i=j)) \nabla f( \cos\theta_{i,j}) \cdot \frac{\partial \cos\theta_{i,j}}{\partial \vec{W_j}} \tag{5}
</code></pre><p>The function $\mathbb{1}(y_i=j)$ is $1$ when $j=y_i$ and $0$ otherwise.
In addition, $\frac{\partial \cos\theta_{i,j}}{\partial \vec{x_i}}$ and $\frac{\partial \cos\theta_{i,j}}{\partial \vec{ When W_j}}$ is calculated, the following vector values are obtained.</p>
<pre><code class="language-math" data-lang="math">\frac{\partial \cos\theta_{i,j}}{\partial \vec{x_i}} = \frac{1}{\|\vec{x_i}\|_2}(\boldsymbol{\hat{W_j }}-\cos\theta_{i,j} \cdot \boldsymbol{\hat{x_i}}) \tag{6}
</code></pre><pre><code class="language-math" data-lang="math">\frac{\partial \cos\theta_{i,j}}{\partial \vec{W_j}} = \frac{1}{\|\vec{W_j}\|_2}(\boldsymbol{\hat{x_i }}-\cos\theta_{i,j} \cdot \boldsymbol{\hat{W_j}}) \tag{7}
</code></pre><p>Next, calculate the part of $\nabla f(\cos\theta_{i,j})$. Since $f$ is a function that represents the logits part, $f(\cos\theta_{i,j})$ without an angular margin is $s \cdot \cos\theta_{i,j}$. So $\nabla f(\cos\theta_{i,j}) = s$.
For Cosface, $f(\cos\theta_{i,y_i})=s \cdot (\cos\theta_{i,y_i}-m)$, so $\nabla f(\cos\theta_{i,y_i} ) = s$, and for Arcface $f(\cos\theta_{i,y_i})=s \cdot \cos(\theta_{i,y_i} + m)$, so $\nabla f(\cos \theta_{i,y_i}) = s \cdot \frac{\sin(\theta_{i,y_i} + m)}{\sin\theta_{i,y_i}}$.</p>
<p>In any case, $\nabla f(\cos\theta_{i,j})$ is a scalar value determined by the parameters $s$, $m$ and $\cos\theta_{i,y_i}$. Also, since the value of $s&gt;1$ is usually used, all of the above are $\nabla f(\cos\theta_{i,j})&gt; 1$.</p>
<p>Now, let&rsquo;s reorganize $\frac{\partial L_{CE}(\vec{x_i})}{\partial \vec{W_j}}$ by dividing it by $j$.</p>
<p>For $j \neq y_i$</p>
<pre><code class="language-math" data-lang="math">\frac{\partial L_{CE}(\vec{x_i})}{\partial \vec{W_j}} = P_{i,j} \cdot \nabla f(\cos\theta_{i,j}) \ cdot \frac{\partial \cos\theta_{i,j}}{\partial \vec{W_j}} = T \cdot \frac{\partial \cos\theta_{i,j}}{\partial \vec{ W_j}} \tag{8}
</code></pre><p>If $j = y_i$</p>
<pre><code class="language-math" data-lang="math">\frac{\partial L_{CE}(\vec{x_i})}{\partial \vec{W_j}} = (P_{i,j}-1) \cdot \nabla f(\cos\theta_{i, j}) \cdot \frac{\partial \cos\theta_{i,j}}{\partial \vec{W_j}} = U \cdot \frac{\partial \cos\theta_{i,j}}{\ partial \vec{W_j}} \tag{9}
</code></pre><p>Since $P_{i,j}$ is a Softmax value, the range of possible values is $P_{i,j} \in [0, 1]$. So $T$ and $U$ in the above equation are scalar values in the range $T&gt;0$ and $U&lt;0$ respectively.</p>
<p>Backpropagation optimizes the weight vector $\vec{W_j}$ as follows.</p>
<pre><code class="language-math" data-lang="math">\vec{W_j} \leftarrow \vec{W_j}-\eta\cdot \frac{\partial L_{CE}}{\partial \vec{W_j}} \tag{10}
</code></pre><p>Based on these facts, we can see the following.</p>
<ul>
<li>
<p>Weight parameter $\vec{W_{y_i}}$ of the correct answer class is $\frac{\partial \cos\theta_{i,j}}{perpendicular to $\vec{W_{y_i}}$ by backpropagation. It is updated in the direction of \partial \vec{W_j}}$. And the length of the gradient vector is $(P_{i,j}-\mathbb{1}(y_i=j)) \nabla f(\cos\theta_{i,j})$.</p>
</li>
<li>
<p>The weight parameter $\vec{W_j}$ of the class which is not correct is $-\frac{\partial \cos\theta_{i,j}}{\partial \ which is perpendicular to $\vec{W_j}$ by Backpropagation. It is updated in the direction of vec{W_j}}$. And the length of the gradient vector is $(P_{i,j}-\mathbb{1}(y_i=j)) \nabla f(\cos\theta_{i,j})$.</p>
</li>
<li>
<p>$\frac{\partial \cos\theta_{i,j}}{\partial \vec{W_j}}$ is perpendicular to $\vec{W_j}$, so this is the fastest and most optimal This is a reasonable update.</p>
</li>
<li>
<p>There are various Cosine Based Softmax Loss such as CosFace and ArcFace, but the gradient directions to be updated are all the same. The essential difference between them is the length of the gradient, which greatly influences the model optimization.</p>
</li>
</ul>
<img width="50%" alt="Screenshot 2020-02-05 8.19.29.png" src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/146511/811dc1a6-29bd-75fb-e2c4-294e7fbb47f2.png">
<h2 id="slope-length">slope length</h2>
<p>As we have seen, the length of the gradient is $(P_{i,j}-\mathbb{1}(y_i=j)) \cdot \nabla f(\cos\theta_{i,j})$ , **The value of $P_{i,j}$ has a great influence on the length of the gradient**. In addition, $P_{i,j}$ is positively correlated with logits $f_{i,j}$. And logits $f_{i,j}$ are strongly influenced by hyperparameters $s$ and $m$.</p>
<p>Let&rsquo;s look at an example of ArcFace. $f_{i,y_i}$ is $s \cdot \cos(\theta_{i,y_i} + m)$. It can be seen that if the hyperparameters $s$ and $m$ are different, even if the same $\theta_{i,y_i}$, the value of $P_{i,y_i}$ changes significantly.</p>
<img width="80%" alt="Screenshot 2020-02-05 10.30.36.png" src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/146511/985999ab-0f40-035b-e573-e96c3151acc3.png">
<p>What you should think about here is the original purpose of ArcFace. ArcFace does not want to classify, but wants to know if the angle $\theta$ formed by the two extracted vectors $\vec{x_1}$ and $\vec{x_2}$ is close. In other words, the size of $\theta_{i,y_i}$ originally has to influence the length of the gradient (that is, $P_{i,j}$) that is important in Backpropagation. , Hyperparameters $s$ and $m$ are strongly involved.
Furthermore, another one. The value of $P_{i,y_i}$ also changes depending on the number of classes $C$. As the number of classes $C$ increases, the probability of being assigned to the incorrect class $P_{i,j}$ is expected to decrease more or less. This makes sense for closed classification problems. However, this is not suitable for face recognition, which is a problem with Open-Set.</p>
<p>There is also $\nabla f(\cos\theta_{i,j})$ as a value that affects the length of the gradient. This depends on the type of Cosine Based Softmax Loss.</p>
<p>In case of Cosine Based Softmax Loss without angle margin,</p>
<pre><code class="language-math" data-lang="math">\nabla f(\cos\theta_{i,j}) = s \tag{11}
</code></pre><p>For Cosface,</p>
<pre><code class="language-math" data-lang="math">\nabla f(\cos\theta_{i,j}) = s \tag{12}
</code></pre><p>For Arcface,</p>
<pre><code class="language-math" data-lang="math">\nabla f(\cos\theta_{i,j}) = s \cdot \frac{\sin(\theta_{i,j} + m)}{\sin\theta_{i,j}} \tag{13 }
</code></pre><p>Will be.In other words, in Cosine Based Softmax Loss without angular margin and Cosface, the effect of $\nabla f(\cos\theta_{i,j})$ on the gradient length is always constant $s$. It shows a thing. But on Arcface, the gradient length and $\theta_{i,j}$ are negatively correlated, which is completely unexpected. Gradually decreasing $\theta_{i,y_i}$ generally tends to reduce the slope of $P_{i,y_i}$, but Arcface tends to lengthen it. Therefore, Arcface does not explain the geometrical meaning of $\nabla f(\cos\theta_{i,j})$ on the length of the gradient.</p>
<img width="90%" alt="Screenshot 2020-02-05 11.59.19.png" src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/146511/4d7640f5-42fd-05d4-c8ca-182848dfd60b.png">
<h2 id="p2s-grads-proposal-to-determine-the-length-of-the-gradient">P2S Grad&rsquo;s proposal to determine the length of the gradient</h2>
<p>From here, the main subject is to explain a new method <strong>P2SGrad</strong> that determines the length of the gradient only by $\theta_{i,j}$, without depending on hyperparameters, number of classes, or logits. I will. Since P2SGrad does not define a specific loss function but directly calculates the slope, we will review the equations $(4)$ and $(5)$.
First of all, as explained so far, there is no need to change here because it is the most optimal direction. Next, consider $P_{i,j}$ and $\nabla f(\cos\theta_{i,j})$, which are the lengths of the gradient. Regarding $\nabla f(\cos\theta_{i,j})$, I am reviewing it for the purpose of excluding hyperparameters from the beginning, so I will set it to $1$ here.</p>
<p>Next, consider $P_{i,j}$, but use $\cos\theta_{i,j}$ instead of $P_{i,j}$. There are three reasons.</p>
<ul>
<li>$P_{i,j}$ and $\cos\theta_{i,j}$ have the same theoretical range $[0, 1]$. However, it is $\theta_{i,j} \in [0, \pi/2]$.</li>
<li>Unlike $P_{i,j}$, $\cos\theta_{i,j}$ is not affected by either hyperparameters or the number of classes $C$.</li>
<li>During inference, face recognition is performed based on the similarity of $\cos\theta$. $P_{i,j}$ is only the probability that it applies only to closed classification tasks.</li>
</ul>
<p>Taking these into account, we formulate a new P2SGrad. The expressions $(4)$ and $(5)$ are replaced by the expressions $(14)$ and $(15)$, respectively.</p>
<pre><code class="language-math" data-lang="math">\tilde{G}_{P2SGrad}(\vec{x_i}) = \sum_{j=1}^{C}(\cos\theta_{i,j}-\mathbb{1}(y_i=j)) \cdot \frac{\partial \cos\theta_{i,j}}{\partial \vec{x_i}} \tag{14}
</code></pre><pre><code class="language-math" data-lang="math">\tilde{G}_{P2SGrad}(\vec{W_j}) = (\cos\theta_{i,j}-\mathbb{1}(y_i=j)) \cdot \frac{\partial \cos\theta_ {i,j}}{\partial \vec{W_j}} \tag{15}
</code></pre><p>This P2S Grad is concise but very reasonable. If $y_i = j$, the gradient length and $\theta_{i,j}$ have a positive correlation, and if $j \neq y_i$, a negative correlation holds.</p>
<p>Finally, I will explain how to learn. In general learning, foward processing is used to obtain the Cross Entropy Loss of the equation $(3)$, and then backpropagation back propagates the equations $(4)$ and $(5)$ from the final layer, Gradually calculate the slope of the bias parameter. In learning using P2SGrad, the formulas $(14)$ and $(15)$ are obtained directly from the feature amount $\vec{x_i}$ obtained by the foward process. After that, backpropagation is carried out in the same procedure as normal learning, and the weights of each layer and the gradient of the bias parameter are obtained in order.</p>
<h2 id="experiment">Experiment</h2>
<p>For detailed experiment results, see the original paper, and for your reference, I will post the results of experiments on the LWF data set. The CNN part uses ResNet-50 and shows the relationship between the learning iteration and the average value of $\theta_{i,y_i}$.
<img width="80%" alt="Screenshot 2020-02-05 16.13.14.png" src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/146511/cd7f203f-9497-cf22-beb2-875488f553e4.png">
It can be seen that learning by P2S Grad decreases $\theta_{i,y_i}$ faster than other Cosine Based Softmax Loss.</p>
<section class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1" role="doc-endnote">
<p>Y. Wen, K Zhang, Z Li, and Y. Qiao. “A Discriminative Feature Learning Approach for Deep Face Recognition.” In ECCV, 2016. <a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2" role="doc-endnote">
<p>W. Liu, Y. Wen, Z. Yu, M. Li, B. Raj, and L. Song. “Sphereface: Deep hypersphere embedding for face recognition.” In CVPR, 2017. <a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3" role="doc-endnote">
<p>H. Wang, Y. Wang, Z. Zhou, X. Ji, D. Gong, J. Zhou, Z. Li, and W. Liu, &ldquo;CosFace: Large Margin Cosine Loss for Deep Face Recognition, &ldquo;in Proc. of CVPR, 2018. <a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:4" role="doc-endnote">
<p>J. Deng, J. Guo, N. Xue, and S. Zafeiriou. &ldquo;ArcFace: Additive Angular Margin Loss for Deep Face Recognition,&rdquo; In CVPR, 2019. <a href="#fnref:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:5" role="doc-endnote">
<p>X. Zhang, R. Zhao, Y. Qiao, X. Wang, and H. Li. &ldquo;AdaCos: Adaptively Scaling Cosine Logits for Effectively Learning Deep Face Representations.&rdquo; in CVPR, 2019. <a href="#fnref:5" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:6" role="doc-endnote">
<p>X. Zhang, R. Zhao, J. Yan, M. Gao, Y. Qiao, X. Wang, and H. Li. &ldquo;P2SGrad: Refined Gradients for Optimizing Deep Face Models.&rdquo; in CVPR, 2019 . <a href="#fnref:6" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</section>

    </article>
  </div>

  
  
  
  <script>
  window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
  ga('create', 'UA-169005401-1', 'auto');
  ga('send', 'pageview');
  </script>
  <script async src='https://www.google-analytics.com/analytics.js'></script>
  

  

  
<link rel="stylesheet" type="text/css" href="//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.0/cookieconsent.min.css" />
<script src="//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.0/cookieconsent.min.js"></script>
<script>
window.addEventListener("load", function(){
window.cookieconsent.initialise({
  "palette": {
    "popup": {
      "background": "#216942",
      "text": "#b2d192"
    },
    "button": {
      "background": "#afed71"
    }
  }
})});
</script>

</body>

</html>
