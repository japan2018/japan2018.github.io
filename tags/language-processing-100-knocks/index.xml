<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title> language processing 100 knocks on Some Title</title>
    <link>https://japan2018.github.io/tags/language-processing-100-knocks/</link>
    <description>Recent content in  language processing 100 knocks on Some Title</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 03 May 2020 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://japan2018.github.io/tags/language-processing-100-knocks/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>[Chapter 4] Introduction to Python with 100 knocks</title>
      <link>https://japan2018.github.io/chapter-4-introduction-to-python-with-100-knocks-fc4ad/</link>
      <pubDate>Sun, 03 May 2020 00:00:00 +0000</pubDate>
      
      <guid>https://japan2018.github.io/chapter-4-introduction-to-python-with-100-knocks-fc4ad/</guid>
      <description>This article is a sequel to my book [Introduction to Python with 100 knocks](https://qiita.com/hi-asano/items/8e303425052781d95f09).[100knocksChapter4](https://nlp100.github.io/ja/ch04.html) will be explained.  First, install the morphological analyzer MeCab, download neko.txt, and perform morphological analysis to check the contents.
$ mecab &amp;lt;neko.txt&amp;gt; neko.txt.mecab It is &amp;ldquo;I am a cat&amp;rdquo; from Aozora Bunko.
MeCab&amp;rsquo;s default dictionary system is similar to school grammar, but adjective verbs are nouns + auxiliary verbs, and Sa verbs are nouns + verbs.</description>
    </item>
    
    <item>
      <title>100 language processing knocks-38 (using pandas): histogram</title>
      <link>https://japan2018.github.io/100-language-processing-knocks-38-using-pandas-histogram-f9e48/</link>
      <pubDate>Mon, 02 Mar 2020 00:00:00 +0000</pubDate>
      
      <guid>https://japan2018.github.io/100-language-processing-knocks-38-using-pandas-histogram-f9e48/</guid>
      <description>[100 language processing knocks 2015](http://www.cl.ecei.tohoku.ac.jp/nlp100/)[&amp;quot;Chapter4:Morphologicalanalysis&amp;quot;](http://www.cl.ecei.tohokuItisa[38th&amp;quot;histogram&amp;quot;](http://www.cl.ecei.tohoku.ac.jp/nlp100/#sec38)recordof.ac.jp/nlp100/#ch4).  It&amp;rsquo;s easy as long as you&amp;rsquo;ve overcome the tofu from the previous knock. If you don&amp;rsquo;t put out a label, you don&amp;rsquo;t have to deal with &amp;ldquo;tofu&amp;rdquo;.
Reference link    Links Remarks     038.Histogram.ipynb Answer Program GitHub Link   [Amateur language processing 100 knocks: 38 Copy and paste source of many source parts   MeCab Official MeCab page to check first    #Environment</description>
    </item>
    
    <item>
      <title>Language processing 100 knocks-88: 10 words with high similarity</title>
      <link>https://japan2018.github.io/language-processing-100-knocks-88-10-words-with-high-similarity-fa498/</link>
      <pubDate>Thu, 16 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>https://japan2018.github.io/language-processing-100-knocks-88-10-words-with-high-similarity-fa498/</guid>
      <description>[Language Processing 100 Knock 2015](http://www.cl.ecei.tohoku.ac.jp/nlp100/) 88th record &amp;quot;10 words with high similarity&amp;quot; is recorded.  Extract similar words from all words. This is also the process I want to do from my mailbox and minutes. Technically it is almost the same as the previous content.
Reference link    Links Remarks     088. 10 words with high similarity.ipynb Answer program GitHub link   Amateur language processing 100 knocks: 88 I have always taken care of 100 language processing knocks    #Environment</description>
    </item>
    
    <item>
      <title>09. Typoglycemia</title>
      <link>https://japan2018.github.io/09.-typoglycemia-fb353/</link>
      <pubDate>Fri, 13 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://japan2018.github.io/09.-typoglycemia-fb353/</guid>
      <description>## 09. Typoglycemia  Create a program that leaves the leading and trailing characters of each word in a word string separated by spaces and rearranges the order of the other characters randomly. However, words with a length of 4 or less are not sorted. Give an appropriate English sentence (for example, &amp;ldquo;I couldn&amp;rsquo;t believe that I could actually understand what I was reading :the phenomenal power of the human mind .</description>
    </item>
    
  </channel>
</rss>