<!doctype html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  
  <meta name="generator" content="Hugo 0.72.0" />

  
  <meta name="description" content="Some description">
  

  
  <link rel="apple-touch-icon" sizes="180x180" href="https://memotut.com/apple-touch-icon.png">

  
  <link rel="icon" type="image/png" sizes="32x32" href="https://memotut.com/favicon-32x32.png">

  
  <link rel="icon" type="image/png" sizes="16x16" href="https://memotut.com/favicon-16x16.png">

  
  <link rel="manifest" href="https://memotut.com/site.webmanifest">

  
  <link rel="mask-icon" href="https://memotut.com/safari-pinned-tab.svg" color="#5bbad5">

  <meta name="msapplication-TileColor" content="#da532c">

  <meta name="theme-color" content="#ffffff">

  
  <link rel="stylesheet" href="https://memotut.com/css/bootstrap.min.css" />

  
  <title>Gaussian process and machine learning Gaussian process regression implemented only by numpy of Python | Memo Tut</title>
  

  <style>
body {
  min-width: 300px;
}

.custom-navbar {
  margin-bottom: 1em;
  height: 60px;
}

.custom-navbar a {
  display: inline-block; 
  padding: 18px 0;
  margin-right: 1em; 
  font-weight: bold; 
}

.custom-navbar a:hover,
.custom-navbar a:focus {
  text-decoration: none; 
}

@media print {
  .custom-navbar {
    display: none;
  }
}

article {
  padding-bottom: 1em;
}

img {
  max-width: 100%;
}


body {
  background-color: #fff;
}



body {
  color: #212529;
}



a {
  color: #007bff;
}



a:hover,
a:focus {
  color: #0056b3;
}



.custom-navbar {
  background-color: #212529;
}



.custom-navbar a {
  color: rgba(255, 255, 255, 0.75);
}



.custom-navbar a:hover,
.custom-navbar a:focus {
  color: rgba(255, 255, 255, 1);
}



.container {
  max-width: 800px;
}



pre {
  display: block;
  padding: 9.5px;
  word-break: break-all;
  word-wrap: break-word;
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 4px;
}

pre code {
  padding: 0;
  font-size: inherit;
  color: inherit; 
  white-space: pre-wrap;
  background-color: transparent;
  border: none;
  border-radius: 0;
}

code {
  padding: 2px 4px;
  color: inherit; 
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 4px;
  font-size: .9em;
}



blockquote,
.blockquote {
  padding: 10px 20px;
  margin: 0 0 20px;
  font-size: 1em;
  border-left: 5px solid #6c757d;
}

</style>
</head>

<body>
  <nav class="custom-navbar">
  <div class="container">
    
    <a href="/">Posts</a>
    
    <a href="/tags/">Tags</a>
    
    <a href="/about/">About</a>
    
    <a href="/index.xml">RSS</a>
    
  </div>
</nav>
  
  <div class="container">
    <article>
      <h1>Gaussian process and machine learning Gaussian process regression implemented only by numpy of Python</h1>
<p>
  <small class="text-secondary">
  
  
  Nov 3, 2019
  </small>
  

<small><code><a href="https://memotut.com/tags/python">Python</a></code></small>


<small><code><a href="https://memotut.com/tags/machine-learning"> Machine Learning</a></code></small>


<small><code><a href="https://memotut.com/tags/gaussian"> Gaussian</a></code></small>


<small><code><a href="https://memotut.com/tags/machine-learning-professional-series"> Machine Learning Professional Series</a></code></small>


<small><code><a href="https://memotut.com/tags/gaussian-process"> Gaussian Process</a></code></small>

</p>
<pre><code>I recently read [&quot;Gaussian Process and Machine Learning&quot;](https://www.amazon.co.jp/GaussianProcessandMachineLearning-MachineLearningProfessionalSeries-Mochihashi-Daichi/dp/4061529269) I implemented it.
</code></pre>
<p>The content is &ldquo;3.4.2 Gaussian Process Regression Calculation&rdquo; in Chapter 3.
<img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/357086/04226327-32bd-3b5d-8ae9-d45483485cda.jpeg" alt="MLP_GaussianProcess.jpg">
Deer is a landmark. It is easy to understand and highly recommended.</p>
<h1 id="what-is-the-gauss-process">What is the Gauss process?</h1>
<p>First of all, to explain the Gaussian process in one word,</p>
<p>At Y=f(X)
<strong>&ldquo;If input X is similar, output Y is similar.&quot;</strong></p>
<p>It is a tool for mathematically expressing that property.</p>
<p>This time, we will implement &ldquo;Gaussian process regression&rdquo; which is a regression analysis using this Gaussian process!</p>
<h1 id="4-steps-to-gaussian-process-regression">4 steps to Gaussian process regression</h1>
<p>Here, we will explain the following steps.</p>
<p>Linear regression
↓
Ridge regression
↓
Consider linear regression and ridge regression with Gaussian distribution (probability distribution)
↓
Gaussian process regression</p>
<p>There are two reasons to take the above steps.</p>
<p>**First, Gaussian process regression is a nonlinear model that extends Ridge regression. **
Therefore, it is necessary to understand Ridge regression as a premise for understanding Gaussian process regression.
It is also necessary to understand the linear regression that is the premise of ridge regression. Therefore, I will explain from the linear regression, which is a major assumption. (It&rsquo;s a &ldquo;rush around&rdquo; thing.)</p>
<p>** Secondly, Gaussian process regression obtains predicted values by sampling from a Gaussian distribution (probability distribution). **
As will be explained in detail later, the main difference between Gaussian process regression and ridge regression (or linear regression) is that the variance of the Gaussian distribution to be sampled changes according to the Gaussian process.
Therefore, in Ridge Regression (or Linear Regression), we also put a step of predicting by sampling from Gaussian distribution so that we can understand how Gaussian Process Regression is different from Ridge Regression (or Linear Regression). ..</p>
<h2 id="1-linear-regression">1. Linear regression</h2>
<p>As a first step in understanding Gaussian process regression, let&rsquo;s start with a review of linear regression.</p>
<p>In the linear regression, if the input matrix is X, the coefficient vector is w, and the output vector is y,</p>
<pre><code class="language-math" data-lang="math">y=Xw \\
s.t. \min_{x}|Y-Xw|^2
</code></pre><p>Can be expressed as
s.t. shows the linear regression constraint,</p>
<p>**Minimize the squared error between the measured value Y and the predicted value (y=)Xw. **</p>
<p>is what it means.</p>
<p>In particular, by differentiating this constraint with respect to w, the solution of coefficient w can be obtained.</p>
<pre><code class="language-math" data-lang="math">w=(X^TX)^{-1}X^Ty
</code></pre><p>The following results are obtained by actually performing linear regression.
<img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/357086/f4fb0c6d-36f3-8395-b2fc-7a28a77543de.png" alt="image.png"></p>
<p>This time, we predicted with a linear function, but if the input X is added to the data with high-order values such as squares, cubes, etc., we can also obtain prediction values with higher-order functions such as quadratic functions and cubic functions. can also do.</p>
<h2 id="2-ridge-regression">2. Ridge regression</h2>
<p>Next is ridge regression. Ridge regression is a linear regression with additional constraints.</p>
<pre><code class="language-math" data-lang="math">y=Xw \\
s.t. \min_{x} |Y-Xw|^2 + \alpha|w|^2
</code></pre><p>The constraint condition represented by s.t. has a square (constant multiple) term of the coefficient vector w.
this is,</p>
<p>Minimize the squared error between the measured value Y and the predicted value (y=)Xw.
**Also, make the coefficient w as small as possible. **</p>
<p>is what it means.</p>
<p>This additional constraint offers two benefits.</p>
<ol>
<li>Stabilization of calculation</li>
</ol>
<p>When calculating the coefficient w of linear regression, the following inverse matrix is actually</p>
<pre><code class="language-math" data-lang="math">(X^TX)^{-1}
</code></pre><p>Was calculated on the assumption that</p>
<p>Therefore, if this inverse matrix does not exist, it has the disadvantage that calculation becomes impossible.</p>
<p>Therefore, in ridge regression, by adding the square of the coefficient vector w (a constant multiple of) to the constraint condition, differentiating the constraint condition with w as in the case of linear regression,</p>
<pre><code class="language-math" data-lang="math">w=(X^TX + \alpha I)^{-1}X^Ty
</code></pre><p>Therefore, the identity matrix αI intentionally produces an inverse matrix, which allows calculation.</p>
<ol start="2">
<li>Can prevent over-learning.
<img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/357086/293cc8c7-38e4-0e53-b68f-aee4302be2f3.png" alt="rl_title2-1024x641.png">
The figure on the left shows the result of linear regression that overtrained, and the figure on the right shows the result of ridge regression.
(<a href="Graphedfromhttps://www.n-insight.co.jp/niblog/20190917-1351/">Let&rsquo;s suppress over-learning of linear regression-Ridge regression and Lasso regression-</a>)</li>
</ol>
<p>As I explained earlier, linear regression can predict even higher-order functions, so the regression equation may become overly complicated, and this phenomenon is called overfitting.</p>
<p>Therefore, as shown in the graph, in ridge regression, by limiting the coefficient to a small value, it is possible to prevent over-learning when calculating the predicted value.</p>
<h2 id="3-gaussian-distribution-and-linear-regression-ridge-regression">3. Gaussian distribution and linear regression, ridge regression</h2>
<p>When the linear regression and ridge regression described above are actually used, the predicted value y and the measured value Y
There will be some error between.</p>
<p>In other words, considering that &ldquo;a prediction error will occur&rdquo;, the predicted value y given a certain x is the Gaussian distribution</p>
<pre><code class="language-math" data-lang="math">N(w^Tx, \sigma ^2)
</code></pre><p>Can be said to be sampled according to.</p>
<p>Here, since the coefficient w of the linear regression and the ridge regression has already been obtained, if each is transformed,</p>
<p>Linear regression</p>
<pre><code class="language-math" data-lang="math">N \Bigl(\bigl((X^TX)^{-1}X^Ty \bigr)^Tx, \sigma ^2 \Bigr)
</code></pre><p>Ridge regression</p>
<pre><code class="language-math" data-lang="math">N \Bigl(\bigl((X^TX + \alpha I)^{-1}X^Ty \bigr)^Tx, \sigma ^2 \Bigr)
</code></pre><p>It means that each predicted value y can be obtained by sampling from.</p>
<p>The point I would like you to keep in particular here is</p>
<p>**Linear regression and ridge regression have different Gaussian distribution means. **</p>
<p>**But the variance does not change and both assume &ldquo;equal variance&rdquo;. **</p>
<p>That is the point.</p>
<h2 id="4-gaussian-process-regression">4. Gaussian process regression</h2>
<p>Finally, I will explain the Gaussian process regression!</p>
<p>Gaussian process regression is a ridge regression</p>
<p><strong>&ldquo;If input X is similar, output Y is similar.&quot;</strong></p>
<p>The characteristic of the Gaussian process is added as a constraint condition.</p>
<p>That is, when the predicted value y of the Gaussian process regression is considered to be a value obtained by sampling from a certain Gaussian distribution, as in the linear regression and ridge regression described above,</p>
<p>The Gaussian distribution that Gaussian process regression follows is</p>
<p>**Don&rsquo;t assume “equal variance” like Ridge regression,</p>
<p>** The variance of the predicted values y, y&rsquo;changes according to the distance (that is, whether they are similar) of the inputs x, x&rsquo;. **</p>
<p>Can be said.</p>
<p>With this in mind, let&rsquo;s check the Gaussian distribution that Gaussian process regression follows.</p>
<pre><code class="language-math" data-lang="math">N(k_*^TK^{-1}y, k_{**}-k_*^TK^{-1}k_*)
</code></pre><p>Here, k and K that I am not familiar with came out.</p>
<p>These k and K are called kernels and are &ldquo;values obtained from two inputs x&rdquo;.</p>
<p>I will explain in detail later, but here</p>
<p>K: Kernel obtained from input x between training data
k*: Kernel obtained from input x of training data and input x of test data
k**: Kernel obtained from input x between test data</p>
<p>As the mean and variance of the Gaussian distribution followed by the Gaussian process regression contains one of the above kernels</p>
<p>** The positional relationship between various inputs x affects both the mean and variance of the Gaussian distribution that Gaussian process regression follows. **</p>
<p>You can see that.</p>
<h1 id="gaussian-process-regression-implementation">Gaussian process regression implementation</h1>
<h2 id="creating-data">Creating data</h2>
<p>This time, prepare the following data.</p>
<p>Original data: artificially created mixed signal with noise
Training data: Random sample points obtained partially from the original data
Predicted data: Original noisy mixed signal</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> numpy <span style="color:#f92672">as</span> np

<span style="color:#75715e">#Create original data</span>
n<span style="color:#f92672">=</span><span style="color:#ae81ff">100</span>
data_x <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>linspace(<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">4</span><span style="color:#f92672">*</span>np<span style="color:#f92672">.</span>pi, n)
data_y <span style="color:#f92672">=</span> <span style="color:#ae81ff">2</span><span style="color:#f92672">*</span>np<span style="color:#f92672">.</span>sin(data_x) <span style="color:#f92672">+</span> <span style="color:#ae81ff">3</span><span style="color:#f92672">*</span>np<span style="color:#f92672">.</span>cos(<span style="color:#ae81ff">2</span><span style="color:#f92672">*</span>data_x) <span style="color:#f92672">+</span> <span style="color:#ae81ff">5</span><span style="color:#f92672">*</span>np<span style="color:#f92672">.</span>sin(<span style="color:#ae81ff">2</span><span style="color:#f92672">/</span><span style="color:#ae81ff">3</span><span style="color:#f92672">*</span>data_x) <span style="color:#f92672">+</span> np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>randn(len(data_x))

<span style="color:#75715e"># Missing signal to get partial sample points</span>
missing_value_rate <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.2</span>
sample_index <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>sort(np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>choice(np<span style="color:#f92672">.</span>arange(n), int(n<span style="color:#f92672">*</span>missing_value_rate), replace<span style="color:#f92672">=</span>False))
</code></pre></div><p>Now, let&rsquo;s check with pyplot in matplotlib.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">from</span> matplotlib <span style="color:#f92672">import</span> pyplot <span style="color:#66d9ef">as</span> plt
<span style="color:#f92672">%</span>matplotlib inline

plt<span style="color:#f92672">.</span>figure(figsize<span style="color:#f92672">=</span>(<span style="color:#ae81ff">12</span>, <span style="color:#ae81ff">5</span>))
plt<span style="color:#f92672">.</span>title(<span style="color:#e6db74">&#39;signal data&#39;</span>, fontsize<span style="color:#f92672">=</span><span style="color:#ae81ff">20</span>)

<span style="color:#75715e">#Original signal</span>
plt<span style="color:#f92672">.</span>plot(data_x, data_y,<span style="color:#e6db74">&#39;x&#39;</span>, color<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;green&#39;</span>, label<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;correct signal&#39;</span>)

<span style="color:#75715e">#Partial sample points</span>
plt<span style="color:#f92672">.</span>plot(data_x[sample_index], data_y[sample_index],<span style="color:#e6db74">&#39;o&#39;</span>, color<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;red&#39;</span>, label<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;sample dots&#39;</span>)

plt<span style="color:#f92672">.</span>legend(bbox_to_anchor<span style="color:#f92672">=</span>(<span style="color:#ae81ff">1.05</span>, <span style="color:#ae81ff">1</span>), loc<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;upper left&#39;</span>, borderaxespad<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>, fontsize<span style="color:#f92672">=</span><span style="color:#ae81ff">12</span>)
plt<span style="color:#f92672">.</span>show()
</code></pre></div><p>The results are as follows.
*Since random sample points are obtained by np.random.choice(), the graph may change.
<img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/357086/5e22ae13-4192-00b0-7602-585723d77830.png" alt="signal_data.png"></p>
<p>It&rsquo;s quite complicated data, but how do you predict the Gaussian process?</p>
<h2 id="kernel-definition">Kernel definition</h2>
<h3 id="1-about-the-kernel">1. About the kernel</h3>
<p>The kernel that suddenly appeared here, but if you explain roughly,</p>
<p>** Find the covariance matrix for Gaussian process regression without calculating the coefficient vector w **</p>
<p>It is a tool.</p>
<p>Earlier, I explained that the Gaussian process is &ldquo;ridge regression with mean and variance in predicted value y&rdquo;.In addition, this mean and variance are calculated according to the characteristic of the Gaussian process, that is, if the input X is similar, the output Y is similar.</p>
<p>For example, if the input X is one-dimensional data and the interval from -10 to 10 is 1.0, the predicted value y(=xw) of the Gaussian process is derived from the similarity of the input X, that is, the positional relationship. The vector w has 21 dimensions.</p>
<p>If the input X is two-dimensional, then the coefficient vector is replaced by a matrix and its elements are increased to 441.
If the number of elements in the coefficient matrix is increased exponentially as the number of elements increases in the 3rd dimension, 4th dimension,&hellip;</p>
<p>Moreover, not only the dimension but also the range of the input X may be further expanded, so this does not have enough computing power.</p>
<p>That&rsquo;s where the kernel comes in.</p>
<p>When the kernel is used, the coefficient vector (or matrix) w is not calculated, and the similarity between the two outputs y and y&rsquo;corresponding to the two inputs x and x&rsquo;is calculated directly, which is necessary for Gaussian process regression. You can find the exact covariance matrix!</p>
<h3 id="2-kernel-implementation">2. Kernel implementation</h3>
<p>There are several types of kernels, but this time we will use a Gaussian kernel (RBF kernel) with Gaussian error.</p>
<p>Gaussian kernel has
*** &ldquo;The value decreases exponentially like a Gaussian distribution according to the distance of input x, x&rsquo;.&rdquo; ***
There is such a feature.</p>
<p>The formula is expressed as:
<img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/357086/7e803b5a-8c8b-248d-be62-05f353f6a1e9.png" alt="Gaussian kernel with noize.png">
Here, the parameter values this time are the following three.
θ1, θ2: Gaussian kernel parameters
θ3: Gaussian error variance</p>
<p>The Gaussian error function δ is
Two input values x and x&rsquo;are equal: δ(x, x&rsquo;) = 1
Otherwise: δ(x, x&rsquo;) = 0
The condition is branched as follows.</p>
<p>Now implement the expression as a function.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e">#Functionalize Gaussian kernel</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">kernel</span>(x, x_prime, p, q, r):
    <span style="color:#66d9ef">if</span> x <span style="color:#f92672">==</span> x_prime:
        delta <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span>
    <span style="color:#66d9ef">else</span>:
        delta <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>

    <span style="color:#66d9ef">return</span> p<span style="color:#f92672">*</span>np<span style="color:#f92672">.</span>exp(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span> <span style="color:#f92672">*</span> (x<span style="color:#f92672">-</span>x_prime)<span style="color:#f92672">**</span><span style="color:#ae81ff">2</span> <span style="color:#f92672">/</span> q) <span style="color:#f92672">+</span> (r <span style="color:#f92672">*</span> delta)
</code></pre></div><p>θ1, θ2, θ3 are p, q, r
x, x&rsquo;is x, x_prime
Has become.</p>
<h2 id="algorithm-implementation">Algorithm implementation</h2>
<p>From here, we will proceed according to &ldquo;Figure 3.17 Basic Algorithm for Computation of Gaussian Process Regression&rdquo; in Chapter 3.</p>
<p>For the time being, according to the notation in the book, divide the signal data created earlier into &ldquo;learning data&rdquo; and &ldquo;test data&rdquo; and redefine them in an easy-to-understand manner.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e">#Definition of data</span>
xtrain <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>copy(data_x[sample_index])
ytrain <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>copy(data_y[sample_index])

xtest <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>copy(data_x)
</code></pre></div><p>Then we will start the implementation.</p>
<p>Here, the Gaussian distribution that the previously presented Gaussian process regression follows</p>
<pre><code class="language-math" data-lang="math">N(k_*^TK^{-1}y, k_{**}-k_*^TK^{-1}k_*)
</code></pre><p>Compute both the mean and the variance at using the Gaussian kernel.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e">#Average</span>
mu <span style="color:#f92672">=</span> []
<span style="color:#75715e"># Dispersion</span>
var <span style="color:#f92672">=</span> []

<span style="color:#75715e"># Parameter value</span>
Theta_1 <span style="color:#f92672">=</span> <span style="color:#ae81ff">1.0</span>
Theta_2 <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.4</span>
Theta_3 <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.1</span>

Below, basic algorithm <span style="color:#66d9ef">for</span> calculating Gaussian process regression
train_length <span style="color:#f92672">=</span> len(xtrain)
<span style="color:#75715e"># Prepare the background of the kernel matrix between training data</span>
K <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>zeros((train_length, train_length))

<span style="color:#66d9ef">for</span> x <span style="color:#f92672">in</span> range(train_length):
    <span style="color:#66d9ef">for</span> x_prime <span style="color:#f92672">in</span> range(train_length):
        K[x, x_prime] <span style="color:#f92672">=</span> kernel(xtrain[x], xtrain[x_prime], Theta_1, Theta_2, Theta_3)
        
<span style="color:#75715e"># Dot product calculates with dot</span>
yy <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>dot(np<span style="color:#f92672">.</span>linalg<span style="color:#f92672">.</span>inv(K), ytrain)

test_length <span style="color:#f92672">=</span> len(xtest)
<span style="color:#66d9ef">for</span> x_test <span style="color:#f92672">in</span> range(test_length):
    
    <span style="color:#75715e"># Prepare kernel matrix background between test data and training data</span>
    k <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>zeros((train_length,))
    <span style="color:#66d9ef">for</span> x <span style="color:#f92672">in</span> range(train_length):
        k[x] <span style="color:#f92672">=</span> kernel(xtrain[x], xtest[x_test], Theta_1, Theta_2, Theta_3)
        
    s <span style="color:#f92672">=</span> kernel(xtest[x_test], xtest[x_test], Theta_1, Theta_2, Theta_3)
    
    <span style="color:#75715e"># Dot product is calculated in dots and added to the array of mean values</span>
    mu<span style="color:#f92672">.</span>append(np<span style="color:#f92672">.</span>dot(k, yy))
    <span style="color:#75715e"># Calculate &#34;k * K^-1&#34; part first (in dots because it is an inner product)</span>
    kK_ <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>dot(k, np<span style="color:#f92672">.</span>linalg<span style="color:#f92672">.</span>inv(K))
    <span style="color:#75715e"># Dot calculation of dot product with the second half and add to array of variance</span>
    var<span style="color:#f92672">.</span>append(s<span style="color:#f92672">-</span>np<span style="color:#f92672">.</span>dot(kK_, k<span style="color:#f92672">.</span>T))
</code></pre></div><p>It&rsquo;s a little long, so I will explain it in order.</p>
<p>First we want to find the predicted mean and variance of the signal.
(Since the interval of predicted values is the same as the original data, 100 averages and variances are stored in the list.)</p>
<p>Also, specify the parameter value first.</p>
<p>The contents of the algorithm are the same as in the book, but the following changes are required when implementing in Python.</p>
<ul>
<li>Before calculating K[x, x&rsquo;], the background preparation of the kernel matrix K is necessary.
(This time, I created it with a zero matrix and rewritten the elements.)
・The “*” in the book represents the dot product of matrices, so rewrite it as np.dot().
・K<input checked="" disabled="" type="checkbox"> also requires ground preparation before calculation.</li>
<li>The inner product of the three matrices in the calculation of variance is divided into two steps and rewritten as np.dot().</li>
</ul>
<p>If you suppress the above points, Gaussian process regression is perfect!</p>
<p>Finally, let&rsquo;s check with pyplot in matplotlib.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">plt<span style="color:#f92672">.</span>figure(figsize<span style="color:#f92672">=</span>(<span style="color:#ae81ff">12</span>, <span style="color:#ae81ff">5</span>))
plt<span style="color:#f92672">.</span>title(<span style="color:#e6db74">&#39;signal prediction by Gaussian process&#39;</span>, fontsize<span style="color:#f92672">=</span><span style="color:#ae81ff">20</span>)

<span style="color:#75715e">#Original signal</span>
plt<span style="color:#f92672">.</span>plot(data_x, data_y,<span style="color:#e6db74">&#39;x&#39;</span>, color<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;green&#39;</span>, label<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;correct signal&#39;</span>)
<span style="color:#75715e">#Partial sample points</span>
plt<span style="color:#f92672">.</span>plot(data_x[sample_index], data_y[sample_index],<span style="color:#e6db74">&#39;o&#39;</span>, color<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;red&#39;</span>, label<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;sample dots&#39;</span>)

<span style="color:#75715e"># Convert variance to standard deviation</span>
std <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>sqrt(var)

<span style="color:#75715e"># Signal the average value obtained in the Gaussian process</span>
plt<span style="color:#f92672">.</span>plot(xtest, mu, color<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;blue&#39;</span>, label<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;mean by Gaussian process&#39;</span>)
<span style="color:#75715e"># Range of standard deviation obtained in Gaussian process * See end of code for range</span>
plt<span style="color:#f92672">.</span>fill_between(xtest, mu <span style="color:#f92672">+</span> <span style="color:#ae81ff">2</span><span style="color:#f92672">*</span>std, mu <span style="color:#f92672">-</span><span style="color:#ae81ff">2</span><span style="color:#f92672">*</span>std, alpha<span style="color:#f92672">=.</span><span style="color:#ae81ff">2</span>, color<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;blue&#39;</span>, label<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;standard deviation by Gaussian process&#39;</span>)

plt<span style="color:#f92672">.</span>legend(bbox_to_anchor<span style="color:#f92672">=</span>(<span style="color:#ae81ff">1.05</span>, <span style="color:#ae81ff">1</span>), loc<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;upper left&#39;</span>, borderaxespad<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>, fontsize<span style="color:#f92672">=</span><span style="color:#ae81ff">12</span>)
plt<span style="color:#f92672">.</span>show()

<span style="color:#75715e">#Mean value ± (standard deviation × 2) …The specified number appears within the range with a probability of 95.4%</span>
</code></pre></div><p>The results are as follows.
*Although it was described at the beginning, the graph changes depending on the position of sample points obtained at random.
<img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/357086/ead371b7-6984-224e-baf0-9106ea0e288d.png" alt="GP_predict.png"></p>
<p>As I mentioned in the code, this time I am converting the predicted variance to standard deviation.
The expected range for the Gaussian process was set to twice the standard deviation.</p>
<ul>
<li>Regarding the method of determining the range, the following described in <a href="https://to-kei.net/basic/glossary/standard-deviation/">to-kei.net -Statistics that understands all humankind-</a> I referenced the table.</li>
</ul>
<table>
<thead>
<tr>
<th align="left">Range</th>
<th align="left">Probability that a specified number will appear in the range</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">Mean ± standard deviation</td>
<td align="left">68.3%</td>
</tr>
<tr>
<td align="left">Mean ± (standard deviation × 2)</td>
<td align="left">95.4%</td>
</tr>
<tr>
<td align="left">Mean ± (standard deviation × 3)</td>
<td align="left">99.7%</td>
</tr>
</tbody>
</table>
<h1 id="advantages-of-gaussian-process">Advantages of Gaussian process</h1>
<p>Gaussian process regression has two major advantages.</p>
<h2 id="1-nonlinear-model-can-be-easily-obtained">1. Nonlinear model can be easily obtained</h2>
<p>In linear regression and ridge regression, when creating a non-linear model, multiple n-th order functions were prepared and learned.
Therefore, the point of &ldquo;how many functions to consider&rdquo; ** becomes a bottleneck, and considerable effort must be spent on model building.</p>
<p>However, in Gaussian process regression, you can obtain a non-linear model without preparing an nth-order function by **using a kernel. **</p>
<p>Therefore, the calculation process and algorithm for learning become complicated, but it is possible to construct a complicated model with less effort.</p>
<h2 id="2-partial-prediction-accuracy-can-be-confirmed">2. Partial prediction accuracy can be confirmed</h2>
<p>Observing the graph of the Gaussian process regression obtained by implementing the algorithm earlier, we can grasp the approximate characteristics of the original data, but due to the nature of the Gaussian process, the points where the sample point spacing is large (horizontal axis: 8 (For example, in the vicinity)**, the prediction is quite wrong.</p>
<p>However, such a mispredicted part has a wider &ldquo;dispersion&rdquo; than other parts**, and it seems that the Gaussian process itself is confident that it is not confident in the prediction.</p>
<p>In other words, the Gaussian process is a model that can confirm the partial prediction accuracy.</p>
<h1 id="references-and-links">References and links</h1>
<p><a href="https://www.amazon.co.jp/GaussianProcessandMachineLearning-MachineLearningProfessionalSeries-Mochihashi-Daichi/dp/4061529269">&ldquo;Gaussian Process and Machine Learning&rdquo;</a>
<a href="https://www.n-insight.co.jp/niblog/20190917-1351/">Let&rsquo;s suppress over-learning of linear regression-Ridge regression and Lasso regression-</a>
<a href="https://to-kei.net/basic/glossary/standard-deviation/">to-kei.net -Statistics that understand all humankind-</a></p>

    </article>
  </div>

  
  
  
  <script>
  window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
  ga('create', 'UA-169005401-1', 'auto');
  ga('send', 'pageview');
  </script>
  <script async src='https://www.google-analytics.com/analytics.js'></script>
  

  

  
<link rel="stylesheet" type="text/css" href="//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.0/cookieconsent.min.css" />
<script src="//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.0/cookieconsent.min.js"></script>
<script>
window.addEventListener("load", function(){
window.cookieconsent.initialise({
  "palette": {
    "popup": {
      "background": "#216942",
      "text": "#b2d192"
    },
    "button": {
      "background": "#afed71"
    }
  }
})});
</script>

</body>

</html>
