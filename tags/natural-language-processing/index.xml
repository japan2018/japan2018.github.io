<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title> natural language processing on Memo Tut</title>
    <link>https://memotut.com/tags/natural-language-processing/</link>
    <description>Recent content in  natural language processing on Memo Tut</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 15 May 2020 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://memotut.com/tags/natural-language-processing/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>10 techniques to improve BERT accuracy</title>
      <link>https://memotut.com/10-techniques-to-improve-bert-accuracy-34330/</link>
      <pubDate>Fri, 15 May 2020 00:00:00 +0000</pubDate>
      
      <guid>https://memotut.com/10-techniques-to-improve-bert-accuracy-34330/</guid>
      <description>## Introduction  It has become commonplace to fine-tune and use BERT for natural language processing tasks. It is expected that there will be more and more scenes where you want to improve the accuracy even a little when doing competitions such as Kaggle or projects where accuracy requirements are tight. Therefore, we will summarize the accuracy improvement method. Classification task is assumed as a task.
Adjust number of characters You can enter up to 512 words in the learned BERT.</description>
    </item>
    
    <item>
      <title>[Natural language processing] I tried to visualize the comments of each member in the Slack community</title>
      <link>https://memotut.com/natural-language-processing-i-tried-to-visualize-the-comments-of-each-member-in-the-slack-community-dca47/</link>
      <pubDate>Sun, 10 May 2020 00:00:00 +0000</pubDate>
      
      <guid>https://memotut.com/natural-language-processing-i-tried-to-visualize-the-comments-of-each-member-in-the-slack-community-dca47/</guid>
      <description>## About this article  In this article, I will introduce a method to visualize what each member says in the Slack community with Wordcloud.
The source code can be found here [https://github.com/sota0121/slack-msg-analysis) :octocat:
I would also like to read: [Natural language processing] I tried to visualize the topics raised this week in the Slack community
table of contents  Usage and output example Get message from Slack Pre-processing: table creation/cleaning/morphological analysis/normalization/stopword removal Pre-processing: Extracting important words (tf-idf) Visualization processing with Wordcloud Bonus  *I would like to summarize the preprocessing in another article in the future</description>
    </item>
    
    <item>
      <title>[Natural language processing] I tried to visualize the topics raised this week in the Slack community</title>
      <link>https://memotut.com/natural-language-processing-i-tried-to-visualize-the-topics-raised-this-week-in-the-slack-community-41630/</link>
      <pubDate>Sat, 09 May 2020 00:00:00 +0000</pubDate>
      
      <guid>https://memotut.com/natural-language-processing-i-tried-to-visualize-the-topics-raised-this-week-in-the-slack-community-41630/</guid>
      <description>## About this article  In this article, I will introduce a method of using Wordcloud to visualize what topics were raised within a certain period (here one week) of the Slack community.
The source code can be found here [https://github.com/sota0121/slack-msg-analysis) :octocat:
I would also like to read: [Natural language processing] I tried to visualize the comments of each member in the Slack community
table of contents  Usage and output example Get message from Slack Pre-processing: message mart table creation Pre-treatment: Cleaning Pre-processing: Morphological analysis (Janome) Pre-processing: Normalization Pre-processing: Stop word removal Pre-processing: Extract important words (tf-idf) Visualization processing with Wordcloud Bonus  *I would like to summarize the preprocessing in another article in the future</description>
    </item>
    
    <item>
      <title>[The strongest English word book bomb ww] Automatically generate the English word book required by engineers in Python-Part 1</title>
      <link>https://memotut.com/the-strongest-english-word-book-bomb-ww-automatically-generate-the-english-word-book-required-by-engineers-in-python-part-1-5bf18/</link>
      <pubDate>Wed, 22 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>https://memotut.com/the-strongest-english-word-book-bomb-ww-automatically-generate-the-english-word-book-required-by-engineers-in-python-part-1-5bf18/</guid>
      <description>#Introduction  I think that everyone has bought once If you automatically create an English word book in Python,
 You don&#39;t need to buy an English word book anymore?  I will verify the hypothesis.   **If LGTM exceeds 10,** We will make more practical ones in the second part (or the second part), so please LGTM if you find it interesting~! About the author Biography After graduating from the Department of Electrical and Information Engineering at the age of 20, at a technical trading company, translating specifications etc.</description>
    </item>
    
    <item>
      <title>That&#39;s right, let&#39;s eat Bubu-zuke. [Natural language processing starting with Kyoto dialect]</title>
      <link>https://memotut.com/thats-right-lets-eat-bubu-zuke.-natural-language-processing-starting-with-kyoto-dialect-a81d5/</link>
      <pubDate>Sun, 15 Mar 2020 00:00:00 +0000</pubDate>
      
      <guid>https://memotut.com/thats-right-lets-eat-bubu-zuke.-natural-language-processing-starting-with-kyoto-dialect-a81d5/</guid>
      <description>#Introduction  I will do natural language processing for the first time. Excited. This article is [Qiita x COTOHA API present project] Let&amp;rsquo;s do text analysis with COTOHA API! ](https://zine.qiita.com/event/collaboration-cotoha-api/?utm_source=qiita&amp;amp;utm_medium=banner) ~~ I want more prizes! ~~ The post was in time.
Immediately the main subject. What to do now First, I will briefly explain what to do. I could do the following ↓
python3 bubuduke.py &amp;quot;Getta&amp;quot; &amp;quot;Do not be good&amp;quot; I will make a Kyoto dialect translator like this.</description>
    </item>
    
    <item>
      <title>Will COTOHA exceed humans? Challenge the Japanese test!</title>
      <link>https://memotut.com/will-cotoha-exceed-humans-challenge-the-japanese-test-668e0/</link>
      <pubDate>Fri, 13 Mar 2020 00:00:00 +0000</pubDate>
      
      <guid>https://memotut.com/will-cotoha-exceed-humans-challenge-the-japanese-test-668e0/</guid>
      <description>What is # COTOHA API  The following are the main things you can do with APIs that process natural language and voice provided by NTT Communications.
 Parsing Named entity extraction Proper noun (company name) correction Anaphora analysis Keyword extraction Similarity judgment Sentence type judgment User attribute estimation Stagnation removal Voice recognition error detection Sentiment analysis voice recognition Speech synthesis wrap up  For details, see here This time, I tried the Japanese test using the similarity judgment.</description>
    </item>
    
    <item>
      <title>Responding to Pokemon&#39;s wonderful message by using COTOHA▼</title>
      <link>https://memotut.com/responding-to-pokemons-wonderful-message-by-using-cotoha-3e507/</link>
      <pubDate>Wed, 11 Mar 2020 00:00:00 +0000</pubDate>
      
      <guid>https://memotut.com/responding-to-pokemons-wonderful-message-by-using-cotoha-3e507/</guid>
      <description>Welcome to the # COTOHA world! ▼  This kiji is squid Because I saw a secret ▼▼ Let&amp;#39;s try It’s a lottery ▼▼ [Qiita x COTOHA API present project] Let&amp;rsquo;s do text analysis with COTOHA API! 
#What is Pokemon Nice Message ▼ It is a message display used on the screen of the famous game &amp;ldquo;Pokemon&amp;rdquo; (released from Pokemon)(TheimagebelowisPokemonredandgreen.Fromthegamescreen).The point is that we want to reproduce this (why do you want to do that?</description>
    </item>
    
    <item>
      <title>[With Japanese model] Sentence vector model recommended for people who will process natural language in 2020</title>
      <link>https://memotut.com/with-japanese-model-sentence-vector-model-recommended-for-people-who-will-process-natural-language-in-2020-1df94/</link>
      <pubDate>Mon, 09 Mar 2020 00:00:00 +0000</pubDate>
      
      <guid>https://memotut.com/with-japanese-model-sentence-vector-model-recommended-for-people-who-will-process-natural-language-in-2020-1df94/</guid>
      <description>## Summary   Create and publish a Japanese model of Sentence-BERT (Papers,Implementation). did. By using this Japanese model, anyone can easily create high-quality sentence vectors. Please use it. Sentence-BERT uses pre-trained BERTmodelandSiameseNetwork , Is a method of creating a highly accurate sentence vector. Cosine similarity using accuracy evaluation of English version (STSbenchmarkandrankcorrelationcoefficientofspearmanofcorrectlabel.1Inthecaseof(closerisbetter),themethodusingtheaverageofsimplewordvectors(GloVe)is0.58,andthemodelcorrespondingtothescalecreatedthistimeisaround0.85(PapersSeeTable2). The accuracy is 0.17 when using the CLS vector of the bare BERT and 0.46 when using the average of the BERT embeddings, which is worse than the result of the average of the word vector (0.</description>
    </item>
    
    <item>
      <title>18 beautiful Python terms you want to read aloud. R18 with example sentence</title>
      <link>https://memotut.com/18-beautiful-python-terms-you-want-to-read-aloud.-r18-with-example-sentence-519b2/</link>
      <pubDate>Wed, 04 Mar 2020 00:00:00 +0000</pubDate>
      
      <guid>https://memotut.com/18-beautiful-python-terms-you-want-to-read-aloud.-r18-with-example-sentence-519b2/</guid>
      <description>#Background  Python, which is known as a frozen tuna script language, It is also famous for its many beautifully named packages.
 PyPy pypan pypants  Reference: 7 Python terms to read aloud http://doloopwhile.hatenablog.com/entry/20120120/1327062714
Enchanted by these beautifully named packages, How beautifully named packages exist **I really decided to investigate. **
The reference information is a little old from 2012, If you look again now, you will surely find a more beautiful name!</description>
    </item>
    
    <item>
      <title>I have no emotions... ← Try to analyze if there are no emotions using various emotion estimators</title>
      <link>https://memotut.com/i-have-no-emotions...-try-to-analyze-if-there-are-no-emotions-using-various-emotion-estimators-2f70a/</link>
      <pubDate>Wed, 04 Mar 2020 00:00:00 +0000</pubDate>
      
      <guid>https://memotut.com/i-have-no-emotions...-try-to-analyze-if-there-are-no-emotions-using-various-emotion-estimators-2f70a/</guid>
      <description>#Introduction  Human communication includes not only language but also many non-verbal information such as facial expressions and gestures. When analyzing and interpreting human behavior mechanically and programmatically, more information can be obtained by adding meaning to such non-verbal information. These technologies are being researched and developed in various fields such as image processing, natural language processing, and voice recognition, and are expected to be applied in many fields such as marketing and robot interaction systems.</description>
    </item>
    
    <item>
      <title>Humans are not so sad creatures</title>
      <link>https://memotut.com/humans-are-not-so-sad-creatures-17761/</link>
      <pubDate>Sat, 29 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>https://memotut.com/humans-are-not-so-sad-creatures-17761/</guid>
      <description>With the birth of the Internet, the world has changed dramatically. Now with just one smartphone, you can instantly do various things such as research, shopping, communication, entertainment consumption, and information dissemination.  As efficiency went on, people&amp;rsquo;s tasks continued to diminish, and even ** began to disappear. **The everyday life is spreading, in which people act like the information recommended by AI and like the posts that everyone likes.</description>
    </item>
    
    <item>
      <title>I have lived a lot of happiness. [Make human disqualification happy using COTOHA API]</title>
      <link>https://memotut.com/i-have-lived-a-lot-of-happiness.-make-human-disqualification-happy-using-cotoha-api-972c4/</link>
      <pubDate>Fri, 28 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>https://memotut.com/i-have-lived-a-lot-of-happiness.-make-human-disqualification-happy-using-cotoha-api-972c4/</guid>
      <description>## Introduction  During this time, I read Osamu Dazai&amp;rsquo;s &amp;ldquo;Human Disqualification&amp;rdquo;. Those who have read it will understand, I was reading in a very muddy atmosphere and I felt quite dark.
Well then **Let&amp;rsquo;s rewrite it into a happy sentence. **
Used ・COTOHA API Using the sentiment analysis function, obtain the keyword (emotion keyword) that made the emotion judgment in the sentence and the emotion represented by the emotion keyword.</description>
    </item>
    
    <item>
      <title>A program that automatically corrects Takenoko no Sato to Mushroom Mountain</title>
      <link>https://memotut.com/a-program-that-automatically-corrects-takenoko-no-sato-to-mushroom-mountain-a8c7b/</link>
      <pubDate>Tue, 25 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>https://memotut.com/a-program-that-automatically-corrects-takenoko-no-sato-to-mushroom-mountain-a8c7b/</guid>
      <description>#Introduction-Introduction of wonderful sweets and the existence of late-deteriorated products-  As you probably know, there is a wonderful sweets called &amp;ldquo;Mushroom Mountain&amp;rdquo;1. A chocolate snack made and sold by Meiji Co., Ltd. since 1975. It has a cute mushroom-like shape with crackers on the stem and chocolate on the umbrella. It was said that the best combination was tried and errored over a development period of 5 years, and while it was easy to hold and functional, it had plenty of chocolate and a moderately salty and crispy feel.</description>
    </item>
    
    <item>
      <title>Challenges for large-scale models such as BERT</title>
      <link>https://memotut.com/challenges-for-large-scale-models-such-as-bert-e72d8/</link>
      <pubDate>Tue, 25 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>https://memotut.com/challenges-for-large-scale-models-such-as-bert-e72d8/</guid>
      <description># Breakthrough model of natural language processing-BERT  BERT[BidirectionalEncoderRepresentationsfromTransformers]wasannouncedtoGoogle&amp;rsquo;steaminthefallof2018.Thisistheresultoflearningamodelinarevolutionarywayforaverylargenetworkwithalargeamountofdatausingatransformerarchitecture.PleaserefertoThisarticle for learning method and accuracy.
In the field of machine learning where open source and free exchange of information are important, new ideas are published as papers at arxiv.org, and models are shared on github. Doing is the basic way. As soon as new ideas are published, they can be referenced and reused by machine learning researchers and development teams around the world.</description>
    </item>
    
    <item>
      <title>Automatic quiz generation with COTOHA</title>
      <link>https://memotut.com/automatic-quiz-generation-with-cotoha-0329e/</link>
      <pubDate>Sun, 23 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>https://memotut.com/automatic-quiz-generation-with-cotoha-0329e/</guid>
      <description>#1. At the beginning  Qiita made her debut laugh after being taken by something. Previously, I tried to automatically generate a quiz QA using Natural language processing API COTOHA provided by NTT Communications. Every time, a present project with Mr. Qiita was held, so I&amp;rsquo;m really happy. So, I will write about automatic generation of QA quiz using news articles. #2. Development environment *Google Colaboratory *Python3 *COTOHA api (parsing) #3.</description>
    </item>
    
    <item>
      <title>Is it possible to extract the profile information of the person from the chat log?</title>
      <link>https://memotut.com/is-it-possible-to-extract-the-profile-information-of-the-person-from-the-chat-log-f4cf7/</link>
      <pubDate>Thu, 20 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>https://memotut.com/is-it-possible-to-extract-the-profile-information-of-the-person-from-the-chat-log-f4cf7/</guid>
      <description>#Purpose   There was an interesting campaign I was wondering how much a person has exposed their profile inadvertently  #Environment or data
Analysis API There is not enough knowledge to do the analysis from scratch by yourself, so this time NTT Communications has published COTOHA API I used.
##raw data This time I&amp;rsquo;m thinking of using the chat log as a base. In Japan, LINE is the mainstream when it comes to chat, but LINE has a chat log export function.</description>
    </item>
    
    <item>
      <title>To people who are offered but not recruited</title>
      <link>https://memotut.com/to-people-who-are-offered-but-not-recruited-0f811/</link>
      <pubDate>Wed, 19 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>https://memotut.com/to-people-who-are-offered-but-not-recruited-0f811/</guid>
      <description>** [Twitter](https://twitter.com/omiita_atiimo)introducesartificialintelligenceandarticleswritteninothermedia**,soifyouwanttoknowmoreaboutartificialintelligence,**Pleasefeelfreetofollowus(https://twitter.com/omiita_atiimo)! **  ** Add Shinjiro Koizumi syntax [Add](https://qiita.com/omiita/items/0f811f15e569bf2539b8#6-%E7%95%AA%E5%A4%96%E7%B7%A8%E5%B0%8F%E6%B3%89%E9%80%B2%E6%AC%A1%E9%83%8E%E6%A7%8B%E6%96%87) Did. ** -- 1. I teach, but I don&amp;rsquo;t explain Prime Minister Abe&amp;rsquo;s Remark &amp;ldquo;I am recruiting but not recruiting&amp;rdquo; I was inspired and created a program that automatically converts the input sentence into a sentence &amp;ldquo;I&amp;rsquo;m recruiting but not recruiting&amp;rdquo;**!
If you enter &amp;ldquo;Recruit people&amp;rdquo;, it will be converted into the sentence &amp;ldquo;We are recruiting people but not recruiting&amp;rdquo;.</description>
    </item>
    
    <item>
      <title>Introduction of ALBERT (using MeCab&#43;Sentencepiece) model that learned large-scale Japanese business news corpus</title>
      <link>https://memotut.com/introduction-of-albert-using-mecab-sentencepiece-model-that-learned-large-scale-japanese-business-news-corpus-b41dc/</link>
      <pubDate>Mon, 17 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>https://memotut.com/introduction-of-albert-using-mecab-sentencepiece-model-that-learned-large-scale-japanese-business-news-corpus-b41dc/</guid>
      <description>#Introduction  Previously, Japanese BERT pre-learned modelandXLNetpre-learnedmodel It is the stockmark Morinaga who posted the introduction articles such as. Thank you for reading the article on model release to many people.
This time, we will release the pre-learned Japanese model of ALBERT. Now, while various pre-learned models have been proposed, why do you publish ALBERT Japanese model?ALBERT is not just SOTA as described as A Lite BERT , Because it is a model that makes the BERT lighter while maintaining and improving accuracy.</description>
    </item>
    
    <item>
      <title>The winner of the one with the highest degree of similarity to the original story in The Most Stupid Hototogisu Won&#39;t Winner</title>
      <link>https://memotut.com/the-winner-of-the-one-with-the-highest-degree-of-similarity-to-the-original-story-in-the-most-stupid-hototogisu-wont-winner-33b69/</link>
      <pubDate>Mon, 17 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>https://memotut.com/the-winner-of-the-one-with-the-highest-degree-of-similarity-to-the-original-story-in-the-most-stupid-hototogisu-wont-winner-33b69/</guid>
      <description># Overview  A long time ago, there was a story that said, &amp;ldquo;The most stinky hottogigisu who won the championship 1&amp;rdquo;. If it&#39;s a crow, invite me to nothingness (Izanae) Totogi (Hototogisu) It&amp;rsquo;s like Growing my jet black flame, Firetogisu.
I like all the works, but it&amp;rsquo;s useless even if this kind of hand is too far from the original material, and the best taste is to keep the shape and meaning similar to the original material and to have a sense of jerk I wonder if it is.</description>
    </item>
    
    <item>
      <title>A story of trying to reproduce Katsuno Isono who does not react to inconvenient things by natural language processing.</title>
      <link>https://memotut.com/a-story-of-trying-to-reproduce-katsuno-isono-who-does-not-react-to-inconvenient-things-by-natural-language-processing.-57eb5/</link>
      <pubDate>Sun, 16 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>https://memotut.com/a-story-of-trying-to-reproduce-katsuno-isono-who-does-not-react-to-inconvenient-things-by-natural-language-processing.-57eb5/</guid>
      <description>#Introduction  COTOHA API is a Japanese natural language processing API. It is an API that has various functions and is quite fun to use even a free account can be used 1000/day.
COTOHA API
By the way, do you know Isono-kun? Yes, this is Katsuno Isono from a certain national anime.
The name of this Isono-kun is called something in the play, but when it&amp;rsquo;s inconvenient, he doesn&amp;rsquo;t hear or reacts so-called &amp;ldquo;human-like&amp;rdquo;.</description>
    </item>
    
    <item>
      <title>COTOHA API Cornflakes</title>
      <link>https://memotut.com/cotoha-api-cornflakes-474ca/</link>
      <pubDate>Sun, 16 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>https://memotut.com/cotoha-api-cornflakes-474ca/</guid>
      <description>#Introduction  &amp;ldquo;I&amp;rsquo;m a milk boy, please.&amp;rdquo; &amp;ldquo;Now, I received the unsold chocolates for Valentine&amp;rsquo;s Day. I don&amp;rsquo;t mind if there&amp;rsquo;s anything like this.&amp;rdquo; &amp;ldquo;I&amp;rsquo;ll take you.&amp;rdquo;
What is a milk boy? This is a comedy duo belonging to Yoshimoto Kogyo who won the M-1 Grand Prix 2019. My Navi News has a video of corn flakes, so please check it out. https://www.youtube.com/watch?v=VjBQtr4lH0k
What is COTOHA API?  Natural language processing technology and speech recognition/synthesis technology that utilize the research results of NTT Group for over 40 years This service is provided so that it can be used easily with API.</description>
    </item>
    
    <item>
      <title>Let&#39;s do difficult things with COTOHA API-Learn by using Introduction to natural language processing-</title>
      <link>https://memotut.com/lets-do-difficult-things-with-cotoha-api-learn-by-using-introduction-to-natural-language-processing-0d5b5/</link>
      <pubDate>Sat, 15 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>https://memotut.com/lets-do-difficult-things-with-cotoha-api-learn-by-using-introduction-to-natural-language-processing-0d5b5/</guid>
      <description>Recently, the world of natural language processing (NLP) has continued to make remarkable progress. I think many people would like to enter the world of such natural language processing. However, even though it is generally natural language processing, the number of tasks contained in the word is enormous.  &amp;ldquo;What should I start with after all!?&amp;rdquo;
This article is to answer such a voice.
1.First of all This article aims to raise the level of &amp;ldquo;natural language processing.</description>
    </item>
    
    <item>
      <title>Youtube live It analyzes the sentiment of the comment and judges the emo part from Youtube LIve.</title>
      <link>https://memotut.com/youtube-live-it-analyzes-the-sentiment-of-the-comment-and-judges-the-emo-part-from-youtube-live.-fa198/</link>
      <pubDate>Fri, 14 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>https://memotut.com/youtube-live-it-analyzes-the-sentiment-of-the-comment-and-judges-the-emo-part-from-youtube-live.-fa198/</guid>
      <description># TL;DR  It is a suggestion and experiment that it is possible to extract &amp;ldquo;emo&amp;rdquo; and &amp;ldquo;highlight&amp;rdquo; with better accuracy than just the number of comments per hour by performing sentiment analysis of comments on Youtube live. ..
In fact, we were able to get the result that the accuracy is likely to improve compared to just looking at the number of comments.
#Introduction
Last time, I borrowed COTOHA API and tried to think freely, not practical Article.</description>
    </item>
    
    <item>
      <title>Miya is not enough for our communication.</title>
      <link>https://memotut.com/miya-is-not-enough-for-our-communication.-e63f6/</link>
      <pubDate>Thu, 13 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>https://memotut.com/miya-is-not-enough-for-our-communication.-e63f6/</guid>
      <description>#TL;DR  By using the COTOHA API, we have proposed a poem that is close to what you want to convey from the Hyakunin Isshu. As a gateway to more &amp;ldquo;elegant&amp;rdquo; communication, you can bridge between engineers and non-engineers. (May be)
| What I want to convey | Proposed Waka | Meaning | |&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;- &amp;mdash;&amp;mdash;&amp;mdash;|&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;- &amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;|&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;- &amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;ndash; &amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;ndash; &amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;| | I feel very lonely to hear that I will be transferred.</description>
    </item>
    
    <item>
      <title>Summary about Doc2Vec</title>
      <link>https://memotut.com/summary-about-doc2vec-5ea94/</link>
      <pubDate>Thu, 13 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>https://memotut.com/summary-about-doc2vec-5ea94/</guid>
      <description>#Introduction  This time, I studied Doc2Vec as an extension of Word2Vec. &amp;ldquo;Document classification&amp;rdquo; and &amp;ldquo;document grouping (clustering)&amp;rdquo; are tasks that are often requested in natural language processing, but in order to implement them, a distributed representation of the document itself is required. If you use Doc2Vec, you can directly get the distributed representation.
##reference I have referred to the following to understand Doc2Vec.
 (doc2vec(Paragraph Vector) algorithm ](https://kitayamalab.wordpress.com/2016/12/10/doc2vecparagraph-vector-%e3%81%ae%e3%82%a2%e3%83%ab%e3%82%b4%e3%83%aa%e3%82%ba%e3%83%a0/) Distributed Representations of Sentences and Documents (original paper) Doc2Vec mechanism and document similarity calculation tutorial using gensim How to use natural language processing technology-I tried to predict the quality of papers using Doc2Vec and DAN!</description>
    </item>
    
    <item>
      <title>Natural language processing for busy people</title>
      <link>https://memotut.com/natural-language-processing-for-busy-people-40710/</link>
      <pubDate>Mon, 10 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>https://memotut.com/natural-language-processing-for-busy-people-40710/</guid>
      <description>#Overview for busy people  I tried linguistic processing, inspired by the article Ore program Ugocas Omae Genshizinaru. I like the old story &amp;ldquo;Busy People Series&amp;rdquo; 1, so I used the summary API of COTOHA API I tried to make a song for busy people.
*Note Although the lyrics summarized in the output result of the code below (black background part) appear, it is used for the purpose of research on language processing in accordance with Article 32 of the Copyright Act.</description>
    </item>
    
    <item>
      <title>The result of having Mr. COTOHA summarize My memories of Mentos and Go. With COTOHA fastest tutorial</title>
      <link>https://memotut.com/the-result-of-having-mr.-cotoha-summarize-my-memories-of-mentos-and-go.-with-cotoha-fastest-tutorial-16e67/</link>
      <pubDate>Sun, 09 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>https://memotut.com/the-result-of-having-mr.-cotoha-summarize-my-memories-of-mentos-and-go.-with-cotoha-fastest-tutorial-16e67/</guid>
      <description># Memories of Mentos and Go  Mentos has a deep feeling. At that time, I was participating in a Go club, Playing a Go stone to drop the opponent&amp;rsquo;s Go stone He was playing &amp;ldquo;shooting Go&amp;rdquo;. Heian nobles must have been entertaining, The club president, who couldn&amp;rsquo;t understand the naive play, went mad. According to him, the important go stone breaks, so stop it. Go stones cannot be fixed if they break.</description>
    </item>
    
    <item>
      <title>A story about trying magic tricks of live coding of natural language processing and dependency analysis with LT in an instant from an empty place</title>
      <link>https://memotut.com/a-story-about-trying-magic-tricks-of-live-coding-of-natural-language-processing-and-dependency-analysis-with-lt-in-an-instant-from-an-empty-place-b0472/</link>
      <pubDate>Sun, 26 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>https://memotut.com/a-story-about-trying-magic-tricks-of-live-coding-of-natural-language-processing-and-dependency-analysis-with-lt-in-an-instant-from-an-empty-place-b0472/</guid>
      <description># wrap up  GiNZA, which carries out ultra-high-precision natural language processing and dependency analysis, is amazing, With Colaboratory, you don&amp;rsquo;t need to build an environment and you can use it easily with just a browser.
During the LT (Lightning Talk) in order to emphasize that crispness On-the-fly environment construction &amp;amp; natural code processing by writing code, Moreover, I tried the &amp;ldquo;magic trick&amp;rdquo; that says I can achieve high precision and high functionality.</description>
    </item>
    
    <item>
      <title>[PyTorch] How to use BERT-fine tuning Japanese pre-trained models to solve classification problem</title>
      <link>https://memotut.com/pytorch-how-to-use-bert-fine-tuning-japanese-pre-trained-models-to-solve-classification-problem-7f3a5/</link>
      <pubDate>Sat, 18 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>https://memotut.com/pytorch-how-to-use-bert-fine-tuning-japanese-pre-trained-models-to-solve-classification-problem-7f3a5/</guid>
      <description>#Introduction  Although BERT is updating SOTA with various tasks of natural language processing, what is published by [Google] on Github is based on Tensorflow. It has been implemented. People who use PyTorch want to use the PyTorch version, but I have not created the PyTorch version, so please use the one made by HuggingFace,butwedevelopedAskthemformoreinformationastheyarenotinvolvedin!AndQA.
Although it is a BERT made by Hugging Face, there was no Japanese pre-trained model until December 2019.</description>
    </item>
    
    <item>
      <title>Understanding Word2Vec</title>
      <link>https://memotut.com/understanding-word2vec-69afa/</link>
      <pubDate>Mon, 13 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>https://memotut.com/understanding-word2vec-69afa/</guid>
      <description>#Introduction  I have summarized what I have learned about Word2Vec, which is now a standard method for natural language processing. The outline of the algorithm is organized and the model is created using the library.
##reference I have referred to the following to understand Word2Vec.
 Deep Learning from Zero ❷-Natural Language Processing EditionK.Saito(Author) How to understand Word2vec with pictures  Efficient Estimation of Word Representations in Vector Space (Original paper) gensim API reference  #Word2Vec overview The following describes the concept of natural language processing, which is the premise of Word2Vec.</description>
    </item>
    
    <item>
      <title>[For beginners] Language analysis using natural language processing tool GiNZA (from morphological analysis to vectorization)</title>
      <link>https://memotut.com/for-beginners-language-analysis-using-natural-language-processing-tool-ginza-from-morphological-analysis-to-vectorization-63ffd/</link>
      <pubDate>Sat, 04 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>https://memotut.com/for-beginners-language-analysis-using-natural-language-processing-tool-ginza-from-morphological-analysis-to-vectorization-63ffd/</guid>
      <description>#Introduction  Recently, I started using the Python language processing tool &amp;ldquo;GiNZA&amp;rdquo;. I used to use MeCab so far, but I recently (shyly) found out that Python has a library that incorporates state-of-the-art machine learning techniques, so I&amp;rsquo;m currently moving to GiNZA. This time it is the first GiNZA, so I tried to summarize the processing flow as a memorandum while referring to various sites. There are many places that I am new to natural language analysis and I can not reach it, so please refer to the official documents etc.</description>
    </item>
    
    <item>
      <title>Creation of negative/positive classifier using BERT</title>
      <link>https://memotut.com/creation-of-negative-positive-classifier-using-bert-b0125/</link>
      <pubDate>Wed, 25 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://memotut.com/creation-of-negative-positive-classifier-using-bert-b0125/</guid>
      <description>This post is the 25th day of &amp;quot;Natural Language Processing Advent Calendar 2019-Qiita&amp;quot;.  siny.
In this article, I summarized the creation of a negative-positive classifier using BERT, which plays a major role in natural language processing in 2019.
#Introduction I think that knowledge about BERT has appeared in books, blogs, Qiita, etc. However, since most datasets that can be used for natural language processing are based on English, and there are not many Japanese datasets, it is quite difficult to use BERT using Japanese text.</description>
    </item>
    
    <item>
      <title>I tried implementing sentence classification by Self Attention in PyTorch</title>
      <link>https://memotut.com/i-tried-implementing-sentence-classification-by-self-attention-in-pytorch-98ff5/</link>
      <pubDate>Wed, 25 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://memotut.com/i-tried-implementing-sentence-classification-by-self-attention-in-pytorch-98ff5/</guid>
      <description>This article is from the 25th day of [Pytorch Advent Calendar 2019](https://qiita.com/advent-calendar/2019/pytorch)!  #Introduction
Previous implemented Attention in Encoder-Decoder model, but this time, I will implement sentence classification in Self Attention.
The embedded expression of sentences in Self Attention is introduced in the following paper, and is also quoted in the famous paper &amp;ldquo;Attention Is All You Need&amp;rdquo; by Transformer.
 A STRUCTURED SELF-ATTENTIVE SENTENCE EMBEDDING  This article implements the Self Attention introduced in this paper.</description>
    </item>
    
    <item>
      <title>Me, NER, Flair</title>
      <link>https://memotut.com/me-ner-flair-efda0/</link>
      <pubDate>Tue, 24 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://memotut.com/me-ner-flair-efda0/</guid>
      <description>This article is on the 23rd day of [MYJLab Advent Calendar](https://qiita.com/advent-calendar/2019/myjlab).  Introduction Hello. This is marutaku from MYJLab m1. This time, I had the opportunity to use a technique called proper expression extraction, so I would like to introduce the library used at that time.
About proper expression extraction (NER) Named Entity Recognition is a technology that acquires proper nouns such as company names and place names with labels such as company and place names.</description>
    </item>
    
    <item>
      <title>I tried sentiment analysis of the whole novel Weather&#39;s Child ☔️</title>
      <link>https://memotut.com/i-tried-sentiment-analysis-of-the-whole-novel-weathers-child-%EF%B8%8F-10f52/</link>
      <pubDate>Sun, 22 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://memotut.com/i-tried-sentiment-analysis-of-the-whole-novel-weathers-child-%EF%B8%8F-10f52/</guid>
      <description>![752B7BFA-4985-442D-904C-BEB091269C6C.gif](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/327405/05e5f803-dc12-4db0-bcf5-47661a8e5329.gif)  #1. A brief overview In this article, I will explain how to analyze sentiment by natural language processing the text of the novel **&amp;ldquo;Weather Child&amp;rdquo;**!
In general, emotional analysis is to discover and quantify &amp;ldquo;emotion&amp;rdquo; contained in a sentence and judge the opinion of the sentence. This is an area of current interest because it allows us to mechanically categorize user opinions about our products and services.
On the other hand, &amp;ldquo;Isn&amp;rsquo;t it possible to use sentiment analysis in addition to reviews and reviews?</description>
    </item>
    
    <item>
      <title>I made a learning kit of word2vec/doc2vec/GloVe/fastText</title>
      <link>https://memotut.com/i-made-a-learning-kit-of-word2vec-doc2vec-glove-fasttext-8240c/</link>
      <pubDate>Sat, 21 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://memotut.com/i-made-a-learning-kit-of-word2vec-doc2vec-glove-fasttext-8240c/</guid>
      <description>This article is the 21st day of [Natural Language Processing #2 Advent Calendar 2019](https://qiita.com/advent-calendar/2019/nlp2).  By the way, it&amp;rsquo;s my birthday today. Please celebrate ~~ M&amp;rsquo;s book that sets a deadline for birthday ~ ~
#Introduction
In the word embedding world, BERT has been terrifying over the past year, and even ELMo is becoming less visible. Sometimes I still want to use legacy distributed representations like word2vec or GloVe. In addition, you may want to learn with your data (at least for me) So, I made a learning kit for word2vec/doc2vec/GloVe/fastText for myself, so I will publish it.</description>
    </item>
    
    <item>
      <title>Creating an API that returns negative/positive inference results using BERT with Django REST framework</title>
      <link>https://memotut.com/creating-an-api-that-returns-negative-positive-inference-results-using-bert-with-django-rest-framework-30e10/</link>
      <pubDate>Fri, 20 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://memotut.com/creating-an-api-that-returns-negative-positive-inference-results-using-bert-with-django-rest-framework-30e10/</guid>
      <description>This post is the 20th day of &amp;quot;Django Advent Calendar 2019-Qiita&amp;quot;.  siny.
In this article, I summarized the content of the challenge to create a Rest API that returns negative/positive inference results using Django REST framework and BERT model.
 Since DRF and BERT were implemented for the first time this time, I would appreciate it if you could point out any mistakes or points such as &amp;ldquo;It&amp;rsquo;s better to do this more!</description>
    </item>
    
    <item>
      <title>The guy who predicts the number of views from the title of Jarjar&#39;s video</title>
      <link>https://memotut.com/the-guy-who-predicts-the-number-of-views-from-the-title-of-jarjars-video-06f6f/</link>
      <pubDate>Fri, 20 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://memotut.com/the-guy-who-predicts-the-number-of-views-from-the-title-of-jarjars-video-06f6f/</guid>
      <description>#Introduction  This time, I will talk about making a model that predicts the number of views from the title of Jarjar&amp;rsquo;s video. Since NLP is a complete amateur, I tried to imitate the appearance by referring to the articles of other people.
What is jar jar JALJAL is a comedy group with the most momentum*, consisting of Junpei Goto and Shusuke Fukutoku, who belong to the Yoshimoto Kogyo Tokyo headquarters.</description>
    </item>
    
    <item>
      <title>Answers and impressions of 100 language processing knocks-Part 2</title>
      <link>https://memotut.com/answers-and-impressions-of-100-language-processing-knocks-part-2-434ab/</link>
      <pubDate>Thu, 19 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://memotut.com/answers-and-impressions-of-100-language-processing-knocks-part-2-434ab/</guid>
      <description>advent calendar day 17  I&amp;rsquo;m late .. ..
this is Language processing 100 knocksIsolvedit,soIwillwriteoneanswerandoneimpression(secondpart) It took a lot more time than last time ~~~
Assumptions Environment etc. here(previouslink)
Main story Chapter 5: Dependency Analysis  Use Sobo Natsume&amp;rsquo;s novel &amp;ldquo;I am a cat&amp;rdquo; (neko.txt) for dependency analysis using CaboCha, and save the result in a file called neko.txt.cabocha. Use this file to implement the program corresponding to the following questions.</description>
    </item>
    
    <item>
      <title>Ver 4 to make sandwich man&#39;s control by machine learning</title>
      <link>https://memotut.com/ver-4-to-make-sandwich-mans-control-by-machine-learning-2a164/</link>
      <pubDate>Sun, 15 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://memotut.com/ver-4-to-make-sandwich-mans-control-by-machine-learning-2a164/</guid>
      <description>The fourth volume to make a sandwich man&#39;s control with deep learning! ! ! ! ! ! ! ! ! ! ! !  I&amp;rsquo;m in charge of Natural Language Processing Advent Calendar 2019 (I&amp;rsquo;m sorry for delaying one day!!!) https://qiita.com/advent-calendar/2019/nlp
I want to automatically make a control like Sandwich Man, and I want to sell it to a comedy office for 4000 trillion yen. And I want to see Sandwich Man!</description>
    </item>
    
    <item>
      <title>I tried to make an emotion radar chart of Aozora Bunko&#39;s work</title>
      <link>https://memotut.com/i-tried-to-make-an-emotion-radar-chart-of-aozora-bunkos-work-7a764/</link>
      <pubDate>Sat, 14 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://memotut.com/i-tried-to-make-an-emotion-radar-chart-of-aozora-bunkos-work-7a764/</guid>
      <description># Overview  Introduction This is the 14th day article of &amp;ldquo;Fujitsu Cloud Technologies Advent Calendar 2019&amp;rdquo;. Yesterday was @213&amp;rsquo;sThestoryofthefailurethatoccurredthisyear. ..
Has 213 really survived Friday the 13th? This is where I am concerned.
Main subject People suddenly want to read a book, but depending on the mood at that time, the content may be bright or dark, and the tendency of the book you want to read may change.</description>
    </item>
    
    <item>
      <title>I tried summarizing what was output with Qiita with Word cloud</title>
      <link>https://memotut.com/i-tried-summarizing-what-was-output-with-qiita-with-word-cloud-8c424/</link>
      <pubDate>Mon, 09 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://memotut.com/i-tried-summarizing-what-was-output-with-qiita-with-word-cloud-8c424/</guid>
      <description>This is the 10th article from the Proto Out Studio Advent Calendar!  #Overview After entering the Proto-Out Studio, I started to output with Qiita. (Although it is still very few)
So, this time, I also looked back on what I wrote so far, I want to visualize what I have output in Word Cloud of Python.
About Word Cloud WordCloud is to select words with a high frequency of occurrence from the text and to display them in a size according to the frequency of occurrence of the words.</description>
    </item>
    
    <item>
      <title>Automatically generate a polarity dictionary for emotion analysis</title>
      <link>https://memotut.com/automatically-generate-a-polarity-dictionary-for-emotion-analysis-1b7c7/</link>
      <pubDate>Sat, 07 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://memotut.com/automatically-generate-a-polarity-dictionary-for-emotion-analysis-1b7c7/</guid>
      <description>#Introduction  This is the 7th day of &amp;ldquo;Natural Language Processing Advent Calendar 2019&amp;rdquo;. I felt the problem in the article of &amp;ldquo;Score the negative/positive degree of news articles by sentiment analysis&amp;rdquo; that I posted earlier, **I can use polar dictionary too much I tried to solve the &amp;ldquo;no problem&amp;rdquo; **. This time we will challenge the automatic generation of a polar dictionary using fasttext.
#reference I have referred to the following when learning fasttext.</description>
    </item>
    
    <item>
      <title>Let&#39;s use the distributed expression of words quickly with fastText!</title>
      <link>https://memotut.com/lets-use-the-distributed-expression-of-words-quickly-with-fasttext-c26fa/</link>
      <pubDate>Thu, 05 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://memotut.com/lets-use-the-distributed-expression-of-words-quickly-with-fasttext-c26fa/</guid>
      <description>In this article, we&#39;ll look at how to use fastText to get a distributed word representation. [Article on the previous day](https://qiita.com/BlueRayi/items/eb7949594839771c1b70) I thought it would be nice if I could talk about a little link.  What is #fastText
fastTextisamethodtoacquiredistributedexpressions(wordsexpressedasnumbers)announcedbyFacebook.ItisbasedonthefamiliarWord2Vec(CBOW/skip-gram). As for Word2Vec, it doesn&amp;rsquo;t need to be explained because it has been changed.
Paper: Enriching Word Vectors with Subword Information
The difference between Word2Vec and fastText is how to take a vector.</description>
    </item>
    
    <item>
      <title>1 Click to become an executive button</title>
      <link>https://memotut.com/1-click-to-become-an-executive-button-1c3de/</link>
      <pubDate>Wed, 04 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://memotut.com/1-click-to-become-an-executive-button-1c3de/</guid>
      <description>The looming year-end, is it cold because of winter or bosom...  We created an app that is similar to a faint match this season when you have a feeling of gray. With 1 Click, you can become an executive of a listed company! Its name is &amp;ldquo;Executive Button&amp;rdquo;. Click the image below to start it.
 Executive button here is amazing!
Get paid for executives at listed companies with the push of a button!</description>
    </item>
    
    <item>
      <title>I&#39;m in Singapore right now, a story about creating a LineBot and wanting to do a memorable job</title>
      <link>https://memotut.com/im-in-singapore-right-now-a-story-about-creating-a-linebot-and-wanting-to-do-a-memorable-job-014aa/</link>
      <pubDate>Wed, 04 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://memotut.com/im-in-singapore-right-now-a-story-about-creating-a-linebot-and-wanting-to-do-a-memorable-job-014aa/</guid>
      <description># &amp;quot;Sorry, don&#39;t go to class&amp;quot;  This CM with a strong impact. I love this, so if you invite me to a class &amp;ldquo;I&amp;rsquo;m in Singapore now&amp;rdquo; I have made a LineBot that will reply to me. In addition, various specifications that further enhance the feeling of ** Even if it doesn&amp;rsquo;t remain on the map, I want to make it a memorable job of the person who used it!</description>
    </item>
    
    <item>
      <title>Answers and impressions of 100 language processing knocks-Part 1</title>
      <link>https://memotut.com/answers-and-impressions-of-100-language-processing-knocks-part-1-234e7/</link>
      <pubDate>Sat, 30 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>https://memotut.com/answers-and-impressions-of-100-language-processing-knocks-part-1-234e7/</guid>
      <description>advent calendar The first day, but write something easy to write without any special feeling  this is I have written 100 knocks of language processing,soIwillwriteeachanswerandimpression(currently11/30PM20:30Sothepartthatcanbewrittenisthefirstpart)
Assumptions  Environment -Link of Dockerfile(includingunrelatedones) Ability -I have touched mecab and gensim without knowing anything How to solve -For the time being, I solved it myself, and I checked the question every 10 questions by annoying -There is also a part to put two answers Excuse -Correction of wrong answer noticed during compilation is not in time Thanks -It is very appreciated as a self-taught person who goes through the darkness to release these teaching materials  Reflection I was re-reading the code myself to draw this article, so it was a pseudo code review</description>
    </item>
    
    <item>
      <title>Visualize Wikidata knowledge with Neo4j</title>
      <link>https://memotut.com/visualize-wikidata-knowledge-with-neo4j-cc64c/</link>
      <pubDate>Thu, 28 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>https://memotut.com/visualize-wikidata-knowledge-with-neo4j-cc64c/</guid>
      <description>This is the 8th day of NTT Docomo Service Innovation Department Advent Calendar 2019.  This time, I will visualize the data of Wikidata, which is one of the structured knowledge bases, using Neo4j, which is one of the graph DBs.
#What is Graph DB Simply put, it is a database that can handle graph structures. Compared to the popular RDB, it is a database designed to handle relationships between data 1.</description>
    </item>
    
    <item>
      <title>Summarize the method of preprocessing text (natural language processing) with tf.data.Dataset api</title>
      <link>https://memotut.com/summarize-the-method-of-preprocessing-text-natural-language-processing-with-tf.data.dataset-api-c4f6f/</link>
      <pubDate>Tue, 26 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>https://memotut.com/summarize-the-method-of-preprocessing-text-natural-language-processing-with-tf.data.dataset-api-c4f6f/</guid>
      <description>It is the 11th day of [TensorFlow2.0 Advent Calendar 2019](https://qiita.com/advent-calendar/2019/tensorflow2).  I would like to summarize the method of preprocessing text using tf.data.Dataset API.
This article will explain in the following order.
 Explain what the tf.data.Dataset API is and how effective it is Explain the actual text preprocessing procedure Summary of tips for improving performance  Since the explanation is long (the code is long&amp;hellip;) If you want to take a bird&amp;rsquo;s eye view by looking at only the code, here.</description>
    </item>
    
    <item>
      <title>[Python] I tried to visualize the night of the Galactic Railway with WordCloud!</title>
      <link>https://memotut.com/python-i-tried-to-visualize-the-night-of-the-galactic-railway-with-wordcloud-07baa/</link>
      <pubDate>Thu, 14 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>https://memotut.com/python-i-tried-to-visualize-the-night-of-the-galactic-railway-with-wordcloud-07baa/</guid>
      <description># at first   I created it because I wanted to make WordCloud. Code may be wrong (sorry)  #Environment
 Python 3.7.3 Jupyter Notebook Windows  #Flow 1. Extracting text with scraping 2. Divide words using MeCab 3. WordCloud creation
#1.Scraping Here has &amp;ldquo;Galaxy Railway Night&amp;rdquo; on the site, so extract only the text from here.
 &amp;lt;div class = &amp;#34;main-text&amp;#34;&amp;gt; As you can see, if you extract the text of the lower level of this&amp;rsquo;div&amp;rsquo;, it will be ok!</description>
    </item>
    
    <item>
      <title>I made a library konoha to switch the tokenizer to a nice one.</title>
      <link>https://memotut.com/i-made-a-library-konoha-to-switch-the-tokenizer-to-a-nice-one.-bb9ff/</link>
      <pubDate>Thu, 14 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>https://memotut.com/i-made-a-library-konoha-to-switch-the-tokenizer-to-a-nice-one.-bb9ff/</guid>
      <description># TL; DR  I will introduce konoha, a library for tokenizing sentences. (Formerly tiny_tokenizer) You can use it like ↓. What?
from konoha import WordTokenizer sentence = &amp;#34;I am studying natural language processing&amp;#34; tokenizer = WordTokenizer(&amp;#39;MeCab&amp;#39;) print(tokenizer.tokenize(sentence)) # -&amp;gt; [natural, language, processing, study, study, do, do, masu] tokenizer = WordTokenizer(&amp;#39;Kytea&amp;#39;) print(tokenizer.tokenize(sentence)) # -&amp;gt; [natural, language, processing, study, study, te, i, ma, su] tokenizer = WordTokenizer(&amp;#39;Sentencepiece&amp;#39;, model_path=&amp;#34;data/model.spm&amp;#34;) print(tokenizer.tokenize(sentence)) # -&amp;gt; [, natural, language, processing, studying, doing, doing] Introduction: What is a tokenizer?</description>
    </item>
    
    <item>
      <title>[Python] I visualized Arashi&#39;s lyrics on WordCloud and tried to understand what I wanted to tell the fans 20 years after the formation</title>
      <link>https://memotut.com/python-i-visualized-arashis-lyrics-on-wordcloud-and-tried-to-understand-what-i-wanted-to-tell-the-fans-20-years-after-the-formation-122ca/</link>
      <pubDate>Mon, 11 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>https://memotut.com/python-i-visualized-arashis-lyrics-on-wordcloud-and-tried-to-understand-what-i-wanted-to-tell-the-fans-20-years-after-the-formation-122ca/</guid>
      <description>#Trigger  It&amp;rsquo;s only one year left until Arashi&amp;rsquo;s activities stop. It&amp;rsquo;s been 20 years since the appearance of invisibility costumes. What did the national idol, who is active in multiplayer, want to tell the fans 20 years after the formation? I would like to meet you in person, but that&amp;rsquo;s why. So I decided to &amp;ldquo;visualize the lyrics&amp;rdquo; and convey the message I want to convey to the fans, ~~ the 6th member~~ to Arashi fans.</description>
    </item>
    
    <item>
      <title>I tried to implement Attention Seq2Seq in PyTorch</title>
      <link>https://memotut.com/i-tried-to-implement-attention-seq2seq-in-pytorch-64604/</link>
      <pubDate>Fri, 08 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>https://memotut.com/i-tried-to-implement-attention-seq2seq-in-pytorch-64604/</guid>
      <description>#Introduction  Last time Following the implementation of Seq2Seq, this time I tried to implement Attention Seq2Seq which added Attention to Seq2Seq with PyTorch.
Even beginners like me can&amp;rsquo;t find the source code that implements Attention in PyTorch so easily, and there is also PyTorch Attention Tutorial.Thereis,butitseemsthatIhavenotlearnedmini-batch(?),andIwantedtotryimplementingsimplerplain(?) Attention with such feeling that it was customized for this task. I tried to implement Attention by myself. It would be greatly appreciated if we could provide some helpful information to those who are having trouble implementing Attention.</description>
    </item>
    
    <item>
      <title>What about polarity analysis with order?</title>
      <link>https://memotut.com/what-about-polarity-analysis-with-order-eb794/</link>
      <pubDate>Fri, 08 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>https://memotut.com/what-about-polarity-analysis-with-order-eb794/</guid>
      <description>I learned a little about polarity analysis (emotion analysis) in Japanese, and I was playing a little bit while being an amateur.  In general polarity analysis (emotion analysis) , the scores of each word against the polarity dictionary are averaged over the entire sentence. However, does that mean that you have a score that matches your intuition?
Natural Language Processing Advent Calendar 2019 4 day.Yesterdaywas kabayan55 &amp;lsquo;s “Astoryaboutapersonwhowantstostudynaturallanguageprocessing” and KazumasaYamamoto &amp;lsquo;s “LearndocumentcategoryclassificationwithCLIofspaCy” It was .</description>
    </item>
    
  </channel>
</rss>