<!doctype html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  
  <meta name="generator" content="Hugo 0.72.0" />

  
  <meta name="description" content="Some description">
  

  
  <link rel="apple-touch-icon" sizes="180x180" href="https://memotut.com/apple-touch-icon.png">

  
  <link rel="icon" type="image/png" sizes="32x32" href="https://memotut.com/favicon-32x32.png">

  
  <link rel="icon" type="image/png" sizes="16x16" href="https://memotut.com/favicon-16x16.png">

  
  <link rel="manifest" href="https://memotut.com/site.webmanifest">

  
  <link rel="mask-icon" href="https://memotut.com/safari-pinned-tab.svg" color="#5bbad5">

  <meta name="msapplication-TileColor" content="#da532c">

  <meta name="theme-color" content="#ffffff">

  
  <link rel="stylesheet" href="https://memotut.com/css/bootstrap.min.css" />

  
  <title>[Python] Introduction to Machine Learning from Simple Perceptron | Memo Tut</title>
  

  <style>
body {
  min-width: 300px;
}

.custom-navbar {
  margin-bottom: 1em;
  height: 60px;
}

.custom-navbar a {
  display: inline-block; 
  padding: 18px 0;
  margin-right: 1em; 
  font-weight: bold; 
}

.custom-navbar a:hover,
.custom-navbar a:focus {
  text-decoration: none; 
}

@media print {
  .custom-navbar {
    display: none;
  }
}

article {
  padding-bottom: 1em;
}

img {
  max-width: 100%;
}


body {
  background-color: #fff;
}



body {
  color: #212529;
}



a {
  color: #007bff;
}



a:hover,
a:focus {
  color: #0056b3;
}



.custom-navbar {
  background-color: #212529;
}



.custom-navbar a {
  color: rgba(255, 255, 255, 0.75);
}



.custom-navbar a:hover,
.custom-navbar a:focus {
  color: rgba(255, 255, 255, 1);
}



.container {
  max-width: 800px;
}



pre {
  display: block;
  padding: 9.5px;
  word-break: break-all;
  word-wrap: break-word;
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 4px;
}

pre code {
  padding: 0;
  font-size: inherit;
  color: inherit; 
  white-space: pre-wrap;
  background-color: transparent;
  border: none;
  border-radius: 0;
}

code {
  padding: 2px 4px;
  color: inherit; 
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 4px;
  font-size: .9em;
}



blockquote,
.blockquote {
  padding: 10px 20px;
  margin: 0 0 20px;
  font-size: 1em;
  border-left: 5px solid #6c757d;
}

</style>
</head>

<body>
  <nav class="custom-navbar">
  <div class="container">
    
    <a href="/">Posts</a>
    
    <a href="/tags/">Tags</a>
    
    <a href="/about/">About</a>
    
    <a href="/index.xml">RSS</a>
    
  </div>
</nav>
  
  <div class="container">
    <article>
      <h1>[Python] Introduction to Machine Learning from Simple Perceptron</h1>
<p>
  <small class="text-secondary">
  
  
  Jan 9, 2016
  </small>
  

<small><code><a href="https://memotut.com/tags/python">Python</a></code></small>


<small><code><a href="https://memotut.com/tags/machine-learning"> machine learning</a></code></small>

</p>
<pre><code>#1. What is a simple perceptron?
</code></pre>
<p>The input data is classified into two classes using the following linear discriminant function.</p>
<pre><code class="language-math" data-lang="math">f(x)=g(w^{T}x+\theta)\\
Here, if \theta = x_0, \\
    =g(\sum _{i=0}^{d}w_ix_i)
</code></pre><pre><code class="language-math" data-lang="math">Non-linear activation function: g(a) = \left\{
\begin{array}{ll}
+1 &amp; (a \geq 0) \\
- 1 &amp; (a \lt 0)
\end{array}
\right.
</code></pre><pre><code class="language-math" data-lang="math">Weight: w=(w_0,w_1,...,w_{d}), \\
Input data: x=(x_0=\theta,x_1,x_2,...,x_{d}),
\\d: number of dimensions,
</code></pre><pre><code class="language-math" data-lang="math">Bias: \theta (arbitrary number)
</code></pre><pre><code class="language-math" data-lang="math">When f(x) \geq 0, x\in C_{1}
</code></pre><pre><code class="language-math" data-lang="math">When f(x) \lt 0, x\in C_{2}
</code></pre><pre><code class="language-math" data-lang="math">C_1: Class 1, C_2: Class 2
</code></pre><p>Of the above, the process for determining the parameter of weight_w_ is called “learning”.
Bias_θ_ (bias parameter) is not disabled by default, but it is manually adjusted (tuned) to obtain better results.
What is manually adjusted like this bias is generally called a parameter (hyperparameter or tuning parameter), and adjusting a parameter is sometimes called parameter tuning.</p>
<p>～Example of parameter～
&ldquo;Ngo that resulted as a result of parameter tuning with a simple perceptron&rdquo;</p>
<p>There are various tuning methods, such as a method in which a person manually tries, a grid search, and random sampling.</p>
<p>##1.1. What can you use it for?
It can be used for two-class problems that can be linearly separated.
A roughly separable problem is, roughly speaking, a problem in which in the case of two dimensions, a class 1 set and a class 2 set can be separated by one straight line.
By generalizing this, it is called linear separability that the set of class 1 and the set of class 2 on <em>n</em> dimensional space can be separated by <em>n</em>-1 dimensional hyperplane.
The figure below shows examples of cases where linear separation is possible and cases where linear separation is not possible.
<img src="https://qiita-image-store.s3.amazonaws.com/0/105699/88488629-2875-e071-5434-b27bd3e7dfc4.png" alt="senkeibunri.png"></p>
<p>A common application of supervised learning algorithms such as Perceptron is spam mail detection.
Roughly, the following procedure is used to judge spam mails.</p>
<ol>
<li>Assume two classes, spam mail and normal mail.</li>
<li>Next, create a feature vector (or feature quantity or feature) and a teacher label (spam or normal label) from both emails.</li>
</ol>
<ul>
<li>As an aside, the term &ldquo;feature&rdquo; is often used in the field of natural language processing.</li>
</ul>
<ol start="3">
<li>Input the feature vector and the teacher label into the learning device (this time, simple perceptron) and perform learning.</li>
<li>Make a judgment using the completed mathematical model (linear discriminant function model this time).</li>
</ol>
<p>The feature vector needs to be quantified from the qualitative data by leaving the features of the data well.
This is a very important task, as accuracy may change significantly.
A specific example of the method of creating a feature vector is given below.</p>
<p>[Example: Feature vector creation method]
Even if there is the following data, I want to determine whether it is spam by the subject.</p>
<p>・Subject of spam mail: My boyfriend fought a giant anteater and died</p>
<ul>
<li>Regular email subject: Request to change meeting time</li>
</ul>
<p>A technique often used in the field of natural language processing is morphological analysis.
Famous tools for morphological analysis are MeCab and Cabocha.
By morphological analysis, it is possible to divide the input sentence into morphemes (the smallest unit that has meaning in the language).
This will create the following sentence. (The accuracy is not guaranteed because it is a morphological analysis by a person who is not good at Japanese)</p>
<p>・Subject of spam mail: Boyfriend / / / Giant anteater / / Fighting / Dead / Lost /</p>
<ul>
<li>Regular email subject: Meeting / time / change / request /</li>
</ul>
<p>Next, create a morpheme dictionary using the above data.
As a result, the dictionary will be as follows.</p>
<pre><code class="language-math" data-lang="math">    Dictionary \in\{boyfriend, anteater, fight, dead, dead, meeting, time, change, of, request}
</code></pre><p>*The above is not a general method in the field of natural language processing because it is created by using the data of a dictionary created by someone or by manually inputting it.
When creating this dictionary, we will improve the accuracy by making efforts such as modifying all morphemes into basic forms for verbs and putting them in the dictionary.</p>
<p>Finally, using the dictionary created earlier, create a feature vector as follows.</p>
<pre><code class="language-math" data-lang="math">Spam mail feature vector x_1=\{1,1,1,1,1,1,1,0,0,0,0,0\}, teacher label t=\{-1\}
</code></pre><pre><code class="language-math" data-lang="math">Normal email feature vector x_2=\{0,0,0,0,0,0,0,1,1,1,1,1 \}, teacher label t=\{+1\}
</code></pre><p>How to create a feature vector is that the presence/absence of words included in the dictionary (1: Yes, 0: No) is stored in an array.
This method is called the Bag-Of-Words method (word bagging method).
In some cases, it is created based on the number of times a word appears, instead of whether or not a word appears.
Also, as this extension, there is a weighting method called Tf-idf.</p>
<ul>
<li>This is used to reduce the importance of words that appear in many documents and to increase the importance of words that appear only in specific documents.
Although the teacher label is decided, generally, 0 and 1 are used when used in a probabilistic model, and -1 and 1 that are compatible with activation functions such as perceptron are often applied.</li>
</ul>
<p>The above is the procedure for creating the feature vector.</p>
<p>In addition, the reason why it can be used in linearly separable problems can be explained by &ldquo;Concept theorem of Perceptron&rdquo;.
See below for proof.
<a href="http://ocw.nagoya-u.jp/files/253/haifu%2804-4%29.pdf">http://ocw.nagoya-u.jp/files/253/haifu%2804-4%29.pdf</a></p>
<p>#2. Perceptron learning (stochastic steepest descent algorithm)
The concept of the learning method is simple. If you make a mistake, update the value of <em>w</em> by the formula below, and continue updating until there is no mistake.
The series of learning data is defined as follows.</p>
<pre><code class="language-math" data-lang="math">X=\{X_1,X_2,...,X_M\} \\
M: Number of data
</code></pre><p>Here, a set of misclassified learning data is described as follows.</p>
<pre><code class="language-math" data-lang="math">X_n=\{x_1,x_2,...,x_n\}
</code></pre><p>We would like to find the weight <em>w</em> such that _f(x)_&gt; 0 for class 1 data and <em>f(x)</em> &lt;0 for class 2 data.
In this case, using the teacher label _t_={+1,-1}, all data satisfy the following.</p>
<pre><code class="language-math" data-lang="math">w^Tx_it_i&gt;0
</code></pre><p>The following error function E(•) is conceivable, which assigns an error of 0 to correctly classified data.</p>
<pre><code class="language-math" data-lang="math">E(w) =-\sum_{n \in X} w^Tx_nt_n
</code></pre><p>If you set the value of <em>w</em> so that it will be minimized (it will be 0), you can classify well.
The stochastic gradient descent method is used as a method for minimizing this.
In the stochastic gradient descent method, when the error function consists of the sum of data points like <em>E(w)</em> this time, when data <em>n</em> is given, w is updated by the following calculation.</p>
<pre><code class="language-math" data-lang="math">w^{(r+1)}=w^{(r)}-\mu \nabla E_n \\
r: number of iterations, \mu: learning rate parameter \\
w^{(r)}: weight w after updating r times
</code></pre><p>From the above, the update formula can be derived as follows.</p>
<pre><code class="language-math" data-lang="math">w^{(r+1)}=w^{(r)}-\mu \nabla E(w) \\
=w^{(r)}+\mu x_nt_n
</code></pre><p>In the area where data is misclassified depending on the value of <em>w</em>, whether the value of <em>t</em> is +1 or -1, the error of misclassified data to <em>E(w)</em> The contribution of is a linear function.
Further, depending on the value of <em>w</em>, the contribution of the data to the error <em>E(w)</em> is 0 within the area where the data is correctly classified.
Therefore, <em>E(w)</em> is a piecewise linear function.
Therefore, the gradient of <em>E(w)</em> is calculated as follows.</p>
<pre><code class="language-math" data-lang="math">E(w) = \frac{\partial E(w)}{\partial w} \\
=x_nt_n
</code></pre><p>#3. Implementation with Python
The code below is a code that says that the AND operation, which is neither designed nor broken, can be solved by Perceptron.
The state of learning is plotted.
It may sound like it should be a class or method, but please understand.
Rewrite if you feel like it.
To run, you need to install <em>numpy</em> and <em>matplotlib</em> libraries.
<img src="https://qiita-image-store.s3.amazonaws.com/0/105699/f53eeb2c-20ed-ebd6-62e9-52fa01ba4488.gif" alt="output.gif"></p>
<pre><code># coding:utf-8
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.animation as animation

# Method of deactivation function g(x)
def predict(w,x):
  out = np.dot(w,x)
  if out &gt;= 0:
    o = 1.0
  else:
    o = -1.0
  return o

# Method to plot
def plot(wvec,x1,x2):
  x_fig=np.arange(-2,5,0.1)
  fig = plt.figure(figsize=(8, 8),dpi=100)
  ims = []
  plt.xlim(-1,2.5)
  plt.ylim(-1,2.5)
  # Plot
  for w in wvec:
    y_fig = [-(w[0] / w[1]) * xi-(w[2] / w[1]) for xi in x_fig]
    plt.scatter(x1[:,0],x1[:,1],marker='o',color='g',s=100)
    plt.scatter(x2[:,0],x2[:,1],marker='s',color='b',s=100)
    ims.append(plt.plot(x_fig,y_fig,&quot;r&quot;))
  for i in range(10):
    ims.append(plt.plot(x_fig,y_fig,&quot;r&quot;))
  ani = animation.ArtistAnimation(fig, ims, interval=1000)
  plt.show()

if __name__=='__main__':
  wvec=[np.array([1.0,0.5,0.8])]# initial value of weight vector, appropriate
  mu = 0.3 # learning coefficient
  sita = 1 # bias component# AND function data (last column is bias component: 1)
  x1=np.array([[0,0],[0,1],[1,0]]) #class 1 (operation result is 0) matrix generation
  x2=np.array([[1,1]]) # Matrix generation for class 2 (operation result is 1)
  bias = np.array([sita for i in range(len(x1))])
  x1 = np.c_[x1,bias] # Concatenate bias to end of class 1 data
  bias = np.array([sita for i in range(len(x2))])
  x2 = np.c_[x2,bias] # Concatenate bias to end of class 2 data
  class_x = np.r_[x1,x2] # concatenation of matrices

  t = [-1,-1,-1,1] # AND function label
  #o: ask for output
  o=[]
  while t != o:
    o = [] # initialization
    # Learning phase
    for i in range(class_x.shape[0]):
      out = predict(wvec[-1], class_x[i,:])
      o.append(out)
      if t[i]*out&lt;0: # When output differs from teacher label
        wvectmp = mu*class_x[i,:]*t[i] #w change amount
        wvec.append(wvec[-1] + wvectmp) # update weight
  plot(wvec,x1,x2)
</code></pre><p>#4. Representative algorithm that extends simple perceptron
・SVM
・Multilayer perceptron</p>
<p>#5. References
On pattern recognition and machine learning</p>
<p>First pattern recognition</p>
<p><a href="http://home.hiroshima-u.ac.jp/tkurita/lecture/prnn/node20.html">http://home.hiroshima-u.ac.jp/tkurita/lecture/prnn/node20.html</a></p>
<p><a href="https://speakerdeck.com/kirikisinya/xin-zhe-renaiprmlmian-qiang-hui-at-ban-zang-men-number-2">https://speakerdeck.com/kirikisinya/xin-zhe-renaiprmlmian-qiang-hui-at-ban-zang-men-number-2</a></p>
<p><a href="http://tjo.hatenablog.com/entry/2013/05/01/190247">http://tjo.hatenablog.com/entry/2013/05/01/190247</a></p>
<p><a href="http://nlp.dse.ibaraki.ac.jp/~shinnou/zemi1ppt/iwasaki.pdf">http://nlp.dse.ibaraki.ac.jp/~shinnou/zemi1ppt/iwasaki.pdf</a></p>
<p><a href="http://ocw.nagoya-u.jp/files/253/haifu%2804-4%29.pdf">http://ocw.nagoya-u.jp/files/253/haifu%2804-4%29.pdf</a></p>

    </article>
  </div>

  
  
  
  <script>
  window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
  ga('create', 'UA-169005401-1', 'auto');
  ga('send', 'pageview');
  </script>
  <script async src='https://www.google-analytics.com/analytics.js'></script>
  

  

  
<link rel="stylesheet" type="text/css" href="//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.0/cookieconsent.min.css" />
<script src="//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.0/cookieconsent.min.js"></script>
<script>
window.addEventListener("load", function(){
window.cookieconsent.initialise({
  "palette": {
    "popup": {
      "background": "#216942",
      "text": "#b2d192"
    },
    "button": {
      "background": "#afed71"
    }
  }
})});
</script>

</body>

</html>
