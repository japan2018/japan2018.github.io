<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title> GPU on Memo Tut</title>
    <link>https://memotut.com/tags/gpu/</link>
    <description>Recent content in  GPU on Memo Tut</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 28 Mar 2020 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://memotut.com/tags/gpu/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Machine learning on a gaming PC and comparing the differences in CPU/GPU performance with Colabo [Windows machine learning environment construction procedure definitive version. TF2.0 compatible]</title>
      <link>https://memotut.com/machine-learning-on-a-gaming-pc-and-comparing-the-differences-in-cpu-gpu-performance-with-colabo-windows-machine-learning-environment-construction-procedure-definitive-version.-tf2.0-compatible-b6fa2/</link>
      <pubDate>Sat, 28 Mar 2020 00:00:00 +0000</pubDate>
      
      <guid>https://memotut.com/machine-learning-on-a-gaming-pc-and-comparing-the-differences-in-cpu-gpu-performance-with-colabo-windows-machine-learning-environment-construction-procedure-definitive-version.-tf2.0-compatible-b6fa2/</guid>
      <description># In a nutshell  Purchase a cheap gaming PC (Windows), Enable to switch CPU/GPU in virtual environment of Anaconda, Tensorflow-Running the GPU (v2.0) code, With 4 patterns including CPU/GPU of Colaboratory, I compared the performance. I will summarize the performance comparison results and the Windows version environment construction procedure. Article.
*I often hear that GPU is effective for machine learning
 Gaming PCs have GPUs and are cheap these days  ⇒ ** Let&amp;rsquo;s use GPU of gaming PC for machine learning!</description>
    </item>
    
    <item>
      <title>Problem that torch.cuda.is_available() of pytorch gives false</title>
      <link>https://memotut.com/problem-that-torch.cuda.is_available-of-pytorch-gives-false-7962d/</link>
      <pubDate>Tue, 03 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://memotut.com/problem-that-torch.cuda.is_available-of-pytorch-gives-false-7962d/</guid>
      <description>#Environment  ・Ubuntu 16.04 ・Python 3.7.3 ・Pytorch 1.2.0
GPU cannot be used with #pytorch
I thought about deeplearning, but it&amp;rsquo;s slow so check if you can see cuda in ipython.
In [1]: import torch In [2]: torch.cuda.is_available() Out[2]: False why? As a possible cause, I thought that NVIDIA-driver is not working,
(base) user@user:~$ nvidia-smi Tue Dec 3 11:29:10 2019 +------------------------------------------------- ----------------------------+ | NVIDIA-SMI 390.116 Driver Version: 390.116 | |-------------------------------+----------------- -----+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.</description>
    </item>
    
    <item>
      <title>P100-PCIE-16GB was added to the GPU of Google Colab somehow</title>
      <link>https://memotut.com/p100-pcie-16gb-was-added-to-the-gpu-of-google-colab-somehow-56633/</link>
      <pubDate>Sun, 24 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>https://memotut.com/p100-pcie-16gb-was-added-to-the-gpu-of-google-colab-somehow-56633/</guid>
      <description>* * This article is based on November 24, 2019.  While thinking that everyone may already know. .. .. ..
About Google colab GPU gacha I think it&amp;rsquo;s well known that you have a GPU gacha. See below for details. https://qiita.com/koshian2/items/d33edc963ed6cfcad77e
Trigger I thought that it was faster than usual, and when I checked the GPU, P100 was added as follows. GPU speed comparison Immediately, I compared the training speed.</description>
    </item>
    
  </channel>
</rss>