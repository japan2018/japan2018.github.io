<!doctype html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  
  <meta name="generator" content="Hugo 0.72.0" />

  
  <meta name="description" content="Some description">
  

  
  <link rel="apple-touch-icon" sizes="180x180" href="https://memotut.com/apple-touch-icon.png">

  
  <link rel="icon" type="image/png" sizes="32x32" href="https://memotut.com/favicon-32x32.png">

  
  <link rel="icon" type="image/png" sizes="16x16" href="https://memotut.com/favicon-16x16.png">

  
  <link rel="manifest" href="https://memotut.com/site.webmanifest">

  
  <link rel="mask-icon" href="https://memotut.com/safari-pinned-tab.svg" color="#5bbad5">

  <meta name="msapplication-TileColor" content="#da532c">

  <meta name="theme-color" content="#ffffff">

  
  <link rel="stylesheet" href="https://memotut.com/css/bootstrap.min.css" />

  
  <title>Try running wav2pix, which creates a face image from audio (there is also an anime face generator) | Memo Tut</title>
  

  <style>
body {
  min-width: 300px;
}

.custom-navbar {
  margin-bottom: 1em;
  height: 60px;
}

.custom-navbar a {
  display: inline-block; 
  padding: 18px 0;
  margin-right: 1em; 
  font-weight: bold; 
}

.custom-navbar a:hover,
.custom-navbar a:focus {
  text-decoration: none; 
}

@media print {
  .custom-navbar {
    display: none;
  }
}

article {
  padding-bottom: 1em;
}

img {
  max-width: 100%;
}


body {
  background-color: #fff;
}



body {
  color: #212529;
}



a {
  color: #007bff;
}



a:hover,
a:focus {
  color: #0056b3;
}



.custom-navbar {
  background-color: #212529;
}



.custom-navbar a {
  color: rgba(255, 255, 255, 0.75);
}



.custom-navbar a:hover,
.custom-navbar a:focus {
  color: rgba(255, 255, 255, 1);
}



.container {
  max-width: 800px;
}



pre {
  display: block;
  padding: 9.5px;
  word-break: break-all;
  word-wrap: break-word;
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 4px;
}

pre code {
  padding: 0;
  font-size: inherit;
  color: inherit; 
  white-space: pre-wrap;
  background-color: transparent;
  border: none;
  border-radius: 0;
}

code {
  padding: 2px 4px;
  color: inherit; 
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 4px;
  font-size: .9em;
}



blockquote,
.blockquote {
  padding: 10px 20px;
  margin: 0 0 20px;
  font-size: 1em;
  border-left: 5px solid #6c757d;
}

</style>
</head>

<body>
  <nav class="custom-navbar">
  <div class="container">
    
    <a href="/">Posts</a>
    
    <a href="/tags/">Tags</a>
    
    <a href="/about/">About</a>
    
    <a href="/index.xml">RSS</a>
    
  </div>
</nav>
  
  <div class="container">
    <article>
      <h1>Try running wav2pix, which creates a face image from audio (there is also an anime face generator)</h1>
<p>
  <small class="text-secondary">
  
  
  May 13, 2020
  </small>
  

<small><code><a href="https://memotut.com/tags/python">Python</a></code></small>


<small><code><a href="https://memotut.com/tags/deeplearning">DeepLearning</a></code></small>


<small><code><a href="https://memotut.com/tags/docker">Docker</a></code></small>


<small><code><a href="https://memotut.com/tags/gan">GAN</a></code></small>


<small><code><a href="https://memotut.com/tags/vtuber">Vtuber</a></code></small>

</p>
<pre><code>#Introduction
</code></pre>
<p>The world of deep learning, especially Generative Adversarial Networks (GAN), has grown dramatically in recent years, and I think that research is progressing in various fields such as text-to-image, voice conversion, and sound source separation.</p>
<p>In this talk, I will loosely write about wav2pix, which generates face images from voice.</p>
<p>Paper: <a href="https://arxiv.org/abs/1903.10195">WAV2PIX: SPEECH-CONDITIONED FACE GENERATION USING GENERATIVEADVERSARIAL NETWORKS</a></p>
<p>#Overview
<img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/472706/82bfb62d-9b88-a6a7-de5a-f81d751e9358.png" alt="Untitled 45.png">
<a href="https://imatge-upc.github.io/wav2pix/">https://imatge-upc.github.io/wav2pix/</a></p>
<p>The proposed model consists of the following three modules.</p>
<ul>
<li>Speech Encoder</li>
<li>Generator Network</li>
<li>Discriminator Network</li>
</ul>
<p>I will briefly explain each module.</p>
<p>First, regarding the Speech Encoder, it seems that the decoder of <a href="https://arxiv.org/abs/1703.09452">Speech Enhancement Generative Adversarial Network(SEGAN)</a>isused.SEGANisanend-to-endmodelofspeechenhancementusingGAN.Detailedexplanationisomitted,butpleasereferto<a href="http://veu.talp.cat/segan/">thisdemo</a>.</p>
<p>Next, it seems that the Generator Network and Discriminator Network are inspired by <a href="https://arxiv.org/abs/1611.04076">Least Squares Generative Adversarial Networks (LSGAN)</a>.LSGANhasadeeperunderstandingon<a href="https://qiita.com/inoudayo/items/a98da29b735c610fd7de">thissite</a>.</p>
<h1 id="quick-start">Quick Start</h1>
<p>After this, I will explain the sample execution of wav2pix.</p>
<h2 id="execution-environment">Execution environment</h2>
<p>The environment I tried this time is as follows.
OS: Ubuntu 18.04 LTS
CPU: i3-4130 3.40GHz
Memory: 16GB
GPU: GeForce GTX 1660 Ti (6GB)</p>
<p>Docker Version: Docker version 19.03.8</p>
<h2 id="1-obtaining-and-building-a-dockerfile">1. Obtaining and building a Dockerfile</h2>
<p><a href="https://github.com/imatge-upc/wav2pix">imatge-upc/wav2pix</a> describes how to execute it, but I made a Dockerfile myself for those who are troubled to prepare the execution environment.
So, this time, I will write the operation in Docker as the main.</p>
<p>First, let&rsquo;s get the Dockerfile that I made.
★Note the following points!</p>
<ul>
<li>The image size to create is about 5.5GB</li>
<li>It takes a long time to create an image</li>
</ul>
<pre><code class="language-:" data-lang=":">$ git clone https://github.com/Nahuel-Mk2/docker-wav2pix.git
$ cd docker-wav2pix/
$ docker build .-t docker-wav2pix
</code></pre><p>When you&rsquo;re finished, make sure you have an image.</p>
<pre><code class="language-:" data-lang=":">$ docker images
REPOSITORY TAG IMAGE ID CREATED SIZE
docker-wav2pix latest 8265bc421f7a 4 hours ago 5.36GB
</code></pre><h2 id="2-start-docker-traintest">2. Start Docker /Train/Test</h2>
<h3 id="21-start-docker">2.1. Start Docker</h3>
<p>Let&rsquo;s start Docker.</p>
<pre><code class="language-:" data-lang=":">$ docker run -it --rm --gpus all --ipc=host docker-wav2pix
</code></pre><h3 id="22-train">2.2. Train</h3>
<p>Add one step before train.
Save the necessary path by overwriting the config file.
★If you do not do this, train or test will throw an error at runtime, so be careful!</p>
<pre><code class="language-:" data-lang=":">$ echo -e &quot;# training pickle files path:\n&quot;\
&quot;train_faces_path: /home/user/wav2pix/pickle/faces/train_pickle.pkl\n&quot;\
&quot;train_audios_path: /home/user/wav2pix/pickle/audios/train_pickle.pkl\n&quot;\
&quot;# inference pickle files path:\n&quot;\
&quot;inference_faces_path: /home/user/wav2pix/pickle/faces/test_pickle.pkl\n&quot;\
&quot;inference_audios_path: /home/user/wav2pix/pickle/audios/test_pickle.pkl&quot;&gt; /home/user/wav2pix/config.yaml
</code></pre><p>If you can do the above, run Train.</p>
<pre><code class="language-:" data-lang=":">$ cd wav2pix
$python runtime.py
</code></pre><p>★It took about 3 hours to finish the Train in my environment. If you wait patiently or specify epoch at runtime as follows, it will end early.
★You can ignore the Visdom error.</p>
<pre><code class="language-:" data-lang=":">$ python runtime.py --epochs 100
</code></pre><ul>
<li>-epochs: Specify number of epochs (default 200)</li>
</ul>
<h3 id="23-test">2.3. Test</h3>
<p>After the Train ends, execute Test.
Since it is necessary to call the learned model, it is necessary to add an argument more than when Train is executed.</p>
<pre><code class="language-:" data-lang=":">$ python runtime.py --pre_trained_disc /home/user/wav2pix/checkpoints/disc_200.pth --pre_trained_gen /home/user/wav2pix/checkpoints/gen_200.pth --inference
</code></pre><ul>
<li>-pre_trained_disc: Discriminator path that has been trained</li>
<li>-pre_trained_gen: Trained Generator path</li>
<li>-inference: Flag for inference execution</li>
</ul>
<p>After executing, let&rsquo;s check the generated image.</p>
<pre><code class="language-:" data-lang=":">$ docker cp 89c8d43b0765:/home/user/wav2pix/results/.
</code></pre><p>★If you don&rsquo;t know the CONTAINER ID, run &ldquo;docker ps&rdquo; to check.</p>
<pre><code class="language-:" data-lang=":">$ docker ps
CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES
89c8d43b0765 docker-wav2pix &quot;/bin/bash&quot; 4 hours ago Up 4 hours vigilant_murdock
</code></pre><h2 id="3-image-generated-from-audio-excerpt">3. Image generated from audio (excerpt)</h2>
<p>←: Generated image: Real image: → ←: Generated image: Real image: →
<img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/472706/23af45ac-c2db-62d4-70bb-cdff651890f2.jpeg" alt="jaimealtozano_1550.jpg"><img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/472706/0078e178-2252-028c-2b6d-7db596941ce7.png" alt="1550_2.png"><img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/472706/89f4afad-51d5-f987-1b24-53a31d51363c.jpeg" alt="jaimealtozano_1600.jpg"><img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/472706/c8ac1586-5572-030c-2319-cd7bbec57791.png" alt="1560_2.png"></p>
<p><img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/472706/7be418a6-dc13-5453-4c09-56e38117ab22.jpeg" alt="javiermuniz_1250.jpg"><img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/472706/e1d08ae5-18a8-290f-f663-0bd96bfb30df.png" alt="1250_2.png"><img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/472706/d4980a3c-6ba3-bb17-79af-64961508c8ca.jpeg" alt="javiermuniz_1300.jpg"><img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/472706/a4896133-93c5-0f68-97e4-3a6982e8991e.png" alt="1300_2.png"></p>
<p>For two of the sample data sets this time, I think that the generated images that show that they are relatively faces were created. I also found that I could understand the individuality to some extent. However, it was pointed out in the paper that the images were rough.</p>
<h1 id="bonus">bonus</h1>
<h2 id="ex1-creating-a-data-set-required-for-animation-face-image-generation">Ex.1 Creating a data set required for animation face image generation</h2>
<p>From here, I&rsquo;d like to use wav2pix to generate an anime face image.That said, there is no data set that includes both voice and animated facial images, so you need to create your own. Therefore, referring to the YouTuber dataset created in the paper, we will create a Virtual YouTuber (VTuber) dataset.</p>
<p>The figure below shows how to create the dataset described in the paper.
The video from YouTuber is divided into video and Speech, processed, and finally created as a pair of data.
The only major change is the face detection cascade file.
The cascade file used is <a href="https://github.com/nagadomi/lbpcascade_animeface">here</a>.</p>
<p><img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/472706/0a792cd9-938c-e6f4-103c-f7e7a100d8c2.png" alt="Untitled 50.png"></p>
<p>The VTuber people and the videos used for creating the data are as follows.
For voice, the data is the section without BGM or SE.
(Titles omitted)</p>
<ul>
<li>Kizunaai</li>
</ul>
<p><img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/472706/ce3c1869-5481-d75a-289a-fa9a4b3336c1.png" alt="image.png"></p>
<p><a href="https://www.youtube.com/watch?v=C3Fx-9s9Y68">[Broadcast accidents will also be released! ] Thank you for 1 million people LIVE delivery! ! </a>
<a href="https://www.youtube.com/watch?v=xN9snff2yes">[Live Broadcast] We talked about anime together! </a></p>
<ul>
<li>
<p>Cat
<img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/472706/c70d172b-e952-2c81-5a9f-a8aab81647c8.png" alt="image.png">
<a href="https://www.youtube.com/watch?v=HFSKMWVLJGE">To become a Virtual Youtuber [Live008]</a></p>
</li>
<li>
<p>Hoshigai
<img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/472706/701696c3-697a-ff1d-7e59-cb9a9a89eb80.png" alt="image.png">
<a href="https://www.youtube.com/watch?v=zPRpfK7a2I0">[Official] &ldquo;Hoshigai Seisei no MUSIC SPACE&rdquo; #01 first half (broadcast April 5, 2020)</a>
<a href="https://www.youtube.com/watch?v=Ki8pVCeC4To">[Official] &ldquo;Hoshigai Seisei no MUSIC SPACE&rdquo; #01 latter half (broadcast April 5, 2020)</a>
<a href="https://www.youtube.com/watch?v=H19jbzSWtjI">[Official] &ldquo;Hoshigai Seisei no MUSIC SPACE&rdquo; #04 first half (broadcast on April 26, 2020)</a></p>
</li>
</ul>
<h2 id="ex2-image-generated-from-audio-animated-face">Ex.2 Image generated from audio (animated face)</h2>
<p>First, we will show the animated facial images generated for each epoch.</p>
<ul>
<li>
<p>epoch10
<img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/472706/a5c67379-c1d5-ae65-a8fd-dc6dbbe57b46.png" alt="epoch10-sample.png"></p>
</li>
<li>
<p>epoch50
<img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/472706/f316ff93-d7cf-aa1f-2fa1-c4b2f4750cad.png" alt="epoch50-sample.png"></p>
</li>
<li>
<p>epoch100
<img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/472706/acd75b60-455f-3170-1eaa-9bfa73a1843e.png" alt="epoch100-sample.png"></p>
</li>
<li>
<p>epoch200
<img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/472706/40cc5d61-d12a-4880-5459-71e556d002aa.png" alt="epoch200-sample.png"></p>
</li>
</ul>
<p>From each epoch number, a similar facial-like image was generated in epoch 10, but it can be seen that as the number increases, individuality is clearly reflected in the generated image.
Now, let&rsquo;s look at the following two types of comparison as to how close the generated image is to the real thing.</p>
<p>◆Comparison with the generated image (epoch200)</p>
<p>part1 part2 part2</p>
<p>←: generated image: real image: → ←: generated image: real image: →
<img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/472706/5e9728f5-6079-795a-e8ed-532b9a292bcf.png" alt="image.png"><img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/472706/a554d66a-b296-a09b-ee34-0c064fd55d12.png" alt="image.png"><img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/472706/ad067448-5ffe-4cc7-6edd-afa02172ace3.png" alt="image.png"><img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/472706/7554aec4-a83b-338d-8986-d9d69849f37d.png" alt="image.png">
<img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/472706/282a29ce-9474-90bf-7b47-6a093bae5cd6.png" alt="image.png">![image.png]](<a href="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/472706/45b4ed25-9619-391e-32cc-a989b67afb1e.png)!%5Bimage.png%5D(https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/472706/67466b64-1017-6a11-8959-e987dd16067f.png)!%5Bimage.png%5D(https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/472706/248b0108-28fe-8e3d-4b9b-77a5a2e95b7f.png">https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/472706/45b4ed25-9619-391e-32cc-a989b67afb1e.png)![image.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/472706/67466b64-1017-6a11-8959-e987dd16067f.png)![image.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/472706/248b0108-28fe-8e3d-4b9b-77a5a2e95b7f.png</a>)
<img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/472706/218aa35e-1926-ef85-54f3-bd666292c15b.png" alt="image.png"><img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/472706/64930fba-c2a7-fade-ed29-240e2dc4e380.png" alt="image.png"><img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/472706/63de680b-ed63-6217-51eb-564bec0468d8.png" alt="image.png"><img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/472706/48b32883-1e32-bd0b-a528-94b479021791.png" alt="image.png"></p>
<p>It was confirmed from part1 that each VTuber voice generated an image that could firmly learn individuality. However, I think it should be kept in mind that an image different from the real one may be generated from part2.</p>
<p>#Summary
This time, I explained wav2pix that generates face image from voice and ran a sample.
I also tried changing the data set to generate an anime face image.
As for the anime face image, we could generate something more shaped than we expected, so it may be a good idea to challenge high resolution in the future.
Also, I wonder if it is possible to generate illustrations from audio in the future if there are various face images.</p>
<h1 id="reference-site">Reference site</h1>
<p><a href="https://github.com/imatge-upc/wav2pix">SPEECH-CONDITIONED FACE GENERATION USING GENERATIVE ADVERSARIAL NETWORKS</a>
<a href="https://qiita.com/KSRG_Miyabi/items/2a3b5bdca464ec1154d7">I did machine learning to switch the voices of Kizuna AI and Nekomasu</a>
<a href="https://discuss.pytorch.org/t/unable-to-write-to-file-torch-18692-1954506624/9990">Unable to write to file &lt;/torch_18692_1954506624&gt;</a>
<a href="http://docs.docker.jp/engine/reference/run.html">Docker run reference</a></p>

    </article>
  </div>

  
  
  
  <script>
  window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
  ga('create', 'UA-169005401-1', 'auto');
  ga('send', 'pageview');
  </script>
  <script async src='https://www.google-analytics.com/analytics.js'></script>
  

  

  
<link rel="stylesheet" type="text/css" href="//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.0/cookieconsent.min.css" />
<script src="//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.0/cookieconsent.min.js"></script>
<script>
window.addEventListener("load", function(){
window.cookieconsent.initialise({
  "palette": {
    "popup": {
      "background": "#216942",
      "text": "#b2d192"
    },
    "button": {
      "background": "#afed71"
    }
  }
})});
</script>

</body>

</html>
