<!doctype html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  
  <meta name="generator" content="Hugo 0.72.0" />

  
  <meta name="description" content="Some description">
  

  
  <link rel="apple-touch-icon" sizes="180x180" href="https://memotut.com/apple-touch-icon.png">

  
  <link rel="icon" type="image/png" sizes="32x32" href="https://memotut.com/favicon-32x32.png">

  
  <link rel="icon" type="image/png" sizes="16x16" href="https://memotut.com/favicon-16x16.png">

  
  <link rel="manifest" href="https://memotut.com/site.webmanifest">

  
  <link rel="mask-icon" href="https://memotut.com/safari-pinned-tab.svg" color="#5bbad5">

  <meta name="msapplication-TileColor" content="#da532c">

  <meta name="theme-color" content="#ffffff">

  
  <link rel="stylesheet" href="https://memotut.com/css/bootstrap.min.css" />

  
  <title>[Python] I don&#39;t want to search for high para because it is IQ1 (how to use lightgbm_tuner) | Memo Tut</title>
  

  <style>
body {
  min-width: 300px;
}

.custom-navbar {
  margin-bottom: 1em;
  height: 60px;
}

.custom-navbar a {
  display: inline-block; 
  padding: 18px 0;
  margin-right: 1em; 
  font-weight: bold; 
}

.custom-navbar a:hover,
.custom-navbar a:focus {
  text-decoration: none; 
}

@media print {
  .custom-navbar {
    display: none;
  }
}

article {
  padding-bottom: 1em;
}

img {
  max-width: 100%;
}


body {
  background-color: #fff;
}



body {
  color: #212529;
}



a {
  color: #007bff;
}



a:hover,
a:focus {
  color: #0056b3;
}



.custom-navbar {
  background-color: #212529;
}



.custom-navbar a {
  color: rgba(255, 255, 255, 0.75);
}



.custom-navbar a:hover,
.custom-navbar a:focus {
  color: rgba(255, 255, 255, 1);
}



.container {
  max-width: 800px;
}



pre {
  display: block;
  padding: 9.5px;
  word-break: break-all;
  word-wrap: break-word;
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 4px;
}

pre code {
  padding: 0;
  font-size: inherit;
  color: inherit; 
  white-space: pre-wrap;
  background-color: transparent;
  border: none;
  border-radius: 0;
}

code {
  padding: 2px 4px;
  color: inherit; 
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 4px;
  font-size: .9em;
}



blockquote,
.blockquote {
  padding: 10px 20px;
  margin: 0 0 20px;
  font-size: 1em;
  border-left: 5px solid #6c757d;
}

</style>
</head>

<body>
  <nav class="custom-navbar">
  <div class="container">
    
    <a href="/">Posts</a>
    
    <a href="/tags/">Tags</a>
    
    <a href="/about/">About</a>
    
    <a href="/index.xml">RSS</a>
    
  </div>
</nav>
  
  <div class="container">
    <article>
      <h1>[Python] I don&rsquo;t want to search for high para because it is IQ1 (how to use lightgbm_tuner)</h1>
<p>
  <small class="text-secondary">
  
  
  Dec 1, 2019
  </small>
  

<small><code><a href="https://memotut.com/tags/python">Python</a></code></small>


<small><code><a href="https://memotut.com/tags/machine-learning"> Machine Learning</a></code></small>


<small><code><a href="https://memotut.com/tags/machinelearning"> MachineLearning</a></code></small>


<small><code><a href="https://memotut.com/tags/lightgbm">lightgbm</a></code></small>


<small><code><a href="https://memotut.com/tags/optuna">Optuna</a></code></small>

</p>
<pre><code>This article is posted on [IQ1's 2nd Advent Calendar 2019](https://adventar.org/calendars/4711).
</code></pre>
<h1 id="iq1-machine-learning">IQ1 machine learning</h1>
<p>For machine learning, data preprocessing and model hyper search are inevitable. However, I have an IQ of 1, so I don&rsquo;t want to do it if I can do preprocessing of data and high-para search.
If you put the preprocessing of data into this modern gradient boosting tree system algorithm (such as lightgbm), you can handle the missing values as they are and you do not have to preprocess the categorical variables, so it will be a world relatively friendly to IQ1. It was.
On the other hand, high para search is too difficult for IQ1. It&rsquo;s very difficult because we have to know what kind of high para is in each model and how much range to search.</p>
<h1 id="lightgbm_tuner">lightgbm_tuner</h1>
<p>Optuna has recently released a module called <code>optuna.integration.lightgbm_tuner</code> to automate the light parabola high para search. This module is also IQ1 friendly for many reasons.</p>
<ol>
<li>Fully automatic search for Hypara</li>
<li>You can search at high speed because you search by step-wise (search for each parameter).</li>
<li>Can be used immediately by rewriting the existing lightgbm code in several places (described later)</li>
</ol>
<h1 id="iq1-automatic-high-para-search">IQ1 automatic high para search</h1>
<p>Please install the latest version of optuna. I think it was 0.19.0. Enter with <code>pip install optuna --upgrade</code>.</p>
<h2 id="data-set-preparation">Data set preparation</h2>
<p>The data set used this time is Kaggle&rsquo;s House Prices.
(<a href="https://www.kaggle.com/c/house-prices-advanced-regression-techniques/overview">https://www.kaggle.com/c/house-prices-advanced-regression-techniques/overview</a>)
This data set is a data set that predicts the price of the house from land information and building information.</p>
<p>First, download the data using the API of kaggle</p>
<pre><code>$ kaggle competitions download -c house-prices-advanced-regression-techniques
</code></pre><p>If you unzip the zip properly, train.csv and test.csv will appear. Since I don&rsquo;t want to submit at this time, I use only train.csv.
Anything can be used with a jupyter notebook, so open python and read with pandas.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> pandas <span style="color:#f92672">as</span> pd

df<span style="color:#f92672">=</span>pd<span style="color:#f92672">.</span>read_csv(<span style="color:#e6db74">&#34;train.csv&#34;</span>)
<span style="color:#66d9ef">print</span>(df<span style="color:#f92672">.</span>shape)
</code></pre></div><pre><code class="language-.out" data-lang=".out">(1460, 81)
</code></pre><p>It turns out that there are 81 columns in 1460 records. Since the <code>SalePrice</code> that predicts the ID is included in this, the feature quantity that can be used is 79 dimensions.</p>
<p>For the time being, let&rsquo;s drop the columns that we don&rsquo;t need and keep the target variables separately.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">y<span style="color:#f92672">=</span>df<span style="color:#f92672">.</span>SalePrice
X<span style="color:#f92672">=</span>df<span style="color:#f92672">.</span>drop([<span style="color:#e6db74">&#34;Id&#34;</span>,<span style="color:#e6db74">&#34;SalePrice&#34;</span>],axis<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
</code></pre></div><p>Next, confirm the missing value. However, since it plunges into lightgbm, it is not processed. Just confirmation.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">X<span style="color:#f92672">.</span>loc[:,pd<span style="color:#f92672">.</span>isnull(X)<span style="color:#f92672">.</span>any(axis<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)]<span style="color:#f92672">.</span>columns
</code></pre></div><pre><code class="language-.out" data-lang=".out">Index(['LotFrontage','Alley','MasVnrType','MasVnrArea','BsmtQual',
       'BsmtCond','BsmtExposure','BsmtFinType1','BsmtFinType2',
       'Electrical','FireplaceQu','GarageType','GarageYrBlt',
       'GarageFinish','GarageQual','GarageCond','PoolQC','Fence',
       'MiscFeature'],
      dtype='object')
</code></pre><p>Next, label the column whose element is <code>String</code> as a categorical variable in lightgbm, and then perform label encoding and then set dtype to <code>category</code>. In a word, label encoding simply replaces the string with an integer value so that the elements are not duplicated.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">from</span> sklearn.preprocessing <span style="color:#f92672">import</span> LabelEncoder

<span style="color:#66d9ef">for</span> name <span style="color:#f92672">in</span> X<span style="color:#f92672">.</span>columns:
    <span style="color:#66d9ef">if</span> X[name]<span style="color:#f92672">.</span>dtype<span style="color:#f92672">==</span><span style="color:#e6db74">&#34;object&#34;</span>:
        <span style="color:#75715e">#NAN cannot be entered in Label Encoder, so set to &#34;NAN&#34;</span>
        X[name]<span style="color:#f92672">=</span>X[name]<span style="color:#f92672">.</span>fillna(<span style="color:#e6db74">&#34;NAN&#34;</span>)
        le <span style="color:#f92672">=</span> LabelEncoder()
        le<span style="color:#f92672">.</span>fit(X[name])
        encoded <span style="color:#f92672">=</span> le<span style="color:#f92672">.</span>transform(X[name])
        X[name] <span style="color:#f92672">=</span> pd<span style="color:#f92672">.</span>Series(encoded)<span style="color:#f92672">.</span>astype(<span style="color:#e6db74">&#39;category&#39;</span>)
</code></pre></div><h2 id="learning-with-ordinary-lightgbm">Learning with ordinary lightgbm</h2>
<p>Since pre-processing is finished, let lightgbm learn. Here is the code when I rushed into a normal lightgbm</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> lightgbm <span style="color:#f92672">as</span> lgb
<span style="color:#f92672">from</span> sklearn.model_selection <span style="color:#f92672">import</span> train_test_split

X_train,X_test,y_train,y_test<span style="color:#f92672">=</span>train_test_split(X,y,random_state<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)

train_dataset<span style="color:#f92672">=</span>lgb<span style="color:#f92672">.</span>Dataset(X_train,y_train)
valid_dataset<span style="color:#f92672">=</span>lgb<span style="color:#f92672">.</span>Dataset(X_test,y_test,reference<span style="color:#f92672">=</span>train_dataset)

<span style="color:#75715e"># For time measurement</span>
<span style="color:#75715e">#%%time</span>
params<span style="color:#f92672">=</span>{<span style="color:#e6db74">&#34;objective&#34;</span>:<span style="color:#e6db74">&#34;regression&#34;</span>,
                    <span style="color:#e6db74">&#34;learning_rate&#34;</span>:<span style="color:#ae81ff">0.05</span>}
model<span style="color:#f92672">=</span>lgb<span style="color:#f92672">.</span>train(params,
                train_set<span style="color:#f92672">=</span>train_dataset,
                valid_sets<span style="color:#f92672">=</span>[valid_dataset],
                num_boost_round<span style="color:#f92672">=</span><span style="color:#ae81ff">300</span>,
                early_stopping_rounds<span style="color:#f92672">=</span><span style="color:#ae81ff">50</span>)
</code></pre></div><pre><code class="language-.out" data-lang=".out">...(Omitted)...
Early stopping, best iteration is:
[113] valid_0's l2: 6.65263e+08
</code></pre><pre><code>CPU times: user 3.11 s, sys: 537 ms, total: 3.65 s
Wall time: 4.47 s
</code></pre><p>I was able to learn. The learning time was 4.47 seconds.
By the way, it was like this when plotting the prediction results. The horizontal axis is the predicted value and the vertical axis is the true value.
<img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/188403/be269c1c-b7e2-a0c3-b808-b382655321fa.png" alt="image.png"></p>
<h2 id="learning-with-lightgbm_tuner">Learning with lightgbm_tuner</h2>
<p>Rewrite the above code to do IQ1 high para search</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> lightgbm <span style="color:#f92672">as</span> lgb
<span style="color:#f92672">import</span> optuna.integration.lightgbm_tuner <span style="color:#f92672">as</span> lgb_tuner
<span style="color:#f92672">from</span> sklearn.model_selection <span style="color:#f92672">import</span> train_test_split

X_train,X_test,y_train,y_test<span style="color:#f92672">=</span>train_test_split(X,y,random_state<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)

train_dataset<span style="color:#f92672">=</span>lgb<span style="color:#f92672">.</span>Dataset(X_train,y_train)
valid_dataset<span style="color:#f92672">=</span>lgb<span style="color:#f92672">.</span>Dataset(X_test,y_test,reference<span style="color:#f92672">=</span>train_dataset)

params<span style="color:#f92672">=</span>{<span style="color:#e6db74">&#34;objective&#34;</span>:<span style="color:#e6db74">&#34;regression&#34;</span>,
                    <span style="color:#e6db74">&#34;learning_rate&#34;</span>:<span style="color:#ae81ff">0.05</span>,
                    <span style="color:#e6db74">&#34;metric&#34;</span>:<span style="color:#e6db74">&#34;l2&#34;</span>}
model<span style="color:#f92672">=</span>lgb_tuner<span style="color:#f92672">.</span>train(params,
                      train_set<span style="color:#f92672">=</span>train_dataset,
                      valid_sets<span style="color:#f92672">=</span>[valid_dataset],
                      num_boost_round<span style="color:#f92672">=</span><span style="color:#ae81ff">300</span>,
                      early_stopping_rounds<span style="color:#f92672">=</span><span style="color:#ae81ff">50</span>)
</code></pre></div><p>Do you know where it was rewritten? IQ1 is looking for mistakes.</p>
<p>The following three places have been rewritten.</p>
<ol>
<li>Add import statement</li>
<li>Change from <code>lgb.train</code> to <code>lgb_tuner.train</code></li>
<li>Add optimized value <code>metric</code> to <code>params</code></li>
</ol>
<p>By the way, the learning time is as follows. It was slower than I expected&hellip;</p>
<pre><code class="language-.out" data-lang=".out">CPU times: user 3min 24s, sys: 33.8 s, total: 3min 58s
Wall time: 3min 48s
</code></pre><p>The score of validation data is as follows.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">model<span style="color:#f92672">.</span>best_score
</code></pre></div><pre><code class="language-.out" data-lang=".out">defaultdict(dict, {'valid_0': {'l2': 521150494.1730755}})
</code></pre><p>It is like this when comparing the experimental results this time. The performance has improved properly after tuning.</p>
<table>
<thead>
<tr>
<th align="left"></th>
<th align="right">lightgbm</th>
<th align="right">lightgbm_tuner</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">Study Time</td>
<td align="right">4.47 s</td>
<td align="right">228 s</td>
</tr>
<tr>
<td align="left">valid Data accuracy (MSE)</td>
<td align="right">6.65263e+08</td>
<td align="right">5.21150e+08</td>
</tr>
</tbody>
</table>
<p>Looking at the plot, you can see that the one with the higher price (the one on the right) feels better
<img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/188403/2b89bc71-70bc-d4a7-705c-4539c2ae50c8.png" alt="image.png"></p>
<h1 id="end">end</h1>
<p>It seems that IQ1 can be used for machine learning if you use this! !!
By the way, when I submitted it with the model made with this, it was about 2000. (There were 4900 people who participated in sample_submission.csv or more, so there were many people with IQ1 or less)</p>
<h1 id="additionin-the-mood-when-i-tried-high-para-search-using-optuna-the-validation-score-went-up-but-the-submit-score-got-a-little-worse-when-submitting-i-relearn-with-learning_rate005-num_boosting_round1000early_stopping_rounds50">AdditionIn the mood, when I tried high para search using optuna, the validation score went up, but the submit score got a little worse. (When submitting, I relearn with <code>learning_rate=0.05, num_boosting_round=1000,early_stopping_rounds=50</code>)</h1>
<p>Did you overfit the validation data? After all, it is difficult for IQ1 to explore high para.</p>
<p>**If you have any practical tips for exploring high para, please comment! !! **</p>
<p>The tuning strategy this time is as follows.</p>
<ul>
<li>Arrange the parameters that are likely to be tuned, and set a wide range</li>
<li>Roughly set <code>learning_rate</code> to increase the number of trials</li>
</ul>
<p>(The lower the score, the better)</p>
<ul>
<li>Defopara&rsquo;s lightgbm submission score: 0.13852</li>
<li>Submission score for lightgbm_tuner: 0.13174</li>
<li>Submission score for optuna: 0.13401</li>
</ul>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> optuna

<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">objective</span>(trial):
    <span style="color:#e6db74">&#39;&#39;&#39;
</span><span style="color:#e6db74">    trial:set of hyperparameter
</span><span style="color:#e6db74">    &#39;&#39;&#39;</span>
    <span style="color:#75715e"># hypyer param</span>
    bagging_fraction <span style="color:#f92672">=</span> trial<span style="color:#f92672">.</span>suggest_uniform(<span style="color:#e6db74">&#34;bagging_fraction&#34;</span>,<span style="color:#ae81ff">0</span>,<span style="color:#ae81ff">1</span>)
    bagging_freq <span style="color:#f92672">=</span> trial<span style="color:#f92672">.</span>suggest_int(<span style="color:#e6db74">&#34;bagging_freq&#34;</span>,<span style="color:#ae81ff">0</span>,<span style="color:#ae81ff">10</span>)
    feature_fraction <span style="color:#f92672">=</span> trial<span style="color:#f92672">.</span>suggest_uniform(<span style="color:#e6db74">&#34;feature_fraction&#34;</span>,<span style="color:#ae81ff">0</span>,<span style="color:#ae81ff">1</span>)
    lambda_l1 <span style="color:#f92672">=</span> trial<span style="color:#f92672">.</span>suggest_uniform(<span style="color:#e6db74">&#34;lambda_l1&#34;</span>,<span style="color:#ae81ff">0</span>,<span style="color:#ae81ff">50</span>)
    lambda_l2 <span style="color:#f92672">=</span> trial<span style="color:#f92672">.</span>suggest_uniform(<span style="color:#e6db74">&#34;lambda_l2&#34;</span>,<span style="color:#ae81ff">0</span>,<span style="color:#ae81ff">50</span>)
    min_child_samples <span style="color:#f92672">=</span> trial<span style="color:#f92672">.</span>suggest_int(<span style="color:#e6db74">&#34;min_child_samples&#34;</span>,<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">50</span>)
    num_leaves <span style="color:#f92672">=</span> trial<span style="color:#f92672">.</span>suggest_int(<span style="color:#e6db74">&#34;num_leaves&#34;</span>,<span style="color:#ae81ff">2</span>,<span style="color:#ae81ff">50</span>)
    max_depth <span style="color:#f92672">=</span> trial<span style="color:#f92672">.</span>suggest_int(<span style="color:#e6db74">&#34;max_depth&#34;</span>,<span style="color:#ae81ff">0</span>,<span style="color:#ae81ff">8</span>)
    params<span style="color:#f92672">=</span>{<span style="color:#e6db74">&#34;learning_rate&#34;</span>:<span style="color:#ae81ff">0.5</span>,
                    <span style="color:#e6db74">&#34;objective&#34;</span>:<span style="color:#e6db74">&#34;regression&#34;</span>,
                    <span style="color:#e6db74">&#34;bagging_fraction&#34;</span>:bagging_fraction,
                    <span style="color:#e6db74">&#34;bagging_freq&#34;</span>:bagging_freq,
                    <span style="color:#e6db74">&#34;feature_fraction&#34;</span>:feature_fraction,
                    <span style="color:#e6db74">&#34;lambda_l1&#34;</span>:lambda_l1,
                    <span style="color:#e6db74">&#34;lambda_l2&#34;</span>: lambda_l2,
                    <span style="color:#e6db74">&#34;min_child_samples&#34;</span>:min_child_samples,
                    <span style="color:#e6db74">&#34;num_leaves&#34;</span>:num_leaves,
                    <span style="color:#e6db74">&#34;max_depth&#34;</span>:max_depth}

    model_opt <span style="color:#f92672">=</span> lgb<span style="color:#f92672">.</span>train(params,train_set<span style="color:#f92672">=</span>train_dataset,valid_sets<span style="color:#f92672">=</span>[valid_dataset],
                          (num_boost_round<span style="color:#f92672">=</span><span style="color:#ae81ff">70</span>,early_stopping_rounds<span style="color:#f92672">=</span><span style="color:#ae81ff">10</span>)
 
    <span style="color:#66d9ef">return</span> model_opt<span style="color:#f92672">.</span>best_score[<span style="color:#e6db74">&#34;valid_0&#34;</span>][<span style="color:#e6db74">&#34;l2&#34;</span>]

study <span style="color:#f92672">=</span> optuna<span style="color:#f92672">.</span>create_study()
study<span style="color:#f92672">.</span>optimize(objective, n_trials<span style="color:#f92672">=</span><span style="color:#ae81ff">500</span>)
</code></pre></div><pre><code>...(Omitted)...
[I 2019-12-01 15:02:35,075] Finished trial#499 resulted in value: 537618254.528029. Current best value is 461466711.4731979 with parameters: ('bagging_fraction': 0.9973929186258068,'bagging_freq': 2,'feature_fraction': 0.9469601028256658, 'lambda_l1': 10.1589501379876,'lambda_l2': 0.0306013767707684,'min_child_samples': 2,'num_leaves': 35,'max_depth': 2}.
</code></pre><p>validation score was 4.61467e+08</p>

    </article>
  </div>

  
  
  
  <script>
  window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
  ga('create', 'UA-169005401-1', 'auto');
  ga('send', 'pageview');
  </script>
  <script async src='https://www.google-analytics.com/analytics.js'></script>
  

  

  
<link rel="stylesheet" type="text/css" href="//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.0/cookieconsent.min.css" />
<script src="//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.0/cookieconsent.min.js"></script>
<script>
window.addEventListener("load", function(){
window.cookieconsent.initialise({
  "palette": {
    "popup": {
      "background": "#216942",
      "text": "#b2d192"
    },
    "button": {
      "background": "#afed71"
    }
  }
})});
</script>

</body>

</html>
