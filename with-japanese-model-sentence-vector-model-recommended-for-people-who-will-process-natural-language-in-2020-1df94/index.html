<!doctype html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  
  <meta name="generator" content="Hugo 0.72.0" />

  
  <meta name="description" content="Some description">
  

  
  <link rel="apple-touch-icon" sizes="180x180" href="https://memotut.com/apple-touch-icon.png">

  
  <link rel="icon" type="image/png" sizes="32x32" href="https://memotut.com/favicon-32x32.png">

  
  <link rel="icon" type="image/png" sizes="16x16" href="https://memotut.com/favicon-16x16.png">

  
  <link rel="manifest" href="https://memotut.com/site.webmanifest">

  
  <link rel="mask-icon" href="https://memotut.com/safari-pinned-tab.svg" color="#5bbad5">

  <meta name="msapplication-TileColor" content="#da532c">

  <meta name="theme-color" content="#ffffff">

  
  <link rel="stylesheet" href="https://memotut.com/css/bootstrap.min.css" />

  
  <title>[With Japanese model] Sentence vector model recommended for people who will process natural language in 2020 | Memo Tut</title>
  

  <style>
body {
  min-width: 300px;
}

.custom-navbar {
  margin-bottom: 1em;
  height: 60px;
}

.custom-navbar a {
  display: inline-block; 
  padding: 18px 0;
  margin-right: 1em; 
  font-weight: bold; 
}

.custom-navbar a:hover,
.custom-navbar a:focus {
  text-decoration: none; 
}

@media print {
  .custom-navbar {
    display: none;
  }
}

article {
  padding-bottom: 1em;
}

img {
  max-width: 100%;
}


body {
  background-color: #fff;
}



body {
  color: #212529;
}



a {
  color: #007bff;
}



a:hover,
a:focus {
  color: #0056b3;
}



.custom-navbar {
  background-color: #212529;
}



.custom-navbar a {
  color: rgba(255, 255, 255, 0.75);
}



.custom-navbar a:hover,
.custom-navbar a:focus {
  color: rgba(255, 255, 255, 1);
}



.container {
  max-width: 800px;
}



pre {
  display: block;
  padding: 9.5px;
  word-break: break-all;
  word-wrap: break-word;
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 4px;
}

pre code {
  padding: 0;
  font-size: inherit;
  color: inherit; 
  white-space: pre-wrap;
  background-color: transparent;
  border: none;
  border-radius: 0;
}

code {
  padding: 2px 4px;
  color: inherit; 
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 4px;
  font-size: .9em;
}



blockquote,
.blockquote {
  padding: 10px 20px;
  margin: 0 0 20px;
  font-size: 1em;
  border-left: 5px solid #6c757d;
}

</style>
</head>

<body>
  <nav class="custom-navbar">
  <div class="container">
    
    <a href="/">Posts</a>
    
    <a href="/tags/">Tags</a>
    
    <a href="/about/">About</a>
    
    <a href="/index.xml">RSS</a>
    
  </div>
</nav>
  
  <div class="container">
    <article>
      <h1>[With Japanese model] Sentence vector model recommended for people who will process natural language in 2020</h1>
<p>
  <small class="text-secondary">
  
  
  Mar 9, 2020
  </small>
  

<small><code><a href="https://memotut.com/tags/python">Python</a></code></small>


<small><code><a href="https://memotut.com/tags/natural-language-processing"> natural language processing</a></code></small>


<small><code><a href="https://memotut.com/tags/machine-learning"> machine learning</a></code></small>


<small><code><a href="https://memotut.com/tags/deeplearning"> DeepLearning</a></code></small>


<small><code><a href="https://memotut.com/tags/ai"> AI</a></code></small>

</p>
<pre><code>## Summary
</code></pre>
<ul>
<li>Create and publish a Japanese model of Sentence-BERT (<a href="https://arxiv.org/abs/1908.10084">Papers</a>,<a href="https://github.com/UKPLab/sentence-transformers">Implementation</a>). did.</li>
<li>By using this Japanese model, anyone can easily create high-quality sentence vectors. Please use it.</li>
<li>Sentence-BERT uses pre-trained <a href="https://arxiv.org/abs/1810.04805">BERT</a>modeland<a href="https://qiita.com/hkambe/items/f6405a21e5fcea96fcc1">SiameseNetwork</a> , Is a method of creating a highly accurate sentence vector.</li>
<li>Cosine similarity using accuracy evaluation of English version (<a href="http://ixa2.si.ehu.es/stswiki/index.php/STSbenchmark">STSbenchmark</a>andrankcorrelationcoefficientofspearmanofcorrectlabel.1Inthecaseof(closerisbetter),themethodusingtheaverageofsimplewordvectors(GloVe)is0.58,andthemodelcorrespondingtothescalecreatedthistimeisaround0.85(<a href="https://arxiv.org/abs/1908.10084">Papers</a>SeeTable2).</li>
<li>The accuracy is 0.17 when using the CLS vector of the bare BERT and 0.46 when using the average of the BERT embeddings, which is worse than the result of the average of the word vector (0.58) (<a href="https:SeeTables1and2of//arxiv.org/abs/1908.10084">Paper</a>). As stated in the original BERT paper, it is not appropriate to use these as sentence vectors.</li>
</ul>
<p>image
<img width="700" alt="UMAP.png" src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/26062/ef61887a-f5a9-5410-c322-dda35c5c8ebc.png"></p>
<h2 id="introduction">Introduction</h2>
<p>It is a story of making a vector with high quality (in Japanese), that is, the closer the meaning of a sentence is to the vector, the closer it is to the context, not the character.</p>
<p>I think that there are a certain number of people who are creating sentence vectors for the purpose of searching sentences that have similar meanings in hobbies and practice.</p>
<p>However, even if the average of the word vectors is not bad, I wonder if I am wondering if there is a bad scene and I am wandering into a maze called heuristics (trying a specific word or part of speech as a stopword, I tried adding a mystery weight.
In addition, the average of word vectors may not be able to handle polysemous words, or it may be a similar sentence search that focuses on the word &ldquo;not there&rdquo; because the context is not considered.
On the other hand, learning a sentence vector model using an existing deep neural network such as Universal Sentence Encoder is too computationally expensive, and it may be difficult to train by yourself.</p>
<p>For comrades who have such problems, high quality Japanese sentence vectors using the method proposed in <a href="https://arxiv.org/abs/1908.10084">Paper Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks</a> Make a model and publish it.</p>
<h2 id="prerequisite-knowledge">Prerequisite knowledge</h2>
<ul>
<li>Basic knowledge of modern natural language processing
-What is BERT
-What is a sentence vector (distributed representation of a sentence)</li>
<li>How to use Google Colaboratory</li>
<li>Experience using sentence vectors</li>
</ul>
<h2 id="example-of-using-japanese-sentence-vector-model">Example of using Japanese sentence vector model</h2>
<p>Before the technical explanation of Sentence-BERT, we will explain how to use it. it&rsquo;s simple.
The example does two things: You can try these examples out at Google Colaboratory.</p>
<ol>
<li>Search for a sentence whose meaning is close to the given query sentence.</li>
<li>Visualize the latent semantic space of the title sentence with UMAP.</li>
</ol>
<ul>
<li>Colaboratory Notebook: <a href="https://colab.research.google.com/github/sonoisa/sentence-transformers/blob/master/sentence_transformers_ja.ipynb">https://colab.research.google.com/github/sonoisa/sentence-transformers/blob/master/sentence_transformers_ja.ipynb</a>
*A GPU is required. Open it in Chrome to use TensorBoard.</li>
</ul>
<h3 id="setup">setup</h3>
<p>Install the relevant library and download the Japanese model.
No detailed explanation is necessary.
For the convenience of trialing with Colaboratory, instead of installing Japanese sentence-transformers this time, I moved to the directory with the source code, but in production use, like In[2] commented out It&rsquo;s better to install with setup.py.</p>
<pre><code class="language-bash:In[1]" data-lang="bash:In[1]">!git clone https://github.com/sonoisa/sentence-transformers
!cd sentence-transformers; pip install -r requirements.txt
</code></pre><pre><code class="language-bash:In[2]" data-lang="bash:In[2]">#!cd sentence-transformers; python setup.py install
</code></pre><pre><code class="language-bash:In[3]" data-lang="bash:In[3]">!wget -O sonobe-datasets-sentence-transformers-model.tar &quot;https://www.floydhub.com/api/v1/resources/JLTtbaaK5dprnxoJtUbBbi?content=true&amp;download=true&amp;rename=sonobe-datasets-sentence-transformers-model- 2&quot;
!tar -xvf sonobe-datasets-sentence-transformers-model.tar
</code></pre><p>By decompressing the tar, the directory training_bert_japanese containing the model is created.</p>
<pre><code class="language-python:In[4]" data-lang="python:In[4]">%cd sentence-transformers
</code></pre><h3 id="load-japanese-model">Load Japanese model</h3>
<p>The model is loaded by creating a SentenceTransformer instance with the path /content/training_bert_japanese to the directory where the Japanese model is placed as an argument.</p>
<pre><code class="language-python:In[5]" data-lang="python:In[5]">%tensorflow_version 2.x
from sentence_transformers import SentenceTransformer
import numpy as np

model_path = &quot;/content/training_bert_japanese&quot;
model = SentenceTransformer(model_path, show_progress_bar=False)
</code></pre><h3 id="statement-vector-calculation">Statement vector calculation</h3>
<p>Compute the sentence vector. Just call model.encode (list of statements).
In this example, as the sentence, I will use the title of &ldquo;Irasutoya&rdquo; (a slightly modified version) published in <a href="https://qiita.com/sonoisa/items/775ac4c7871ced6ed4c3">another article</a>. ..
(Actually, it&rsquo;s better that a sentence that is several times longer is more effective, but I could not prepare it immediately, so I will use the title for the time being. I will add an explanation when I can prepare more suitable sentences.)</p>
<pre><code class="language-python:In[6]" data-lang="python:In[6]"># Source: https://qiita.com/sonoisa/items/775ac4c7871ced6ed4c3 Excerpt from the image title of &quot;Irasutoya&quot; (I deleted the words &quot;illustration&quot;, &quot;mark&quot; and &quot;character&quot;)
sentences = [&quot;Male office worker bowing&quot;, &quot;Laughing bag&quot;, &quot;Technical evangelist (female)&quot;, &quot;Fighting AI&quot;, &quot;Laughing man (5 levels)&quot;,
...
&quot;Men who are staring at money&quot;, &quot;Thank you&quot;, &quot;Retirement (female)&quot;, &quot;Technical evangelist (male)&quot;, &quot;Standing ovation&quot;]
</code></pre><pre><code class="language-python:In[7]" data-lang="python:In[7]">sentence_vectors = model.encode(sentences)
</code></pre><p>This is the only sentence vector calculation.</p>
<!--
### Cluster sentences with similar meanings

```python:In[8]
from sklearn.cluster import KMeans

num_clusters = 8
clustering_model = KMeans(n_clusters=num_clusters)
clustering_model.fit(sentence_vectors)
cluster_assignment = clustering_model.labels_

clustered_sentences = [[] for i in range(num_clusters)]
for sentence_id, cluster_id in enumerate(cluster_assignment):
    clustered_sentences[cluster_id].append(sentences[sentence_id])

for i, cluster in enumerate(clustered_sentences):
    print("Cluster ", i+1)
    print(cluster)
    print("")
```

```python:Out[8]
Cluster 1
['Technical evangelist (female)','Various Man' marks','Hina dolls'King and Sannin Uedo'','Retirement (male)','Athletic meet'Sports race/White group'','Technical books',' "Kaya no Kuruhiguri", "Technical book (separated)", "Continued" of various movies, "Halysen", "Pico Pico hammer", "Singularity", "English alphabet", "Technical evangelist (male)"]

Cluster 2
['Laughing men (5 levels)','Laughing women (5 levels)','Drunken with various facial expressions (males)','Clapping boys','Daybreak smile (males)','Mirror Who sees (smiling man)']

Cluster 3
['Fighting AI','AI family','AI with weapons','Human friend with AI','AI with heart','ON AIR lamp','AI for image recognition',' Artificial intelligence/AI']

Cluster 4
['Woman with bitter laughter','Woman who can resist laughter','Dance'dancing woman','Woman with skipping rope','Girl clapping','People giving peace sign ( "Women)", "Recipients (Women)", "Girls riding a unicycle", "Running cats (smiles)", "Excursion "Bento/Boys/Girls", "Girls happy to receive gifts", "Drunken with various expressions (female)", "Smile after dawn (female)", "Old lady doing banzai", "Cooking "female", "Retirement (female)"]

Cluster 5['Laughter bag','Happy laughter','Happiness laugh (female)','Applause person','Happiness laughter','Love laughter','Happiness laughter )','A man with a bitter smile','Thanks for trouble','A good smile (male)','A good smile','Facial expression','A person clapping (stickman)','Laughter "Holder (male)", "A man staring at money and grinning," "Thank you", "Standing ovation"]

Cluster 6
['Growing Artificial Intelligence','People fighting with Artificial Intelligence','Artificial Intelligence','Man who sends mail with artificial intelligence (male)','Artificial intelligence to compose','Mail with artificial intelligence' "People (women)", "Go players who fight against artificial intelligence", "Artificial intelligence to search", "Artificial intelligence to work", "Shogi players to fight artificial intelligence", "Artificial intelligence that robs work", " "Artificial intelligence to write sentences", "Artificial intelligence to draw pictures", "People who make friends with artificial intelligence", "Persons who leave work to artificial intelligence", "Smart speakers", "Learn artificial intelligence"]

Cluster 7
["Male office worker bowing", "Doctor bowing (female)", "Pharmacist bowing", "Dog bowing", "Doctor bowing" , "Nurse bowing (male)", "Bear bowing", "Cat bowing", "Rabbit bowing", "Female office worker bowing" ']

Cluster 8
['Manzai','Comte','Dance'Dancing Man'','Imitation Performer','Comedy'Manzai'','Boy of Entertainer (Future Dream)']
```
- ->


### Find sentences with similar meanings

Using the calculated sentence vector, try searching for a sentence with a similar meaning (title of Irasu).
Look for a sentence vector with a small cosine distance.


```python:In[9]
import scipy.spatial

queries = ['Runaway AI','Runaway Artificial Intelligence','Thank you Irasu Toya-san','Continue']
query_embeddings = model.encode(queries)

closest_n = 5
for query, query_embedding in zip(queries, query_embeddings):
    distances = scipy.spatial.distance.cdist([query_embedding], sentence_vectors, metric="cosine")[0]

    results = zip(range(len(distances)), distances)
    results = sorted(results, key=lambda x: x[1])

    print("\n\n======================\n\n")
    print("Query:", query)
    print("\nTop 5 most similar sentences in corpus:")

    for idx, distance in results[0:closest_n]:
        print(sentences[idx].strip(), "(Score: %.4f)" %(distance / 2))
```

Below is the output result.
Runaway ≒ battle, it is considered a weapon, so I think it is a natural result.
It can be understood that having a heart can make a runaway.

```bash:Out[9]
======================
Query: A runaway AI

Top 5 most similar sentences in corpus:
Fighting AI (Score: 0.1521)
AI with a heart (Score: 0.1666)
AI with weapons (Score: 0.1994)
Artificial Intelligence/AI (Score: 0.2130)
AI for image recognition (Score: 0.2306)
```

AI has been reworded as artificial intelligence. The results have changed.
It seems that those with similar notations tend to rank higher.

```bash:Out[9]
======================
Query: Runaway artificial intelligence

Top 5 most similar sentences in corpus:
Artificial intelligence that steals work (Score: 0.1210)
People who fight with artificial intelligence (Score: 0.1389)
Artificial Intelligence (Score: 0.1411)
Growing artificial intelligence (Score: 0.1482)
Artificial Intelligence/AI (Score: 0.1629)
```

Thanks = I know you're thankful.

```bash:Out[9]
======================
Query: Thanks to Mr. Iriya

Top 5 most similar sentences in corpus:
People who say "Thank you" (Score: 0.1381)
Happy smile (Score: 0.1693)
Good Smile (Score: 0.1715)
Good Smile (Score: 0.1743)
A person who can resist laughter (male) (Score: 0.1789)
```

You can search for a sentence that is close to a single word instead of a sentence.

```bash:Out[9]
======================
Query: Continue

Top 5 most similar sentences in corpus:
"Continued" of various movies (Score: 0.1878)
Singularity (Score: 0.2703)
Laughter (Score: 0.2811)
Thank you (Score: 0.2881)
Hallisen (Score: 0.2931)
```


### Visualize latent semantic space with TensorBoard

Execute the following code and use Colaboratory's TensorBoard extension to map the sentence vector space to a low-dimensional space and visualize it.
*It may not be displayed unless it is Chrome.

```python:In[10]
%load_ext tensorboard
import os
logs_base_dir = "runs"
os.makedirs(logs_base_dir, exist_ok=True)
```

```python:In[11]
import torch
from torch.utils.tensorboard import SummaryWriter

import tensorflow as tf
import tensorboard as tb
tf.io.gfile = tb.compat.tensorflow_stub.io.gfile

summary_writer = SummaryWriter()
summary_writer.add_embedding(mat=np.array(sentence_vectors), metadata=sentences)
```

```python:In[12]
%tensorboard --logdir {logs_base_dir}
```

- When TensorBoard starts, select PROJECTOR from the menu on the upper right.
- Visualization algorithm (lower left pane of TensorBoard) should be set to 2D for UMAP and neighbors (right pane of TensorBoard) should be set to 10.

Hopefully the space of sentence vectors will be visualized as follows.
In the upper left is the "artificial intelligence" system, and immediately to the right you can see the "AI" system.
In the lower left you can see the "bowing" system, in the center a few other miscellaneous objects, in the right you can see the "female" system, and at the bottom you can see the "male" system.

<img width="700" alt="TensorBoard.png" src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/26062/33ecd130-5461-237b-2caf-d13086555aad.png">






## Sentence-BERT overview

(The paper is very easy to read, so I don't think there is a need for commentary.)

- Paper: [Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks](https://arxiv.org/abs/1908.10084)
- Original implementation: [UKPLab/sentence-transformers](https://github.com/UKPLab/sentence-transformers)(*NotethattheJapaneseversiondoesnotworkwiththeoriginalimplementation)

In short, the sentence vector created by embedding tokens in BERT and performing mean pooling is used for distance learning (finetune) using Siamese Network. Sounds simple.
If you look at Fig. 1 and 2 of the paper cited below, you can understand it.

<img width="700" alt="SBERT_architecture.png" src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/26062/4dc3f534-5ab5-89f3-fb6a-02db2d07ddc9.png">

[Papers](https://arxiv.org/abs/1908.10084) have been experimented with other methods, but the Japanese model construction uses this network structure, which has the highest performance.

As shown in Table 2 quoted below, the cosine similarity and correct answer label using the English version accuracy evaluation ([STSbenchmark](http://ixa2.si.ehu.es/stswiki/index.php/STSbenchmark))Inspearman'srankcorrelationcoefficient,thecloserto1,thebetter)is0.58forthemethodusingtheaverageofnaivewordvectors(GloVe), and around 0.85 for the model equivalent to the scale created this time.

<img width="400" alt="Performance.png" src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/26062/19019fba-38e3-1b6c-f62f-1143ac2871dc.png">

According to Table 1 of [Papers](https://arxiv.org/abs/1908.10084),theaccuracyis0.17whenusingtheCLSvectorofthebareBERTand0.46whenusingtheaverageofembeddingtheBERT.Itgivesworseresultsthanthevectoraverageresult(0.58).AsyoucanseeintheoriginalBERTpaper,itturnsoutthatitisnotappropriatetousetheseassentencevectors(whichisworsethanIimagined).

[Huggingface/transformers](https://github.com/huggingface/transformers)(Modelis[TohokuUniversityInui/SuzukiLab.]](https://github.com/cl-tohoku/bert-japanese))JapaneseversionBERTmodelisused.AndregardingthelearningmethodoftheJapanesemodelthistime(datasetusedforlearning,accuracyevaluationmethod), I'm sorry, but due to various reasons, it is a secret. As a result, it seems that it is possible to create a sentence vector with context added, which is comparable to the English version.



## Download Japanese source code and model

The Japanese version source code and model used in the above example can be downloaded from the following.

* [Source code of Japanese version of sentence-transformers](https://github.com/sonoisa/sentence-transformers)
* [Japanese model](https://www.floydhub.com/api/v1/resources/JLTtbaaK5dprnxoJtUbBbi?content=true&download=true&rename=sonobe-datasets-sentence-transformers-model-2)(442.8MB)
The tar file contains the following files:
  * training_bert_japanese/0_BERTJapanese/added_tokens.json
  * training_bert_japanese/0_BERTJapanese/config.json
  * training_bert_japanese/0_BERTJapanese/pytorch_model.bin
  * training_bert_japanese/0_BERTJapanese/sentence_bert_config.json
  * training_bert_japanese/0_BERTJapanese/special_tokens_map.json
  * training_bert_japanese/0_BERTJapanese/tokenizer_config.json
  * training_bert_japanese/0_BERTJapanese/vocab.txt
  * training_bert_japanese/1_Pooling/config.json
  * training_bert_japanese/config.json
  * training_bert_japanese/modules.json


## Disclaimer

The author pays close attention to the contents, functions, etc. of this article, but does not guarantee that the contents are accurate or safe, and does not guarantee anything. We are not responsible.
Even if any inconvenience or damage occurs to the user by using the contents of this article, the author and the organization to which the author belongs (Nittetsu Solutions Co., Ltd. (NSSOL, former Nippon Steel & Sumikin Solutions Co., Ltd.)) We do not take any responsibility.


## Summary

I made the Japanese version code and model of Sentence-BERT.
Now anyone can easily create high-quality sentence vectors.
Please use it.
    </article>
  </div>

  
  
  
  <script>
  window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
  ga('create', 'UA-169005401-1', 'auto');
  ga('send', 'pageview');
  </script>
  <script async src='https://www.google-analytics.com/analytics.js'></script>
  

  

  
<link rel="stylesheet" type="text/css" href="//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.0/cookieconsent.min.css" />
<script src="//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.0/cookieconsent.min.js"></script>
<script>
window.addEventListener("load", function(){
window.cookieconsent.initialise({
  "palette": {
    "popup": {
      "background": "#216942",
      "text": "#b2d192"
    },
    "button": {
      "background": "#afed71"
    }
  }
})});
</script>

</body>

</html>
