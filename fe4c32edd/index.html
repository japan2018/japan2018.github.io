<!doctype html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  
  <meta name="generator" content="Hugo 0.72.0" />

  
  <meta name="description" content="Some description">
  

  
  <link rel="apple-touch-icon" sizes="180x180" href="https://memotut.com/apple-touch-icon.png">

  
  <link rel="icon" type="image/png" sizes="32x32" href="https://memotut.com/favicon-32x32.png">

  
  <link rel="icon" type="image/png" sizes="16x16" href="https://memotut.com/favicon-16x16.png">

  
  <link rel="manifest" href="https://memotut.com/site.webmanifest">

  
  <link rel="mask-icon" href="https://memotut.com/safari-pinned-tab.svg" color="#5bbad5">

  <meta name="msapplication-TileColor" content="#da532c">

  <meta name="theme-color" content="#ffffff">

  
  <link rel="stylesheet" href="https://memotut.com/css/bootstrap.min.css" />

  
  <title>[Python] Machine learning beginners try out Naive Bayes (1)-theory | Memo Tut</title>
  

  <style>
body {
  min-width: 300px;
}

.custom-navbar {
  margin-bottom: 1em;
  height: 60px;
}

.custom-navbar a {
  display: inline-block; 
  padding: 18px 0;
  margin-right: 1em; 
  font-weight: bold; 
}

.custom-navbar a:hover,
.custom-navbar a:focus {
  text-decoration: none; 
}

@media print {
  .custom-navbar {
    display: none;
  }
}

article {
  padding-bottom: 1em;
}

img {
  max-width: 100%;
}


body {
  background-color: #fff;
}



body {
  color: #212529;
}



a {
  color: #007bff;
}



a:hover,
a:focus {
  color: #0056b3;
}



.custom-navbar {
  background-color: #212529;
}



.custom-navbar a {
  color: rgba(255, 255, 255, 0.75);
}



.custom-navbar a:hover,
.custom-navbar a:focus {
  color: rgba(255, 255, 255, 1);
}



.container {
  max-width: 800px;
}



pre {
  display: block;
  padding: 9.5px;
  word-break: break-all;
  word-wrap: break-word;
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 4px;
}

pre code {
  padding: 0;
  font-size: inherit;
  color: inherit; 
  white-space: pre-wrap;
  background-color: transparent;
  border: none;
  border-radius: 0;
}

code {
  padding: 2px 4px;
  color: inherit; 
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 4px;
  font-size: .9em;
}



blockquote,
.blockquote {
  padding: 10px 20px;
  margin: 0 0 20px;
  font-size: 1em;
  border-left: 5px solid #6c757d;
}

</style>
</head>

<body>
  <nav class="custom-navbar">
  <div class="container">
    
    <a href="/">Posts</a>
    
    <a href="/tags/">Tags</a>
    
    <a href="/about/">About</a>
    
    <a href="/index.xml">RSS</a>
    
  </div>
</nav>
  
  <div class="container">
    <article>
      <h1>[Python] Machine learning beginners try out Naive Bayes (1)-theory</h1>
<p>
  <small class="text-secondary">
  
  
  Apr 20, 2016
  </small>
  

<small><code><a href="https://memotut.com/tags/python">Python</a></code></small>


<small><code><a href="https://memotut.com/tags/machine-learning">Machine Learning</a></code></small>

</p>
<pre><code>## This page
</code></pre>
<p>Since I&rsquo;m a machine learning beginner and a Python beginner, I think that it will be a bit like reading the pages that were basically referenced.</p>
<h2 id="what-is-conditional-probability-bayes-theorem-">What is conditional probability (Bayes&rsquo; theorem)? ?</h2>
<p>It&rsquo;s a bit of math. Have you heard the term <code>conditional probability</code>?
I heard a little about it in the class before, but I didn&rsquo;t remember much.</p>
<pre><code class="language-math" data-lang="math">P(B|A) = \frac{P(A \cap B)}{P(A)} = \frac{P(B)P(A|B)}{P(A)}
</code></pre><p><code>P(B|A)</code> is the probability of event B occurring when event A occurs.</p>
<p>A famous problem is the story of children.</p>
<blockquote>
<p>A couple has two children. It turns out that at least one of the two is a boy. At this time, find the probability that both are boys. However, the probability of having a boy and the probability of having a girl are both 1/2.</p>
</blockquote>
<p>The idea of this is</p>
<ul>
<li>Event A: <code>It turns out that at least one of the two is a boy. </code></li>
<li>Event B: <code>Both are boys</code></li>
</ul>
<p>If you think that, you can think as follows.</p>
<h3 id="event-a">Event A</h3>
<p>Since at least one is said to be a boy, all the patterns in the case of having two children are listed as <code>man&amp;man</code>, <code>man&amp;woman</code>, <code>woman&amp;woman</code>, <code>woman&amp;man</code>. Pattern. (Including differences between siblings)
Of these, the probability of event A is <code>3/4</code>, which omits <code>woman&amp;woman</code>, both of which are <code>woman</code>.</p>
<h3 id="event-b">Event B</h3>
<p>Needless to say, this is <code>1/4</code>.</p>
<p>So the probability of P(B|A) is</p>
<pre><code class="language-math" data-lang="math">P(B|A) = \frac{\frac{1}{4}}{\frac{3}{4}} = \frac{1}{3}
</code></pre><p>Will be.</p>
<p>Reference: <a href="http://mathtrain.jp/bayes">The beautiful story of high school mathematics</a></p>
<h2 id="challenge-naive-bayes">Challenge Naive Bayes</h2>
<p>Now for the main subject. Naive Bayes is also called Naive Bayes.
It seems that it is used for the purpose of categorizing sentences by slightly modifying the Bayesian theorem.</p>
<pre><code class="language-math" data-lang="math">P(cat|doc) = \frac{P(cat)P(doc|cat)}{P(doc)}\propto P(cat)P(doc|cat)
</code></pre><p>This is the basic form.</p>
<p>I would like to explain each element.</p>
<p><code>P(cat|doc)</code> is the probability that the given category (cat) is the given category (cat).
To put it a bit more, it seems to be the probability that it is a category such as &ldquo;IT&rdquo; when the sentence contains words such as &ldquo;Apple&rdquo;, &ldquo;iPhone&rdquo;, and &ldquo;MacBook Pro&rdquo;.</p>
<pre><code class="language-math" data-lang="math">\frac{P(cat)P(doc|cat)}{P(doc)}
</code></pre><p>Since <code>P(doc)</code> is common to all, we will ignore it and calculate it.</p>
<pre><code class="language-math" data-lang="math">P(cat)P(doc|cat)
</code></pre><p><code>P(cat)</code> is easy.</p>
<p>If the total number of sentences is 100 and the number of sentences in the IT category is 50</p>
<pre><code class="language-math" data-lang="math">P(IT) = \frac{50}{100} = \frac{1}{2}
</code></pre><p>You can calculate like that.</p>
<p><code>P(doc|cat)</code> may be a bit confusing. Probability of being a sentence (doc) when a category (cat) is given&hellip;? And here is the core of Naive Bayes.</p>
<p>Naive Bayes does not issue <code>P(doc|cat)</code> exactly, but assumes independence between word occurrences.</p>
<p><code>Independence between the appearance of words</code> may be a word that often comes out when <code>machine</code> and <code>learning</code> are in the field of machine learning, but the words <code>machine</code> and <code>learning</code> are independent. It will be calculated by simplifying the assumption that it appears.</p>
<pre><code class="language-math" data-lang="math">P(doc|cat) = P(word_1 \wedge .... \wedge word_k|cat) = \prod_{i=0}^k P(word_k|cat)
</code></pre><p>And calculate.</p>
<pre><code class="language-math" data-lang="math">\prod
</code></pre><p>Is the (x) version of <code>Î£</code>. Next, let&rsquo;s calculate <code>P(word|cat)</code>. This is the probability that a word (word) will appear when a category (cat) is given, so calculate as follows. (The formula is on the reference page, so please refer to it..)</p>
<pre><code class="language-math" data-lang="math">P(word_k|cat) = \frac{Number of occurrences of word (word) in category (cat)}{Total number of words that appear in category (cat)}
</code></pre><p>All you have to do now is calculate them! !</p>
<ul>
<li>Feed learning data</li>
<li>Target sentence scoring</li>
<li>Returns the category with the highest number.</li>
</ul>
<h2 id="technique">Technique</h2>
<p>There are two keywords.</p>
<ul>
<li>Laplace smoothing</li>
<li>Logarithm</li>
</ul>
<h4 id="laplace-smoothing"><Laplace smoothing></h4>
<p>I&rsquo;m just doing <code>+1</code>. There seems to be a <code>zero frequency problem</code>. As for what this is, I think that P(doc|cat) was expressed by the total power of the occurrence probability of words, but the product means that if there is one <code>0</code>, it will be 0, so the whole It will be 0.</p>
<p>For example, let&rsquo;s say that when you were learning in the category &ldquo;IT&rdquo;, you got a new word &ldquo;Python&rdquo; that was not the learning data. Then..</p>
<pre><code class="language-math" data-lang="math">P(word_k|cat) = \frac{Number of occurrences of word (word) in category (cat)}{Total number of words that appear in category (cat)}
</code></pre><p>The numerator of becomes 0,</p>
<pre><code class="language-math" data-lang="math">P(Python|IT) = 0
</code></pre><p>Will be. So, add 1 to the number of appearances in advance to prevent the probability from becoming 0! ! That seems to be a technique called Laplace smoothing. Of course, the denominator will also change. Write the formula again..</p>
<pre><code class="language-math" data-lang="math">P(word_k|cat) = \frac{Number of occurrences of word (word) in category (cat) + 1}{Total number of words in category (cat) + Total number of words}
</code></pre><p>Will be.</p>
<h4 id="take-logarithm"><take logarithm></h4>
<p>In the sample data, it is no big deal, but when applied to actual calculation, the denominator of <code>P(word|cat)</code> may become very large.</p>
<p>The reason is that the number of words can be very large. Underflow may occur at such time.</p>
<p>The opposite of underflow or overflow is that the number after the decimal point becomes too large to handle.</p>
<p>That&rsquo;s where the logarithm comes in. If you take the logarithm, you can replace it with the addition of the same base, so the result will be the same even if you take the logarithm and add it.</p>
<p>So, change the following into a logarithmic form.</p>
<pre><code class="language-math" data-lang="math">\prod_{i=0}^k P(word_k|cat)
</code></pre><pre><code class="language-math" data-lang="math">\prod_{i=0}^k \log P(word_k|cat)
</code></pre><p>Combining these, the final formula is:</p>
<pre><code class="language-math" data-lang="math">P(cat|doc) = \log P(cat) + \sum_{i=0}^k \log P(word_k|cat)
</code></pre><p>Reference: <a href="http://kenyu.red/archives/3132.html">Logarithm (LOG) calculation and formula! This is already perfect! ! </a></p>
<h3 id="next-time"><next time></h3>
<p><a href="http://qiita.com/RINDO/items/8caef9daba5f10caebbe">Machine learning beginners try Naive Bayes (2)-Implementation</a></p>
<p>So I would like to implement it using Python. (Refer to the site)</p>
<h4 id="reference">Reference</h4>
<p>The following sites were very helpful. Thank you very much.</p>
<ul>
<li><a href="http://aidiary.hatenablog.com/entry/20100613/1276389337">Text classification using Naive Bayes</a></li>
<li><a href="https://hogehuga.com/post-563/#3">[WIP] Simple Bayes classifier is not simple at all so get started</a></li>
</ul>
<h2 id="edit-history">Edit history</h2>
<ul>
<li>2017.11.15: The final formula was incorrect, so I fixed it to Sigma</li>
<li>2018.10.31: Corrected the description of conditional probability because it was wrong</li>
</ul>

    </article>
  </div>

  
  
  
  <script>
  window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
  ga('create', 'UA-169005401-1', 'auto');
  ga('send', 'pageview');
  </script>
  <script async src='https://www.google-analytics.com/analytics.js'></script>
  

  

  
<link rel="stylesheet" type="text/css" href="//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.0/cookieconsent.min.css" />
<script src="//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.0/cookieconsent.min.js"></script>
<script>
window.addEventListener("load", function(){
window.cookieconsent.initialise({
  "palette": {
    "popup": {
      "background": "#216942",
      "text": "#b2d192"
    },
    "button": {
      "background": "#afed71"
    }
  }
})});
</script>

</body>

</html>
