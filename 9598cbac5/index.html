<!doctype html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  
  <meta name="generator" content="Hugo 0.72.0" />

  
  <meta name="description" content="Some description">
  

  
  <link rel="apple-touch-icon" sizes="180x180" href="https://memotut.com/apple-touch-icon.png">

  
  <link rel="icon" type="image/png" sizes="32x32" href="https://memotut.com/favicon-32x32.png">

  
  <link rel="icon" type="image/png" sizes="16x16" href="https://memotut.com/favicon-16x16.png">

  
  <link rel="manifest" href="https://memotut.com/site.webmanifest">

  
  <link rel="mask-icon" href="https://memotut.com/safari-pinned-tab.svg" color="#5bbad5">

  <meta name="msapplication-TileColor" content="#da532c">

  <meta name="theme-color" content="#ffffff">

  
  <link rel="stylesheet" href="https://memotut.com/css/bootstrap.min.css" />

  
  <title>[Python] Image registration: From SIFT to deep learning | Memo Tut</title>
  

  <style>
body {
  min-width: 300px;
}

.custom-navbar {
  margin-bottom: 1em;
  height: 60px;
}

.custom-navbar a {
  display: inline-block; 
  padding: 18px 0;
  margin-right: 1em; 
  font-weight: bold; 
}

.custom-navbar a:hover,
.custom-navbar a:focus {
  text-decoration: none; 
}

@media print {
  .custom-navbar {
    display: none;
  }
}

article {
  padding-bottom: 1em;
}

img {
  max-width: 100%;
}


body {
  background-color: #fff;
}



body {
  color: #212529;
}



a {
  color: #007bff;
}



a:hover,
a:focus {
  color: #0056b3;
}



.custom-navbar {
  background-color: #212529;
}



.custom-navbar a {
  color: rgba(255, 255, 255, 0.75);
}



.custom-navbar a:hover,
.custom-navbar a:focus {
  color: rgba(255, 255, 255, 1);
}



.container {
  max-width: 800px;
}



pre {
  display: block;
  padding: 9.5px;
  word-break: break-all;
  word-wrap: break-word;
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 4px;
}

pre code {
  padding: 0;
  font-size: inherit;
  color: inherit; 
  white-space: pre-wrap;
  background-color: transparent;
  border: none;
  border-radius: 0;
}

code {
  padding: 2px 4px;
  color: inherit; 
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 4px;
  font-size: .9em;
}



blockquote,
.blockquote {
  padding: 10px 20px;
  margin: 0 0 20px;
  font-size: 1em;
  border-left: 5px solid #6c757d;
}

</style>
</head>

<body>
  <nav class="custom-navbar">
  <div class="container">
    
    <a href="/">Posts</a>
    
    <a href="/tags/">Tags</a>
    
    <a href="/about/">About</a>
    
    <a href="/index.xml">RSS</a>
    
  </div>
</nav>
  
  <div class="container">
    <article>
      <h1>[Python] Image registration: From SIFT to deep learning</h1>
<p>
  <small class="text-secondary">
  
  
  Jan 22, 2020
  </small>
  

<small><code><a href="https://memotut.com/tags/python">Python</a></code></small>


<small><code><a href="https://memotut.com/tags/image-processing"> image processing</a></code></small>


<small><code><a href="https://memotut.com/tags/opencv"> OpenCV</a></code></small>


<small><code><a href="https://memotut.com/tags/deeplearning"> DeepLearning</a></code></small>


<small><code><a href="https://memotut.com/tags/registration"> registration</a></code></small>

</p>
<pre><code>## Overview
</code></pre>
<p>There are few Japanese articles about image registration (as of January 22, 2020), so I thought it was very easy to understand <a href="https://www.sicara.ai/blog/2019-07-16-image-registration-deep-learning">Image Registration: From SIFT to Deep Learning</a> was translated or summarized. Some parts have been omitted or added, so please read the original article if you want to get the original text.</p>
<p><a href="https://github.com/suuungwoo/image-registration">Source code</a></p>
<h2 id="what-is-image-registration">What is image registration?</h2>
<p><img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/207053/679f3f93-5944-eddd-5f44-7f1e581d4b79.png" alt="image.png"></p>
<p>Image registration is the process of correcting the misalignment between two images. Image registration is used when comparing multiple images of the same scene. For example, it often appears in the fields of satellite image analysis, optical flow, and medical imaging.
Let&rsquo;s look at a concrete example. The image above is an alignment of the images of Muscat Bonbons I ate at Shin-Okubo. It was delicious, but not recommended for those who are not sweet tooth.
In this example, you can see that the position of the second Muscat bonbon from the left is adjusted to the position of the Muscat bonbon at the left end while keeping the brightness and the like. In the following, we refer to the unreferenced image, such as the left edge, as the reference image, and the converted image, such as the second from the left, as the floating image.
This article describes some techniques for aligning between floating and reference images. Note that the iteration/signal strength based methods are not so common and are not covered in this article.</p>
<h2 id="feature-based-approach">Feature-based approach</h2>
<p>Since the early 2000s, a feature-based approach to image registration has been used. This approach consists of three steps: keypoint detection and feature description, feature matching, and image transformation. Briefly, it selects points of interest in both images, associates each point of interest in the reference image with the corresponding point in the floating image, and transforms the floating image so that both images are aligned.</p>
<h3 id="keypoint-detection-and-description-of-characteristics">Keypoint detection and description of characteristics</h3>
<p>Key points define what is important and distinctive in the image (corners, edges, etc.). Each keypoint is represented by a descriptor. The descriptor is a feature vector containing the essential features of the keypoint. Descriptors must be robust to image transformations (localization, scale, brightness, etc.). There are many algorithms for detecting keypoints and characterizing features.</p>
<ul>
<li><a href="https://www.cs.ubc.ca/~lowe/papers/ijcv04.pdf">SIFT</a>(Scale-invariantfeaturetransform) is the original algorithm used to detect keypoints, but commercial There is a charge for use. SIFT feature descriptors are invariant to uniform scaling, orientation, and intensity transformations, and partially invariant to affine distortions.</li>
<li><a href="https://www.vision.ee.ethz.ch/~surf/eccv06.pdf">SURF</a>(SpeededUpRobustFeatures) is a detector and descriptor affected by SIFT. It is several times faster than SIFT. We have also obtained a patent.</li>
<li><a href="https://www.edwardrosten.com/work/rosten_2006_machine.pdf">ORB FAST Brief</a>(OrientedFASTandRotatedBRIEF)is<a href="https://www.edwardrosten.com/work/rosten_2006_machine.pdf">FAST</a>pdf)Afastbinarydescriptorbasedonacombinationofakeypointdetectoranda<a href="https://www.cs.ubc.ca/~lowe/525/papers/calonder_eccv10.pdf">Brief</a> descriptor. It is rotation invariant and robust against noise. Developed by OpenCV Labs, it is an efficient and free alternative to SIFT.</li>
<li><a href="http://www.bmva.org/bmvc/2013/Papers/paper0013/paper0013.pdf">AKAZE</a>(Accelerated-KAZE)is<a href="http://citeseerx.ist.psu.edu/speedupversionofviewdoc/download?doi=10.1.1.304.4980&amp;rep=rep1&amp;type=pdf">KAZE</a>. It provides a fast multiscale feature detection and description approach for nonlinear scale spaces. It is invariant to both scale and rotation and is free.</li>
</ul>
<p>These algorithms are easy to use with OpenCV. In the example below, we used AKAZE&rsquo;s OpenCV implementation. Other algorithms can be used simply by renaming the algorithm.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> cv2 <span style="color:#f92672">as</span> cv

<span style="color:#75715e"># Image is read in grayscale to make it easier to see key points etc.</span>
img <span style="color:#f92672">=</span> cv<span style="color:#f92672">.</span>imread(<span style="color:#e6db74">&#39;img/float.jpg&#39;</span>)
gray <span style="color:#f92672">=</span> cv<span style="color:#f92672">.</span>cvtColor(img, cv<span style="color:#f92672">.</span>COLOR_BGR2GRAY)

<span style="color:#75715e">#Keypoint detection and description of features</span>
akaze <span style="color:#f92672">=</span> cv<span style="color:#f92672">.</span>AKAZE_create()
kp, descriptor <span style="color:#f92672">=</span> akaze<span style="color:#f92672">.</span>detectAndCompute(gray, None)

keypoints_img <span style="color:#f92672">=</span> cv<span style="color:#f92672">.</span>drawKeypoints(gray, kp, img)
cv<span style="color:#f92672">.</span>imwrite(<span style="color:#e6db74">&#39;keypoints.jpg&#39;</span>, keypoints_img)
</code></pre></div><p><img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/207053/3477c24c-da00-6698-e2e5-5f056fb3f5ee.png" alt="image.png"></p>
<p>For more information on feature detection and descriptors, see <a href="https://docs.opencv.org/3.4/d7/d66/tutorial_feature_detection.html">OpenCV Tutorial</a>.</p>
<h3 id="feature-matching">Feature matching</h3>
<p>After finding the keypoints in both images, the corresponding keypoints need to be associated or &ldquo;matched&rdquo;. One of the methods for that is <code>BFMatcher.knnMatch()</code>. It measures the distance between each pair of keypoint descriptors and, for each keypoint, matches the k keypoints with the closest distances.
Then apply a ratio filter to keep only the correct matches. To achieve reliable matching, the matched keypoints must be much closer than the closest false match.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> cv2 <span style="color:#f92672">as</span> cv

float_img <span style="color:#f92672">=</span> cv<span style="color:#f92672">.</span>imread(<span style="color:#e6db74">&#39;img/float.jpg&#39;</span>, cv<span style="color:#f92672">.</span>IMREAD_GRAYSCALE)
ref_img <span style="color:#f92672">=</span> cv<span style="color:#f92672">.</span>imread(<span style="color:#e6db74">&#39;img/ref.jpg&#39;</span>, cv<span style="color:#f92672">.</span>IMREAD_GRAYSCALE)

akaze <span style="color:#f92672">=</span> cv<span style="color:#f92672">.</span>AKAZE_create()
float_kp, float_des <span style="color:#f92672">=</span> akaze<span style="color:#f92672">.</span>detectAndCompute(float_img, None)
ref_kp, ref_des <span style="color:#f92672">=</span> akaze<span style="color:#f92672">.</span>detectAndCompute(ref_img, None)

<span style="color:#75715e"># Feature matching</span>
bf <span style="color:#f92672">=</span> cv<span style="color:#f92672">.</span>BFMatcher()
matches <span style="color:#f92672">=</span> bf<span style="color:#f92672">.</span>knnMatch(float_des, ref_des, k<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>)

<span style="color:#75715e"># Keep only correct matching</span>
good_matches <span style="color:#f92672">=</span> []
<span style="color:#66d9ef">for</span> m, n <span style="color:#f92672">in</span> matches:
    <span style="color:#66d9ef">if</span> m<span style="color:#f92672">.</span>distance <span style="color:#f92672">&lt;</span><span style="color:#ae81ff">0.75</span> <span style="color:#f92672">*</span> n<span style="color:#f92672">.</span>distance:
        good_matches<span style="color:#f92672">.</span>append([m])

matches_img <span style="color:#f92672">=</span> cv<span style="color:#f92672">.</span>drawMatchesKnn(
    float_img,
    float_kp,
    ref_img,
    ref_kp,
    good_matches,
    None,
    flags<span style="color:#f92672">=</span>cv<span style="color:#f92672">.</span>DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)
cv<span style="color:#f92672">.</span>imwrite(<span style="color:#e6db74">&#39;matches.jpg&#39;</span>, matches_img)
</code></pre></div><p><img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/207053/216eea75-1db6-49b9-7783-7a665e949a7c.png" alt="image.png"></p>
<p>For other feature matching methods implemented in OpenCV, see <a href="https://docs.opencv.org/trunk/dc/dc3/tutorial_py_matcher.html">Documentation</a>.</p>
<h3 id="image-conversion">Image conversion</h3>
<p>After matching at least 4 keypoints, one image is transformed relative to the other. This is called <a href="https://docs.opencv.org/3.0-beta/modules/cudawarping/doc/warping.html">image warping</a>.Twoimagesonthesameplaneinspacearerelatedby<a href="https://docs.opencv.org/3.4.1/d9/dab/tutorial_homography.html">homography</a>.Homographyisageometrictransformationthathas8freeparametersandisrepresentedbya3x3matrix.Theyrepresentthedistortionaddedtotheentireimage(asopposedtothelocaltransformation). Therefore, to get the transformed floating image, compute the homography matrix and apply it to the floating image.
To ensure optimal conversion, we detect outliers using the <a href="https://en.wikipedia.org/wiki/Random_sample_consensus">RANSAC</a>algorithmandremovethemtodeterminethefinalhomography.Iwill.Itisdirectlyincorporatedintothe<a href="https://docs.opencv.org/2.4/modules/calib3d/doc/camera_calibration_and_3d_reconstruction.html?highlight=findhomography#findhomography">findHomography</a>methodofOpenCV.AsanalternativetoRANSAC<a href="https://docs.opencv.org/2.4/modules/calib3d/doc/camera_calibration_and_3d_reconstruction.html?highlight=findhomography">LMEDS</a>: There is also a robust estimation method such as the minimum median method.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> numpy <span style="color:#f92672">as</span> np
<span style="color:#f92672">import</span> cv2 <span style="color:#f92672">as</span> cvfloat_img <span style="color:#f92672">=</span> cv<span style="color:#f92672">.</span>imread(<span style="color:#e6db74">&#39;img/float.jpg&#39;</span>, cv<span style="color:#f92672">.</span>IMREAD_GRAYSCALE)
ref_img <span style="color:#f92672">=</span> cv<span style="color:#f92672">.</span>imread(<span style="color:#e6db74">&#39;img/ref.jpg&#39;</span>, cv<span style="color:#f92672">.</span>IMREAD_GRAYSCALE)

akaze <span style="color:#f92672">=</span> cv<span style="color:#f92672">.</span>AKAZE_create()
float_kp, float_des <span style="color:#f92672">=</span> akaze<span style="color:#f92672">.</span>detectAndCompute(float_img, None)
ref_kp, ref_des <span style="color:#f92672">=</span> akaze<span style="color:#f92672">.</span>detectAndCompute(ref_img, None)

bf <span style="color:#f92672">=</span> cv<span style="color:#f92672">.</span>BFMatcher()
matches <span style="color:#f92672">=</span> bf<span style="color:#f92672">.</span>knnMatch(float_des, ref_des, k<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>)

good_matches <span style="color:#f92672">=</span> []
<span style="color:#66d9ef">for</span> m, n <span style="color:#f92672">in</span> matches:
    <span style="color:#66d9ef">if</span> m<span style="color:#f92672">.</span>distance <span style="color:#f92672">&lt;</span> <span style="color:#ae81ff">0.75</span> <span style="color:#f92672">*</span> n<span style="color:#f92672">.</span>distance:
        good_matches<span style="color:#f92672">.</span>append([m])

<span style="color:#75715e"># 適切なキーポイントを選択</span>
ref_matched_kpts <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>float32(
    [float_kp[m[<span style="color:#ae81ff">0</span>]<span style="color:#f92672">.</span>queryIdx]<span style="color:#f92672">.</span>pt <span style="color:#66d9ef">for</span> m <span style="color:#f92672">in</span> good_matches])<span style="color:#f92672">.</span>reshape(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>)
sensed_matched_kpts <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>float32(
    [ref_kp[m[<span style="color:#ae81ff">0</span>]<span style="color:#f92672">.</span>trainIdx]<span style="color:#f92672">.</span>pt <span style="color:#66d9ef">for</span> m <span style="color:#f92672">in</span> good_matches])<span style="color:#f92672">.</span>reshape(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>)

<span style="color:#75715e"># ホモグラフィを計算</span>
H, status <span style="color:#f92672">=</span> cv<span style="color:#f92672">.</span>findHomography(
    ref_matched_kpts, sensed_matched_kpts, cv<span style="color:#f92672">.</span>RANSAC, <span style="color:#ae81ff">5.0</span>)

<span style="color:#75715e"># 画像を変換</span>
warped_image <span style="color:#f92672">=</span> cv<span style="color:#f92672">.</span>warpPerspective(
    float_img, H, (float_img<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">1</span>], float_img<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>]))

cv<span style="color:#f92672">.</span>imwrite(<span style="color:#e6db74">&#39;warped.jpg&#39;</span>, warped_image)
</code></pre></div><p><img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/207053/95d3388e-2fba-3805-9e76-5e5332e47e77.png" alt="image.png"></p>
<p>これらの3つのステップの詳細に関心がある場合、OpenCVは<a href="https://docs.opencv.org/3.1.0/db/d27/tutorial_py_table_of_contents_feature2d.html">一連の便利なチュートリアル</a>をまとめています。</p>
<h2 id="深層学習アプローチ">深層学習アプローチ</h2>
<p>最近の画像位置合わせのほとんどの研究は、深層学習の使用に関するものです。過去数年間で、深層学習により、分類、検出、セグメンテーションなどのコンピュータビジョンタスクの最先端のパフォーマンスが可能になりました。画像位置合わせに関しても例外ではありません。</p>
<h3 id="特徴抽出">特徴抽出</h3>
<p>最初に深層学習が画像位置合わせに使用されたのは、特徴抽出のためでした。畳み込みニューラルネットワーク（CNN）の連続層は、ますます複雑な画像特徴をとらえ、タスク固有の特徴を学習します。2014年以来、研究者はこれらのネットワークをSIFTまたは同様のアルゴリズムではなく、特徴抽出ステップに適用しています。</p>
<ul>
<li>2014年、Dosovitskiyらは、教師なしデータのみを用いてCNNを学習することを提案しました。これらの特徴の汎用性により、変換に対して頑健になりました。これらの特徴または記述子は、SIFT記述子よりも優れていました。</li>
<li>2018年、Yangらは、同じ考えに基づいた<a href="https://ieeexplore.ieee.org/document/8404075">非剛体位置合わせ方法</a>を開発しました。彼らは事前に学習されたVGGネットワークの層を用いて、畳み込み情報とローカリゼーション特徴の両方を保持する特徴記述子を生成しました。これらの記述子は、特にSIFTに多くの外れ値が含まれているか、十分な数の特徴点とマッチングできない場合に、SIFTのような検出器よりも優れているようです。
後者の論文のコードは<a href="https://github.com/yzhq97/cnn-registration">ここ</a>にあります。この位置合わせ方法は15分以内に手元の画像で試せますが、前半で実装したSIFTのような方法よりも約70倍遅いです。</li>
</ul>
<h3 id="ホモグラフィ学習">ホモグラフィ学習</h3>
<p>研究者たちは、深層学習の使用を特徴抽出に限定せず、ニューラルネットワークを用いて幾何学的変換を直接学習し、位置合わせを実現しようとしました。</p>
<h4 id="教師あり学習">教師あり学習</h4>
<p>2016年、DeToneらは、2つの画像に関連するホモグラフィを学習するVGGスタイルモデルである回帰ホモグラフィネットを説明する<a href="https://arxiv.org/pdf/1606.03798.pdf">深層画像ホモグラフィ推定</a>を公開しました。このアルゴリズムには、ホモグラフィとCNNモデルのパラメータをエンドツーエンドで同時に学習するという利点があります。特徴抽出とマッチングのプロセスは不要です。</p>
<p><img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/207053/b2fb2349-9dde-8f85-3906-5ab9595cea27.png" alt="image.png"></p>
<p>ネットワークは、出力として8つの実数値を生成します。出力とグランドトゥルースホモグラフィ間の損失によって教師あり学習を行います。</p>
<p><img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/207053/c445e9a0-1d8f-0965-9cf5-3e82e74791bb.png" alt="image.png"></p>
<p>他の教師あり学習アプローチと同様に、このホモグラフィ推定法は教師データのペアが必要です。しかし、実際のデータでグランドトゥルースホモグラフィを得るのは簡単ではありません。</p>
<h4 id="教師なし学習">教師なし学習</h4>
<p>Nguyenらは、深層画像ホモグラフィ推定への教師なし学習アプローチを提示しました。彼らは同じCNNを用いましたが、教師なしアプローチに適した損失関数を使用する必要がありました。そこで、グランドトゥルースラベルを必要としないフォトメトリック損失を選択しました。参照画像と変換した浮動画像の類似度を計算します。</p>
<p>$$
\mathbf{L}_{PW} = \frac{1}{|\mathbf{x} _i|}
\sum _{\mathbf{x} _i}|I^A(\mathscr{H}(\mathbf{x} _i))-I^B(\mathbf{x} _i)|
$$</p>
<p>彼らのアプローチは、Tensor Direct Linear TransformとSpatial Transformation Layerという2つの新しいネットワーク構造を導入しています。</p>
<p><img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/207053/57077ebc-ec4d-fd43-f6b7-fbe6bb4676f5.png" alt="image.png"></p>
<p>著者は、この教師なしの方法は、従来の特徴に基づく方法と比較して、より早い推論速度であり、同等以上の精度と照明変動に対して頑健性を持つと主張しています。さらに、教師あり手法と比較して、適応性とパフォーマンスに優れています。</p>
<h2 id="その他のアプローチ">その他のアプローチ</h2>
<h3 id="強化学習">強化学習</h3>
<p>深層強化学習は、医療アプリケーションの位置合わせ方法として注目されています。事前定義された最適化アルゴリズムとは対照的に、このアプローチでは、訓練されたエージェントを用いて位置合わせを実行します。</p>
<p><img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/207053/a42a92ed-69d1-0295-ac0e-281a665c1d23.png" alt="image.png"></p>
<ul>
<li>2016年、Liaoらは画像位置合わせに強化学習を最初に使用しました。彼らの方法は、エンドツーエンド学習のための貪欲な教師ありアルゴリズムに基づいています。その目標は、モーションアクションの最適なシーケンスを見つけることにより画像を位置合わせすることです。このアプローチはいくつかの最先端の方法より優れていましたが、剛体変換にのみ使用されました。</li>
<li>強化学習はより複雑な変換にも使用されています。Krebsらはエージェントに基づくアクション学習により頑健な非剛体位置合わせを行いました。変換モデルのパラメータを最適化するために人工エージェントを適用します。この方法は、前立腺MRIの被験者間位置合わせで評価され、2次元および3次元で有望な結果を示しました。</li>
</ul>
<h3 id="複雑な変換">複雑な変換</h3>
<p>かなりの割合の画像位置合わせにおける現在の研究が、医用画像の分野に関係しています。多くの場合、2つの医用画像間の変換は、被験者の局所的な変換（呼吸や解剖学的変化など）のために、ホモグラフィ行列によって単純に記述することはできません。変位ベクトル場で表現できる微分同相写像など、より複雑な変換モデルが必要です。</p>
<p><img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/207053/55fe9948-6f0d-0fe0-e64c-08dfea9f3ae0.png" alt="image.png"></p>
<p>研究者は、ニューラルネットワークを用いて、多くのパラメータを持つこれらの大きな変換モデルを推定しようとしました。</p>
<ul>
<li>最初の例は、上記のKrebsらの強化学習法です。</li>
<li>2017年、De Vosらは<a href="https://arxiv.org/pdf/1704.06065.pdf">DIRNet</a>を提案しました。これは、CNNを用いてコントロールポイントのグリッドを予測するネットワークであり、このグリッドを用いて変位ベクトル場を生成し、参照画像に従って浮動画像をワープします。</li>
</ul>
<p><img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/207053/3ccf3d6a-cac9-286e-e09b-650816ceadd7.png" alt="image.png"></p>
<ul>
<li><a href="https://www.researchgate.net/publication/315748621_Quicksilver_Fast_Predictive_Image_Registration_-_a_Deep_Learning_Approach">Quicksilver位置合わせ</a>は、同様の問題に対処します。Quicksilverは、深層符号化復号化ネットワークを用いて、画像の外観でパッチごとの変換を直接予測します。</li>
</ul>
<h2 id="まとめ">まとめ</h2>
<p>画像位置合わせを行なうための手法をいくつか紹介してきました。特徴点を用いる手法から、深層学習で画像を直接変換する手法に変化しつつあることがわかりました。</p>

    </article>
  </div>

  
  
  
  <script>
  window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
  ga('create', 'UA-169005401-1', 'auto');
  ga('send', 'pageview');
  </script>
  <script async src='https://www.google-analytics.com/analytics.js'></script>
  

  

  
<link rel="stylesheet" type="text/css" href="//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.0/cookieconsent.min.css" />
<script src="//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.0/cookieconsent.min.js"></script>
<script>
window.addEventListener("load", function(){
window.cookieconsent.initialise({
  "palette": {
    "popup": {
      "background": "#216942",
      "text": "#b2d192"
    },
    "button": {
      "background": "#afed71"
    }
  }
})});
</script>

</body>

</html>
