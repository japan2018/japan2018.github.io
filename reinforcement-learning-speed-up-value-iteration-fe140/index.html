<!doctype html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  
  <meta name="generator" content="Hugo 0.72.0" />

  
  <meta name="description" content="Some description">
  

  
  <link rel="apple-touch-icon" sizes="180x180" href="https://japan2018.github.io/apple-touch-icon.png">

  
  <link rel="icon" type="image/png" sizes="32x32" href="https://japan2018.github.io/favicon-32x32.png">

  
  <link rel="icon" type="image/png" sizes="16x16" href="https://japan2018.github.io/favicon-16x16.png">

  
  <link rel="manifest" href="https://japan2018.github.io/site.webmanifest">

  
  <link rel="mask-icon" href="https://japan2018.github.io/safari-pinned-tab.svg" color="#5bbad5">

  <meta name="msapplication-TileColor" content="#da532c">

  <meta name="theme-color" content="#ffffff">

  
  <link rel="stylesheet" href="https://japan2018.github.io/css/bootstrap.min.css" />

  
  <title>Reinforcement Learning: Speed up Value Iteration | Some Title</title>
  

  <style>
body {
  min-width: 300px;
}

.custom-navbar {
  margin-bottom: 1em;
  height: 60px;
}

.custom-navbar a {
  display: inline-block; 
  padding: 18px 0;
  margin-right: 1em; 
  font-weight: bold; 
}

.custom-navbar a:hover,
.custom-navbar a:focus {
  text-decoration: none; 
}

@media print {
  .custom-navbar {
    display: none;
  }
}

article {
  padding-bottom: 1em;
}

img {
  max-width: 100%;
}


body {
  background-color: #fff;
}



body {
  color: #212529;
}



a {
  color: #007bff;
}



a:hover,
a:focus {
  color: #0056b3;
}



.custom-navbar {
  background-color: #212529;
}



.custom-navbar a {
  color: rgba(255, 255, 255, 0.75);
}



.custom-navbar a:hover,
.custom-navbar a:focus {
  color: rgba(255, 255, 255, 1);
}



.container {
  max-width: 800px;
}



pre {
  display: block;
  padding: 9.5px;
  word-break: break-all;
  word-wrap: break-word;
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 4px;
}

pre code {
  padding: 0;
  font-size: inherit;
  color: inherit; 
  white-space: pre-wrap;
  background-color: transparent;
  border: none;
  border-radius: 0;
}

code {
  padding: 2px 4px;
  color: inherit; 
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 4px;
  font-size: .9em;
}



blockquote,
.blockquote {
  padding: 10px 20px;
  margin: 0 0 20px;
  font-size: 1em;
  border-left: 5px solid #6c757d;
}

</style>
</head>

<body>
  <nav class="custom-navbar">
  <div class="container">
    
    <a href="/">Posts</a>
    
    <a href="/tags/">Tags</a>
    
    <a href="/about/">About</a>
    
    <a href="/index.xml">RSS</a>
    
  </div>
</nav>
  
  <div class="container">
    <article>
      <h1>Reinforcement Learning: Speed up Value Iteration</h1>
<p>
  <small class="text-secondary">
  
  
  May 19, 2020
  </small>
  

<small><code><a href="https://japan2018.github.io/tags/python">Python</a></code></small>


<small><code><a href="https://japan2018.github.io/tags/reinforcement-learning"> reinforcement learning</a></code></small>


<small><code><a href="https://japan2018.github.io/tags/cython"> Cython</a></code></small>


<small><code><a href="https://japan2018.github.io/tags/ai"> AI</a></code></small>


<small><code><a href="https://japan2018.github.io/tags/dynamic-planning-method"> dynamic planning method</a></code></small>

</p>
<pre><code>#Introduction
</code></pre>
<p>In recent years, the field of model-free deep reinforcement learning has been actively studied due to the success of AlphaGo and DQN. These algorithms are one of the effective approaches when the state-action space is very large or when mathematical modeling of dynamics is difficult. However, in many of the problems encountered in reality, mathematical modeling of the environment is relatively easy, and the state-action space can be reduced by devising it in many cases. For such problems, I think that using model-based table reinforcement learning is more advantageous in terms of development and operation costs.</p>
<p>However, the size of the state-action space that can be handled by table reinforcement learning greatly depends on the speed of the program, and speeding up is very important. Therefore, in this paper, we will introduce the know-how for high-speed execution of __ value iteration __, which is the basic algorithm for reinforcement learning. In the end, we were able to achieve __500 times faster __ than the naive implementation.</p>
<p>#Background</p>
<h3 id="markov-decision-process">Markov decision process</h3>
<p>Markov Decision Processes (MDP) is a framework used in problem setting for reinforcement learning. The &ldquo;environment&rdquo; takes a state $s$ at each time, and the &ldquo;agent&rdquo; who makes a decision arbitrarily selects the action $a$ available in that state. The environment then randomly transitions to a new state, at which time the agent receives a reward $r$ corresponding to the state transition.
The basic problem setting in MDP is to seek a policy that is a mapping (probability distribution) to the optimal behavior of an agent in a certain state. Then, if the objective function is the cumulative discounted reward, we end up with the problem of finding the optimal value function as follows.</p>
<pre><code class="language-math" data-lang="math">\pi^* = \text{arg}\max_{\pi} \text{E}_{s \sim \mu, \tau \sim \pi}[\sum _t \gamma^tr(s_t, a_t)| s_0=s] = \text{arg}\max_{\pi} \text{E}_{s \sim \mu} [V_{\pi}(s)]
</code></pre><p>State value function $V(s)$ and action value function $Q(s, a)$ are defined as follows, respectively.</p>
<pre><code class="language-math" data-lang="math">V_{\pi}(s) = \text{E}_{\tau \sim \pi}[\sum _t \gamma^t r(s_t, a_t)|s_0=s] \\
Q_{\pi}(s, a) = \text{E}_{\tau \sim \pi}[\sum _t \gamma^t r(s_t, a_t)|s_0=s, a_0=a]
</code></pre><h3 id="value-iteration">Value iteration</h3>
<p>State value function in optimal policy $\pi^<em>$ is $V^</em>(s) = V_{\pi^*}(s)$, behavior value function is $Q^*(s,a) = Q_{\ Define it as pi^*}(s,a)$. From the definition of the value function, we see that the optimal value function satisfies the Bellman equation</p>
<pre><code class="language-math" data-lang="math">Q^*(s, a) = \sum_{s', r} p(s', r|s, a) [r + \gamma V^*(s')] \\
V^*(s) = \max _a Q^*(s, a)
</code></pre><p>The Value Iteration method is an algorithm that starts from an appropriate initial value and repeatedly adapts the Bellman equation to update $V$ and $Q$ alternately. The worst case complexity is a polynomial for the number of states and the number of actions.
If we can find the action value function $Q^*(s,a)$, we can find the optimal policy by choosing the action with the highest action value among the possible actions $a$ in the state $s$. I can do it.</p>
<pre><code class="language-math" data-lang="math">\pi(a|s) = \text{arg}\max _a Q^*(s,a)
</code></pre><h1 id="algorithm">Algorithm</h1>
<h3 id="experiment-preparation">Experiment preparation</h3>
<p>The value iteration method is very simple as an idea, but there are various ways to implement it. Here, I would like to introduce some implementations and their processing speeds.
We will create an experimental MDP for comparison of each implementation. For ease of implementation, consider a deterministic MDP, that is, a certain behavior $a$, where the reward $r$ and the next state $s'$ are deterministically determined. The Bellman equation is then simplified to</p>
<pre><code class="language-math" data-lang="math">Q^*(s, a) = r + \gamma V^*(s')
</code></pre><p>A deterministic MDP can be represented by a graph in which states are nodes, behaviors are edges, and rewards are edge weights (attributes). The following function creates the MDP used in the experiment.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> networkx <span style="color:#f92672">as</span> nx
<span style="color:#f92672">import</span> random

<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">create_mdp</span>(num_states, num_actions, reward_ratio<span style="color:#f92672">=</span><span style="color:#ae81ff">0.01</span>, neighbor_count<span style="color:#f92672">=</span><span style="color:#ae81ff">30</span>):
    get_reward <span style="color:#f92672">=</span> <span style="color:#66d9ef">lambda</span>: <span style="color:#ae81ff">1.0</span> <span style="color:#66d9ef">if</span> random<span style="color:#f92672">.</span>random() <span style="color:#f92672">&lt;</span>reward_ratio <span style="color:#66d9ef">else</span> <span style="color:#ae81ff">0.0</span>
    get_neighbor <span style="color:#f92672">=</span> <span style="color:#66d9ef">lambda</span> u: random<span style="color:#f92672">.</span>randint(u<span style="color:#f92672">-</span>neighbor_count, u <span style="color:#f92672">+</span> neighbor_count) <span style="color:#f92672">%</span>(num_states<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>)
    edges <span style="color:#f92672">=</span> [
        (i, (i <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span>) <span style="color:#f92672">%</span>(num_states<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>), get_reward())
        <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(num_states)
    ]
    <span style="color:#66d9ef">for</span> _ <span style="color:#f92672">in</span> range(num_states <span style="color:#f92672">*</span> (num_actions<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>)):
        u <span style="color:#f92672">=</span> random<span style="color:#f92672">.</span>randint(<span style="color:#ae81ff">0</span>, num_states<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>)
        v <span style="color:#f92672">=</span> get_neighbor(u)
        r <span style="color:#f92672">=</span> get_reward()
        edges<span style="color:#f92672">.</span>append((u, v, r))
    G <span style="color:#f92672">=</span> nx<span style="color:#f92672">.</span>DiGraph()
    G<span style="color:#f92672">.</span>add_weighted_edges_from(edges)
    <span style="color:#66d9ef">return</span> G
</code></pre></div><p>By specifying the number of states and the number of actions (average number of actions in each state), a random strongly connected graph and sparse reward are generated. It is expressed in DiGraph where the node of networkx is the state, edge is the action, and weight attribute of edge is the reward.</p>
<p>In future experiments, we will use MDPs with 10,000 states and an average of 3 actions in each state.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">num_states <span style="color:#f92672">=</span> <span style="color:#ae81ff">10000</span>
num_actions <span style="color:#f92672">=</span> <span style="color:#ae81ff">3</span>
G <span style="color:#f92672">=</span> create_mdp(num_states, num_actions)
</code></pre></div><h3 id="implement-naive-value-iteration">Implement naive value iteration</h3>
<p>The simplest algorithm is a method that repeats &ldquo;updating the state value of all states&rdquo; and &ldquo;updating the action value of all actions&rdquo;, and is sometimes called Synchronous Dynamic Programming.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">NonConvergenceError</span>(<span style="color:#a6e22e">Exception</span>):
    <span style="color:#66d9ef">pass</span>

<span style="color:#66d9ef">class</span> <span style="color:#a6e22e">SyncDP</span>:
    
    <span style="color:#66d9ef">def</span> __init__(self, G, gamma, max_sweeps, threshold):
        self<span style="color:#f92672">.</span>G <span style="color:#f92672">=</span> G
        self<span style="color:#f92672">.</span>gamma <span style="color:#f92672">=</span> gamma
        self<span style="color:#f92672">.</span>max_sweeps <span style="color:#f92672">=</span> max_sweeps
        self<span style="color:#f92672">.</span>threshold <span style="color:#f92672">=</span> threshold
        self<span style="color:#f92672">.</span>V <span style="color:#f92672">=</span> {state :<span style="color:#ae81ff">0</span> <span style="color:#66d9ef">for</span> state <span style="color:#f92672">in</span> G<span style="color:#f92672">.</span>nodes}
        self<span style="color:#f92672">.</span>TD <span style="color:#f92672">=</span> {state :<span style="color:#ae81ff">0</span> <span style="color:#66d9ef">for</span> state <span style="color:#f92672">in</span> G<span style="color:#f92672">.</span>nodes}
        self<span style="color:#f92672">.</span>Q <span style="color:#f92672">=</span> {(state, action) :<span style="color:#ae81ff">0</span> <span style="color:#66d9ef">for</span> state, action <span style="color:#f92672">in</span> G<span style="color:#f92672">.</span>edges}

    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">get_reward</span>(self, s, a):
        <span style="color:#66d9ef">return</span> self<span style="color:#f92672">.</span>G<span style="color:#f92672">.</span>edges[s, a][<span style="color:#e6db74">&#39;weight&#39;</span>]

    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">sweep</span>(self):
        <span style="color:#66d9ef">for</span> state <span style="color:#f92672">in</span> self<span style="color:#f92672">.</span>G<span style="color:#f92672">.</span>nodes:
            <span style="color:#66d9ef">for</span> action <span style="color:#f92672">in</span> self<span style="color:#f92672">.</span>G<span style="color:#f92672">.</span>successors(state):
                self<span style="color:#f92672">.</span>Q[state, action] <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>get_reward(state, action) <span style="color:#f92672">+</span> self<span style="color:#f92672">.</span>gamma <span style="color:#f92672">*</span> self<span style="color:#f92672">.</span>V[action]
        <span style="color:#66d9ef">for</span> state <span style="color:#f92672">in</span> self<span style="color:#f92672">.</span>G<span style="color:#f92672">.</span>nodes:
            v_new <span style="color:#f92672">=</span> max([self<span style="color:#f92672">.</span>Q[state, action] <span style="color:#66d9ef">for</span> action <span style="color:#f92672">in</span> self<span style="color:#f92672">.</span>G<span style="color:#f92672">.</span>successors(state)])
            self<span style="color:#f92672">.</span>TD[state] <span style="color:#f92672">=</span> abs(self<span style="color:#f92672">.</span>V[state]<span style="color:#f92672">-</span>v_new)
            self<span style="color:#f92672">.</span>V[state] <span style="color:#f92672">=</span> v_new

    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">run</span>(self):
        <span style="color:#66d9ef">for</span> _ <span style="color:#f92672">in</span> range(self<span style="color:#f92672">.</span>max_sweeps):
            self<span style="color:#f92672">.</span>sweep()
            <span style="color:#66d9ef">if</span> (np<span style="color:#f92672">.</span>array(list(self<span style="color:#f92672">.</span>TD<span style="color:#f92672">.</span>values())) <span style="color:#f92672">&lt;</span>self<span style="color:#f92672">.</span>threshold)<span style="color:#f92672">.</span>all():
                <span style="color:#66d9ef">return</span> self<span style="color:#f92672">.</span>V
        <span style="color:#66d9ef">raise</span> NonConvergenceError

</code></pre></div><p>The parameters such as the discount rate gamma, the convergence threshold threshold, and the maximum number of sweeps are originally determined by the requirements of the application, but here, set appropriate values.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">gamma <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.95</span>
threshold <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.01</span>
max_sweeps <span style="color:#f92672">=</span> <span style="color:#ae81ff">1000</span>
</code></pre></div><p>When executed under this condition, the processing time is as follows.</p>
<pre><code>%timeit V = SyncDP(G, gamma, max_sweeps, threshold).run()
8.83 s ± 273 ms per loop (mean ± std.dev. of 7 runs, 1 loop each)
</code></pre><p>If the number of states is 10,000 and it takes such a long time, the applications that can be used are likely to be quite limited. This processing time will be the baseline for future improvements.</p>
<h3 id="asynchronous-dynamic-programming-async-dp">Asynchronous Dynamic Programming (Async DP)</h3>
<p>Since the synchronous value update algorithm updates all the states in one sweep, even a single sweep can take a considerable amount of time if the number of states is very large. Asynchronous DP updates the state value on an iterative basis. In other words, instead of preparing a new array to store the updated value each time like synchronous DP and updating all state values and storing it in a new array, calculate each time by updating Repeat the value update, taking advantage of another available state value. It is necessary to keep updating all states to ensure convergence, but the update order can be freely selected. For example, value updates can be sped up by skipping conditions that are less relevant to the best strategy.</p>
<h3 id="prioritized-sweepingin-the-case-of-asynchronous-dp-the-order-of-value-updates-can-be-decided-arbitrarily-not-all-states-are-equally-useful-in-value-updates-in-other-states-during-value-updates-and-it-is-expected-that-some-states-will-have-a-significant-impact-on-the-value-of-other-states-for-example-in-an-mdp-that-gives-sparse-rewards-as-we-are-thinking-it-is-important-to-efficiently-propagate-the-state-in-which-rewards-are-obtained-to-other-states-therefore-the-following-algorithm-using-a-priority-queue-can-be-considered">Prioritized SweepingIn the case of asynchronous DP, the order of value updates can be decided arbitrarily. Not all states are equally useful in value updates in other states during value updates, and it is expected that some states will have a significant impact on the value of other states. For example, in an MDP that gives sparse rewards as we are thinking, it is important to efficiently propagate the state in which rewards are obtained to other states. Therefore, the following algorithm using a priority queue can be considered.</h3>
<ol>
<li>Manage the amount of change due to value update in all states with priority queue</li>
<li>Update the value of the state at the top of the queue</li>
<li>If the amount of change from the previous value update exceeds the threshold, push the state and amount of change pair to the queue</li>
</ol>
<p>This algorithm is called Prioritized Sweeping. The implementation looks like this:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> heapq

<span style="color:#66d9ef">class</span> <span style="color:#a6e22e">Prioritized</span> DP(SyncDP):
    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">run</span>(self):
        self<span style="color:#f92672">.</span>sweep()
        pq <span style="color:#f92672">=</span> [
            (<span style="color:#f92672">-</span>abs(td_error), state)
            <span style="color:#66d9ef">for</span> state, td_error <span style="color:#f92672">in</span> self<span style="color:#f92672">.</span>TD<span style="color:#f92672">.</span>items()
            <span style="color:#66d9ef">if</span> abs(td_error)<span style="color:#f92672">&gt;</span> self<span style="color:#f92672">.</span>threshold
            ]
        heapq<span style="color:#f92672">.</span>heapify(pq)
        <span style="color:#66d9ef">while</span> pq:
            _, action <span style="color:#f92672">=</span> heapq<span style="color:#f92672">.</span>heappop(pq)
            <span style="color:#66d9ef">if</span> self<span style="color:#f92672">.</span>TD[action] <span style="color:#f92672">&lt;</span>self<span style="color:#f92672">.</span>threshold:
                <span style="color:#66d9ef">continue</span>
            self<span style="color:#f92672">.</span>TD[action] <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
            <span style="color:#66d9ef">for</span> state <span style="color:#f92672">in</span> self<span style="color:#f92672">.</span>G<span style="color:#f92672">.</span>predecessors(action):
                self<span style="color:#f92672">.</span>Q[state, action] <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>get_reward(state, action) <span style="color:#f92672">+</span> self<span style="color:#f92672">.</span>gamma <span style="color:#f92672">*</span> self<span style="color:#f92672">.</span>V[action]
                v_new <span style="color:#f92672">=</span> max([self<span style="color:#f92672">.</span>Q[state, action] <span style="color:#66d9ef">for</span> action <span style="color:#f92672">in</span> self<span style="color:#f92672">.</span>G<span style="color:#f92672">.</span>successors(state)])
                td_error <span style="color:#f92672">=</span> abs(v_new<span style="color:#f92672">-</span>self<span style="color:#f92672">.</span>V[state])
                self<span style="color:#f92672">.</span>TD[state] <span style="color:#f92672">+=</span> td_error
                <span style="color:#66d9ef">if</span> td_error <span style="color:#f92672">&gt;</span>self<span style="color:#f92672">.</span>threshold:
                    heapq<span style="color:#f92672">.</span>heappush(pq, (<span style="color:#f92672">-</span>td_error, state))
                self<span style="color:#f92672">.</span>V[state] <span style="color:#f92672">=</span> v_new
        <span style="color:#66d9ef">return</span> self<span style="color:#f92672">.</span>V
</code></pre></div><p>First, execute the value update of all states with the sweep function, and configure the heap based on the state in which the TD error exceeds the threshold value. After that, update is repeated until the queue is exhausted. First, update the action value $Q$ so that the state fetched from the queue becomes the next state (=action). Then, update the state value $V$ that depends on the updated action value. If the difference between before update and after update (td_error) exceeds the threshold, push it to the queue. The processing time was as follows, and we were able to achieve about twice the speedup.</p>
<pre><code>%timeit V = PrioritizedDP(G, gamma, max_sweeps, threshold).run()
4.06 s ± 115 ms per loop (mean ± std.dev. of 7 runs, 1 loop each)
</code></pre><h3 id="vector-operation">Vector operation</h3>
<p>So far we have implemented networkx object-based algorithms, but frequently called methods such as successors are mostly time consuming. Let&rsquo;s consider how to utilize the vector operation by numpy while making the data structure more efficient. By expressing the graph with a numpy array, it becomes possible to use the vector operation for the action value operation as follows.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">ArraySyncDP</span>:
    
    <span style="color:#66d9ef">def</span> __init__(self, A: ArrayGraph, gamma, max_sweeps, threshold):
        self<span style="color:#f92672">.</span>A <span style="color:#f92672">=</span> A
        self<span style="color:#f92672">.</span>gamma <span style="color:#f92672">=</span> gamma
        self<span style="color:#f92672">.</span>max_sweeps <span style="color:#f92672">=</span> max_sweeps
        self<span style="color:#f92672">.</span>threshold <span style="color:#f92672">=</span> threshold
        self<span style="color:#f92672">.</span>V <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>zeros(A<span style="color:#f92672">.</span>num_states)
        self<span style="color:#f92672">.</span>TD <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>full(A<span style="color:#f92672">.</span>num_states, np<span style="color:#f92672">.</span>inf)
        self<span style="color:#f92672">.</span>Q <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>zeros(A<span style="color:#f92672">.</span>num_actions)

    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">run</span>(self):
        <span style="color:#66d9ef">for</span> _ <span style="color:#f92672">in</span> range(self<span style="color:#f92672">.</span>max_sweeps):
            self<span style="color:#f92672">.</span>Q[:] <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>A<span style="color:#f92672">.</span>reward <span style="color:#f92672">+</span> self<span style="color:#f92672">.</span>gamma <span style="color:#f92672">*</span> self<span style="color:#f92672">.</span>V[self<span style="color:#f92672">.</span>A<span style="color:#f92672">.</span>action2next_state]
            <span style="color:#66d9ef">for</span> state_id <span style="color:#f92672">in</span> range(self<span style="color:#f92672">.</span>A<span style="color:#f92672">.</span>num_states):
                start, end <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>A<span style="color:#f92672">.</span>state2action_start[state_id], self<span style="color:#f92672">.</span>A<span style="color:#f92672">.</span>state2action_start[state_id <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span>]
                v_new <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>Q[start :end]<span style="color:#f92672">.</span>max()
                self<span style="color:#f92672">.</span>TD[state_id] <span style="color:#f92672">=</span> abs(self<span style="color:#f92672">.</span>V[state_id]<span style="color:#f92672">-</span>v_new)
                self<span style="color:#f92672">.</span>V[state_id] <span style="color:#f92672">=</span> v_new

            <span style="color:#66d9ef">if</span> (self<span style="color:#f92672">.</span>TD <span style="color:#f92672">&lt;</span>self<span style="color:#f92672">.</span>threshold)<span style="color:#f92672">.</span>all():
                <span style="color:#66d9ef">return</span> self<span style="color:#f92672">.</span>V
        <span style="color:#66d9ef">raise</span> NonConvergenceError
</code></pre></div><pre><code>%timeit V = ArraySyncDP(A, gamma, max_sweeps, threshold).run()
3.5 s ± 99.1 ms per loop (mean ± std.dev. of 7 runs, 1 loop each)
</code></pre><p>Slightly faster than Prioritized Sweeping, but less effective. It seems that the main reason is that we have not been able to vectorize everything and still use for minutes in updating the state value.</p>
<h3 id="cython">Cython</h3>
<p>Originally, compared to a list that refers to objects scattered in memory, an array holds basic data types in a continuous area in memory, so it should be possible to access it at high speed. However, it seems that in Python, the element access is slower than that of a list or a dictionary because the conversion to a Python object is performed to refer to each element of the array, which causes overhead.</p>
<p>Therefore, consider using Cython to speed up array access. Cython is a compiler that converts Python with type annotations into a compiled extension. The converted extension module can be loaded by import as with a normal Python module. If you use Cython, it seems that you can speed up the process of element access using the numpy array as an interface for the following reasons.</p>
<ul>
<li>When processing an array in sequence, since the memory area is continuous, you can instruct the compiler to directly obtain the address of the next element without asking Python for the address of the next element.</li>
<li>numpy array can be accessed at low level as an object that implements a generic buffer interface called memoryview</li>
<li>Therefore, the memory area can be easily shared with C libraries, eliminating the need to convert Python objects to other formats</li>
</ul>
<p>Implement asynchronous DP in Cython. I would like to use a priority queue, but since processing of Python objects occurs, only the state value update with a small TD error is skipped.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">%%</span>cython
<span style="color:#f92672">import</span> numpy <span style="color:#f92672">as</span> np
cimport numpy <span style="color:#66d9ef">as</span> np
cimport cython

ctypedef np<span style="color:#f92672">.</span>float64_t FLOAT_t
ctypedef np<span style="color:#f92672">.</span>int64_t INT_t

<span style="color:#a6e22e">@cython.boundscheck</span>(False)
<span style="color:#a6e22e">@cython.wraparound</span>(False)
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">cythonic_backup</span>(
    FLOAT_t[:] V, FLOAT_t[:] Q, FLOAT_t[:] TD, FLOAT_t[:] reward,
    INT_t[:] state2action_start, INT_t[:] action2next_state,
    INT_t[:] next_state2inv_action, INT_t[:, :] inv_action2state_action,
    FLOAT_t gamma, FLOAT_t threshold
):
    cdef INT_t num_updates, state, action, next_state, inv_action, start_inv_action, end_inv_action, start_action, end_action
    cdef FLOAT_t v
    num_updates <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
    <span style="color:#66d9ef">for</span> next_state <span style="color:#f92672">in</span> range(len(V)):
        <span style="color:#66d9ef">if</span> TD[next_state] <span style="color:#f92672">&lt;</span>threshold:
            <span style="color:#66d9ef">continue</span>
            
        num_updates <span style="color:#f92672">+=</span> <span style="color:#ae81ff">1</span>
        TD[next_state] <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
        start_inv_action <span style="color:#f92672">=</span> next_state2inv_action[next_state]
        end_inv_action <span style="color:#f92672">=</span> next_state2inv_action[next_state <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span>]
        <span style="color:#66d9ef">for</span> inv_action <span style="color:#f92672">in</span> range(start_inv_action, end_inv_action):
            state <span style="color:#f92672">=</span> inv_action2state_action[inv_action][<span style="color:#ae81ff">0</span>]
            action <span style="color:#f92672">=</span> inv_action2state_action[inv_action][<span style="color:#ae81ff">1</span>]
            Q[action] <span style="color:#f92672">=</span> reward[action] <span style="color:#f92672">+</span> gamma <span style="color:#f92672">*</span> V[next_state]
            start_action <span style="color:#f92672">=</span> state2action_start[state]end_action <span style="color:#f92672">=</span> state2action_start[state <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span>]
            v <span style="color:#f92672">=</span> <span style="color:#f92672">-</span><span style="color:#ae81ff">1e9</span>
            <span style="color:#66d9ef">for</span> action <span style="color:#f92672">in</span> range(start_action, end_action):
                <span style="color:#66d9ef">if</span> v <span style="color:#f92672">&lt;</span> Q[action]:
                    v <span style="color:#f92672">=</span> Q[action]
            <span style="color:#66d9ef">if</span> v <span style="color:#f92672">&gt;</span> V[state]:
                TD[state] <span style="color:#f92672">+=</span> v <span style="color:#f92672">-</span> V[state]
            <span style="color:#66d9ef">else</span>:
                TD[state] <span style="color:#f92672">+=</span> V[state] <span style="color:#f92672">-</span> v
            V[state] <span style="color:#f92672">=</span> v
    <span style="color:#66d9ef">return</span> num_updates
</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">CythonicAsyncDP</span>(ArraySyncDP):
    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">run</span>(self):
        A <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>A
        <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(self<span style="color:#f92672">.</span>max_sweeps):
            num_updates <span style="color:#f92672">=</span> cythonic_backup(
                self<span style="color:#f92672">.</span>V, self<span style="color:#f92672">.</span>Q, self<span style="color:#f92672">.</span>TD, A<span style="color:#f92672">.</span>reward, A<span style="color:#f92672">.</span>state2action_start, A<span style="color:#f92672">.</span>action2next_state,
                A<span style="color:#f92672">.</span>next_state2inv_action, A<span style="color:#f92672">.</span>inv_action2state_action, self<span style="color:#f92672">.</span>gamma, self<span style="color:#f92672">.</span>threshold
            )
            <span style="color:#66d9ef">if</span> num_updates <span style="color:#f92672">==</span> <span style="color:#ae81ff">0</span>:
                <span style="color:#66d9ef">return</span> self<span style="color:#f92672">.</span>V
        <span style="color:#66d9ef">raise</span> NonConvergenceError
</code></pre></div><pre><code>%timeit V = CythonicAsyncDP(A, gamma, max_sweeps, threshold).run()
18.6 ms ± 947 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
</code></pre><p><strong>18.6ms</strong> と劇的な高速化が達成できました。この処理速度であれば、利用用途にもよりますが、状態数が100万程度の問題でも十分対応可能です。</p>
<h1 id="参考文献">参考文献</h1>
<p>[1] ハイパフォーマンスPython, Micha Gorelick and Ian Ozsvald, オライリージャパン, 2015.
[2] Reinforcement Learning, R. S. Sutton and A. G. Barto, The MIT Press, 2018.</p>

    </article>
  </div>

  
  
  
  <script>
  window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
  ga('create', 'UA-123456789-1', 'auto');
  ga('send', 'pageview');
  </script>
  <script async src='https://www.google-analytics.com/analytics.js'></script>
  

  

  
<link rel="stylesheet" type="text/css" href="//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.0/cookieconsent.min.css" />
<script src="//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.0/cookieconsent.min.js"></script>
<script>
window.addEventListener("load", function(){
window.cookieconsent.initialise({
  "palette": {
    "popup": {
      "background": "#216942",
      "text": "#b2d192"
    },
    "button": {
      "background": "#afed71"
    }
  }
})});
</script>

</body>

</html>
