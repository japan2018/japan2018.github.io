<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title> albert on Memo Tut</title>
    <link>https://memotut.com/tags/albert/</link>
    <description>Recent content in  albert on Memo Tut</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 17 Feb 2020 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://memotut.com/tags/albert/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>[Python] Introduction of ALBERT (using MeCab&#43;Sentencepiece) model that learned large-scale Japanese business news corpus</title>
      <link>https://memotut.com/b41dcf018/</link>
      <pubDate>Mon, 17 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>https://memotut.com/b41dcf018/</guid>
      <description>#Introduction  Previously, Japanese BERT pre-learned modelandXLNetpre-learnedmodel It is the stockmark Morinaga who posted the introduction articles such as. Thank you for reading the article on model release to many people.
This time, we will release the pre-learned Japanese model of ALBERT. Now, while various pre-learned models have been proposed, why do you publish ALBERT Japanese model?ALBERT is not just SOTA as described as A Lite BERT , Because it is a model that makes the BERT lighter while maintaining and improving accuracy.</description>
    </item>
    
  </channel>
</rss>