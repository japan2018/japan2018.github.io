<!doctype html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  
  <meta name="generator" content="Hugo 0.72.0" />

  
  <meta name="description" content="Some description">
  

  
  <link rel="apple-touch-icon" sizes="180x180" href="https://memotut.com/apple-touch-icon.png">

  
  <link rel="icon" type="image/png" sizes="32x32" href="https://memotut.com/favicon-32x32.png">

  
  <link rel="icon" type="image/png" sizes="16x16" href="https://memotut.com/favicon-16x16.png">

  
  <link rel="manifest" href="https://memotut.com/site.webmanifest">

  
  <link rel="mask-icon" href="https://memotut.com/safari-pinned-tab.svg" color="#5bbad5">

  <meta name="msapplication-TileColor" content="#da532c">

  <meta name="theme-color" content="#ffffff">

  
  <link rel="stylesheet" href="https://memotut.com/css/bootstrap.min.css" />

  
  <title>Deep learning of Raspberry Pi 3 B&#43; &amp; PyTorch for real-time classification of multiple objects in camera images | Memo Tut</title>
  

  <style>
body {
  min-width: 300px;
}

.custom-navbar {
  margin-bottom: 1em;
  height: 60px;
}

.custom-navbar a {
  display: inline-block; 
  padding: 18px 0;
  margin-right: 1em; 
  font-weight: bold; 
}

.custom-navbar a:hover,
.custom-navbar a:focus {
  text-decoration: none; 
}

@media print {
  .custom-navbar {
    display: none;
  }
}

article {
  padding-bottom: 1em;
}

img {
  max-width: 100%;
}


body {
  background-color: #fff;
}



body {
  color: #212529;
}



a {
  color: #007bff;
}



a:hover,
a:focus {
  color: #0056b3;
}



.custom-navbar {
  background-color: #212529;
}



.custom-navbar a {
  color: rgba(255, 255, 255, 0.75);
}



.custom-navbar a:hover,
.custom-navbar a:focus {
  color: rgba(255, 255, 255, 1);
}



.container {
  max-width: 800px;
}



pre {
  display: block;
  padding: 9.5px;
  word-break: break-all;
  word-wrap: break-word;
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 4px;
}

pre code {
  padding: 0;
  font-size: inherit;
  color: inherit; 
  white-space: pre-wrap;
  background-color: transparent;
  border: none;
  border-radius: 0;
}

code {
  padding: 2px 4px;
  color: inherit; 
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 4px;
  font-size: .9em;
}



blockquote,
.blockquote {
  padding: 10px 20px;
  margin: 0 0 20px;
  font-size: 1em;
  border-left: 5px solid #6c757d;
}

</style>
</head>

<body>
  <nav class="custom-navbar">
  <div class="container">
    
    <a href="/">Posts</a>
    
    <a href="/tags/">Tags</a>
    
    <a href="/about/">About</a>
    
    <a href="/index.xml">RSS</a>
    
  </div>
</nav>
  
  <div class="container">
    <article>
      <h1>Deep learning of Raspberry Pi 3 B+ &amp; PyTorch for real-time classification of multiple objects in camera images</h1>
<p>
  <small class="text-secondary">
  
  
  Mar 29, 2020
  </small>
  

<small><code><a href="https://memotut.com/tags/python">Python</a></code></small>


<small><code><a href="https://memotut.com/tags/opencv">OpenCV</a></code></small>


<small><code><a href="https://memotut.com/tags/raspberrypi">RaspberryPi</a></code></small>


<small><code><a href="https://memotut.com/tags/deeplearning">DeepLearning</a></code></small>


<small><code><a href="https://memotut.com/tags/pytorch">PyTorch</a></code></small>

</p>
<pre><code>I got Razpai 3B+ and picamera because of university class. Since I'm free, I decided to let Razpai classify using deep learning. However, instead of classifying the pictures taken in advance, it classifies the objects in the real-time image from picamera and displays them in a nice way.
</code></pre>
<p>It may be at the student level, but I&rsquo;d appreciate it if you can refer to even a part of it.</p>
<h1 id="what-you-envision">What you envision</h1>
<p>I thought that I would try to create &ldquo;a function to classify and display ** in real time when multiple personal items are placed within the fixed picamera&rsquo;s field of view&rdquo; in Razpi.</p>
<p>Specifically, the object is extracted by <strong>background difference</strong> (a method of extracting the changed part from the background image), and deep learning is performed by <strong>PyTorch [Pitoch]</strong> (similar to Keras, TensorFlow). Take the policy of classifying over.</p>
<p>** (* YOLO, SSD etc. are not handled!) **</p>
<p>So I implemented it in the next step.</p>
<p><em>Step1</em>: Prepare your own training data
<em>Step2</em>: Build and train a neural network
<em>Step3</em>: Implement the mechanism to extract the object from the picamera image and display the result classified by the learned parameter</p>
<p>Razpai is slow in processing, so I tried to do <strong>learning on my own PC and use the obtained parameter file to classify on a raspapi</strong>. So I put PyTorch on my PC and Raspberry Pi.</p>
<p>The following is a series of steps.
Make a note of where you struggled with the <strong>[⚠Note]</strong> symbol.</p>
<h1 id="first-preparation">First preparation</h1>
<p>Set up the execution environment on the PC and Raspberry Pi.</p>
<h2 id="execution-environment">Execution environment</h2>
<p>The version of the same package is different between PC and Razz pie, but don&rsquo;t worry.
My own PC is for learning.</p>
<h3 id="own-pc-windows-10">Own PC (Windows 10)</h3>
<ul>
<li>Python 3.6.4</li>
<li>PyTorch 1.4.0+cpu</li>
<li>Torchvision 0.5.0+cpu</li>
</ul>
<p>** * Torchvision ** is a set with PyTorch and is a library used for ** image pre-processing and data set creation **.</p>
<h3 id="raspberry-pi-3-model-b-raspbian-stretch">Raspberry Pi 3 Model B+ (Raspbian Stretch)</h3>
<ul>
<li>Python 3.5.3</li>
<li>PyTorch 1.3.0+cpu</li>
<li>Torchvision 0.5.0+cpu</li>
<li>OpenCV 3.4.7</li>
</ul>
<p>I used <strong>Raspberry Pi Camera Module V2</strong> for the camera that plugs into the Raspberry Pi.
In addition, I put <a href="https://www.realvnc.com/en/connect/download/viewer/">VNC Viewer</a> in my PC and operated Raspberry Pi with <strong>SSH connection</strong>.</p>
<h2 id="construction-method">Construction method</h2>
<p>Put the above version of the package on each computer.
I will omit the details, but I have referred to the site around the link.</p>
<h3 id="pytorch--torchvision">PyTorch / Torchvision</h3>
<p>For <strong>PC</strong>, select the environment from <a href="https://pytorch.org/">PyTorch official</a> and install.</p>
<p>** [⚠Note] **GPU cannot be used unless NVIDIA product is included, so those who have &ldquo;intel&rdquo; select <strong>CUDA → None</strong> (usually use CPU).</p>
<p>For <strong>Raspberry</strong>, <a href="https://qiita.com/yyojiro/items/d91b02149aa6480ded80">&ldquo;PyTorch v1.3.0 is put in Raspberry Pi 3&rdquo;</a>and[&ldquo;PyTorchDeepLearningFrameworkissourcecodeforRaspberryPi.Howtobuildfrom]&quot;](<a href="http://www.neko.ne.jp/~freewing/raspberry_pi/raspberry_pi_build_pytorch_deep_learning_framework/">http://www.neko.ne.jp/~freewing/raspberry_pi/raspberry_pi_build_pytorch_deep_learning_framework/</a>) Thank you for the build.</p>
<p><strong>[⚠Note]</strong> Specify the version as <code>git clone ~~~ -b v1.3.0</code> etc.
** 【⚠ Note】 ** In PyTorch 1.4.0, <code>fatal error: immintrin.h</code> does not exist, so the build stopped at about 80%. Mystery. (2020/3/20)</p>
<h3 id="opencv">OpenCV</h3>
<p>[Install OpenCV 3 on Raspberry Pi + Python 3 as easily as possible]](<a href="https://qiita.com/masaru/items/658b24b0806144cfeb1c">https://qiita.com/masaru/items/658b24b0806144cfeb1c</a>) etc. and install it on <strong>Raspian</strong>.</p>
<p>Both take several hours to build &hellip;</p>
<h1 id="implement-immediately">Implement immediately</h1>
<p>After repeated trial and error, I created a Python script.</p>
<h2 id="step1-create-the-training-data-of-the-image-used-for-learning-by-yourself"><em>Step1</em>: Create the training data of the image used for learning by yourself</h2>
<p>We have created image data of personal items used for learning.
It is supposed to be used by attaching the picamera to the Razz pie and fixing it so that the picamera does not move **.</p>
<h3 id="program-to-create">Program to create</h3>
<p>Press the &ldquo;r&rdquo; key to rotate the screen, then press &ldquo;p&rdquo; to capture the background without displaying anything.
If you put a personal item you want to shoot and shoot it again with &ldquo;p&rdquo;, it subtracts the background and saves the picture of the <strong>green frame part</strong>.</p>
<p>This time, I&rsquo;m going to classify the &ldquo;certain Phone&rdquo;, &ldquo;wrist watch&rdquo;, and &ldquo;wallet&rdquo; **, so I&rsquo;ll just take those three pictures.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-take_photo.py" data-lang="take_photo.py"><span style="color:#75715e"># coding: utf-8</span>
<span style="color:#f92672">import</span> cv2
<span style="color:#f92672">from</span> datetime <span style="color:#f92672">import</span> datetime
<span style="color:#f92672">import</span> picamera
<span style="color:#f92672">import</span> picamera.array

MIN_LEN <span style="color:#f92672">=</span> <span style="color:#ae81ff">50</span> <span style="color:#75715e"># Minimum length of one side of object detection frame</span>
GRAY_THR <span style="color:#f92672">=</span> <span style="color:#ae81ff">20</span> <span style="color:#75715e"># Concentration change threshold</span>
CUT_MODE <span style="color:#f92672">=</span> True <span style="color:#75715e"># True: Cut the detected object and save it, False: Save the entire image as it is</span>


<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">imshow_rect</span>(img, contour, minlen<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>):
<span style="color:#e6db74">&#34;&#34;&#34;
</span><span style="color:#e6db74">Enclose all the object detection points in the acquired image with a square frame
</span><span style="color:#e6db74">argument:
</span><span style="color:#e6db74">    img: camera image
</span><span style="color:#e6db74">    contour: contour
</span><span style="color:#e6db74">    minlen: Threshold of detection size (excluding points where one side of the frame is shorter than this)
</span><span style="color:#e6db74">&#34;&#34;&#34;</span>
    <span style="color:#66d9ef">for</span> pt <span style="color:#f92672">in</span> contour:
        x, y, w, h <span style="color:#f92672">=</span> cv2<span style="color:#f92672">.</span>boundingRect(pt)
        <span style="color:#66d9ef">if</span> w <span style="color:#f92672">&lt;</span>minlen <span style="color:#f92672">and</span> h <span style="color:#f92672">&lt;</span>minlen: <span style="color:#66d9ef">continue</span>
        cv2<span style="color:#f92672">.</span>rectangle(img, (x, y), (x<span style="color:#f92672">+</span>w, y<span style="color:#f92672">+</span>h), (<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">255</span>, <span style="color:#ae81ff">0</span>), <span style="color:#ae81ff">2</span>)
    cv2<span style="color:#f92672">.</span>imshow(<span style="color:#e6db74">&#39;Preview&#39;</span>, img)


<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">save_cutimg</span>(img, contour, minlen<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>):
<span style="color:#e6db74">&#34;&#34;&#34;
</span><span style="color:#e6db74">Cut out and save all the object detection points in the acquired image
</span><span style="color:#e6db74">argument:
</span><span style="color:#e6db74">    Same as above
</span><span style="color:#e6db74">&#34;&#34;&#34;</span>
    <span style="color:#75715e"># Get date and time and use for file name</span>
    dt <span style="color:#f92672">=</span> datetime<span style="color:#f92672">.</span>now()
    f_name <span style="color:#f92672">=</span><span style="color:#e6db74">&#39;{}.jpg&#39;</span><span style="color:#f92672">.</span>format(dt<span style="color:#f92672">.</span>strftime(<span style="color:#e6db74">&#39;%y%m</span><span style="color:#e6db74">%d</span><span style="color:#e6db74">%H%M%S&#39;</span>))
    imgs_cut <span style="color:#f92672">=</span> []
    <span style="color:#66d9ef">for</span> pt <span style="color:#f92672">in</span> contour:
        x, y, w, h <span style="color:#f92672">=</span> cv2<span style="color:#f92672">.</span>boundingRect(pt)
        <span style="color:#66d9ef">if</span> w <span style="color:#f92672">&lt;</span>minlen <span style="color:#f92672">and</span> h <span style="color:#f92672">&lt;</span>minlen: <span style="color:#66d9ef">continue</span>
        imgs_cut<span style="color:#f92672">.</span>append(img[y:y<span style="color:#f92672">+</span>h, x:x<span style="color:#f92672">+</span>w])

    <span style="color:#75715e"># Cut out objects and save</span>
    <span style="color:#66d9ef">if</span> <span style="color:#f92672">not</span> imgs_cut: <span style="color:#66d9ef">return</span> <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>
    <span style="color:#66d9ef">if</span> len(imgs_cut) <span style="color:#f92672">&gt;</span><span style="color:#ae81ff">1</span>:
        <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(len(imgs_cut)):
            cv2<span style="color:#f92672">.</span>imwrite(f_name[:<span style="color:#f92672">-</span><span style="color:#ae81ff">4</span>]<span style="color:#f92672">+</span><span style="color:#e6db74">&#39;_&#39;</span><span style="color:#f92672">+</span>str(i<span style="color:#f92672">+</span><span style="color:#ae81ff">1</span>)<span style="color:#f92672">+</span>f_name[<span style="color:#f92672">-</span><span style="color:#ae81ff">4</span>:], imgs_cut[i])
    <span style="color:#66d9ef">else</span>:
        cv2<span style="color:#f92672">.</span>imwrite(f_name, imgs_cut[<span style="color:#ae81ff">0</span>])
    <span style="color:#66d9ef">return</span> len(imgs_cut)


<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">save_img</span>(img):
<span style="color:#e6db74">&#34;&#34;&#34;
</span><span style="color:#e6db74">Save acquired image as it is
</span><span style="color:#e6db74">argument:
</span><span style="color:#e6db74">    Same as above
</span><span style="color:#e6db74">&#34;&#34;&#34;</span>
    dt <span style="color:#f92672">=</span> datetime<span style="color:#f92672">.</span>now()
    fname <span style="color:#f92672">=</span><span style="color:#e6db74">&#39;{}.jpg&#39;</span><span style="color:#f92672">.</span>format(dt<span style="color:#f92672">.</span>strftime(<span style="color:#e6db74">&#39;%y%m</span><span style="color:#e6db74">%d</span><span style="color:#e6db74">%H%M%S&#39;</span>))
    cv2<span style="color:#f92672">.</span>imwrite(fname, img)


<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">take_photo</span>():
<span style="color:#e6db74">&#34;&#34;&#34;
</span><span style="color:#e6db74">Background photography -&gt; object photography, save
</span><span style="color:#e6db74">Key input: 
</span><span style="color:#e6db74">    &#34;p&#34;: take a picture
</span><span style="color:#e6db74">    &#34;q&#34;: stop
</span><span style="color:#e6db74">    &#34;r&#34;: Rotate screen (when shooting background)
</span><span style="color:#e6db74">    &#34;i&#34;: Start over (when shooting an object)
</span><span style="color:#e6db74">&#34;&#34;&#34;</span>
    cnt <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
    <span style="color:#75715e"># start picamera</span>
    <span style="color:#66d9ef">with</span> picamera<span style="color:#f92672">.</span>PiCamera() <span style="color:#66d9ef">as</span> camera:
        camera<span style="color:#f92672">.</span>resolution <span style="color:#f92672">=</span> (<span style="color:#ae81ff">480</span>, <span style="color:#ae81ff">480</span>) <span style="color:#75715e"># resolution</span>
        camera<span style="color:#f92672">.</span>rotation <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span> <span style="color:#75715e"># camera rotation (degrees)</span>
        <span style="color:#75715e"># Start streaming</span>
        <span style="color:#66d9ef">with</span> picamera<span style="color:#f92672">.</span>array<span style="color:#f92672">.</span>PiRGBArray(camera) <span style="color:#66d9ef">as</span> stream:
            <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#39;Set background ...&#39;</span>, end<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;&#39;</span>, flush<span style="color:#f92672">=</span>True)
            <span style="color:#75715e"># Shoot the background at the beginning</span>
            <span style="color:#66d9ef">while</span> True:
                <span style="color:#75715e"># Get and display streaming images</span>
                camera<span style="color:#f92672">.</span>capture(stream,<span style="color:#e6db74">&#39;bgr&#39;</span>, use_video_port<span style="color:#f92672">=</span>True)
                cv2<span style="color:#f92672">.</span>imshow(<span style="color:#e6db74">&#39;Preview&#39;</span>, stream<span style="color:#f92672">.</span>array)

                wkey <span style="color:#f92672">=</span> cv2<span style="color:#f92672">.</span>waitKey(<span style="color:#ae81ff">5</span>) <span style="color:#f92672">&amp;</span> <span style="color:#ae81ff">0xFF</span> <span style="color:#75715e"># Accept key input</span>

                stream<span style="color:#f92672">.</span>seek(<span style="color:#ae81ff">0</span>) <span style="color:#75715e"># New capture spell x2</span>
                stream<span style="color:#f92672">.</span>truncate()

                <span style="color:#66d9ef">if</span> wkey <span style="color:#f92672">==</span> ord(<span style="color:#e6db74">&#39;q&#39;</span>):
                    cv2<span style="color:#f92672">.</span>destroyAllWindows()
                    <span style="color:#66d9ef">return</span> <span style="color:#66d9ef">print</span>()
                <span style="color:#66d9ef">elif</span> wkey <span style="color:#f92672">==</span> ord(<span style="color:#e6db74">&#39;r&#39;</span>):
                    camera<span style="color:#f92672">.</span>rotation <span style="color:#f92672">+=</span> <span style="color:#ae81ff">90</span>
                <span style="color:#66d9ef">elif</span> wkey <span style="color:#f92672">==</span> ord(<span style="color:#e6db74">&#39;p&#39;</span>):
                    camera<span style="color:#f92672">.</span>exposure_mode <span style="color:#f92672">=</span><span style="color:#e6db74">&#39;off&#39;</span> <span style="color:#75715e"># White balance fixed</span>
                    save_img(stream<span style="color:#f92672">.</span>array)
                    <span style="color:#75715e"># Grayscale and set as background image</span>
                    back_gray <span style="color:#f92672">=</span> cv2<span style="color:#f92672">.</span>cvtColor(stream<span style="color:#f92672">.</span>array,(cv2<span style="color:#f92672">.</span>COLOR_BGR2GRAY)
                    <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#39;done&#39;</span>)
                    <span style="color:#66d9ef">break</span>

            <span style="color:#75715e"># After setting the background, shoot the object without moving the camera</span>
            <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#39;Take photos!&#39;</span>)
            <span style="color:#66d9ef">while</span> True:
                camera<span style="color:#f92672">.</span>capture(stream,<span style="color:#e6db74">&#39;bgr&#39;</span>, use_video_port<span style="color:#f92672">=</span>True)
                <span style="color:#75715e"># Make the current frame grayscale</span>
                stream_gray <span style="color:#f92672">=</span> cv2<span style="color:#f92672">.</span>cvtColor(stream<span style="color:#f92672">.</span>array,
                                           (cv2<span style="color:#f92672">.</span>COLOR_BGR2GRAY)

                <span style="color:#75715e"># Calculate absolute value of difference, binarize, create mask</span>
                diff <span style="color:#f92672">=</span> cv2<span style="color:#f92672">.</span>absdiff(stream_gray, back_gray)
                mask <span style="color:#f92672">=</span> cv2<span style="color:#f92672">.</span>threshold(diff, GRAY_THR, <span style="color:#ae81ff">255</span>,
                                     cv2<span style="color:#f92672">.</span>THRESH_BINARY)[<span style="color:#ae81ff">1</span>]
                cv2<span style="color:#f92672">.</span>imshow(<span style="color:#e6db74">&#39;mask&#39;</span>, mask)

                <span style="color:#75715e"># Contour and mask creation for object detection</span>
                contour <span style="color:#f92672">=</span> cv2<span style="color:#f92672">.</span>findContours(mask,
                                           cv2<span style="color:#f92672">.</span>RETR_EXTERNAL,
                                           cv2<span style="color:#f92672">.</span>CHAIN_APPROX_SIMPLE)[<span style="color:#ae81ff">1</span>]

                <span style="color:#75715e"># All detected objects are displayed in a box</span>
                stream_arr <span style="color:#f92672">=</span> stream<span style="color:#f92672">.</span>array<span style="color:#f92672">.</span>copy()
                imshow_rect(stream_arr, contour, MIN_LEN)

                wkey <span style="color:#f92672">=</span> cv2<span style="color:#f92672">.</span>waitKey(<span style="color:#ae81ff">5</span>) <span style="color:#f92672">&amp;</span> <span style="color:#ae81ff">0xFF</span>

                stream<span style="color:#f92672">.</span>seek(<span style="color:#ae81ff">0</span>)
                stream<span style="color:#f92672">.</span>truncate()

                <span style="color:#66d9ef">if</span> wkey <span style="color:#f92672">==</span> ord(<span style="color:#e6db74">&#39;q&#39;</span>):
                    cv2<span style="color:#f92672">.</span>destroyAllWindows()
                    <span style="color:#66d9ef">return</span>
                <span style="color:#66d9ef">elif</span> wkey <span style="color:#f92672">==</span> ord(<span style="color:#e6db74">&#39;i&#39;</span>):
                    <span style="color:#66d9ef">break</span>
                <span style="color:#66d9ef">elif</span> wkey <span style="color:#f92672">==</span> ord(<span style="color:#e6db74">&#39;p&#39;</span>):
                    <span style="color:#66d9ef">if</span> CUT_MODE:
                        num <span style="color:#f92672">=</span> save_cutimg(stream<span style="color:#f92672">.</span>array, contour, MIN_LEN)
                        <span style="color:#66d9ef">if</span> num<span style="color:#f92672">&gt;</span> <span style="color:#ae81ff">0</span>:
                            cnt <span style="color:#f92672">+=</span> num
                            <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#39; Captured: {} (sum: {})&#39;</span><span style="color:#f92672">.</span>format(num, cnt))
                    <span style="color:#66d9ef">else</span>:
                        save_img(stream<span style="color:#f92672">.</span>array)
                        cnt <span style="color:#f92672">+=</span> <span style="color:#ae81ff">1</span>
                        <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#39; Captured: 1 (sum: {})&#39;</span><span style="color:#f92672">.</span>format(cnt))

    <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#39;Initialized&#39;</span>)
    take_photo()


<span style="color:#66d9ef">if</span> __name__ <span style="color:#f92672">==</span><span style="color:#e6db74">&#39;__main__&#39;</span>:
    take_photo()

</code></pre></div><h3 id="run">run</h3>
<p>I&rsquo;m just taking pictures.
The cropped image for each green frame is saved like this.</p>
<p><img width="25%" alt="200328174638.jpg" src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/605747/9f138aad-74cf-3a38-e1de-9ed4ab48de70.jpeg"> ➡ <img width="15%" alt="200328174642_1.jpg" src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/605747/3fa5b27f-2b02-dad7-8b70-ce11f5a55210.jpeg"> ** &amp; ** <img width="15%" alt="200328174642_2.jpg" src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/605747/4f6b2008-f663-b58f-78fe-93ede6611eff.jpeg"> ** &amp; ** <img width="15%" alt="200328174642_3.jpg" src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/605747/a3a63bb0-7214-029b-bca6-14a80e4c5147.jpeg"></p>
<p>** [⚠Note] If you have too few photos, you will not learn well. **
I took more than 50 photos for each class for training data, but I wonder if it is still &hellip;
For the time being, various noises are added during learning, and the amount of data increases.</p>
<p>Put the photos in a folder and use Slack or something to transfer them to your PC**. (Semi-analog)
And store the photos of each personal item ** in the folder structure below. **</p>
<pre><code>image_data
├─train
│ ├─phone
│ │ 191227013419.jpg
│ │ 191227013424.jpg
│ │:
│ ├─wallet
│ │ 191227013300.jpg
│ │ 191227013308.jpg
│ │:
│ └─watch
│ 191227013345.jpg
│ 191227013351.jpg
|:
└─val
    ├─phone
    │ 191227013441.jpg
    │ 191227013448.jpg
    |:
    ├─wallet
    │ 191227013323.jpg
    │ 191227013327.jpg
    |:
    └─watch
            191227013355.jpg
            191227013400.jpg
                   :
</code></pre><h2 id="step2-deep-learning-with-pytorch-on-pc"><em>Step2</em>: Deep learning with PyTorch on PC</h2>
<p>Build a network and train with the image above.</p>
<h3 id="program-to-create-1">Program to create</h3>
<p>When executed, it reads the image from the previous folder and starts learning, outputs the progress file, the loss and accuracy transition diagram, and the final parameter file.</p>
<p>In creating it, I made a lot of reference to <a href="https://www.shuwasystem.co.jp/book/9784798055473.html">&ldquo;PyTorch Neural Network Implementation Handbook&rdquo; (Shuwa System)</a>.</p>
<p>Even if you interrupt with &ldquo;Ctrl+C&rdquo;, the learning progress up to that point is saved as <strong>&ldquo;train_process.ckpt&rdquo;</strong>, and you can continue learning from the next time you run.
You can change the hyper parameter on the way.</p>
<p>By the way, torchvsion&rsquo;s <strong>ImageFolder</strong> creates a dataset with the folder name containing the photos as the class name. Easy! !
The photos in the train folder are used for learning, and the photos in the val folder are used for evaluation.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-train_net.py" data-lang="train_net.py"><span style="color:#75715e"># coding: utf-8</span>
<span style="color:#f92672">import</span> os
<span style="color:#f92672">import</span> re
<span style="color:#f92672">import</span> torch.nn <span style="color:#f92672">as</span> nn
<span style="color:#f92672">import</span> torch.optim <span style="color:#f92672">as</span> optim
<span style="color:#f92672">import</span> torch.utils
<span style="color:#f92672">from</span> torchvision <span style="color:#f92672">import</span> datasets, transforms
<span style="color:#f92672">import</span> matplotlib.pyplot <span style="color:#f92672">as</span> plt

DATA_DIR <span style="color:#f92672">=</span><span style="color:#e6db74">&#39;image_data&#39;</span> <span style="color:#75715e"># Image folder name</span>
CKPT_PROCESS <span style="color:#f92672">=</span><span style="color:#e6db74">&#39;train_process.ckpt&#39;</span> <span style="color:#75715e"># learning progress save file name</span>
CKPT_NET <span style="color:#f92672">=</span><span style="color:#e6db74">&#39;trained_net.ckpt&#39;</span> <span style="color:#75715e"># Trained parameter file name</span>
NUM_CLASSES <span style="color:#f92672">=</span> <span style="color:#ae81ff">3</span> <span style="color:#75715e"># number of classes</span>
NUM_EPOCHS <span style="color:#f92672">=</span> <span style="color:#ae81ff">100</span> <span style="color:#75715e"># Number of learnings</span>

<span style="color:#75715e"># Hyperparameters that change often</span>
LEARNING_RATE <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.001</span> <span style="color:#75715e"># learning rate</span>
MOMENTUM <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.5</span> <span style="color:#75715e"># inertia</span>

checkpoint <span style="color:#f92672">=</span> {} <span style="color:#75715e"># Variable for saving progress</span>


<span style="color:#75715e"># Image data conversion definition (bulk)</span>
The size of <span style="color:#75715e">#Resize is related to the first Linear input size of classifier</span>
data_transforms <span style="color:#f92672">=</span> transforms<span style="color:#f92672">.</span>Compose((
    transforms<span style="color:#f92672">.</span>Resize((<span style="color:#ae81ff">112</span>, <span style="color:#ae81ff">112</span>)), <span style="color:#75715e"># resize</span>
    transforms<span style="color:#f92672">.</span>RandomRotation(<span style="color:#ae81ff">30</span>), <span style="color:#75715e"># Random rotation</span>
    transforms<span style="color:#f92672">.</span>Grayscale(), <span style="color:#75715e">#binarization</span>
    transforms<span style="color:#f92672">.</span>ToTensor(), <span style="color:#75715e"># tensorization</span>
    transforms<span style="color:#f92672">.</span>Normalize(mean<span style="color:#f92672">=</span>[<span style="color:#ae81ff">0.5</span>], std<span style="color:#f92672">=</span>[<span style="color:#ae81ff">0.5</span>]) <span style="color:#75715e"># normalization (numbers are text)</span>
])

val_transforms <span style="color:#f92672">=</span> transforms<span style="color:#f92672">.</span>Compose((
    transforms<span style="color:#f92672">.</span>Resize((<span style="color:#ae81ff">112</span>, <span style="color:#ae81ff">112</span>)),
    transforms<span style="color:#f92672">.</span>Grayscale(),
    transforms<span style="color:#f92672">.</span>ToTensor(),
    transforms<span style="color:#f92672">.</span>Normalize(mean<span style="color:#f92672">=</span>[<span style="color:#ae81ff">0.5</span>], std<span style="color:#f92672">=</span>[<span style="color:#ae81ff">0.5</span>])
])

<span style="color:#75715e">#Create dataset</span>
train_dataset <span style="color:#f92672">=</span> datasets<span style="color:#f92672">.</span>ImageFolder(
    root<span style="color:#f92672">=</span>os<span style="color:#f92672">.</span>path<span style="color:#f92672">.</span>join(DATA_DIR,<span style="color:#e6db74">&#39;train&#39;</span>),
    transform<span style="color:#f92672">=</span>train_transforms
)val_dataset <span style="color:#f92672">=</span> datasets<span style="color:#f92672">.</span>ImageFolder(
    root<span style="color:#f92672">=</span>os<span style="color:#f92672">.</span>path<span style="color:#f92672">.</span>join(DATA_DIR,<span style="color:#e6db74">&#39;val&#39;</span>),
    transform<span style="color:#f92672">=</span>val_transforms
)

<span style="color:#75715e"># Get mini batch</span>
train_loader <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>utils<span style="color:#f92672">.</span>data<span style="color:#f92672">.</span>DataLoader(
    dataset<span style="color:#f92672">=</span>train_dataset,
    batch_size<span style="color:#f92672">=</span><span style="color:#ae81ff">10</span>, <span style="color:#75715e"># Batch size for learning</span>
    shuffle<span style="color:#f92672">=</span>True <span style="color:#75715e"># Shuffle training data</span>
)

val_loader <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>utils<span style="color:#f92672">.</span>data<span style="color:#f92672">.</span>DataLoader(
    dataset<span style="color:#f92672">=</span>val_dataset,
    batch_size<span style="color:#f92672">=</span><span style="color:#ae81ff">10</span>,
    shuffle<span style="color:#f92672">=</span>True
)


<span style="color:#66d9ef">class</span> <span style="color:#a6e22e">NeuralNet</span>(nn<span style="color:#f92672">.</span>Module):
    <span style="color:#e6db74">&#34;&#34;&#34;Network definition. nn.Module inheritance&#34;&#34;&#34;</span>
    <span style="color:#66d9ef">def</span> __init__(self, num_classes):
        super(NeuralNet, self)<span style="color:#f92672">.</span>__init__()
        self<span style="color:#f92672">.</span>features <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Sequential(
            nn<span style="color:#f92672">.</span>Conv2d(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">8</span>, kernel_size<span style="color:#f92672">=</span><span style="color:#ae81ff">11</span>, stride<span style="color:#f92672">=</span><span style="color:#ae81ff">4</span>, padding<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>),
            nn<span style="color:#f92672">.</span>ReLU(inplace<span style="color:#f92672">=</span>True),
            nn<span style="color:#f92672">.</span>MaxPool2d(kernel_size<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>, stride<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>),
            nn<span style="color:#f92672">.</span>Conv2d(<span style="color:#ae81ff">8</span>, <span style="color:#ae81ff">16</span>, kernel_size<span style="color:#f92672">=</span><span style="color:#ae81ff">5</span>, padding<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>),
            nn<span style="color:#f92672">.</span>ReLU(inplace<span style="color:#f92672">=</span>True),
            nn<span style="color:#f92672">.</span>MaxPool2d(kernel_size<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>, stride<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>),
        )

        self<span style="color:#f92672">.</span>classifier <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Sequential(
            nn<span style="color:#f92672">.</span>Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.5</span>),
            nn<span style="color:#f92672">.</span>Linear(<span style="color:#ae81ff">400</span>, <span style="color:#ae81ff">200</span>),
            nn<span style="color:#f92672">.</span>ReLU(inplace<span style="color:#f92672">=</span>True),
            nn<span style="color:#f92672">.</span>Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.5</span>),
            nn<span style="color:#f92672">.</span>Linear(<span style="color:#ae81ff">200</span>, num_classes)
        )

    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x):
        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>features(x)
        x <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>view(x<span style="color:#f92672">.</span>size(<span style="color:#ae81ff">0</span>), <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>)
        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>classifier(x)
        <span style="color:#66d9ef">return</span> x


<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">main</span>():
    <span style="color:#e6db74">&#34;&#34;&#34; Load training data -&gt; learning (-&gt; save training data) -&gt; show results &#34;&#34;&#34;</span>
    <span style="color:#66d9ef">global</span> checkpoint
    <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#39;[Settings]&#39;</span>)
    <span style="color:#75715e"># Device settings</span>
    device <span style="color:#f92672">=</span><span style="color:#e6db74">&#39;cuda&#39;</span> <span style="color:#66d9ef">if</span> torch<span style="color:#f92672">.</span>cuda<span style="color:#f92672">.</span>is_available() <span style="color:#66d9ef">else</span><span style="color:#e6db74">&#39;cpu&#39;</span>

    <span style="color:#75715e"># Network, evaluation function, optimization function setting</span>
    net <span style="color:#f92672">=</span> NeuralNet(NUM_CLASSES)<span style="color:#f92672">.</span>to(device)
    criterion <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>CrossEntropyLoss() <span style="color:#75715e"># evaluation function</span>
    optimizer <span style="color:#f92672">=</span> optim<span style="color:#f92672">.</span>SGD( <span style="color:#75715e"># optimization algorithm</span>
        net<span style="color:#f92672">.</span>parameters(),
        lr<span style="color:#f92672">=</span>LEARNING_RATE,
        momentum<span style="color:#f92672">=</span>MOMENTUM,
        weight_decay<span style="color:#f92672">=</span><span style="color:#ae81ff">5e-4</span>
    )

    <span style="color:#75715e"># Show settings</span>
    <span style="color:#75715e"># print(&#39; Device :&#39;, device)</span>
    <span style="color:#75715e"># print(&#39; Dataset Class-Index :&#39;, train_dataset.class_to_idx)</span>
    <span style="color:#75715e"># print(&#39; Network Model :&#39;, re.findall(&#39;(.*)\(&#39;, str(net))[0])</span>
    <span style="color:#75715e"># print(&#39; Criterion :&#39;, re.findall(&#39;(.*)\(&#39;, str(criterion))[0])</span>
    <span style="color:#75715e"># print(&#39; Optimizer :&#39;, re.findall(&#39;(.*)\(&#39;, str(optimizer))[0])</span>
    <span style="color:#75715e"># print(&#39;-Learning Rate :&#39;, LEARNING_RATE)</span>
    <span style="color:#75715e"># print(&#39; -Momentum :&#39;, MOMENTUM)</span>

    t_loss_list <span style="color:#f92672">=</span> []
    t_acc_list <span style="color:#f92672">=</span> []
    v_loss_list <span style="color:#f92672">=</span> []
    v_acc_list <span style="color:#f92672">=</span> []
    epoch_pre <span style="color:#f92672">=</span> <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>

    <span style="color:#75715e"># Training (on the way) data acquisition</span>
    <span style="color:#66d9ef">if</span> os<span style="color:#f92672">.</span>path<span style="color:#f92672">.</span>isfile(CKPT_PROCESS):
        checkpoint <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>load(CKPT_PROCESS)
        net<span style="color:#f92672">.</span>load_state_dict(checkpoint[<span style="color:#e6db74">&#39;net&#39;</span>])
        optimizer<span style="color:#f92672">.</span>load_state_dict(checkpoint[<span style="color:#e6db74">&#39;optimizer&#39;</span>])
        t_loss_list <span style="color:#f92672">=</span> checkpoint[<span style="color:#e6db74">&#39;t_loss_list&#39;</span>]
        t_acc_list <span style="color:#f92672">=</span> checkpoint[<span style="color:#e6db74">&#39;t_acc_list&#39;</span>]
        v_loss_list <span style="color:#f92672">=</span> checkpoint[<span style="color:#e6db74">&#39;v_loss_list&#39;</span>]
        v_acc_list <span style="color:#f92672">=</span> checkpoint[<span style="color:#e6db74">&#39;v_acc_list&#39;</span>]
        epoch_pre <span style="color:#f92672">=</span> checkpoint[<span style="color:#e6db74">&#39;epoch&#39;</span>]
        <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;Progress until last time = {}/{} epochs&#34;</span>\
              <span style="color:#f92672">.</span>format(epoch_pre<span style="color:#f92672">+</span><span style="color:#ae81ff">1</span>, NUM_EPOCHS))

    <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#39;[Main process]&#39;</span>)
    <span style="color:#66d9ef">for</span> epoch <span style="color:#f92672">in</span> range(epoch_pre<span style="color:#f92672">+</span><span style="color:#ae81ff">1</span>, NUM_EPOCHS):
        t_loss, t_acc, v_loss, v_acc <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>

        <span style="color:#75715e"># Learning ------------------------------------------------ ---------</span>
        net<span style="color:#f92672">.</span>train() <span style="color:#75715e"># learning mode</span>
        <span style="color:#66d9ef">for</span> _, (images, labels) <span style="color:#f92672">in</span> enumerate(train_loader):
            images, labels <span style="color:#f92672">=</span> images<span style="color:#f92672">.</span>to(device), labels<span style="color:#f92672">.</span>to(device)
            optimizer<span style="color:#f92672">.</span>zero_grad()
            outputs <span style="color:#f92672">=</span> net(images)
            loss <span style="color:#f92672">=</span> criterion(outputs, labels)
            t_loss <span style="color:#f92672">+=</span> loss<span style="color:#f92672">.</span>item()
            t_acc <span style="color:#f92672">+=</span> (outputs<span style="color:#f92672">.</span>max(<span style="color:#ae81ff">1</span>)[<span style="color:#ae81ff">1</span>] <span style="color:#f92672">==</span> labels)<span style="color:#f92672">.</span>sum()<span style="color:#f92672">.</span>item()
            loss<span style="color:#f92672">.</span>backward()
            optimizer<span style="color:#f92672">.</span>step()
        avg_t_loss <span style="color:#f92672">=</span> t_loss <span style="color:#f92672">/</span> len(train_loader<span style="color:#f92672">.</span>dataset)
        avg_t_acc <span style="color:#f92672">=</span> t_acc <span style="color:#f92672">/</span> len(train_loader<span style="color:#f92672">.</span>dataset)

        <span style="color:#75715e"># Evaluation ------------------------------------------------ ---------</span>
        net<span style="color:#f92672">.</span>eval() <span style="color:#75715e"># evaluation mode</span>
        <span style="color:#66d9ef">with</span> torch<span style="color:#f92672">.</span>no_grad(): <span style="color:#75715e"># stop updating gradient</span>
            <span style="color:#66d9ef">for</span> images, labels <span style="color:#f92672">in</span> val_loader:
                images, labels <span style="color:#f92672">=</span> images<span style="color:#f92672">.</span>to(device), labels<span style="color:#f92672">.</span>to(device)
                images <span style="color:#f92672">=</span> images<span style="color:#f92672">.</span>to(device)
                labels <span style="color:#f92672">=</span> labels<span style="color:#f92672">.</span>to(device)
                outputs <span style="color:#f92672">=</span> net(images)
                loss <span style="color:#f92672">=</span> criterion(outputs, labels)
                v_loss <span style="color:#f92672">+=</span> loss<span style="color:#f92672">.</span>item()
                v_acc <span style="color:#f92672">+=</span> (outputs<span style="color:#f92672">.</span>max(<span style="color:#ae81ff">1</span>)[<span style="color:#ae81ff">1</span>] <span style="color:#f92672">==</span> labels)<span style="color:#f92672">.</span>sum()<span style="color:#f92672">.</span>item()
        avg_v_loss <span style="color:#f92672">=</span> v_loss <span style="color:#f92672">/</span> len(val_loader<span style="color:#f92672">.</span>dataset)
        avg_v_acc <span style="color:#f92672">=</span> v_acc <span style="color:#f92672">/</span> len(val_loader<span style="color:#f92672">.</span>dataset)
        <span style="color:#75715e"># ------------------------------------------------- -------------</span>
        <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#39;</span><span style="color:#ae81ff">\r</span><span style="color:#e6db74">Epoch [{}/{}] | Train [oss:{:.3f}, acc:{:.3f}] | Val [loss:{:.3f}, acc:{:.3f}] &#39;</span>\
              <span style="color:#f92672">.</span>format(epoch<span style="color:#f92672">+</span><span style="color:#ae81ff">1</span>, NUM_EPOCHS, avg_t_loss, avg_t_acc, avg_v_loss, avg_v_acc), end<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;&#39;</span>)

        <span style="color:#75715e">#Loss, accuracy record</span>
        t_loss_list<span style="color:#f92672">.</span>append(avg_t_loss)
        t_acc_list<span style="color:#f92672">.</span>append(avg_t_acc)
        v_loss_list<span style="color:#f92672">.</span>append(avg_v_loss)
        v_acc_list<span style="color:#f92672">.</span>append(avg_v_acc)

        <span style="color:#75715e">#Process for saving progress</span>
        checkpoint[<span style="color:#e6db74">&#39;net&#39;</span>] <span style="color:#f92672">=</span> net<span style="color:#f92672">.</span>state_dict()
        checkpoint[<span style="color:#e6db74">&#39;optimizer&#39;</span>] <span style="color:#f92672">=</span> optimizer<span style="color:#f92672">.</span>state_dict()
        checkpoint[<span style="color:#e6db74">&#39;t_loss_list&#39;</span>] <span style="color:#f92672">=</span> t_loss_list
        checkpoint[<span style="color:#e6db74">&#39;t_acc_list&#39;</span>] <span style="color:#f92672">=</span> t_acc_list
        checkpoint[<span style="color:#e6db74">&#39;v_loss_list&#39;</span>] <span style="color:#f92672">=</span> v_loss_list
        checkpoint[<span style="color:#e6db74">&#39;v_acc_list&#39;</span>] <span style="color:#f92672">=</span> v_acc_list
        checkpoint[<span style="color:#e6db74">&#39;epoch&#39;</span>] <span style="color:#f92672">=</span> epoch

    graph()
    save_process()
    save_net()<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">save_process</span>():
    <span style="color:#e6db74">&#34;&#34;&#34;途中経過を保存&#34;&#34;&#34;</span>
    <span style="color:#66d9ef">global</span> checkpoint
    <span style="color:#66d9ef">if</span> <span style="color:#f92672">not</span> checkpoint: <span style="color:#66d9ef">return</span>
    torch<span style="color:#f92672">.</span>save(checkpoint, CKPT_PROCESS)


<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">save_net</span>():
    <span style="color:#e6db74">&#34;&#34;&#34;ネットワーク情報のみ保存&#34;&#34;&#34;</span>
    <span style="color:#66d9ef">global</span> checkpoint
    <span style="color:#66d9ef">if</span> <span style="color:#f92672">not</span> checkpoint: <span style="color:#66d9ef">return</span>
    torch<span style="color:#f92672">.</span>save(checkpoint[<span style="color:#e6db74">&#39;net&#39;</span>], CKPT_NET)


<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">graph</span>():
    <span style="color:#e6db74">&#34;&#34;&#34;損失, 精度のグラフ化&#34;&#34;&#34;</span>
    <span style="color:#66d9ef">global</span> checkpoint
    <span style="color:#66d9ef">if</span> <span style="color:#f92672">not</span> checkpoint: <span style="color:#66d9ef">return</span>
    t_loss_list <span style="color:#f92672">=</span> checkpoint[<span style="color:#e6db74">&#39;t_loss_list&#39;</span>]
    t_acc_list <span style="color:#f92672">=</span> checkpoint[<span style="color:#e6db74">&#39;t_acc_list&#39;</span>]
    v_loss_list <span style="color:#f92672">=</span> checkpoint[<span style="color:#e6db74">&#39;v_loss_list&#39;</span>]
    v_acc_list <span style="color:#f92672">=</span> checkpoint[<span style="color:#e6db74">&#39;v_acc_list&#39;</span>]

    plt<span style="color:#f92672">.</span>figure(figsize<span style="color:#f92672">=</span>(<span style="color:#ae81ff">10</span>, <span style="color:#ae81ff">4</span>))
    plt<span style="color:#f92672">.</span>subplot(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">1</span>)
    plt<span style="color:#f92672">.</span>plot(range(len(t_loss_list)), t_loss_list,
             color<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;blue&#39;</span>, linestyle<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;-&#39;</span>, label<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;t_loss&#39;</span>)
    plt<span style="color:#f92672">.</span>plot(range(len(v_loss_list)), v_loss_list,
             color<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;green&#39;</span>, linestyle<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;--&#39;</span>, label<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;v_loss&#39;</span>)
    plt<span style="color:#f92672">.</span>legend()
    plt<span style="color:#f92672">.</span>xlabel(<span style="color:#e6db74">&#39;epoch&#39;</span>)
    plt<span style="color:#f92672">.</span>ylabel(<span style="color:#e6db74">&#39;loss&#39;</span>)
    plt<span style="color:#f92672">.</span>title(<span style="color:#e6db74">&#39;Training and validation loss&#39;</span>)
    plt<span style="color:#f92672">.</span>grid()

    plt<span style="color:#f92672">.</span>subplot(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">2</span>)
    plt<span style="color:#f92672">.</span>plot(range(len(t_acc_list)), t_acc_list,
             color<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;blue&#39;</span>, linestyle<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;-&#39;</span>, label<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;t_acc&#39;</span>)
    plt<span style="color:#f92672">.</span>plot(range(len(v_acc_list)), v_acc_list,
             color<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;green&#39;</span>, linestyle<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;--&#39;</span>, label<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;v_acc&#39;</span>)
    plt<span style="color:#f92672">.</span>legend()
    plt<span style="color:#f92672">.</span>xlabel(<span style="color:#e6db74">&#39;epoch&#39;</span>)
    plt<span style="color:#f92672">.</span>ylabel(<span style="color:#e6db74">&#39;acc&#39;</span>)
    plt<span style="color:#f92672">.</span>title(<span style="color:#e6db74">&#39;Training and validation accuracy&#39;</span>)
    plt<span style="color:#f92672">.</span>grid()
    plt<span style="color:#f92672">.</span>show()


<span style="color:#66d9ef">if</span> __name__ <span style="color:#f92672">==</span> <span style="color:#e6db74">&#34;__main__&#34;</span>:
    <span style="color:#66d9ef">try</span>:
        main()
    <span style="color:#66d9ef">except</span> <span style="color:#a6e22e">KeyboardInterrupt</span>:
        <span style="color:#66d9ef">print</span>()
        graph()
        save_process()

</code></pre></div><p><strong>【⚠注】ネットワークの規模はほどほどに。</strong>
層やノード数を増やし過ぎると、のちにラズパイで分類する際、膨大な量のパラメータがメモリを食い尽すのか<code>DefaultCPUAllocator: can't allocate memory: you tried to allocate 685198800 bytes.</code>のエラーが出ます。</p>
<h3 id="実行">実行</h3>
<p>学習の経過がコチラ。
左が損失、右が精度です。また青線が訓練データ、緑破線が検証データに対するものです。
<img width="110%" alt="0.png" src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/605747/ca0cb9a0-8ce0-b0ee-0c6d-3d679863ecb7.png">
検証データで精度は72%程度。改善の余地があります&hellip;</p>
<p>学習を終えると、学習済みパラメータのみを保存した**&ldquo;&ldquo;trained_net.ckpt&rdquo;&quot;**ファイルが出来るので、それをまたSlackかなんかで**ラズパイに送ります**。</p>
<h2 id="step3-ラズパイでカメラ映像をリアルタイム分類結果表示"><em>Step3</em>: ラズパイでカメラ映像をリアルタイム分類、結果表示</h2>
<p>ゴールとして、カメラ映像内の物体をリアルタイムで分類させ、いい感じに表示します。</p>
<h3 id="作成するプログラム">作成するプログラム</h3>
<p>初めに背景を撮影、その後のフレームからは背景を除算し、浮かび上がった物体を切り取り、定義された前処理を通して4次元のtensorバッチにします。
バッチまるごとネットワークに通して各クラスの確率に変換し、最も確率の高いクラス（物の名前）を重ね描きしてウィンドウに表示します。</p>
<p>先ほど出来た &ldquo;trained_net.ckpt&rdquo; を読み込みます。</p>
<p><strong>【⚠注】バッチサイズ(一度に検出する物体の数)に上限を設けないと、検出した大量の領域を一度に処理しようとして、ラズパイがフリーズする恐れがあります。</strong></p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-raltime_classification.py" data-lang="raltime_classification.py"><span style="color:#75715e"># coding: utf-8</span>
<span style="color:#f92672">import</span> os
<span style="color:#f92672">from</span> PIL <span style="color:#f92672">import</span> Image
<span style="color:#f92672">from</span> time <span style="color:#f92672">import</span> sleep
<span style="color:#f92672">import</span> cv2
<span style="color:#f92672">import</span> picamera
<span style="color:#f92672">import</span> picamera.array
<span style="color:#f92672">import</span> torch
<span style="color:#75715e"># pytorchディレクトリで &#34;export OMP_NUM_THREADS=1 or 2 or 3&#34; 必須(デフォルトは4)</span>
<span style="color:#75715e"># 並列処理コア数は &#34;print(torch.__config__.parallel_info())&#34; で確認</span>
<span style="color:#f92672">import</span> torch.nn <span style="color:#f92672">as</span> nn
<span style="color:#f92672">import</span> torch.utils
<span style="color:#f92672">from</span> torchvision <span style="color:#f92672">import</span> transforms

CKPT_NET <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;trained_net.ckpt&#39;</span>  <span style="color:#75715e"># 学習済みパラメータファイル</span>
OBJ_NAMES <span style="color:#f92672">=</span> [<span style="color:#e6db74">&#39;Phone&#39;</span>, <span style="color:#e6db74">&#39;Wallet&#39;</span>, <span style="color:#e6db74">&#39;Watch&#39;</span>]  <span style="color:#75715e"># 各クラスの表示名</span>
MIN_LEN <span style="color:#f92672">=</span> <span style="color:#ae81ff">50</span>
GRAY_THR <span style="color:#f92672">=</span> <span style="color:#ae81ff">20</span>
CONTOUR_COUNT_MAX <span style="color:#f92672">=</span> <span style="color:#ae81ff">3</span>  <span style="color:#75715e"># バッチサイズ(一度に検出する物体の数)の上限</span>
SHOW_COLOR <span style="color:#f92672">=</span> (<span style="color:#ae81ff">255</span>, <span style="color:#ae81ff">191</span>, <span style="color:#ae81ff">0</span>)  <span style="color:#75715e"># 枠の色(B,G,R)</span>

NUM_CLASSES <span style="color:#f92672">=</span> <span style="color:#ae81ff">3</span>
PIXEL_LEN <span style="color:#f92672">=</span> <span style="color:#ae81ff">112</span>  <span style="color:#75715e"># Resize後のサイズ(1辺)</span>
CHANNELS <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span>  <span style="color:#75715e"># 色のチャンネル数(BGR:3, グレースケール:1)</span>


<span style="color:#75715e"># 画像データ変換定義</span>
<span style="color:#75715e"># Resizeと, classifierの最初のLinear入力が関連</span>
data_transforms <span style="color:#f92672">=</span> transforms<span style="color:#f92672">.</span>Compose([
    transforms<span style="color:#f92672">.</span>Resize((PIXEL_LEN, PIXEL_LEN)),
    transforms<span style="color:#f92672">.</span>Grayscale(),
    transforms<span style="color:#f92672">.</span>ToTensor(),
    transforms<span style="color:#f92672">.</span>Normalize(mean<span style="color:#f92672">=</span>[<span style="color:#ae81ff">0.5</span>], std<span style="color:#f92672">=</span>[<span style="color:#ae81ff">0.5</span>])
])


<span style="color:#66d9ef">class</span> <span style="color:#a6e22e">NeuralNet</span>(nn<span style="color:#f92672">.</span>Module):
    <span style="color:#e6db74">&#34;&#34;&#34;ネットワーク定義. 学習に用いたものと同一である必要&#34;&#34;&#34;</span>
    <span style="color:#66d9ef">def</span> __init__(self, num_classes):
        super(NeuralNet, self)<span style="color:#f92672">.</span>__init__()
        self<span style="color:#f92672">.</span>features <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Sequential(
            nn<span style="color:#f92672">.</span>Conv2d(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">8</span>, kernel_size<span style="color:#f92672">=</span><span style="color:#ae81ff">11</span>, stride<span style="color:#f92672">=</span><span style="color:#ae81ff">4</span>, padding<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>),
            nn<span style="color:#f92672">.</span>ReLU(inplace<span style="color:#f92672">=</span>True),
            nn<span style="color:#f92672">.</span>MaxPool2d(kernel_size<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>, stride<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>),
            nn<span style="color:#f92672">.</span>Conv2d(<span style="color:#ae81ff">8</span>, <span style="color:#ae81ff">16</span>, kernel_size<span style="color:#f92672">=</span><span style="color:#ae81ff">5</span>, padding<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>),
            nn<span style="color:#f92672">.</span>ReLU(inplace<span style="color:#f92672">=</span>True),
            nn<span style="color:#f92672">.</span>MaxPool2d(kernel_size<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>, stride<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>),
        )
        self<span style="color:#f92672">.</span>classifier <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Sequential(
            nn<span style="color:#f92672">.</span>Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.5</span>),
            nn<span style="color:#f92672">.</span>Linear(<span style="color:#ae81ff">400</span>, <span style="color:#ae81ff">200</span>),
            nn<span style="color:#f92672">.</span>ReLU(inplace<span style="color:#f92672">=</span>True),
            nn<span style="color:#f92672">.</span>Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.5</span>),
            nn<span style="color:#f92672">.</span>Linear(<span style="color:#ae81ff">200</span>, num_classes)
        )

    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x):
        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>features(x)
        x <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>view(x<span style="color:#f92672">.</span>size(<span style="color:#ae81ff">0</span>), <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>)
        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>classifier(x)
        <span style="color:#66d9ef">return</span> x


<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">detect_obj</span>(back, target):
    <span style="color:#e6db74">&#34;&#34;&#34;
</span><span style="color:#e6db74">    OpenCVの背景差分処理で, 検出された物体のタプルを作成
</span><span style="color:#e6db74">    引数:
</span><span style="color:#e6db74">        back: 入力背景画像
</span><span style="color:#e6db74">            カラー画像
</span><span style="color:#e6db74">        target: 背景差分対象の画像
</span><span style="color:#e6db74">            カラー画像. 複数の物体を切り抜き, カラー画像タプルにまとめる
</span><span style="color:#e6db74">    &#34;&#34;&#34;</span>
    <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#39;Detecting objects ...&#39;</span>)
    <span style="color:#75715e"># 2値化</span>
    b_gray <span style="color:#f92672">=</span> cv2<span style="color:#f92672">.</span>cvtColor(back, cv2<span style="color:#f92672">.</span>COLOR_BGR2GRAY)
    t_gray <span style="color:#f92672">=</span> cv2<span style="color:#f92672">.</span>cvtColor(target, cv2<span style="color:#f92672">.</span>COLOR_BGR2GRAY)
    <span style="color:#75715e"># 差分を計算</span>
    diff <span style="color:#f92672">=</span> cv2<span style="color:#f92672">.</span>absdiff(t_gray, b_gray)

    <span style="color:#75715e"># 閾値に従ってコンター, マスクを作成, 物体を抽出</span>
    <span style="color:#75715e"># findContoursのインデックスは, cv2.__version__ == 4.2.0-&gt;[0], 3.4.7-&gt;[1]</span>
    mask <span style="color:#f92672">=</span> cv2<span style="color:#f92672">.</span>threshold(diff, GRAY_THR, <span style="color:#ae81ff">255</span>, cv2<span style="color:#f92672">.</span>THRESH_BINARY)[<span style="color:#ae81ff">1</span>]
    cv2<span style="color:#f92672">.</span>imshow(<span style="color:#e6db74">&#39;mask&#39;</span>, mask)
    contour <span style="color:#f92672">=</span> cv2<span style="color:#f92672">.</span>findContours(mask,
                               cv2<span style="color:#f92672">.</span>RETR_EXTERNAL,
                               cv2<span style="color:#f92672">.</span>CHAIN_APPROX_SIMPLE)[<span style="color:#ae81ff">1</span>]

    <span style="color:#75715e"># 一定の縦横幅以上で検出された変化領域の座標, サイズバッチを作成</span>
    pt_list <span style="color:#f92672">=</span> list(filter(
        <span style="color:#66d9ef">lambda</span> x: x[<span style="color:#ae81ff">2</span>] <span style="color:#f92672">&gt;</span> MIN_LEN <span style="color:#f92672">and</span> x[<span style="color:#ae81ff">3</span>] <span style="color:#f92672">&gt;</span> MIN_LEN,
        [cv2<span style="color:#f92672">.</span>boundingRect(pt) <span style="color:#66d9ef">for</span> pt <span style="color:#f92672">in</span> contour]))[:CONTOUR_COUNT_MAX]

    <span style="color:#75715e"># Frame cutout according to position information, converted to tuple of PIL image and returned</span>
    obj_imgaes <span style="color:#f92672">=</span> tuple(map(
        <span style="color:#66d9ef">lambda</span> x: Image<span style="color:#f92672">.</span>fromarray(target[x[<span style="color:#ae81ff">1</span>]:x[<span style="color:#ae81ff">1</span>]<span style="color:#f92672">+</span>x[<span style="color:#ae81ff">3</span>], x[<span style="color:#ae81ff">0</span>]:x[<span style="color:#ae81ff">0</span>]<span style="color:#f92672">+</span>x[<span style="color:#ae81ff">2</span>]]),
        pt_list
    ))
    <span style="color:#66d9ef">return</span> (obj_imgaes, pt_list)


<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">batch_maker</span>(tuple_images, transform):
    <span style="color:#e6db74">&#34;&#34;&#34;
</span><span style="color:#e6db74">    Transform a tuple of PIL format images into a tensor batch that can be processed by a network
</span><span style="color:#e6db74">    argument:
</span><span style="color:#e6db74">        tuple_images: PIL image tuple
</span><span style="color:#e6db74">        transform: torchvision image transformation definition
</span><span style="color:#e6db74">    &#34;&#34;&#34;</span>
    <span style="color:#66d9ef">return</span> torch<span style="color:#f92672">.</span>cat((transform(img) <span style="color:#66d9ef">for</span> img
                      <span style="color:#f92672">in</span> tuple_images])<span style="color:#f92672">.</span>view(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, CHANNELS, PIXEL_LEN, PIXEL_LEN)


<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">judge_what</span>(img, probs_list, pos_list):
    <span style="color:#e6db74">&#34;&#34;&#34;
</span><span style="color:#e6db74">    Determine the object from the probability of belonging to each class, display the frame and name at that position, and return the class index
</span><span style="color:#e6db74">    argument:
</span><span style="color:#e6db74">        probs_list: quadratic array of probabilities. Batch format
</span><span style="color:#e6db74">        pos_list: secondary array of positions. Batch format
</span><span style="color:#e6db74">    &#34;&#34;&#34;</span>
    <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#39;Judging objects ...&#39;</span>)
    <span style="color:#75715e"># Convert to the list of highest probabilities and their indexes</span>
    ip_list <span style="color:#f92672">=</span> list(map(<span style="color:#66d9ef">lambda</span> x: max(enumerate(x), key <span style="color:#f92672">=</span> <span style="color:#66d9ef">lambda</span> y:y[<span style="color:#ae81ff">1</span>]),
                       F<span style="color:#f92672">.</span>softmax(probs_list, dim<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>))) <span style="color:#75715e"># &lt;- 4/30 fixed</span>

    <span style="color:#75715e">#Convert the index to the object name, write the object name and certainty at the position of the object and display</span>
    <span style="color:#66d9ef">for</span> (idx, prob), pos <span style="color:#f92672">in</span> zip(ip_list, pos_list):
        cv2<span style="color:#f92672">.</span>rectangle(img, (pos[<span style="color:#ae81ff">0</span>], pos[<span style="color:#ae81ff">1</span>]), (pos[<span style="color:#ae81ff">0</span>]<span style="color:#f92672">+</span>pos[<span style="color:#ae81ff">2</span>], pos[<span style="color:#ae81ff">1</span>]<span style="color:#f92672">+</span>pos[<span style="color:#ae81ff">3</span>]), SHOW_COLOR, <span style="color:#ae81ff">2</span>)
        cv2<span style="color:#f92672">.</span>putText(img,<span style="color:#e6db74">&#39;</span><span style="color:#e6db74">%s</span><span style="color:#e6db74">:</span><span style="color:#e6db74">%.1f%%</span><span style="color:#e6db74">&#39;</span><span style="color:#f92672">%</span>(OBJ_NAMES[idx], prob<span style="color:#f92672">*</span><span style="color:#ae81ff">100</span>), (pos[<span style="color:#ae81ff">0</span>]<span style="color:#f92672">+</span><span style="color:#ae81ff">5</span>, pos[<span style="color:#ae81ff">1</span>]<span style="color:#f92672">+</span><span style="color:#ae81ff">20</span>),
                    cv2<span style="color:#f92672">.</span>FONT_HERSHEY_SIMPLEX, <span style="color:#ae81ff">0.8</span>, SHOW_COLOR, thickness<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>)
    <span style="color:#66d9ef">return</span> ip_list


<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">realtime_classify</span>():
    <span style="color:#e6db74">&#34;&#34;&#34; Load trained model -&gt; Load test data -&gt; Classification -&gt; Overlay result on image &#34;&#34;&#34;</span>
    <span style="color:#75715e">#Device settings</span>
    device <span style="color:#f92672">=</span><span style="color:#e6db74">&#39;cuda&#39;</span> <span style="color:#66d9ef">if</span> torch<span style="color:#f92672">.</span>cuda<span style="color:#f92672">.</span>is_available() <span style="color:#66d9ef">else</span><span style="color:#e6db74">&#39;cpu&#39;</span>
    <span style="color:#75715e"># network settings</span>
    net <span style="color:#f92672">=</span> NeuralNet(NUM_CLASSES)<span style="color:#f92672">.</span>to(device)

    <span style="color:#75715e"># Get trained data</span>
    <span style="color:#66d9ef">if</span> os<span style="color:#f92672">.</span>path<span style="color:#f92672">.</span>isfile(CKPT_NET):
        checkpoint <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>load(CKPT_NET)
        net<span style="color:#f92672">.</span>load_state_dict(checkpoint)
    <span style="color:#66d9ef">else</span>:
        <span style="color:#66d9ef">raise</span> FileNotFoundError(<span style="color:#e6db74">&#39;No trained network file: {}&#39;</span><span style="color:#f92672">.</span>format(CKPT_NET))

    <span style="color:#75715e">#Evaluation mode</span>
    net<span style="color:#f92672">.</span>eval()
    <span style="color:#75715e"># start picamera</span>
    <span style="color:#66d9ef">with</span> picamera<span style="color:#f92672">.</span>PiCamera() <span style="color:#66d9ef">as</span> camera:
        camera<span style="color:#f92672">.</span>resolution <span style="color:#f92672">=</span> (<span style="color:#ae81ff">480</span>, <span style="color:#ae81ff">480</span>)
        <span style="color:#75715e"># Start streaming</span>
        <span style="color:#66d9ef">with</span> picamera<span style="color:#f92672">.</span>array<span style="color:#f92672">.</span>PiRGBArray(camera) <span style="color:#66d9ef">as</span> stream:
            <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#39;Setting background ...&#39;</span>)
            sleep(<span style="color:#ae81ff">2</span>)
    
            camera<span style="color:#f92672">.</span>exposure_mode <span style="color:#f92672">=</span><span style="color:#e6db74">&#39;off&#39;</span> <span style="color:#75715e"># White balance fixed</span>
            camera<span style="color:#f92672">.</span>capture(stream,<span style="color:#e6db74">&#39;bgr&#39;</span>, use_video_port<span style="color:#f92672">=</span>True)
            <span style="color:#75715e"># Set as background</span>
            img_back <span style="color:#f92672">=</span> stream<span style="color:#f92672">.</span>array

            stream<span style="color:#f92672">.</span>seek(<span style="color:#ae81ff">0</span>)
            stream<span style="color:#f92672">.</span>truncate()
            
            <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#39;Start!&#39;</span>)
            <span style="color:#66d9ef">with</span> torch<span style="color:#f92672">.</span>no_grad():
                <span style="color:#66d9ef">while</span> True:
                    camera<span style="color:#f92672">.</span>capture(stream,<span style="color:#e6db74">&#39;bgr&#39;</span>, use_video_port<span style="color:#f92672">=</span>True)
                    <span style="color:#75715e"># Background difference for future input images</span>
                    img_target <span style="color:#f92672">=</span> stream<span style="color:#f92672">.</span>array
                    <span style="color:#75715e"># Detect object and its position</span>
                    obj_imgs, positions <span style="color:#f92672">=</span> detect_obj(img_back, img_target)
                    <span style="color:#66d9ef">if</span> obj_imgs:
                        <span style="color:#75715e"># Convert detected object to network input format</span>
                        obj_batch <span style="color:#f92672">=</span> batch_maker(obj_imgs, data_transforms)
                        <span style="color:#75715e">#Classification</span>
                        outputs <span style="color:#f92672">=</span> net(obj_batch)
                        <span style="color:#75715e"># Judgment</span>
                        result <span style="color:#f92672">=</span> judge_what(img_target, outputs, positions)
                        <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#39; Result:&#39;</span>, result)

                    <span style="color:#75715e"># Display</span>
                    cv2<span style="color:#f92672">.</span>imshow(<span style="color:#e6db74">&#39;detection&#39;</span>, img_target)

                    <span style="color:#66d9ef">if</span> cv2<span style="color:#f92672">.</span>waitKey(<span style="color:#ae81ff">200</span>) <span style="color:#f92672">==</span> ord(<span style="color:#e6db74">&#39;q&#39;</span>):
                        cv2<span style="color:#f92672">.</span>destroyAllWindows()
                        <span style="color:#66d9ef">return</span>

                    stream<span style="color:#f92672">.</span>seek(<span style="color:#ae81ff">0</span>)
                    stream<span style="color:#f92672">.</span>truncate()


<span style="color:#66d9ef">if</span> __name__ <span style="color:#f92672">==</span> <span style="color:#e6db74">&#34;__main__&#34;</span>:
    <span style="color:#66d9ef">try</span>:
        realtime_classify()
    <span style="color:#66d9ef">except</span> <span style="color:#a6e22e">KeyboardInterrupt</span>:
        cv2<span style="color:#f92672">.</span>destroyAllWindows()

</code></pre></div><h3 id="run-1">run</h3>
<p>Bring the &ldquo;trained_net.ckpt&rdquo; from above to your Raspberry Pi and run it in the same directory.
The detected object name and its certainty factor are displayed.</p>
<p>The results of the execution&hellip;&hellip;&hellip;Satisfied with classifying them with high accuracy from the moment they are placed! !</p>
<p><img width="30%" alt="0.jpg" src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/605747/2a9874d1-6761-db08-cfe2-edd3833cb803.jpeg"> ➡ <img width="30%" alt="1.jpg" src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/605747/fc4a09b1-7070-36b4-b485-0cebef1402f0.jpeg"> ➡ <img width="30%" alt="2.jpg" src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/605747/e77ddd84-ccd8-a1f5-7bf3-9ff3fb61c844.jpeg"></p>
<p>** [⚠Note] It is recommended to change the number of cores used for execution (default 4). **
If you use it with 4 cores full, you are liable to freeze.
Change the command <code>export OMP_NUM_THREADS=2</code> (using 2 cores) in the pytorch directory. The number of cores can be confirmed with <code>print(torch.__config__.parallel_info())</code>.
However, if you close the shell, the changes will be discarded, so to make it persistent, under the bottom &ldquo;&hellip;~ fi&rdquo; of <strong>&quot;.profile&rdquo;</strong> in <code>/home/pi</code>, under the <code>export OMP_NUM_THREADS Write =2</code> and reboot.</p>
<p>#Summary</p>
<p>I could do what I wanted to do! (Sorry for the lack of readability&hellip;)
With OpenCV&rsquo;s face detection, it can be applied immediately to very simple face recognition.</p>
<p>Originally I was thinking of implementing SSD, but I thought it would be difficult to create a dataset with location information, and I gave up because the error that I tried to train with sample data could not be resolved. ..</p>
<p>Unlike SSDs and the like, the background difference this time is that the overlapping objects cannot be separated, and they are judged as one.</p>
<p>It was a good study~</p>

    </article>
  </div>

  
  
  
  <script>
  window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
  ga('create', 'UA-169005401-1', 'auto');
  ga('send', 'pageview');
  </script>
  <script async src='https://www.google-analytics.com/analytics.js'></script>
  

  

  
<link rel="stylesheet" type="text/css" href="//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.0/cookieconsent.min.css" />
<script src="//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.0/cookieconsent.min.js"></script>
<script>
window.addEventListener("load", function(){
window.cookieconsent.initialise({
  "palette": {
    "popup": {
      "background": "#216942",
      "text": "#b2d192"
    },
    "button": {
      "background": "#afed71"
    }
  }
})});
</script>

</body>

</html>
