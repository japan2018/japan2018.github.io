<!doctype html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  
  <meta name="generator" content="Hugo 0.72.0" />

  
  <meta name="description" content="Some description">
  

  
  <link rel="apple-touch-icon" sizes="180x180" href="https://memotut.com/apple-touch-icon.png">

  
  <link rel="icon" type="image/png" sizes="32x32" href="https://memotut.com/favicon-32x32.png">

  
  <link rel="icon" type="image/png" sizes="16x16" href="https://memotut.com/favicon-16x16.png">

  
  <link rel="manifest" href="https://memotut.com/site.webmanifest">

  
  <link rel="mask-icon" href="https://memotut.com/safari-pinned-tab.svg" color="#5bbad5">

  <meta name="msapplication-TileColor" content="#da532c">

  <meta name="theme-color" content="#ffffff">

  
  <link rel="stylesheet" href="https://memotut.com/css/bootstrap.min.css" />

  
  <title>I don&#39;t want to admit... The dynamical system representation of Neural Network | Memo Tut</title>
  

  <style>
body {
  min-width: 300px;
}

.custom-navbar {
  margin-bottom: 1em;
  height: 60px;
}

.custom-navbar a {
  display: inline-block; 
  padding: 18px 0;
  margin-right: 1em; 
  font-weight: bold; 
}

.custom-navbar a:hover,
.custom-navbar a:focus {
  text-decoration: none; 
}

@media print {
  .custom-navbar {
    display: none;
  }
}

article {
  padding-bottom: 1em;
}

img {
  max-width: 100%;
}


body {
  background-color: #fff;
}



body {
  color: #212529;
}



a {
  color: #007bff;
}



a:hover,
a:focus {
  color: #0056b3;
}



.custom-navbar {
  background-color: #212529;
}



.custom-navbar a {
  color: rgba(255, 255, 255, 0.75);
}



.custom-navbar a:hover,
.custom-navbar a:focus {
  color: rgba(255, 255, 255, 1);
}



.container {
  max-width: 800px;
}



pre {
  display: block;
  padding: 9.5px;
  word-break: break-all;
  word-wrap: break-word;
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 4px;
}

pre code {
  padding: 0;
  font-size: inherit;
  color: inherit; 
  white-space: pre-wrap;
  background-color: transparent;
  border: none;
  border-radius: 0;
}

code {
  padding: 2px 4px;
  color: inherit; 
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 4px;
  font-size: .9em;
}



blockquote,
.blockquote {
  padding: 10px 20px;
  margin: 0 0 20px;
  font-size: 1em;
  border-left: 5px solid #6c757d;
}

</style>
</head>

<body>
  <nav class="custom-navbar">
  <div class="container">
    
    <a href="/">Posts</a>
    
    <a href="/tags/">Tags</a>
    
    <a href="/about/">About</a>
    
    <a href="/index.xml">RSS</a>
    
  </div>
</nav>
  
  <div class="container">
    <article>
      <h1>I don&rsquo;t want to admit&hellip; The dynamical system representation of Neural Network</h1>
<p>
  <small class="text-secondary">
  
  
  Dec 18, 2019
  </small>
  

<small><code><a href="https://memotut.com/tags/python">Python</a></code></small>


<small><code><a href="https://memotut.com/tags/deeplearning">DeepLearning</a></code></small>


<small><code><a href="https://memotut.com/tags/neuralnetwork">NeuralNetwork</a></code></small>


<small><code><a href="https://memotut.com/tags/autograd">Autograd</a></code></small>


<small><code><a href="https://memotut.com/tags/hamiltonian">Hamiltonian</a></code></small>

</p>
<pre><code>This article is the 18th day article of [NTT Communications Advent Calendar 2019](https://qiita.com/advent-calendar/2019/nttcom).
</code></pre>
<p>Yesterday was @yusuke84&rsquo;s article, <a href="https://qiita.com/yusuke84/items/73319b9a1dbc61b27c0f">Thinking about WebRTC Platform SkyWay support</a>.
**Merry Christmas! **:santa::christmas_tree::gift::santa::christmas_tree::gift::santa::christmas_tree::gift::santa::christmas_tree::gift::santa::christmas_tree::gift::santa: :christmas_tree::gift:</p>
<p>#Introduction
It&rsquo;s the company&rsquo;s Advent Calendar, so let&rsquo;s write some technical tips at the beginning to keep up with the flow!
So, I was searching for information about Neural Network, but in the end, I settled on a story that I was interested in, which was more theoretical than implementation, and this article is the end of it.
(Well, maybe one person might run out of control, surely)</p>
<p>So, I will write a story that I have been interested in for some time about the expression of physical systems using Neural Networks. In the meantime, I am delusioning that I would like to receive a corresponding evaluation from the company because I will make a novel method into a paper based on the story of this area. (Flicker</p>
<p>*I&rsquo;m still looking forward to it.</p>
<h1 id="who-are-you-">Who are you? ?</h1>
<p>I am a second-year employee who has been working in the Technology Development Department, researching and developing basic technologies that contribute to data science and AI.
This is a post from @BootCamp_2019, because the real one (mostly ROM only) is contaminated.</p>
<p>As you can see from this infamous name, I was a lecturer of this year&rsquo;s data training &amp; AI course of &ldquo;Boot Camp&rdquo; which is our in-house training.</p>
<p>Every time there is something, &ldquo;<a href="http://www.toukei-kentei.jp">Statistical test</a>1stgrade&rdquo;&quot;<a href="https://www.su-gaku.net/suken/">Mathematicstest</a> 1st grade He is showing off a useless qualification in the research industry.</p>
<p>The company mainly researches and develops multivariate time series analysis, sparse estimation, factor analysis, etc., but I personally like nonlinear mechanics, reduction theory, information statistical mechanics.</p>
<p>As a result, this article is also based on my personal taste. (Teppero</p>
<p>#Mochibe
Recently, even a little while ago, the application of Deep Learning has been discussed in various fields.
This is a very delightful thing, and for me, who was originally in the theoretical system of mathematical science, it is a great motivation to concentrate on Deep Learning.
Is it rumored that incorporating deep learning into theories and schemes that are valued in different fields has entered a period of disillusionment? It is synonymous with incorporating a new style into deep learning, and I think it is a wonderful thing to explore the next evolution from many perspectives.</p>
<p>This article was written in hopes of sympathizing with that area.</p>
<p>I&rsquo;m going to put together a simple model and explanation as much as possible, so I hope you will find the atmosphere interesting.
(If you know it well, please tell me about the Hamiltonian mechanics of the multi-body phase oscillator model.</p>
<p>Dynamical system representation using # Neural Network</p>
<h2 id="neural-network-and-differential-equations">Neural Network and differential equations</h2>
<h3 id="the-world-is-made-up-of-differential-equations">The world is made up of differential equations.</h3>
<p>For example, the equation of motion of spring simple vibration learned in high school physics is described as follows.
($m$ is mass, $a$ is acceleration, $k$ is spring constant, $x$ is the position of the object, and the balance position is the origin. Consider a simple one-dimensional motion.)
$$
ma = -kx
$$
<img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/397186/732e4690-386c-e1fc-c0e5-6f99cd8a333f.png" alt="tan.png"><sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup></p>
<p>In the future, when we dig a little deeper and learn, the velocity $v$ of the object is the first-order time derivative of the position $x$, and the acceleration $a$ means the first-order time derivative of the velocity $v$. I will. That is, the acceleration $a$ means the second-order time derivative of the position $x$.
If this is clearly described by explicitly stating that the position is a function of time $x(t)$, the equation of motion can be rewritten as follows.
$$
m\frac{d^2x(t)}{dt^2} = -kx(t)
$$</p>
<p>A differential equation is a functional equation described as a relational expression between an unknown function and its derivative. The equation of motion of spring simple oscillation is also a differential equation.</p>
<p>Better mathematical models are essential for better theoretical interpretation of physical phenomena.
By using a differential equation, a real-world phenomenon can be formulated as a mathematical problem (based on some assumption and approximation), and by interpreting the meaning of the solution of the problem, the original phenomenon itself A theoretical interpretation can be given.</p>
<p>For example, by mathematically solving the differential equation that expresses the simple spring vibration, the following solution is obtained.
(However, $A$ is the amplitude and $\alpha$ is the initial phase, which are determined from the initial position $x(0)$ and the initial velocity $v(0)$.)
$$
x(t)=A\sin\left(\sqrt{\frac{k}{m}} t +\alpha\right)
$$
And when this solution is plotted on the plane of time $t$ and position $x$ as shown in the figure below, it can be seen that it agrees with the behavior of the actual simple vibration.</p>
<p><img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/397186/0937fe5a-6f4c-ace4-26d4-6c12f09622ff.gif" alt="image-phy5-1.gif">[ ^2]</p>
<p>In fact, the formulation of mathematical models using differential equations is useful not only for physical phenomena, but for a wide range of phenomena such as biology, chemistry, politics, economy, and society.
In other words, the phenomena in the world can be expressed by differential equations to a surprising extent, and conversely, the finding of a valid differential equation can be said to be a problem equivalent to the theoretical interpretation of the phenomenon. (You can say it with a dream!)</p>
<p>However, unfortunately, higher education in Japan teaches how to solve differential equations, but does not teach much how to make differential equations. Solving is easy for anyone with the right tools, but new is very difficult to create and much more important than solving.
If you are interested in making differential equations, this book <sup id="fnref:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup> is recommended. It is a good book that is suitable for training model building because it has many application examples.</p>
<p><img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/397186/9611e749-607a-f8a2-7ac3-33c9fc6877c5.jpeg" alt="41oNUWfit7L.jpg"></p>
<h3 id="differential-equation-and-resnet">Differential equation and ResNet</h3>
<p>I have forgotten to say that the differential equations dealt with in this article are technically a group called ordinary differential equations. But what is the ordinary differential equation? Since it will be long, I will leave it elsewhere, so I will call the ordinary differential equation and the differential equation hereafter because it is troublesome.</p>
<p>Now, the basic first order differential equation is generally given in the following form.
$$
\frac{dx(t)}{dt}=f(x(t))
$$
By the way, the derivative can be obtained by the following limit from its definition.
$$
\frac{dx(t)}{dt}=\lim_{\Delta t\to0}\frac{x(t+\Delta t)-x(t)}{\Delta t}
$$
Therefore, if you want to simulate the behavior of a differential equation with a computer, the simplest method is to discretize time at $\Delta t$ intervals and perform numerical calculations based on the discrete time $t_n=n\Delta t$. I will.
$$
\frac{x(t_{n+1})-x(t_n)}{\Delta t}=f(x(t_n))
$$
Let&rsquo;s transform this a little.
$$
x(t_{n+1})=x(t_n)+f(x(t_n))\Delta t
$$
And although it is a bit of a climb, if you replace it with $g(x(t_n))=f(x(t_n))\Delta t$, you can finally express it as follows.
$$
x(t_{n+1})=x(t_n)+g(x(t_n))
$$
Have you ever seen a deep-learning surgeon with this format and good sense somewhere?
Actually, if you stare at it, you can see it as an equation of the same form as ResNet<sup id="fnref:5"><a href="#fn:5" class="footnote-ref" role="doc-noteref">5</a></sup>, which prevailed in the image recognition task.</p>
<p>On the contrary, from the neural network side, try to reach this equation step by step.</p>
<p>Below is the figure taken from Figure 2. of ResNet<sup id="fnref:5"><a href="#fn:5" class="footnote-ref" role="doc-noteref">5</a></sup>.</p>
<p><img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/397186/b0e971ac-ce1b-b721-1f15-74960301cbc7.png" alt="20171104001512.png"></p>
<p>In each layer of any architecture, after all, for some input $x$, it is configured by a suitable combination of linear transformation (Afiine transformation etc.) and nonlinear transformation (activation function etc.). The transformation $\mathcal{F}(x)$ is output and is propagated to the next layer.
The propagation path of the central vertical arrow in the figure illustrates just this schematically. (The figure shows Relu, but in general, other than Relu is also possible.)</p>
<p>The heart of ResNet is the propagation path (detour) marked &ldquo;$x$ identity&rdquo; on the right.
I will leave the details to the original paper <sup id="fnref:5"><a href="#fn:5" class="footnote-ref" role="doc-noteref">5</a></sup>, but this propagation path (detour) is an identity map and merges the input into the output as it is.
As a result, it is known that learning proceeds efficiently even in a network with a very large number of layers, and it is thought that this is because error backpropagation proceeds efficiently.Here, if the input is $x_n$ and the output is $x_{n+1}$ for the $n$th ResNet block, the previous figure can be rewritten as follows.</p>
<p>$$
x_{n+1}=x_n+\mathcal{F}^{(n)}(x_n)
$$</p>
<p>This is exactly the same form of equation as the discretized differential equation shown above.
If you show it more explicitly, it will be easier to understand if you associate $x(t_n)$ in the differential equation with $x_n$ in the ResNet block.</p>
<p>Furthermore, in RevNet<sup id="fnref:6"><a href="#fn:6" class="footnote-ref" role="doc-noteref">6</a></sup>, which is known as a generalization of ResNet, each block has the following symmetric structure.</p>
<p>$$
\begin{eqnarray}
x_{n+1}&amp;=&amp;x_n+\mathcal{F}(y_n)\<br>
y_{n+1}&amp;=&amp;y_n+\mathcal{G}(x_n)
\end{eqnarray}
$$
Considering the same discussion as above, we can see that this is a discretization of the following simultaneous differential equations.
$$
\begin{eqnarray}
\frac{dx(t)}{dt}&amp;=&amp;f(y(t))\<br>
\frac{dy(t)}{dt}&amp;=&amp;g(x(t))
\end{eqnarray}
$$</p>
<p>It should be noted that not all differential equations can be described by ResNet (or RevNet) in light of the above discussions.
For details of this area, see this book <sup id="fnref:7"><a href="#fn:7" class="footnote-ref" role="doc-noteref">7</a></sup>, which has been popular in the streets recently. This book is a very good book, so if you are interested in exploring the deep relationship between deep learning and physics, I would be glad if you could get it.
In particular, for the sake of simplifying the discussion in this article, we have broken down the consideration of the number of units, etc., so please check this book for details.</p>
<img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/397186/4694e395-cdb1-baa1-c885-8320a6c9a7b4.jpeg" width=50%>
<h3 id="odenet-neural-ordinary-differential-equations">ODENet (Neural Ordinary Differential Equations)</h3>
<p>This paper <sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>, which was selected as the best paper of NeurIPS 2018 <sup id="fnref:8"><a href="#fn:8" class="footnote-ref" role="doc-noteref">8</a></sup>, proposes an epoch-making method that connects a time evolution equation driven by a differential equation and a neural network. The author implementation is available on Github<sup id="fnref:9"><a href="#fn:9" class="footnote-ref" role="doc-noteref">9</a></sup>.</p>
<p>ODENet considers layer-by-layer processing such as hidden layers of ResNet and RNN to be a time evolution equation with a continuous limit in the time direction, and constructs a neural network by explicitly solving it as a (normal) differential equation. is.
By making it continuous, the concept of deep learning layer disappears, the efficiency of memory and calculation amount is good, and as an optimization equivalent to Back Propagation, a solver of (normal) differential equations can be used. Proposing a method.</p>
<p>This is a figure taken from Figure 1 of ODENet<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>. The left figure describes the discrete time evolution in normal ResNet, while the ODENet in the right figure can represent the continuous time evolution. There is no longer the concept of discrete layers. (It is perfect if you can feel that there are many atmosphere arrows (vector fields).)
<img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/397186/270a7f0c-ed12-2cdb-61b7-42e1c5295b24.png" alt="68747470733a2f2f692e696d6775722e636f6d2f753650645337522e706e67.png"></p>
<p>In addition, the probability density can be learned efficiently with Continuous Normalizing Flow (CNF), which is an extension of Normalizing Flow<sup id="fnref:10"><a href="#fn:10" class="footnote-ref" role="doc-noteref">10</a></sup>(NF), and time series data can be efficiently obtained by making the time evolution of latent variables continuous. The method of learning is proposed.</p>
<p>Please refer to this site <sup id="fnref:11"><a href="#fn:11" class="footnote-ref" role="doc-noteref">11</a></sup><sup id="fnref:12"><a href="#fn:12" class="footnote-ref" role="doc-noteref">12</a></sup><sup id="fnref:13"><a href="#fn:13" class="footnote-ref" role="doc-noteref">13</a></sup> for detailed theoretical explanations about ODENet.</p>
<p>As an aside, the code is published in this Normalizing Flow, another repository <sup id="fnref:14"><a href="#fn:14" class="footnote-ref" role="doc-noteref">14</a></sup> by the same author. It&rsquo;s fun to actually construct a strange probability density.</p>
<p>For example, the probability density of the guy who always watches us warmly can be calculated beautifully like this.</p>
<p><img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/397186/0759caf0-913a-3b14-a917-1bf181fd093e.gif" alt="traj.gif"></p>
<h2 id="neural-network-and-hamiltonian-mechanics">Neural Network and Hamiltonian mechanics</h2>
<h3 id="hamilton-mechanics">Hamilton mechanics</h3>
<p>Hamilton mechanics is a form of analytical mechanics that analyzes phenomena using a mysterious characteristic function called Hamiltonian.
Hamiltonian is a physical quantity equivalent to energy in physics, and many properties of physical systems can be described in Hamiltonian.
(Hamiltonian is not necessarily limited to physical phenomena, but it is energy here because it makes it easier to imagine the following discussions when using physical phenomena as the subject.)</p>
<p>Well, let&rsquo;s put aside the difficult story and think of it as a physical quantity equivalent to the total energy of high school physics.
That is, if Hamiltonian and kinetic and potential energies are written as $\mathcal{H}, \mathcal{K}, \mathcal{U}$, they have the following relationships.</p>
<p>$$
\mathcal{H}=\mathcal{K}+\mathcal{U}
$$</p>
<p>Another important concept is that Hamiltonian dynamics introduces generalized coordinates and generalized momentum.
However, it is enough here to simply think of generalized coordinates as normal coordinates and generalized momentum as normal momentum (the product of mass and velocity). If these are written as $q,p$, respectively, they can be written in the following relationship with normal physical quantities.</p>
<p>$$
\begin{eqnarray}
q(t)&amp;=&amp;x(t)\<br>
p(t)&amp;=&amp;mv(t)
\end{eqnarray}
$$</p>
<p>It&rsquo;s very natural, but I think that there is such a drunk notation, so I will actually find the Hamiltonian about the differential equation of the spring simple vibration.</p>
<p>This is very simple, and it is enough to rewrite the kinetic and potential energies using generalized coordinates and generalized momentum respectively as follows.
$$
\begin{eqnarray}
\mathcal{K}&amp;=&amp;\frac{1}{2}mv^2(t)=\frac{1}{2m}p^2(t)\<br>
\mathcal{U}&amp;=&amp;\frac{1}{2}kx^2(t)=\frac{1}{2}kq^2(t)
\end{eqnarray}
$$
That is, the Hamiltonian of spring simple vibration is as follows.
$$
\mathcal{H}=\frac{1}{2m}p^2(t)+\frac{1}{2}kq^2(t)
$$</p>
<p>The reason why we introduce quantities such as Hamiltonian is that the symmetry of canonical transformations and canonical equations is beautiful, but there is a convenient relationship.
It&rsquo;s frustrating, but I&rsquo;ll leave the details here and believe that the following relationships exist here.
$$
\begin{eqnarray}
\frac{dq(t)}{dt}&amp;=&amp;\frac{\partial \mathcal{H}}{\partial p}\\frac{dp(t)}{dt}&amp;=&amp;-\frac{\partial \mathcal{H}}{\partial q}
\end{eqnarray}
$$</p>
<p>Then, as an example, let&rsquo;s apply Hamiltonian with simple spring vibration.
$$
\begin{eqnarray}
\frac{dq(t)}{dt}&amp;=&amp;\frac{\partial \mathcal{H}}{\partial p}=\frac{p(t)}{m}\<br>
\frac{dp(t)}{dt}&amp;=&amp;-\frac{\partial \mathcal{H}}{\partial q}=-kq(t)
\end{eqnarray}
$$</p>
<p>It is a little difficult to understand as it is, so restore the generalized coordinates and generalized momentum to the original physical quantities.
$$
\begin{eqnarray}
\frac{dx(t)}{dt}&amp;=&amp;\frac{mv(t)}{m}=v(t)\<br>
m\frac{dv(t)}{dt}&amp;=&amp;-kx(t)
\end{eqnarray}
$$</p>
<p>The first relation expresses that velocity is the time derivative of position.
The problem is the second relational expression, but this is actually the original equation of motion of the spring simple vibration.
Substituting the fact that the velocity is the time derivative of the position (first relational expression), it becomes clear that it agrees with the equation of motion of the spring simple vibration mentioned at the beginning of this article.
$$
m\frac{dv(t)}{dt}=m\frac{d^2x(t)}{dt^2}=-kx(t)
$$</p>
<p>In fact, Hamiltonian contains a great deal of important information that describes phenomena, and it is the essence of Hamiltonian mechanics to analyze phenomena from the Hamiltonian&rsquo;s point of view (if a little misleading).
Here, since we dealt with the simple motion of the spring simple vibration, it is difficult to convey the gratitude, but when the complex phenomenon such as multi-body motion or mutual coupling is brought to its full potential.</p>
<p>For Hamiltonian (and Lagrangian), you may want to read this area <sup id="fnref:15"><a href="#fn:15" class="footnote-ref" role="doc-noteref">15</a></sup> first.</p>
<h3 id="hamilton-neural-network">Hamilton Neural Network</h3>
<p>Here, we will introduce a different approach from the ODENet, which was introduced earlier, among the neural networks that express phenomena.
This method has also been adopted in NeuroIPS 2019 <sup id="fnref:8"><a href="#fn:8" class="footnote-ref" role="doc-noteref">8</a></sup>, and you can try it out immediately with this original paper <sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup> and author implementation <sup id="fnref:16"><a href="#fn:16" class="footnote-ref" role="doc-noteref">16</a></sup>.</p>
<p>This method is intuitively very simple and easy to understand.
By using the canonical equation, which is the key to Hamilton&rsquo;s mechanics, as a loss function as it is, we are learning a neural network that can represent Hamiltonian well from the inputs of generalized coordinates and generalized momentum.</p>
<p>The point here is that you are not necessarily learning Hamiltonian itself, whether good or bad, but it is important that you are learning something similar to Hamiltonian.
Therefore, even if the specific Hamiltonian of the phenomenon is unknown, we can expect to be able to construct a Neural Network that can well express its time evolution.</p>
<p>The general flow of learning in Hamilton Neural Network is as follows.</p>
<p>(1) Generalized coordinates and generalized momentum discretized in the time direction are used as input data.</p>
<p>(2) Neural Network shall be characterized by the parameter $ \theta $</p>
<p>(3) Scalar value $\mathcal{H}_{\theta}$ is output by forward propagation (note that Hamiltonian is not specifically requested at this output stage).</p>
<p>(4) Calculate the following values by back propagation using automatic differentiation.
$$
\frac{\partial \mathcal{H}_\theta}{\partial p},\:\frac{\partial \mathcal{H}_\theta}{\partial q}
$$</p>
<p>(5) Find the time derivative of generalized coordinates and generalized momentum using the information of the next time point (this corresponds to the teacher data).
$$
\frac{dq(t)}{dt},\:\frac{dp(t)}{dt}
$$</p>
<p>(6) Learn to minimize the following loss function based on the canonical equation.
$$
L_{HNN}=\left(\frac{\partial \mathcal{H}_\theta}{\partial p}-\frac{dq(t)}{dt}\right)^2+\left(\ frac{\partial \mathcal{H}_\theta}{\partial q}+\frac{dp(t)}{dt}\right)^2
$$</p>
<p>Below is a figure taken from Figure 1 of Hamiltonian Network<sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup>.</p>
<p><img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/397186/1e49b5c6-f1bb-a3c7-0d6c-9697f78bd8fd.png" alt="overall-idea.png"></p>
<p>It can be seen that the proposed method can better represent the behavior of the original physical phenomenon compared to a simple neural network called Baseline.</p>
<p>As an interesting application, as shown in Figure 4 below, is it possible to successfully reproduce the phenomenon by inputting only the image data of the physical phenomenon?
<img width="640" alt="Screen-Shot-2019-11-18-at-15.24.38.png" src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/397186/15f20ac9-cfc4-c00c-ff81-cf2de131c319.png"></p>
<p>However, the problem is that while the simplicial motion can be expressed as being relatively equivalent to the measured value, we have not been able to learn very well about the many-body motion in which the interaction works.</p>
<p>As you can see from Figure B.3. below, the behavior of the three-body movement is disintegrated from the beginning. (It seems that they will collide with each other due to their attraction.)
However, compared to Baseline, I feel that it is trying to maintain a circular orbit, so I think it is relatively good.</p>
<img width="752" alt="Screenshots 2019-12-18 15.14.38.png" src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/397186/ba416d25-6c7e-954d-ed38-9a10c182ac67.png">
<p>However, it is very interesting to be able to express the single body motion in particular, despite being a very intuitive and simple Neural Network. I think that there is still room for development, so research and development will progress.</p>
<p>For this paper, please refer to the relevant site <sup id="fnref:17"><a href="#fn:17" class="footnote-ref" role="doc-noteref">17</a></sup><sup id="fnref:18"><a href="#fn:18" class="footnote-ref" role="doc-noteref">18</a></sup> as it is easy to understand.</p>
<p>#Summary
It was an article that summarized the contents to be stabbed for those who stab like that.
I wanted to write a virtuous summary, but I am now in Nanki Shirahama at the development camp of the company, and I have less than 10 minutes left until the set time to go to dinner, so if you remember again I&rsquo;ll try! I&rsquo;m hungry. .. ..</p>
<p>It seems that the god of our team has also returned from the public bath, so here is once.</p>
<p>Tomorrow is a very virtuous article by a great senior @kirikei from the same team and the same university! ! !
We are waiting for you to sit down! ! ! I will study! ! !</p>
<section class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1" role="doc-endnote">
<p><a href="https://wakaranai-physics.com/dynamics/simple-harmonic-motion/">Don&rsquo;t know how to solve physical equation of motion [single oscillation]</a>. Taking simple oscillation as an example, we show and compare various solution methods such as Taylor expansion, exponential function, and conservation. <a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2" role="doc-endnote">
<p><a href="https://arxiv.org/abs/1806.07366">Neural Ordinary Differential Equations</a>, Ricky T. Q. Chen, Yulia Rubanova, Jesse Bettencourt, and David Duvenaud, 2018. <a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3" role="doc-endnote">
<p><a href="https://arxiv.org/abs/1906.01563">Hamiltonian Neural Networks</a>, Sam Greydanus, Misko Dzamba, and Jason Yosinski, 2019. <a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:4" role="doc-endnote">
<p><a href="https://www.amazon.co.jp/dp/4535781737/ref=cm_sw_em_r_mt_dp_U_Esc-Db0ZXP6F3">Let&rsquo;s make a mathematical model with differential equations</a>,NipponHyoronsha(1990/4/9). <a href="#fnref:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:5" role="doc-endnote">
<p><a href="https://arxiv.org/abs/1512.03385">Deep Residual Learning for Image Recognition</a>, Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun, 2015. <a href="#fnref:5" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:6" role="doc-endnote">
<p><a href="https://arxiv.org/abs/1707.04585">The Reversible Residual Network: Backpropagation Without Storing Activations</a>, Aidan N. Gomez, Mengye Ren, Raquel Urtasun, and Roger B. Grosse, 2017. <a href="#fnref:6" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:7" role="doc-endnote">
<p><a href="https://www.amazon.co.jp/dp/4065162629/ref=cm_sw_em_r_mt_dp_U_SWf-DbJKCWVAA">Understanding the principles of deep learning and physics, application is possible</a>,Kodansha(2019/6/22)). <a href="#fnref:7" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:8" role="doc-endnote">
<p><a href="https://nips.cc">Neural Information Processing Systems</a>, NeuroIPS for short. At this time of today, the old abbreviation NIPS still fits better. <a href="#fnref:8" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:9" role="doc-endnote">
<p><a href="https://github.com/rtqichen/torchdiffeq">Github rtqichen/torchdiffeq PyTorch Implementation of Differentiable ODE Solvers</a>, author implementation of the paper <sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>. <a href="#fnref:9" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:10" role="doc-endnote">
<p><a href="https://blog.evjang.com/2018/01/nf1.html">Eric Jang Normalizing Flows Tutorial, Part 1: Distributions and Determinants</a> is easy to understand. <a href="#fnref:10" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:11" role="doc-endnote">
<p><a href="https://www.slideshare.net/DeepLearningJP2016/dlneural-ordinary-differential-equations">SlideShare [DL reading] Neural Ordinary Differential Equations</a>, The content of the paper <sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup> is systematic. It is summarized. <a href="#fnref:11" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:12" role="doc-endnote">
<p><a href="https://github.com/yoheikikuta/paper-reading/issues/21">Github yoheikikuta/paper-reading [2018] Neural Ordinary Differential Equations</a>, Paper <sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup> Easy to understand. <a href="#fnref:12" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:13" role="doc-endnote">
<p><a href="https://ainow.ai/2018/12/25/159218/">AINOW [NIPS 2018 Best Paper] From the University of Toronto: A completely new Neural Network model that connects intermediate layers in a continuous space that is differentiable</a>, the contents of the paper <sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup> are very crushed and give an image. <a href="#fnref:13" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:14" role="doc-endnote">
<p><a href="https://github.com/rtqichen/ffjord">Github rtqichen/ffjord code for &ldquo;FFJORD: Free-form Continuous Dynamics for Scalable Reversible Generative Models&rdquo;.</a>.Thisistheauthor&rsquo;simplementationof<a href="https://arxiv.org/abs/1810.01367">FFJORD:Free-formContinuousDynamicsforScalableReversibleGenerativeModels</a>. <a href="#fnref:14" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:15" role="doc-endnote">
<p>Refer to <a href="https://takun-physics.net/?p=4843">Mantis analytic mechanics in space, Lagrangian and Hamiltonian forms</a>. The comparison with the Lagrange format is particularly easy to understand.
By the way, the story like high school physics before that is called Newton mechanics. <a href="#fnref:15" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:16" role="doc-endnote">
<p><a href="https://github.com/greydanus/hamiltonian-nn">Github graydanus/hamiltonian-nn Hamiltonian Neural Networks</a>, author&rsquo;s implementation of the paper <sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup>. <a href="#fnref:16" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:17" role="doc-endnote">
<p><a href="https://ai-scholar.tech/deep-learning/hamiltonian-ai-344/">Hamiltonian Neural Networks that enables prediction of object motion that satisfies the AI-SCHOLAR energy conservation law</a>, Always I am indebted. <a href="#fnref:17" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:18" role="doc-endnote">
<p><a href="https://github.com/yoheikikuta/paper-reading/issues/46">Github yoheikikuta/paper-reading [2019] Hamiltonian Neural Networks</a>, easy to understand. <a href="#fnref:18" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</section>

    </article>
  </div>

  
  
  
  <script>
  window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
  ga('create', 'UA-169005401-1', 'auto');
  ga('send', 'pageview');
  </script>
  <script async src='https://www.google-analytics.com/analytics.js'></script>
  

  

  
<link rel="stylesheet" type="text/css" href="//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.0/cookieconsent.min.css" />
<script src="//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.0/cookieconsent.min.js"></script>
<script>
window.addEventListener("load", function(){
window.cookieconsent.initialise({
  "palette": {
    "popup": {
      "background": "#216942",
      "text": "#b2d192"
    },
    "button": {
      "background": "#afed71"
    }
  }
})});
</script>

</body>

</html>
