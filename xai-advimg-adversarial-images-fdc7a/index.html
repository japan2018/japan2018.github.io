<!doctype html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  
  <meta name="generator" content="Hugo 0.72.0" />

  
  <meta name="description" content="Some description">
  

  
  <link rel="apple-touch-icon" sizes="180x180" href="https://japan2018.github.io/apple-touch-icon.png">

  
  <link rel="icon" type="image/png" sizes="32x32" href="https://japan2018.github.io/favicon-32x32.png">

  
  <link rel="icon" type="image/png" sizes="16x16" href="https://japan2018.github.io/favicon-16x16.png">

  
  <link rel="manifest" href="https://japan2018.github.io/site.webmanifest">

  
  <link rel="mask-icon" href="https://japan2018.github.io/safari-pinned-tab.svg" color="#5bbad5">

  <meta name="msapplication-TileColor" content="#da532c">

  <meta name="theme-color" content="#ffffff">

  
  <link rel="stylesheet" href="https://japan2018.github.io/css/bootstrap.min.css" />

  
  <title>XAI : AdvImg - Adversarial Images | Some Title</title>
  

  <style>
body {
  min-width: 300px;
}

.custom-navbar {
  margin-bottom: 1em;
  height: 60px;
}

.custom-navbar a {
  display: inline-block; 
  padding: 18px 0;
  margin-right: 1em; 
  font-weight: bold; 
}

.custom-navbar a:hover,
.custom-navbar a:focus {
  text-decoration: none; 
}

@media print {
  .custom-navbar {
    display: none;
  }
}

article {
  padding-bottom: 1em;
}

img {
  max-width: 100%;
}


body {
  background-color: #fff;
}



body {
  color: #212529;
}



a {
  color: #007bff;
}



a:hover,
a:focus {
  color: #0056b3;
}



.custom-navbar {
  background-color: #212529;
}



.custom-navbar a {
  color: rgba(255, 255, 255, 0.75);
}



.custom-navbar a:hover,
.custom-navbar a:focus {
  color: rgba(255, 255, 255, 1);
}



.container {
  max-width: 800px;
}



pre {
  display: block;
  padding: 9.5px;
  word-break: break-all;
  word-wrap: break-word;
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 4px;
}

pre code {
  padding: 0;
  font-size: inherit;
  color: inherit; 
  white-space: pre-wrap;
  background-color: transparent;
  border: none;
  border-radius: 0;
}

code {
  padding: 2px 4px;
  color: inherit; 
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 4px;
  font-size: .9em;
}



blockquote,
.blockquote {
  padding: 10px 20px;
  margin: 0 0 20px;
  font-size: 1em;
  border-left: 5px solid #6c757d;
}

</style>
</head>

<body>
  <nav class="custom-navbar">
  <div class="container">
    
    <a href="/">Posts</a>
    
    <a href="/tags/">Tags</a>
    
    <a href="/about/">About</a>
    
    <a href="/index.xml">RSS</a>
    
  </div>
</nav>
  
  <div class="container">
    <article>
      <h1>XAI : AdvImg - Adversarial Images</h1>
<p>
  <small class="text-secondary">
  
  
  Jan 6, 2020
  </small>
  

<small><code><a href="https://japan2018.github.io/tags/python">Python</a></code></small>


<small><code><a href="https://japan2018.github.io/tags/deeplearning"> DeepLearning</a></code></small>


<small><code><a href="https://japan2018.github.io/tags/cntk"> CNTK</a></code></small>


<small><code><a href="https://japan2018.github.io/tags/alexnet"> Alexnet</a></code></small>


<small><code><a href="https://japan2018.github.io/tags/adversarialexamples"> AdversarialExamples</a></code></small>

</p>
<pre><code>#Target
</code></pre>
<p>I tried to generate a hostile image using Microsoft Cognitive Toolkit (CNTK).</p>
<p>It is assumed that CNTK and NVIDIA GPU CUDA are installed.</p>
<p>#Introduction</p>
<h2 id="what-is-a-hostile-image">What is a hostile image?</h2>
<p>A very interesting model in the field of deep learning is the Adversarial Generating Network (GAN) <a href="#reference">[1]</a>.AdversarialExamples<a href="#reference">[2]</a><a href="#reference">[3]</a> influenced the birth of GAN. We will focus on the images here, so we will be dealing with hostile images.</p>
<p>Adversarial images are known to be a very effective way to deceive trained image recognition models. Specifically, by adding a small perturbation to the original image, the image recognition model generates an image that looks almost the same to humans, but outputs a result different from the recognition result of the original image. ..</p>
<h2 id="pre-learned-alexnet">Pre-learned AlexNet</h2>
<p>This time, we used pre-trained AlexNet <a href="#">[4]</a>.AlexNetwonthe<a href="http://image-net.org/challenges/LSVRC/">ILSVRC</a> in 2012 with a big difference over the existing methods, and the famous image that triggered the third neural network boom The recognition model consists of 5 convolutional layers, ReLU function, Local Response Normalization, 3x3-s2 maximum pooling, and fully connected layers.</p>
<p>You can download the pre-trained AlexNet saved in the CNTK format from the following URL.</p>
<p><a href="https://www.cntk.ai/Models/CNTK_Pretrained/AlexNet_ImageNet_CNTK.model">https://www.cntk.ai/Models/CNTK_Pretrained/AlexNet_ImageNet_CNTK.model</a></p>
<h2 id="adversarial-image-generation-algorithm">Adversarial image generation algorithm</h2>
<p>This time, we implemented three algorithms introduced in the paper <a href="#reference">[5]</a>.</p>
<p>###Fast method
The Fast method is very fast as it only needs to calculate backpropagation once.</p>
<pre><code class="language-math" data-lang="math">X^{adversarial} = X + \epsilon sign(\nabla J(X, y_{true}))
</code></pre><p>Where $X$ represents the original image, the backpropagation for the correct category $y_{true}$ node output by the trained recognition model is encoded by the $sign$ function, and the hyperparameter $\epsilon$ is You get a hostile image by multiplying and then adding to the original image.</p>
<p>###Iterative method
Iterative method simply repeats the fast method. However, the pixel value is $Clip$ so that it is not greatly separated from the original image.</p>
<pre><code class="language-math" data-lang="math">X^{adversarial}_{i = 0} = X \\
X^{adversarial}_{i = N} = Clip_{(X-\epsilon, X + \epsilon)} \left\{X^{adversarial}_{i = N-1} + \alpha sign(\nabla J(X^{adversarial}_{i = N}, y_{true})) \right\}
</code></pre><p>Where $N$ is the number of iterations and $\alpha$ is the hyperparameter.</p>
<p>###Iterative Least-Likely class method
The above two methods can be done without identifying the categories that the model should misidentify, and will work if the categories are few or well separated. However, it may not work well when there are many classification categories or when there is no significant difference in the distance of each category.</p>
<p>Therefore, the Iterative least-likey class method takes measures to bring it closer to the category with the lowest recognition rate.</p>
<pre><code class="language-math" data-lang="math">y_{LL} = \arg\min_y p(y|X)
</code></pre><p>Perform the Iterative method on the category $y_{LL}$ with the lowest value for the original image.</p>
<pre><code class="language-math" data-lang="math">X^{adversarial}_{i = 0} = X \\
X^{adversarial}_{i = N} = Clip_{(X-\epsilon, X + \epsilon)} \left\{X^{adversarial}_{i = N-1} + \alpha sign(\nabla J(X^{adversarial}_{i = N}, y_{LL})) \right\}
</code></pre><p>#Implementation</p>
<h2 id="execution-environment">Execution environment</h2>
<p>###hardware
・CPU Intel(R) Core(TM) i7-6700K 4.00GHz
・GPU NVIDIA GeForce GTX 1060 6GB</p>
<p>###software
・Windows 10 Pro 1909
・CUDA 10.0
・CuDNN 7.6
・Python 3.6.6
・Cntk-gpu 2.7
・Numpy 1.17.3
・Opencv-contrib-python 4.1.1.26</p>
<h2 id="program-to-execute">Program to execute</h2>
<p>The implemented program is published on <a href="https://github.com/sho-watari/XAI/tree/master/AdvImg">GitHub</a>.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-Python:advimg.py" data-lang="Python:advimg.py"></code></pre></div><p>‥
#result
The figure below is a visualization of the generated hostile image. The upper left shows the original image, the upper right shows the fast method, the lower left shows the iterative fast method, and the lower right shows the hostile image generated by the iterative least-likely class method.</p>
<p>You can recognize any image as a panda for humans, but the prediction results of the hostile images generated by each method have changed.</p>
<p><img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/176315/cc041d18-9873-b9a8-b53b-4f7020811e17.png" alt="fast_iterative_least.png"></p>
<p>The original image is correctly recognized as giant panda 67.39%, but the hostile image generated by the fast method is gibbon 56.97%, init 98.84% in the iterative method and admiral 99.39% in the iterative least-likely class method. It has become.</p>
<p>Here, if you look closely at the hostile image generated by the fast method, you can see a mysterious pattern. On the other hand, the hostile images generated by the other two methods give the impression that they are more subtle adversarial images, with only some tint changes in the image.</p>
<p>Also, the hyperparameter $\epsilon$ is set to 8 this time, but it has been investigated that increasing the value of $\epsilon$ increases the false recognition rate [[5]](#(Reference), It seems that the recognition rate drops significantly when it is set to about 8 or more.</p>
<p>#reference
<a href="http://image-net.org/challenges/LSVRC/">ImageNet Large Scale Visual Recognition Challenge (ILSVCR)</a></p>
<ol>
<li>Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mira, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. &ldquo;Generative Adversarial Nets&rdquo;, Advances in neural information processing systems. 2014, pp 2672 -2680.</li>
<li>Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rub Fergus. &ldquo;Intriguing properties of neural networks&rdquo;, arXiv preprint arXiv:1312.6199 (2013).</li>
<li>Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy, &ldquo;Explaining and Harnessing Adversarial Examples&rdquo;, arXiv preprint arXiv:1412.6572 (2014),</li>
<li>Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. &ldquo;ImageNet Classifcation with Deep Convolutional Neural Networks&rdquo;, Advances in neural information processing systems. 2012, pp 1097-1105.</li>
<li>Alexey Kurakin, Ian J. Goodfellow, and Samy Bengio. &ldquo;Adversarial Examples in the physical world&rdquo;, arXiv preprint arXiv:1607.02533 (2016).</li>
</ol>

    </article>
  </div>

  
  
  
  <script>
  window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
  ga('create', 'UA-123456789-1', 'auto');
  ga('send', 'pageview');
  </script>
  <script async src='https://www.google-analytics.com/analytics.js'></script>
  

  

  
<link rel="stylesheet" type="text/css" href="//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.0/cookieconsent.min.css" />
<script src="//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.0/cookieconsent.min.js"></script>
<script>
window.addEventListener("load", function(){
window.cookieconsent.initialise({
  "palette": {
    "popup": {
      "background": "#216942",
      "text": "#b2d192"
    },
    "button": {
      "background": "#afed71"
    }
  }
})});
</script>

</body>

</html>
