<!doctype html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  
  <meta name="generator" content="Hugo 0.72.0" />

  
  <meta name="description" content="Some description">
  

  
  <link rel="apple-touch-icon" sizes="180x180" href="https://memotut.com/apple-touch-icon.png">

  
  <link rel="icon" type="image/png" sizes="32x32" href="https://memotut.com/favicon-32x32.png">

  
  <link rel="icon" type="image/png" sizes="16x16" href="https://memotut.com/favicon-16x16.png">

  
  <link rel="manifest" href="https://memotut.com/site.webmanifest">

  
  <link rel="mask-icon" href="https://memotut.com/safari-pinned-tab.svg" color="#5bbad5">

  <meta name="msapplication-TileColor" content="#da532c">

  <meta name="theme-color" content="#ffffff">

  
  <link rel="stylesheet" href="https://memotut.com/css/bootstrap.min.css" />

  
  <title>[Python] I tried to visualize Boeing playing violin with pose estimation | Memo Tut</title>
  

  <style>
body {
  min-width: 300px;
}

.custom-navbar {
  margin-bottom: 1em;
  height: 60px;
}

.custom-navbar a {
  display: inline-block; 
  padding: 18px 0;
  margin-right: 1em; 
  font-weight: bold; 
}

.custom-navbar a:hover,
.custom-navbar a:focus {
  text-decoration: none; 
}

@media print {
  .custom-navbar {
    display: none;
  }
}

article {
  padding-bottom: 1em;
}

img {
  max-width: 100%;
}


body {
  background-color: #fff;
}



body {
  color: #212529;
}



a {
  color: #007bff;
}



a:hover,
a:focus {
  color: #0056b3;
}



.custom-navbar {
  background-color: #212529;
}



.custom-navbar a {
  color: rgba(255, 255, 255, 0.75);
}



.custom-navbar a:hover,
.custom-navbar a:focus {
  color: rgba(255, 255, 255, 1);
}



.container {
  max-width: 800px;
}



pre {
  display: block;
  padding: 9.5px;
  word-break: break-all;
  word-wrap: break-word;
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 4px;
}

pre code {
  padding: 0;
  font-size: inherit;
  color: inherit; 
  white-space: pre-wrap;
  background-color: transparent;
  border: none;
  border-radius: 0;
}

code {
  padding: 2px 4px;
  color: inherit; 
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 4px;
  font-size: .9em;
}



blockquote,
.blockquote {
  padding: 10px 20px;
  margin: 0 0 20px;
  font-size: 1em;
  border-left: 5px solid #6c757d;
}

</style>
</head>

<body>
  <nav class="custom-navbar">
  <div class="container">
    
    <a href="/">Posts</a>
    
    <a href="/tags/">Tags</a>
    
    <a href="/about/">About</a>
    
    <a href="/index.xml">RSS</a>
    
  </div>
</nav>
  
  <div class="container">
    <article>
      <h1>[Python] I tried to visualize Boeing playing violin with pose estimation</h1>
<p>
  <small class="text-secondary">
  
  
  Dec 31, 2019
  </small>
  

<small><code><a href="https://memotut.com/tags/python">Python</a></code></small>


<small><code><a href="https://memotut.com/tags/tensorflow">TensorFlow</a></code></small>


<small><code><a href="https://memotut.com/tags/openpose">OpenPose</a></code></small>


<small><code><a href="https://memotut.com/tags/tf-pose-estimation">tf-pose-estimation</a></code></small>

</p>
<pre><code># I tried to detect the feature point of the right wrist joint
</code></pre>
<p><img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/220817/6c682ab5-e975-6fde-2e08-2d3bc06f6306.gif" alt="ezgif.com-crop-2.gif"></p>
<p>#Introduction</p>
<p>This article was posted on the 22nd day of the Fujitsu Systems Web Technology Advent Calendar 2019.
All content in the article is the personal opinion, and the content of the writing is the author&rsquo;s own responsibility. It does not matter which organization you belong to.
(Goodbye)</p>
<h1 id="overview">Overview</h1>
<p>I enjoy playing violin as a hobby, but playing the violin is difficult.
It is widely known that it is difficult to play, thanks in part to Shizuka-chan, but there are other difficult things besides playing.</p>
<p><img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/220817/d694e127-d038-414b-ae13-f38468ea21ea.png" alt="image.png"></p>
<p>When playing the violin, there are certain things to do before playing the song.
It is called Boeing, and it is the work of deciding whether to play with an upbow or a downbow, one by one, while observing the score.</p>
<ul>
<li>V → Up Bow (bottom to top)</li>
<li>П → Downbow (top to bottom)</li>
</ul>
<p><img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/220817/6e9871a1-a93e-5052-bf0b-ce89597683d2.png" alt="image.png"></p>
<p>I am writing Boeing on the score considering the phrase feeling of the song and ease of playing, but this work is quite laborious. Boeing does not have the only correct answer, but also the player&rsquo;s sensitivity. Also, when playing with a quartet or orchestra, it may be necessary to match it with other instruments such as viola and cello.
It is no exaggeration to say that the development of an automatic Boeing machine is a dream of a string player.</p>
<p>So, even if you don&rsquo;t go to an automatic Boeing machine, can you reduce the trouble by reading Boeing from other people&rsquo;s performance videos and using it as a reference?
I thought.</p>
<p>#Strategy</p>
<p>The strategy is like this.</p>
<ul>
<li>Utilizing an open source library that allows pose estimation, the coordinates of the right wrist holding a bow are recorded momentarily from a performance video.</li>
<li>Overlay the trajectory of the coordinates of your right wrist on the score.</li>
</ul>
<p>Now you should be able to see Boeing in the performance video!
The library used is the open source tf-pose-estimation that implements OpenPose for Tensorflow.</p>
<p>What is #OpenPose?</p>
<p>OpenPose is a technology announced by CMU (Carnegie Mellon University) at CVPR2017, an international conference on computer vision, that detects keypoints and estimates the relationship between keypoints.
Using OpenPose, you can know at which coordinates the characteristic points in the human body are located, such as the positions of joints, as shown below.</p>
<p><img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/220817/6f6f3112-74bc-d752-cda5-2c4b5414aa4f.gif" alt="dance_foot.gif">
<a href="https://github.com/CMU-Perceptual-Computing-Lab/openpose">https://github.com/CMU-Perceptual-Computing-Lab/openpose</a></p>
<p>What is #tf-pose-estimation?</p>
<p>tf-pose-estimation is an implementation of the same neural network as OpenPose for Tensorflow.
I thought about using this time because <a href="https://github.com/errno-mmd/tf-pose-estimation">errno-mmd&rsquo;s tf-pose-estimation extension</a> seemed useful.
We have added an option to output the pose-estimated two-dimensional joint position information to a file in JSON format, and we wanted to use this.</p>
<h1 id="performance-video-to-analyze">Performance video to analyze</h1>
<p>I didn&rsquo;t know the free violin performance video, so I used the recording of my performance for analysis. The song played is the first 10 measures of Mozart&rsquo;s Aine Kleine Nachtzig 1 movement. I played with a metronome at a constant tempo.</p>
<p>#Slightly modified to mark only the joint point of the right wrist</p>
<p>By default, tf-pose-estimation marks various feature points such as eyes, shoulders, elbows, wrists, ankles, and displays them by connecting them with lines. This time, I modified the code a bit to mark only the right wrist.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-tf-pose-estimation/tf_pose/estimator.py" data-lang="tf-pose-estimation/tf_pose/estimator.py">
<span style="color:#ae81ff">440</span> <span style="color:#75715e"># Draw a circle image on the right wrist only</span>
<span style="color:#ae81ff">441</span> <span style="color:#66d9ef">if</span> i <span style="color:#f92672">==</span> CocoPart<span style="color:#f92672">.</span>RWrist<span style="color:#f92672">.</span>value:
<span style="color:#ae81ff">442</span> cv2<span style="color:#f92672">.</span>circle(npimg, center, <span style="color:#ae81ff">8</span>, common<span style="color:#f92672">.</span>CocoColors[i], thickness<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>, lineType<span style="color:#f92672">=</span><span style="color:#ae81ff">8</span>, shift<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)

<span style="color:#ae81ff">449</span> <span style="color:#75715e"># Disable the line drawing connecting feature points</span>
<span style="color:#ae81ff">450</span> <span style="color:#75715e"># cv2.line(npimg, centers[pair[0]], centers[pair[1]], common.CocoColors[pair_order], 3)</span>

</code></pre></div><h1 id="perform-pose-estimation">Perform pose estimation</h1>
<p>The execution environment used Google Colaboratory.
<a href="https://github.com/errno-mmd/tf-pose-estimation">https://github.com/errno-mmd/tf-pose-estimation</a>
After executing the setup command according to Read.md of, execute the following command.</p>
<pre><code>%run -i run_video.py --video &quot;/content/drive/My Drive/violin_playing/EineKleineNachtmusik_20191226.mp4&quot; --model mobilenet_v2_large --write_json &quot;/content/drive/My Drive/violin_playing/json&quot; --no_display - number_people_max 1 --write_video &quot;/content/drive/My Drive/violin_playing/EineKleine_keypoints_20191226.mp4&quot;
</code></pre><p>Using the performance video uploaded in advance to Google Drive as input, a video in which the joint points of the right wrist are drawn and a JSON file in which the coordinates of the joint points for each frame are written are output.</p>
<h1 id="output-video-file">Output video file</h1>
<p>Here is a gif of the output video cut out for a few seconds.</p>
<p><img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/220817/6c682ab5-e975-6fde-2e08-2d3bc06f6306.gif" alt="ezgif.com-crop-2.gif"></p>
<p>As far as you can see, it looks like it is tracing the joint point of the right wrist with a certain degree of accuracy.</p>
<h1 id="output-json-file">Output JSON file</h1>
<p>The JSON file in which the coordinates of feature points are written is output for each frame (1/60 second).
Here is the 10th frame JSON file.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-000000000010_keypoints.json" data-lang="000000000010_keypoints.json">{
<span style="color:#f92672">&#34;version&#34;</span>: <span style="color:#ae81ff">1.2</span>,
<span style="color:#f92672">&#34;people&#34;</span>: [
{
<span style="color:#f92672">&#34;pose_keypoints_2d&#34;</span>: [
<span style="color:#ae81ff">265.5925925925926</span>,
<span style="color:#ae81ff">113.04347826086956</span>,
<span style="color:#ae81ff">0.7988795638084412</span>,
<span style="color:#ae81ff">244.55555555555557</span>,
<span style="color:#ae81ff">147.82608695652175</span>,
<span style="color:#ae81ff">0.762155294418335</span>,
<span style="color:#ae81ff">197.22222222222223</span>,
<span style="color:#ae81ff">149.56521739130434</span>,
<span style="color:#ae81ff">0.6929810643196106</span>,
<span style="color:#ae81ff">165.66666666666669</span>,
<span style="color:#ae81ff">189.56521739130434</span>,
<span style="color:#ae81ff">0.7044630646705627</span>,
<span style="color:#ae81ff">220.88888888888889</span>,
<span style="color:#ae81ff">166.95652173913044</span>,
<span style="color:#ae81ff">0.690696656703949</span>,
<span style="color:#ae81ff">289.2592592592593</span>,
<span style="color:#ae81ff">146.08695652173913</span>,
<span style="color:#ae81ff">0.5453883409500122</span>,
<span style="color:#ae81ff">299.77777777777777</span>,
<span style="color:#ae81ff">212.17391304347825</span>,
<span style="color:#ae81ff">0.6319900751113892</span>,
<span style="color:#ae81ff">339.22222222222223</span>,
<span style="color:#ae81ff">177.3913043478261</span>,
<span style="color:#ae81ff">0.6045356392860413</span>,
<span style="color:#ae81ff">213</span>,
<span style="color:#ae81ff">253.91304347826087</span>,
<span style="color:#ae81ff">0.23064623773097992</span>,
<span style="color:#ae81ff">0</span>,
<span style="color:#ae81ff">0</span>,
<span style="color:#ae81ff">0</span>,
<span style="color:#ae81ff">0</span>,
<span style="color:#ae81ff">0</span>,
<span style="color:#ae81ff">0</span>,
<span style="color:#ae81ff">268.22222222222223</span>,
<span style="color:#ae81ff">276.52173913043475</span>,
<span style="color:#ae81ff">0.2685505151748657</span>,
<span style="color:#ae81ff">0</span>,
<span style="color:#ae81ff">0</span>,
<span style="color:#ae81ff">0</span>,
<span style="color:#ae81ff">0</span>,
<span style="color:#ae81ff">0</span>,
<span style="color:#ae81ff">0</span>,
<span style="color:#ae81ff">257.7037037037037</span>,
<span style="color:#ae81ff">106.08695652173913</span>,
<span style="color:#ae81ff">0.8110038042068481</span>,
<span style="color:#ae81ff">270.85185185185185</span>,
<span style="color:#ae81ff">107.82608695652173</span>,
<span style="color:#ae81ff">0.7383710741996765</span>,
<span style="color:#ae81ff">231.4074074074074</span>,
<span style="color:#ae81ff">107.82608695652173</span>,
<span style="color:#ae81ff">0.7740614414215088</span>,
<span style="color:#ae81ff">0</span>,
<span style="color:#ae81ff">0</span>,
<span style="color:#ae81ff">0</span>
]
}
],
<span style="color:#f92672">&#34;face_keypoints_2d&#34;</span>: [],
<span style="color:#f92672">&#34;hand_left_keypoints_2d&#34;</span>: [],
<span style="color:#f92672">&#34;hand_right_keypoints_2d&#34;</span>: [],
<span style="color:#f92672">&#34;pose_keypoints_3d&#34;</span>: [],
<span style="color:#f92672">&#34;face_keypoints_3d&#34;</span>: [],
<span style="color:#f92672">&#34;hand_left_keypoints_3d&#34;</span>: [],
<span style="color:#f92672">&#34;hand_right_keypoints_3d&#34;</span>: []
}
</code></pre></div><p>Reading the code, the <code>13</code>th element of pose_keypoints_2d (swing from 0) seems to be the value of the y coordinate of the right wrist. In the example of the 10th frame above, it becomes <code>166.95652173913044</code>.</p>
<p>#Graph display of y-coordinate of right wrist</p>
<p>I would like to make a graph with matplotlib.
First, collect the target values from the JSON file output for each frame.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> json
<span style="color:#f92672">import</span> pprint
<span style="color:#f92672">import</span> os
<span style="color:#f92672">import</span> glob

<span style="color:#f92672">import</span> numpy <span style="color:#f92672">as</span> npfiles <span style="color:#f92672">=</span> glob<span style="color:#f92672">.</span>glob(<span style="color:#e6db74">&#39;/content/drive/My Drive/violin_playing/json/*&#39;</span>)
x <span style="color:#f92672">=</span> list(range(len(files)))
y <span style="color:#f92672">=</span> []
<span style="color:#66d9ef">for</span> file <span style="color:#f92672">in</span> files:
    <span style="color:#66d9ef">with</span> open(file) <span style="color:#66d9ef">as</span> f:
        df <span style="color:#f92672">=</span> json<span style="color:#f92672">.</span>load(f)
        y<span style="color:#f92672">.</span>append(df[<span style="color:#e6db74">&#39;people&#39;</span>][<span style="color:#ae81ff">0</span>][<span style="color:#e6db74">&#39;pose_keypoints_2d&#39;</span>][<span style="color:#ae81ff">13</span>])
</code></pre></div><p>The variable x stores the number of frames, and the variable y stores the y-coordinate value of the right wrist joint point.</p>
<p>Then graph it with matplotlib.
The memory of the x coordinate is set to 30 frames.
This is because I played Ainek at a tempo of 4 notes = 120, so 30 frames correspond to exactly one beat of a quarter note, which makes it easier to see the correspondence with the score.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> numpy <span style="color:#f92672">as</span> np
<span style="color:#f92672">import</span> matplotlib.pyplot <span style="color:#f92672">as</span> plt
<span style="color:#f92672">import</span> matplotlib.ticker <span style="color:#f92672">as</span> ticker

fig <span style="color:#f92672">=</span> plt<span style="color:#f92672">.</span>figure(figsize<span style="color:#f92672">=</span>(<span style="color:#ae81ff">120</span>, <span style="color:#ae81ff">10</span>))
ax <span style="color:#f92672">=</span> fig<span style="color:#f92672">.</span>add_subplot(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>)
ax<span style="color:#f92672">.</span>xaxis<span style="color:#f92672">.</span>set_major_locator(ticker<span style="color:#f92672">.</span>MultipleLocator(<span style="color:#ae81ff">30</span>))
ax<span style="color:#f92672">.</span>plot(x, y, <span style="color:#e6db74">&#34;red&#34;</span>, linestyle<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;solid&#39;</span>)
</code></pre></div><p>The output graph is here.</p>
<p><img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/220817/d52ef2b8-7d31-ddf5-d6f1-0b01f11ad080.png" alt="image.png"></p>
<h1 id="overlay">Overlay</h1>
<p>Here are the results overlaid on the score. The overlay was done manually.
<img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/220817/75ac9e0c-9753-7252-b3be-b19f24003bb4.png" alt="image.png"></p>
<p>When you are downbow, you should be heading down, and when you are upbow, you should be heading up, but&hellip; well&hellip; I think it fits well (^_^;)
I feel that.</p>
<p>#Summary</p>
<p>To be honest, the feeling is that the level of utilization will not actually reach this level.
With this technique, it seems to be difficult to identify small passage Boeings such as 16th notes.
If it is a more relaxed song, it may be helpful to some extent.</p>
<p>However, the following issues remain in terms of matching with the musical score.</p>
<ul>
<li>In the score, the width of one bar is not uniform</li>
<li>Similarly, the beats are not evenly spaced</li>
<li>Tempo changes during the song (Adagio → Allegro, etc.)</li>
<li>Even if the tempo notation is the same, the tempo slightly fluctuates according to the phrase</li>
</ul>
<p>The above problem must be overcome because the efficiency of the bowing work will not be improved unless the music score is matched automatically.
Recognizing the pitch and mapping with the musical score may be one solution. The difficulty seems to be high (^_^;)</p>
<p>This time, I focused on improving the efficiency of bowing work, but the approach of visualizing the difference in how to handle bows between expert and beginners and gaining awareness from it is also interesting.
I felt that the technique of pose estimation from this video is a wonderful technique with various potential applications.</p>

    </article>
  </div>

  
  
  
  <script>
  window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
  ga('create', 'UA-169005401-1', 'auto');
  ga('send', 'pageview');
  </script>
  <script async src='https://www.google-analytics.com/analytics.js'></script>
  

  

  
<link rel="stylesheet" type="text/css" href="//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.0/cookieconsent.min.css" />
<script src="//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.0/cookieconsent.min.js"></script>
<script>
window.addEventListener("load", function(){
window.cookieconsent.initialise({
  "palette": {
    "popup": {
      "background": "#216942",
      "text": "#b2d192"
    },
    "button": {
      "background": "#afed71"
    }
  }
})});
</script>

</body>

</html>
