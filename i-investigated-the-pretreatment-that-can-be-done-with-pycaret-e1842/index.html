<!doctype html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  
  <meta name="generator" content="Hugo 0.72.0" />

  
  <meta name="description" content="Some description">
  

  
  <link rel="apple-touch-icon" sizes="180x180" href="https://memotut.com/apple-touch-icon.png">

  
  <link rel="icon" type="image/png" sizes="32x32" href="https://memotut.com/favicon-32x32.png">

  
  <link rel="icon" type="image/png" sizes="16x16" href="https://memotut.com/favicon-16x16.png">

  
  <link rel="manifest" href="https://memotut.com/site.webmanifest">

  
  <link rel="mask-icon" href="https://memotut.com/safari-pinned-tab.svg" color="#5bbad5">

  <meta name="msapplication-TileColor" content="#da532c">

  <meta name="theme-color" content="#ffffff">

  
  <link rel="stylesheet" href="https://memotut.com/css/bootstrap.min.css" />

  
  <title>I investigated the pretreatment that can be done with PyCaret | Memo Tut</title>
  

  <style>
body {
  min-width: 300px;
}

.custom-navbar {
  margin-bottom: 1em;
  height: 60px;
}

.custom-navbar a {
  display: inline-block; 
  padding: 18px 0;
  margin-right: 1em; 
  font-weight: bold; 
}

.custom-navbar a:hover,
.custom-navbar a:focus {
  text-decoration: none; 
}

@media print {
  .custom-navbar {
    display: none;
  }
}

article {
  padding-bottom: 1em;
}

img {
  max-width: 100%;
}


body {
  background-color: #fff;
}



body {
  color: #212529;
}



a {
  color: #007bff;
}



a:hover,
a:focus {
  color: #0056b3;
}



.custom-navbar {
  background-color: #212529;
}



.custom-navbar a {
  color: rgba(255, 255, 255, 0.75);
}



.custom-navbar a:hover,
.custom-navbar a:focus {
  color: rgba(255, 255, 255, 1);
}



.container {
  max-width: 800px;
}



pre {
  display: block;
  padding: 9.5px;
  word-break: break-all;
  word-wrap: break-word;
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 4px;
}

pre code {
  padding: 0;
  font-size: inherit;
  color: inherit; 
  white-space: pre-wrap;
  background-color: transparent;
  border: none;
  border-radius: 0;
}

code {
  padding: 2px 4px;
  color: inherit; 
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 4px;
  font-size: .9em;
}



blockquote,
.blockquote {
  padding: 10px 20px;
  margin: 0 0 20px;
  font-size: 1em;
  border-left: 5px solid #6c757d;
}

</style>
</head>

<body>
  <nav class="custom-navbar">
  <div class="container">
    
    <a href="/">Posts</a>
    
    <a href="/tags/">Tags</a>
    
    <a href="/about/">About</a>
    
    <a href="/index.xml">RSS</a>
    
  </div>
</nav>
  
  <div class="container">
    <article>
      <h1>I investigated the pretreatment that can be done with PyCaret</h1>
<p>
  <small class="text-secondary">
  
  
  Apr 29, 2020
  </small>
  

<small><code><a href="https://memotut.com/tags/python">Python</a></code></small>

</p>
<pre><code>I would like to share what I learned while making corrections and additions as needed.
</code></pre>
<p>*Currently, I focus on regression tasks.
Please note that other tasks may have different specifications.
(As far as we can check roughly, specifications seem to be common between tasks.)</p>
<p>#About this document
The focus is on PyCaret&rsquo;s <strong>preprocessing</strong>.
Basically, I have not touched on modeling and tuning.</p>
<p>While actually running, I am writing while reading the original source code.
<a href="https://github.com/pycaret/pycaret">https://github.com/pycaret/pycaret</a></p>
<p>*Please note that some parts may be incorrect.</p>
<h1 id="implementation-assumptions">Implementation assumptions</h1>
<p>It is assumed that various libraries are imported as follows.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> pandas <span style="color:#f92672">as</span> pd
<span style="color:#f92672">import</span> numpy <span style="color:#f92672">as</span> np
</code></pre></div><p>What is #PyCaret
A library that automates data preprocessing and machine learning model training and can be deployed in a low-code environment.
<a href="https://pycaret.org/">https://pycaret.org/</a></p>
<p>In addition, the installation is one pip command. Very easy. ..</p>
<pre><code>pip install pycaret
</code></pre><p>See this article for an overview and a series of pipeline implementation methods.
<a href="https://qiita.com/tani_AI_Academy/items/62151d7e151024733919">https://qiita.com/tani_AI_Academy/items/62151d7e151024733919</a></p>
<p>#Pre-processing method
With PyCaret, you can specify the preprocessing you want to execute with a parameter.
In addition, PyCaret asks the user about some processing contents before operation.
The operation flow is as follows.</p>
<img width=460 src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/254095/05c067e3-81c9-473c-8c5d-fcdb57ec9559.png">
<h2 id="call-the-data-inputpreprocessing-execution-function">Call the data input/preprocessing execution function</h2>
<p>By calling <code>setup()</code> of the package prepared for each task such as classification and regression, the following preprocessing is executed.</p>
<ul>
<li>Data cleaning and data conversion</li>
<li>Train/test data division</li>
<li>Data sampling</li>
</ul>
<p>I want PyCaret to process ** Preprocessing can be specified by giving it to <code>setup()</code> as an argument **.
Only &ldquo;target (target variable)&rdquo; is required as an argument.</p>
<p>In the following explanation, I would like to acquire and execute the data attached to PyCaret.
You can check the data attached to PyCaret on the home page.
<a href="https://pycaret.org/get-data/">https://pycaret.org/get-data/</a></p>
<ul>
<li>Of course, it is also possible to read and use your own data with pandas.</li>
</ul>
<p>The code to execute data acquisition and preprocessing is as follows.
Here, only the argument &ldquo;target&rdquo; is specified. Other options have default values.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">from</span> pycaret.datasets <span style="color:#f92672">import</span> get_data
dataset <span style="color:#f92672">=</span> get_data(<span style="color:#e6db74">&#34;diamond&#34;</span>)

<span style="color:#f92672">from</span> pycaret.regression <span style="color:#f92672">import</span> <span style="color:#f92672">*</span>
setup(dataset, target<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;Price&#34;</span>)
</code></pre></div><p>*Although the return value is passed through here, multiple values such as preprocessed data are returned.
For the return value, check <a href="dataafterpreprocessingisreturnedasthereturnvalueofsetup()">Contents described later</a>.</p>
<h2 id="check-the-estimation-result-of-the-type-of-each-variable">Check the estimation result of the type of each variable</h2>
<p>When you execute <code>setup()</code>, <strong>PyCaret first estimates the type of each variable (Data Type), and prompts the user to confirm the estimation result and continue processing</strong>.
If the result of type estimation is correct, enter the Enter key in the edit box in the blue frame in the figure to continue the process.
If the inferred type is weird, you can abort the process by typing &ldquo;quit&rdquo;.</p>
<p><img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/254095/8873766a-d161-938c-949f-e96593efe16b.png" alt="image.png"></p>
<p>Variables whose type is inferred can be resolved by explicitly specifying the type in <code>setup()</code>.
(For details, see <a href="NumericFeatures,CategoricalFeatures">Contents described later</a>.)</p>
<h2 id="check-the-execution-summary-of-preprocessing">Check the execution summary of preprocessing</h2>
<p>When the execution of <code>setup()</code> is completed, the processing contents are output in the data frame format.</p>
<table>
<thead>
<tr>
<th align="left"></th>
<th align="left">Description</th>
<th align="left">Value</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">0</td>
<td align="left">session_id</td>
<td align="left">3104</td>
</tr>
<tr>
<td align="left">1</td>
<td align="left">Transform Target</td>
<td align="left">False</td>
</tr>
<tr>
<td align="left">2</td>
<td align="left">Transform Target Method</td>
<td align="left">None</td>
</tr>
<tr>
<td align="left">3</td>
<td align="left">Original Data</td>
<td align="left">(6000, 8)</td>
</tr>
<tr>
<td align="left">4</td>
<td align="left">Missing Values</td>
<td align="left">False</td>
</tr>
<tr>
<td align="left">5</td>
<td align="left">Numeric Features</td>
<td align="left">1</td>
</tr>
<tr>
<td align="left">6</td>
<td align="left">Categorical Features</td>
<td align="left">6</td>
</tr>
<tr>
<td align="left">7</td>
<td align="left">Ordinal Features</td>
<td align="left">False</td>
</tr>
<tr>
<td align="left">8</td>
<td align="left">High Cardinality Features</td>
<td align="left">False</td>
</tr>
<tr>
<td align="left">9</td>
<td align="left">High Cardinality Method</td>
<td align="left">None</td>
</tr>
<tr>
<td align="left">10</td>
<td align="left">Sampled Data</td>
<td align="left">(6000, 8)</td>
</tr>
<tr>
<td align="left">11</td>
<td align="left">Transformed Train Set</td>
<td align="left">(4199, 28)</td>
</tr>
<tr>
<td align="left">12</td>
<td align="left">Transformed Test Set</td>
<td align="left">(1801, 28)</td>
</tr>
<tr>
<td align="left">13</td>
<td align="left">Numeric Imputer</td>
<td align="left">mean</td>
</tr>
<tr>
<td align="left">14</td>
<td align="left">Categorical Imputer</td>
<td align="left">constant</td>
</tr>
<tr>
<td align="left">15</td>
<td align="left">Normalize</td>
<td align="left">False</td>
</tr>
<tr>
<td align="left">16</td>
<td align="left">Normalize Method</td>
<td align="left">None</td>
</tr>
<tr>
<td align="left">17</td>
<td align="left">Transformation</td>
<td align="left">False</td>
</tr>
<tr>
<td align="left">18</td>
<td align="left">Transformation Method</td>
<td align="left">None</td>
</tr>
<tr>
<td align="left">19</td>
<td align="left">PCA</td>
<td align="left">False</td>
</tr>
<tr>
<td align="left">20</td>
<td align="left">PCA Method</td>
<td align="left">None</td>
</tr>
<tr>
<td align="left">21</td>
<td align="left">PCA Components</td>
<td align="left">None</td>
</tr>
<tr>
<td align="left">22</td>
<td align="left">Ignore Low Variance</td>
<td align="left">False</td>
</tr>
<tr>
<td align="left">23</td>
<td align="left">Combine Rare Levels</td>
<td align="left">False</td>
</tr>
<tr>
<td align="left">24</td>
<td align="left">Rare Level Threshold</td>
<td align="left">None</td>
</tr>
<tr>
<td align="left">25</td>
<td align="left">Numeric Binning</td>
<td align="left">False</td>
</tr>
<tr>
<td align="left">26</td>
<td align="left">Remove Outliers</td>
<td align="left">False</td>
</tr>
<tr>
<td align="left">27</td>
<td align="left">Outliers Threshold</td>
<td align="left">None</td>
</tr>
<tr>
<td align="left">28</td>
<td align="left">Remove Multicollinearity</td>
<td align="left">False</td>
</tr>
<tr>
<td align="left">29</td>
<td align="left">Multicollinearity Threshold</td>
<td align="left">None</td>
</tr>
<tr>
<td align="left">30</td>
<td align="left">Clustering</td>
<td align="left">False</td>
</tr>
<tr>
<td align="left">31</td>
<td align="left">Clustering Iteration</td>
<td align="left">None</td>
</tr>
<tr>
<td align="left">32</td>
<td align="left">Polynomial Features</td>
<td align="left">False</td>
</tr>
<tr>
<td align="left">33</td>
<td align="left">Polynomial Degree</td>
<td align="left">None</td>
</tr>
<tr>
<td align="left">34</td>
<td align="left">Trignometry Features</td>
<td align="left">False</td>
</tr>
<tr>
<td align="left">35</td>
<td align="left">Polynomial Threshold</td>
<td align="left">None</td>
</tr>
<tr>
<td align="left">36</td>
<td align="left">Group Features</td>
<td align="left">False</td>
</tr>
<tr>
<td align="left">37</td>
<td align="left">Feature Selection</td>
<td align="left">False</td>
</tr>
<tr>
<td align="left">38</td>
<td align="left">Features Selection Threshold</td>
<td align="left">None</td>
</tr>
<tr>
<td align="left">39</td>
<td align="left">Feature Interaction</td>
<td align="left">False</td>
</tr>
<tr>
<td align="left">40</td>
<td align="left">Feature Ratio</td>
<td align="left">False</td>
</tr>
<tr>
<td align="left">41</td>
<td align="left">Interaction Threshold</td>
<td align="left">None</td>
</tr>
</tbody>
</table>
<p>From this table, you can check **the data size, the number of features, and the presence or absence of various pre-processing specifications.
By default, most options are disabled (False or None).</p>
<p>If you specify an option in the argument of <code>setup()</code>, the corresponding item will be &ldquo;True&rdquo; and will be colored.
<img width=300 src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/254095/b3f9e20a-973c-6eaf-2d02-76902c9108d0.png"></p>
<p>The contents of various items will be explained in the following sections.</p>
<h3 id="information-about-the-session">Information about the session</h3>
<h4 id="session_id">session_id</h4>
<table>
<thead>
<tr>
<th align="left"></th>
<th align="left">Description</th>
<th align="left">Value</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">0</td>
<td align="left">session_id</td>
<td align="left">3104</td>
</tr>
</tbody>
</table>
<p>It is an identifier at the time of execution of PyCaret, and it seems to be used internally as a seed for random numbers.
If not specified, it will be determined randomly.</p>
<p>It can be specified by the argument &ldquo;session_id&rdquo; of <code>setup()</code>.
Specify this value to maintain reproducibility during repeated execution.
(This is an image similar to &ldquo;random_state&rdquo; in scikit-learn.)</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">setup(dataset, target<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;Price&#34;</span>, session_id<span style="color:#f92672">=</span><span style="color:#ae81ff">123</span>)
</code></pre></div><h3 id="information-about-input-data">Information about input data</h3>
<h4 id="original-data">Original Data</h4>
<table>
<thead>
<tr>
<th align="left"></th>
<th align="left">Description</th>
<th align="left">Value</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">3</td>
<td align="left">Original Data</td>
<td align="left">(6000, 8)</td>
</tr>
<tr>
<td align="left">The size (shape) of the input data is output.</td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>When I actually check it, it is certainly the same size.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">dataset<span style="color:#f92672">.</span>shape

<span style="color:#75715e">#Execution result</span>
<span style="color:#75715e"># (6000, 8)</span>
</code></pre></div><h4 id="missing-values">Missing Values</h4>
<table>
<thead>
<tr>
<th align="left"></th>
<th align="left">Description</th>
<th align="left">Value</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">4</td>
<td align="left">Missing Values</td>
<td align="left">False</td>
</tr>
<tr>
<td align="left">Whether the input data is missing is output.</td>
<td></td>
<td></td>
</tr>
<tr>
<td align="left">&ldquo;False&rdquo; is output because the data this time does not include any defects.</td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>This item will be &ldquo;True&rdquo; if a defect is included.
<img width=300 src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/254095/927cacac-2b9c-bf21-0453-83325a9d5102.png"></p>
<p>In addition, if there is a deficiency, filling in the deficiency is processed in <code>setup()</code>.
The specification of the method for filling in defects will be described later.</p>
<h4 id="numeric-features-and-categorical-features">Numeric Features and Categorical Features</h4>
<table>
<thead>
<tr>
<th align="left"></th>
<th align="left">Description</th>
<th align="left">Value</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">5</td>
<td align="left">Numeric Features</td>
<td align="left">1</td>
</tr>
<tr>
<td align="left">6</td>
<td align="left">Categorical Features</td>
<td align="left">6</td>
</tr>
</tbody>
</table>
<p>It can be specified explicitly with the arguments &ldquo;numeric_features&rdquo; and &ldquo;categorical_features&rdquo; of <code>setup()</code>.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">setup(dataset, target<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;Price&#34;</span>,
        categorical_features<span style="color:#f92672">=</span>[<span style="color:#e6db74">&#34;Cut&#34;</span>, <span style="color:#e6db74">&#34;Color&#34;</span>, <span style="color:#e6db74">&#34;Clarity&#34;</span>, <span style="color:#e6db74">&#34;Polish&#34;</span>, <span style="color:#e6db74">&#34;Symmetry&#34;</span>, <span style="color:#e6db74">&#34;Report&#34;</span>],
        numeric_features<span style="color:#f92672">=</span>[<span style="color:#e6db74">&#34;Carat Weight&#34;</span>])
</code></pre></div><p>In the above-mentioned type inference confirmation dialog using **PyCaret, if there is a variable for which the type inference is incorrect, it will be explicitly specified with this argument. **</p>
<h3 id="information-about-traintest-data-division">Information about train/test data division</h3>
<h4 id="transformed-train-set-transformed-test-set">Transformed Train Set, Transformed Test Set</h4>
<table>
<thead>
<tr>
<th align="left"></th>
<th align="left">Description</th>
<th align="left">Value</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">11</td>
<td align="left">Transformed Train Set</td>
<td align="left">(4199, 28)</td>
</tr>
<tr>
<td align="left">12</td>
<td align="left">Transformed Test Set</td>
<td align="left">(1801, 28)</td>
</tr>
<tr>
<td align="left">Each size after division is output to train / test data.</td>
<td></td>
<td></td>
</tr>
<tr>
<td align="left">The split ratio of train / test data can be specified by the argument &ldquo;train_size&rdquo; of <code>setup()</code>.</td>
<td></td>
<td></td>
</tr>
<tr>
<td align="left">The default is 0.7.</td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>Note that the number of columns is different from the input data because the number of features after preprocessing is displayed.
(This time, the number of features has been increased from 7 to 28 by preprocessing.)</p>
<h3 id="information-about-data-sampling">Information about data sampling</h3>
<h4 id="sampled-data">Sampled Data</h4>
<table>
<thead>
<tr>
<th align="left"></th>
<th align="left">Description</th>
<th align="left">Value</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">10</td>
<td align="left">Sampled Data</td>
<td align="left">(6000, 8)</td>
</tr>
<tr>
<td align="left">When data is sampled in ``setup()`, the number of data after sampling is output.</td>
<td></td>
<td></td>
</tr>
<tr>
<td align="left">**PyCaret prompts you to sample the data and perform a series of operations if the number of data rows is greater than 25,000. **</td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>If you execute <code>setup()</code> with data that has more than 25,000 rows, the sampling execution confirmation dialog will be displayed after the type inference confirmation dialog is executed.
When sampling, enter the percentage of data to be sampled in the blue edit box.
If you want to use the total number of lines without sampling, leave it blank and press the Enter key.</p>
<p>(For regression task)
<img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/254095/586d30ee-b771-2c72-8d2b-9d6d43bbc6bf.png" alt="image.png"></p>
<p>(For classification tasks)
<img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/254095/3eb58fae-023e-6166-28b7-6b5efe4ff524.png" alt="image.png"></p>
<p>The graph drawn here shows the standard of accuracy deterioration due to sampling.</p>
<ul>
<li>For regression tasks, a plot of the coefficient of determination (of the linear regression model by default)</li>
<li>In the classification task, plots of various indicators (default is logistic regression model)</li>
</ul>
<p>The model used for this plot can be specified by the argument &ldquo;sample_estimator&rdquo; of <code>setup()</code>.
For example, the code to specify RandomForestRegressor is as follows.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">from</span> sklearn.ensemble <span style="color:#f92672">import</span> RandomForestRegressor

traffic <span style="color:#f92672">=</span> get_data(<span style="color:#e6db74">&#34;traffic&#34;</span>)
setup(traffic, target<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;traffic_volume&#34;</span>, sample_estimator<span style="color:#f92672">=</span>RandomForestRegressor())
</code></pre></div><p>Also, this function itself can be turned off by specifying the argument &ldquo;sampling&rdquo; of <code>setup()</code>.
(The presence or absence of sampling is not confirmed, and the processing is continued using all data.)</p>
<h3 id="other-techniques-for-data-cleaning-and-conversion-of-features">(Other) Techniques for data cleaning and conversion of features</h3>
<p>For other items, it is information about whether or not data cleaning or feature value conversion processing has been executed and the method.
In the next chapter, we will explain the processing corresponding to each.</p>
<h1 id="data-cleaning-and-conversion-of-features">Data cleaning and conversion of features</h1>
<p>We will consider the processing contents and the specification method.</p>
<h2 id="data-after-preprocessing-is-returned-as-the-return-value-of-setup">Data after preprocessing is returned as the return value of setup()</h2>
<p>The preprocessed data and processing pipeline are returned.
It seems that it depends on the type of task you want to solve.</p>
<pre><code class="language-python:regression" data-lang="python:regression">X, y, X_train, X_test, y_train, y_test, seed, prep_pipe, target_inverse_transformer, experiment__ \
    = setup(dataset, target=&quot;Price&quot;)
</code></pre><pre><code class="language-python:classification" data-lang="python:classification">from pycaret.classification import *

dataset = get_data('credit')
X, y, X_train, X_test, y_train, y_test, seed, prep_pipe, experiment__ \
    = setup(dataset, target ='default')
</code></pre><p>The return value differs slightly between regression and classification.
Since the preprocessed data is returned to X and y, you can check the specific processing results.</p>
<p>Is it possible to further process the data preprocessed by PyCaret and set it again in PyCaret? I don&rsquo;t know about the current situation.</p>
<ul>
<li>I would like to continue to investigate.</li>
</ul>
<h2 id="exclude-features">Exclude features</h2>
<p>You can set the features to be excluded in preprocessing and subsequent modeling.</p>
<h3 id="parameter">parameter</h3>
<p>It can be executed by giving the following argument to <code>setup()</code>.</p>
<ul>
<li>ignore_features (list type of string, default = None)
<ul>
<li>Specify the column names of the features you want to exclude in the list.</li>
</ul>
</li>
</ul>
<h3 id="reference">Reference</h3>
<p>**ID and date (datetime) seems to be set to be excluded at the time of modeling ** by default.
In addition, if the date column is not recognized as a date, it seems that it can be explicitly specified with the argument &ldquo;date_features&rdquo;.</p>
<p>Also, while the correct specifications are being confirmed, even if there are columns with exactly the same data, one will automatically be excluded.</p>
<h2 id="fill-in-defects">Fill in defects</h2>
<p>Interpolate the defects by the specified method.</p>
<h3 id="parameter-1">parameter</h3>
<p>It can be executed by giving the following argument to <code>setup()</code>.</p>
<ul>
<li>numeric_imputation (string type, default ='mean&rsquo;)
<ul>
<li>Specify the method of loss filling for numerical data.
<ul>
<li>You can specify&rsquo;mean&rsquo; or&rsquo;median&rsquo;.
*&lsquo;mean&rsquo; fills the gaps with the mean.
*&lsquo;median&rsquo; fills the gap with the median.</li>
</ul>
</li>
</ul>
</li>
</ul>
<p></br></br></p>
<ul>
<li>categorical_imputation (string type, default ='constant&rsquo;)
<ul>
<li>Specify the method of loss filling for categorical data.
<ul>
<li>You can specify&rsquo;constant&rsquo; or&rsquo;mode&rsquo;.
*&lsquo;constant&rsquo; is always filled with the string &ldquo;not_available&rdquo;.
*&lsquo;mode&rsquo; is filled with the mode value for each feature.</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="reference-1">Reference</h3>
<p>At present, it is not possible to specify for each column, it seems that all are processed by a unified method.</p>
<h2 id="sequence-data-encoding">Sequence data encoding</h2>
<p>Label conversion is performed by specifying the column you want to define as ordinal data.</p>
<h3 id="parameter-2">parameter</h3>
<p>It can be executed by giving the following argument to <code>setup()</code>.</p>
<ul>
<li>ordinal_features (dictionary type, default = None)
<ul>
<li>Order Specify the column name and order of values of the data in dictionary format.</li>
</ul>
</li>
</ul>
<p>Specify with the following image.
<code>ordinal_features = {'column_name' :['low','medium','high'] }</code></p>
<p>For the value part of the dictionary data, specify the values in order from the smallest order data.</p>
<h2 id="feature-normalization">Feature normalization</h2>
<p>Normalize each feature.</p>
<h3 id="parameter-3">parameter</h3>
<p>It can be executed by giving the following argument to <code>setup()</code>.</p>
<ul>
<li>normalize (bool type, default = False)
<ul>
<li>Specify whether to execute this process (True/False).</li>
</ul>
</li>
</ul>
<p></br></br></p>
<ul>
<li>normalize_method (string type, default ='zscore&rsquo;)
<ul>
<li>Define the method (one of the following) used for normalization.
*&lsquo;zscore&rsquo;: A method called standardization, where z = (x-u) / s.
*&lsquo;minmax&rsquo;: A method called Min-Max scaling that scales to the 0-1 range.
*&lsquo;maxabs&rsquo;: Scale the maximum and minimum absolute values to 1.
<ul>
<li>`robust&rsquo;: Scale based on the quartile of the data.</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="reference-2">Reference</h3>
<p>See this article for&rsquo;robust&rsquo; scaling.
<a href="https://qiita.com/unhurried/items/7a79d2f3574fb1d0cc27">https://qiita.com/unhurried/items/7a79d2f3574fb1d0cc27</a></p>
<p>It seems that the&rsquo;robust&rsquo; scaling is strong when the dataset contains outliers.</p>
<p>For other scaling, see this article.
<a href="https://qiita.com/Umaremin/items/fbbbc6df11f78532932d">https://qiita.com/Umaremin/items/fbbbc6df11f78532932d</a></p>
<p>In general, linear algorithms tend to be more accurate when normalized, but this is not always the case, and I think that multiple experiments are required.</p>
<h2 id="consolidation-of-rare-values-in-categorical-variables">Consolidation of rare values in categorical variables</h2>
<p>In the categorical variable, the categories that are less than the specified threshold are integrated as one category.</p>
<h3 id="parameter-4">parameter</h3>
<p>It can be executed by giving the following argument to <code>setup()</code>.</p>
<ul>
<li>combine_rare_levels (bool type, default = False)
<ul>
<li>Specify whether to execute this process (True/False).</li>
</ul>
</li>
</ul>
<p></br></br></p>
<ul>
<li>rare_level_threshold (float type, default = 0.1)
<ul>
<li>Specify the threshold value to be regarded as a rare value.</li>
<li>All categories whose occurrence frequency is less than the threshold value are integrated as one category.
<ul>
<li>Effective only when there are two or more categories that are below the threshold.* The integrated category is defined with a name such as &ldquo;XXX_others_infrequent&rdquo;.</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="reference-3">Reference</h3>
<p>Generally, this method avoids the case where a sparse matrix is created by dummyizing variables when there are a large number of categories.</p>
<h2 id="numerical-data-binning">Numerical data binning</h2>
<p>Bin the features of the numerical data.</p>
<h3 id="parameter-5">parameter</h3>
<p>It can be executed by giving the following argument to <code>setup()</code>.</p>
<ul>
<li>bin_numeric_features (list type of string, default = None)</li>
<li>Specify the column name of the numerical data feature to be binned in the list.</li>
</ul>
<h3 id="reference-4">Reference</h3>
<p>Internally, it&rsquo;s an image that runs sklearn.preprocessing.KBinsDiscretizer.
(It seems that an algorithm using the one-dimensional k-means method will be used.)</p>
<p>*I do not understand the details of how to decide the number of bottles, so I would like to study it in the future.</p>
<h2 id="remove-outliers">Remove outliers</h2>
<p>Remove outliers from train data.</p>
<h3 id="parameter-6">parameter</h3>
<p>It can be executed by giving the following argument to <code>setup()</code>.</p>
<ul>
<li>remove_outliers (bool type, default= False)
<ul>
<li>Specify whether to execute this process (True/False).</li>
</ul>
</li>
</ul>
<p></br></br></p>
<ul>
<li>outliers_threshold (float type, default=0.05)
<ul>
<li>Specifies the percentage of outliers in the dataset.
<ul>
<li>For example, the default of 0.05 removes 0.025% of the values on either side of the tail of the distribution.</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="reference-5">Reference</h3>
<p>It seems that singular value decomposition and PCA are used for internal processing.
*I do not understand the details, so I would like to study in the future.</p>
<h2 id="multico-removal">Multico removal</h2>
<p>Features that may cause multico (multicollinearity) are removed.</p>
<h3 id="parameter-7">parameter</h3>
<p>It can be executed by giving the following argument to <code>setup()</code>.</p>
<ul>
<li>remove_multicollinearity (bool type, default= False)</li>
<li>Specify whether to execute this process (True/False).</li>
</ul>
<p></br></br></p>
<ul>
<li>multicollinearity_threshold (float type, default= 0.9)
<ul>
<li>Variables with a cross correlation higher than the threshold defined by this parameter are deleted.
<ul>
<li>(Of the two features, the one with the lowest correlation with the target variable is deleted.)</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="reference-6">Reference</h3>
<p>For Multico, refer to this article.
<a href="https://qiita.com/ynakayama/items/36b7c1640e6a02ce2e00">https://qiita.com/ynakayama/items/36b7c1640e6a02ce2e00</a></p>
<h2 id="characterizing-the-class-ring-result">Characterizing the class ring result</h2>
<p>Clustering is performed using each feature and the class label of each record is added as a new feature.</p>
<h3 id="parameter-8">parameter</h3>
<p>It can be executed by giving the following argument to <code>setup()</code>.</p>
<ul>
<li>create_clusters (bool type, default=False)</li>
<li>Specify whether to execute this process (True/False).</li>
</ul>
<p></br></br></p>
<ul>
<li>cluster_iter (int type, default=20)</li>
<li>Specify the number of iterations when determining the number of clusters.</li>
</ul>
<h3 id="reference-7">Reference</h3>
<p>The number of clusters seems to be determined using a combination of Calinski Harabasz criteria and silhouette criteria.</p>
<p>You can refer to this article for Calinski Harabasz criteria and silhouette criteria.
<a href="https://qiita.com/yasaigirai/items/ec3c3aaaab5bc9b930a2">https://qiita.com/yasaigirai/items/ec3c3aaaab5bc9b930a2</a></p>
<h2 id="feature-removal-by-data-distribution">Feature removal by data distribution</h2>
<p>Remove features with variances that are not statistically significant.</p>
<h3 id="parameter-9">parameter</h3>
<p>It can be executed by giving the following argument to <code>setup()</code>.</p>
<ul>
<li>ignore_low_variance (bool type, default=False)</li>
<li>Specify whether to execute this process (True/False).</li>
</ul>
<h3 id="reference-8">Reference</h3>
<p>The variance of the data here seems to be calculated using the ratio of unique values (unique values) of all samples.
Is it an image that is regarded as a candidate for exclusion, assuming that the more &ldquo;same value&rdquo; is in a certain variable, the lower the variance is?
*I do not understand the details, so I would like to study in the future.</p>
<h2 id="interaction-feature-generation">Interaction feature generation</h2>
<p>Generates interaction features using the specified parameters.</p>
<h3 id="parameter-10">parameter</h3>
<p>It can be executed by giving the following argument to <code>setup()</code>.</p>
<ul>
<li>polynomial_features (bool type, default = False)
<ul>
<li>If True is specified, a new feature will be generated with a combination of polynomials of all numerical data features.</li>
<li>The degree of the polynomial is specified by polynomial_degree param.</li>
<li>However, of the generated features, those that are judged to be insignificant are excluded.
<ul>
<li>Set the judgment threshold with polynomial_threshold.</li>
</ul>
</li>
</ul>
</li>
</ul>
<p></br></br></p>
<ul>
<li>trigonometry_features (bool type, default = False)
<ul>
<li>If True is specified, a new feature will be generated by the combination of trigonometric functions of all numerical data features.</li>
<li>The order and threshold value are specified in the same way as polynomial_features.</li>
</ul>
</li>
</ul>
<p></br></br></p>
<ul>
<li>polynomial_degree (int type, default = 2)
<ul>
<li>Specify the degree of the polynomial feature.</li>
</ul>
</li>
</ul>
<p></br></br></p>
<ul>
<li>polynomial_threshold (float type, default = 0.1)
<ul>
<li>Specify the threshold value to determine whether to keep the newly generated features.
<ul>
<li>(Refer to the &ldquo;Reference&rdquo; below for the judgment method.)</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>For example, if the inputs are two variables [a, b], specifying polynomial_degree=2 is an image that produces features [1, a, b, a^2, ab, b^2].</p>
<p>In addition to the above, you can also specify interaction features.
Generates first-order interaction features for all numerical data features, including dummy variable features for categorical variables and features created by polynomial_features and trigonometry_features.</p>
<ul>
<li>feature_interaction (bool type, default = False)
<ul>
<li>If True is specified, the interaction (a * b) will be calculated and generated as a new feature.</li>
<li>However, of the generated features, those that are judged to be insignificant are excluded.
<ul>
<li>Set the judgment threshold with polynomial_threshold.</li>
</ul>
</li>
</ul>
</li>
</ul>
<p></br></br></p>
<ul>
<li>interaction_threshold (bool type, default = False)
<ul>
<li>If True is specified, the ratio (a / b) will be calculated and a new feature will be generated.</li>
<li>The threshold value is specified in the same way as feature_interaction.</li>
</ul>
</li>
</ul>
<p></br></br></p>
<ul>
<li>interaction_threshold (bool type, default = 0.01)
<ul>
<li>Specify the threshold value to determine whether to keep the newly generated features.
<ul>
<li>(Refer to the &ldquo;Reference&rdquo; below for the judgment method.)</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="reference-9">Reference</h3>
<p>About polynomial_threshold and interaction_threshold
The metric to compare with the threshold seems to be importance based on multiple combinations such as random forest, AdaBoost, and linear correlation.
*I do not understand the details, so I would like to study in the future.</p>
<p>About trigonometry_features, is this literally creating features using trigonometric functions (sin, cos, tan)? Is it?
*I do not understand the details, so I would like to study in the future.</p>
<p>Please note that this function may be inefficient for datasets with large feature space.</p>
<h2 id="generation-of-group-features">Generation of group features</h2>
<p>By specifying the related features in the data set, the statistical features based on them are extracted.
A new feature amount is generated by calculating the following aggregate value between the specified feature amounts.</p>
<ul>
<li>minimum value</li>
<li>Maximum value</li>
<li>Average value</li>
<li>Median</li>
<li>Mode</li>
<li>standard deviation</li>
</ul>
<p><img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/254095/b2ffc0e2-cfdc-e2d5-2989-4655505b3564.png" alt="image.png"></p>
<h3 id="parameter-11">parameter</h3>
<p>It can be executed by giving the following argument to <code>setup()</code>.</p>
<ul>
<li>group_features (list type of string or list type including list, default = None)</li>
<li>Specify the column name of the numerical data feature for which you want to generate group features (related).</li>
</ul>
<p></br></br></p>
<ul>
<li>group_names (list type, default = None)
<ul>
<li>Each group name can be specified by a character string.
<ul>
<li>A column name is added to each aggregated value in the image &ldquo;Group name_Min&rdquo;.</li>
</ul>
</li>
<li>If the length does not match with group_features, or if this argument is not specified, names will be given in order of group_1, group_2.</li>
</ul>
</li>
</ul>
<p>The implementation image is as follows.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">setup(dataset, target<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;Price&#34;</span>, group_features<span style="color:#f92672">=</span>[[<span style="color:#e6db74">&#34;cal1&#34;</span>, <span style="color:#e6db74">&#34;cal2&#34;</span> <span style="color:#e6db74">&#34;cal3&#34;</span>], [<span style="color:#e6db74">&#34;cal4&#34;</span>, <span style="color:#e6db74">&#34;cal5&#34;</span>]], group_names<span style="color:#f92672">=</span>[<span style="color:#e6db74">&#34;gr1&#34;</span>, <span style="color:#e6db74">&#34;gr2&#34;</span>])
</code></pre></div><h2 id="execution-of-feature-selection">Execution of feature selection</h2>
<p>Select the features using some evaluation indicators.</p>
<h3 id="parameter-12">parameter</h3>
<p>It can be executed by giving the following argument to <code>setup()</code>.</p>
<ul>
<li>feature_selection (bool type, default = False)</li>
<li>Specify whether to execute this process (True/False).</li>
</ul>
<p></br></br></p>
<ul>
<li>feature_selection_threshold (float type, default = 0.8)
<ul>
<li>Specifies the percentage threshold used for feature selection. (Including newly generated polynomial features)
<ul>
<li>The larger this value, the more features will be selected.</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="reference-10">Reference</h3>
<p>About feature_selection_threshold
The metric to compare with the threshold seems to be importance based on multiple combinations such as random forest, AdaBoost, and linear correlation.
*I do not understand the details, so I would like to study in the future.</p>
<p>According to the original source comment, when using polynomial_features and feature_interaction, it is better to define this parameter with a low value.
Is it an image that it is better to narrow down the feature amount created by interaction to some extent by this processing?</p>
<h2 id="reduction-of-high-cardinality-features">Reduction of high cardinality features</h2>
<p>Specifying a column with high cardinality reduces the data types in the column and lowers the cardinality.</p>
<h3 id="parameterit-can-be-executed-by-giving-the-following-argument-to-setup">parameterIt can be executed by giving the following argument to <code>setup()</code>.</h3>
<ul>
<li>high_cardinality_features (string type, default = None)
<ul>
<li>Specifies the (high cardinality) column to convert.</li>
</ul>
</li>
</ul>
<p></br></br></p>
<ul>
<li>high_cardinality_method (string type, default ='frequency&rsquo;)
<ul>
<li>Specify the conversion method.
*&lsquo;frequency&rsquo; or&rsquo;clustering&rsquo; can be selected.
<ul>
<li>If you specify&rsquo;frequency&rsquo;, the original data will be replaced with the appearance frequency (numerical value) for each data type.</li>
<li>When&rsquo;clustering&rsquo; is specified, clustering is performed and the result (class label) replaces the original data.</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="reference-11">Reference</h3>
<p>*For cardinality data, please refer to this article.
<a href="https://qiita.com/soyanchu/items/034be19a2e3cb87b2efb">https://qiita.com/soyanchu/items/034be19a2e3cb87b2efb</a></p>
<p>In the method by&rsquo;clustering&rsquo;, k-means is used in the feeling that the source of the original family was roughly inspected.</p>
<p>*I have not fully understood the benefits of reducing cardinality, so I would like to study it in the future.</p>
<h2 id="feature-scaling">Feature scaling</h2>
<p>The feature is scaled by the specified method.</p>
<h3 id="parameter-13">parameter</h3>
<p>It can be executed by giving the following argument to <code>setup()</code>.</p>
<ul>
<li>transformation (bool type, default = False)</li>
<li>Specify whether to execute this process (True/False).</li>
</ul>
<p></br></br></p>
<ul>
<li>transformation_method (string type, default ='yeo-johnson&rsquo;)
<ul>
<li>Specify the conversion method.
*&lsquo;yeo-johnson&rsquo; or&rsquo;quantile&rsquo; can be selected.
*&lsquo;yeo-johnson&rsquo; performs Yeo-Johnson conversion.
<ul>
<li>Is&rsquo;quantile&rsquo; a quartile conversion? to hold.</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="reference-12">Reference</h3>
<p>Both&rsquo;yeo-johnson&rsquo; and&rsquo;quantile&rsquo; seem to transform the data to be normally distributed.
*I do not understand the details, so I would like to study in the future.</p>
<p>After checking the original code,&lsquo;yeo-johnson&rsquo; uses sklearn.preprocessing.PowerTransformer and&rsquo;quantile&rsquo; uses sklearn.preprocessing.QuantileTransformer.</p>
<p>In general, it may be effective at modeling by making the feature value close to the normal distribution.
According to the author&rsquo;s source comments,&lsquo;quantile&rsquo; is non-linear and care must be taken to distort the linear correlation between variables measured on the same scale.</p>
<h2 id="target-variable-scaling">Target variable scaling</h2>
<p>Scales the objective variable according to the specified method.</p>
<p>*At present, this item cannot be specified in the classfication package.
It corresponds only to the transformation to get closer to the normal distribution, and it seems that these are unnecessary processes in the classification task.</p>
<h3 id="parameter-14">parameter</h3>
<p>It can be executed by giving the following argument to <code>setup()</code>.</p>
<ul>
<li>transform_target (bool type, default = False)</li>
<li>Specify whether to execute this process (True/False).</li>
</ul>
<p></br></br></p>
<ul>
<li>transform_target_method (string type, default ='box-cox&rsquo;)
<ul>
<li>Specify the conversion method.</li>
<li>You can select&rsquo;Box-cox&rsquo; or&rsquo;yeo-johnson&rsquo;.
<ul>
<li>`Box-cox&rsquo; performs Box-Cox conversion.
*&lsquo;yeo-johnson&rsquo; performs Yeo-Johnson conversion.</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="reference-13">Reference</h3>
<p>It may be effective at modeling by making the objective variable closer to the normal distribution.</p>
<p>In Box-Cox conversion, there is a constraint that all data are positive values, so if data contains negative values, it seems that Yeo-Johnson conversion is forcibly switched.</p>
<p>*For Box-Cox conversion, this article is helpful.
<a href="https://qiita.com/dyamaguc/items/b468ae66f9ce6ee89724">https://qiita.com/dyamaguc/items/b468ae66f9ce6ee89724</a></p>
<h2 id="dimension-reduction-of-features">Dimension reduction of features</h2>
<p>The dimension of the feature quantity is reduced.</p>
<h3 id="parameter-15">parameter</h3>
<p>It can be executed by giving the following argument to <code>setup()</code>.</p>
<ul>
<li>pca (bool type, default = False)</li>
<li>Specify whether to execute this process (True/False).</li>
</ul>
<p></br></br></p>
<ul>
<li>pca_method (string type, default ='linear&rsquo;)
*&lsquo;linear&rsquo;: Perform dimension reduction by principal component analysis (linear).
*&lsquo;kernel&rsquo;: Dimension reduction by kernel principal component analysis.
*&lsquo;incremental&rsquo;: Dimension reduction by principal component analysis (large amount data ver).</li>
</ul>
<p></br></br></p>
<ul>
<li>pca_components (int/float type, default = 0.99)
<ul>
<li>Specify the number/ratio of features to be retained after dimension reduction.
<ul>
<li>If specified as an int type, it will be treated as the number of features to be retained.
<ul>
<li>It is necessary to specify a value smaller than the original number of features.</li>
</ul>
</li>
<li>If you specify it as a float type, it will be treated as the ratio of the remaining feature amount.</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="reference-14">Reference</h3>
<p>Generally, this is done for the purpose of eliminating unimportant features and saving memory and CPU resources.</p>
<p>This process (dimension reduction) seems to be executed at the end of the pre-processing pipeline.
(The dimension is reduced for the data after other preprocessing is completed.)</p>
<p>You can refer to this article for principal component analysis.
<a href="https://qiita.com/shuva/items/9625bc326e2998f1fa27">https://qiita.com/shuva/items/9625bc326e2998f1fa27</a>
<a href="https://qiita.com/NoriakiOshita/items/460247bb57c22973a5f0">https://qiita.com/NoriakiOshita/items/460247bb57c22973a5f0</a></p>
<p>For&rsquo;incremental&rsquo;, it seems that the method called Incremental PCA is used.
According to the explanation of scikit-learn, Incremental PCA (IPCA) should be used instead of principal component analysis (PCA) when the target data set is too large to fit in memory. With IPCA, it is said that a low-dimensional approximation of the input data is created using the amount of memory that does not depend on the number of input data.
<a href="https://scikit-learn.org/stable/auto_examples/decomposition/plot_incremental_pca.html">https://scikit-learn.org/stable/auto_examples/decomposition/plot_incremental_pca.html</a></p>
<h1 id="implementation-sample">Implementation sample</h1>
<h2 id="create-a-large-amount-of-features">Create a large amount of features</h2>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">from</span> pycaret.regression <span style="color:#f92672">import</span> <span style="color:#f92672">*</span>
X, y, X_train, X_test, y_train, y_test, seed, prep_pipe, target_inverse_transformer, experiment__ \
    <span style="color:#f92672">=</span> setup(dataset, target<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;Price&#34;</span>, session_id<span style="color:#f92672">=</span><span style="color:#ae81ff">123</span>,
             bin_numeric_features <span style="color:#f92672">=</span> [<span style="color:#e6db74">&#34;Carat Weight&#34;</span>],
             create_clusters <span style="color:#f92672">=</span> True,
             polynomial_features <span style="color:#f92672">=</span> True, feature_interaction <span style="color:#f92672">=</span> True, feature_ratio <span style="color:#f92672">=</span> True)
</code></pre></div><p>The execution contents (excerpt) output from <code>setup()</code> are as shown below.
<img width=300 src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/254095/069c8ba0-0b50-626f-1dc6-1139c1789244.png"></p>
<p>Checking the returned data after pre-processing, 72 features were generated as shown below.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">print</span>(X<span style="color:#f92672">.</span>info())

<span style="color:#75715e"># Output result</span>
<span style="color:#75715e"># &lt;class&#39;pandas.core.frame.DataFrame&#39;&gt;</span>
<span style="color:#75715e"># Int64Index: 6000 entries, 0 to 5999</span>
<span style="color:#75715e"># Data columns (total 72 columns):</span>
<span style="color:#75715e"># # Column Non-Null Count Dtype</span>
<span style="color:#75715e"># --- ------ -------------- -----</span>
<span style="color:#75715e"># 0 Carat Weight_Power2 6000 non-null float64</span>
<span style="color:#75715e"># 1 Cut_Fair 6000 non-null float64</span>
<span style="color:#75715e">#2 Cut_Good 6000 non-null float64</span>
<span style="color:#75715e">#3 Cut_Ideal 6000 non-null float64</span>
<span style="color:#75715e"># 4 Cut_Signature-Ideal 6000 non-null float64</span>
<span style="color:#75715e"># 5 Cut_Very Good 6000 non-null float64</span>
<span style="color:#75715e"># 6 Color_D 6000 non-null float64</span>
<span style="color:#75715e"># 7 Color_E 6000 non-null float64</span>
<span style="color:#75715e"># 8 Color_F 6000 non-null float64</span>
<span style="color:#75715e"># 9 Color_G 6000 non-null float64</span>
<span style="color:#75715e"># 10 Color_H 6000 non-null float64</span>
<span style="color:#75715e"># 11 Color_I 6000 non-null float64</span>
<span style="color:#75715e"># 12 Clarity_FL 6000 non-null float64#  13  Clarity_IF                                        6000 non-null   float64</span>
<span style="color:#75715e">#  14  Clarity_SI1                                       6000 non-null   float64</span>
<span style="color:#75715e">#  15  Clarity_VS1                                       6000 non-null   float64</span>
<span style="color:#75715e">#  16  Clarity_VS2                                       6000 non-null   float64</span>
<span style="color:#75715e">#  17  Clarity_VVS1                                      6000 non-null   float64</span>
<span style="color:#75715e">#  18  Clarity_VVS2                                      6000 non-null   float64</span>
<span style="color:#75715e">#  19  Polish_EX                                         6000 non-null   float64</span>
<span style="color:#75715e">#  20  Polish_G                                          6000 non-null   float64</span>
<span style="color:#75715e">#  21  Polish_ID                                         6000 non-null   float64</span>
<span style="color:#75715e">#  22  Polish_VG                                         6000 non-null   float64</span>
<span style="color:#75715e">#  23  Symmetry_EX                                       6000 non-null   float64</span>
<span style="color:#75715e">#  24  Symmetry_G                                        6000 non-null   float64</span>
<span style="color:#75715e">#  25  Symmetry_ID                                       6000 non-null   float64</span>
<span style="color:#75715e">#  26  Symmetry_VG                                       6000 non-null   float64</span>
<span style="color:#75715e">#  27  Report_GIA                                        6000 non-null   float64</span>
<span style="color:#75715e">#  28  Carat Weight_0.0                                  6000 non-null   float64</span>
<span style="color:#75715e">#  29  Carat Weight_1.0                                  6000 non-null   float64</span>
<span style="color:#75715e">#  30  Carat Weight_10.0                                 6000 non-null   float64</span>
<span style="color:#75715e">#  31  Carat Weight_11.0                                 6000 non-null   float64</span>
<span style="color:#75715e">#  32  Carat Weight_12.0                                 6000 non-null   float64</span>
<span style="color:#75715e">#  33  Carat Weight_13.0                                 6000 non-null   float64</span>
<span style="color:#75715e">#  34  Carat Weight_2.0                                  6000 non-null   float64</span>
<span style="color:#75715e">#  35  Carat Weight_3.0                                  6000 non-null   float64</span>
<span style="color:#75715e">#  36  Carat Weight_4.0                                  6000 non-null   float64</span>
<span style="color:#75715e">#  37  Carat Weight_5.0                                  6000 non-null   float64</span>
<span style="color:#75715e">#  38  Carat Weight_6.0                                  6000 non-null   float64</span>
<span style="color:#75715e">#  39  Carat Weight_7.0                                  6000 non-null   float64</span>
<span style="color:#75715e">#  40  Carat Weight_8.0                                  6000 non-null   float64</span>
<span style="color:#75715e">#  41  Carat Weight_9.0                                  6000 non-null   float64</span>
<span style="color:#75715e">#  42  data_cluster_0                                    6000 non-null   float64</span>
<span style="color:#75715e">#  43  Polish_EX_multiply_Carat Weight_Power2            6000 non-null   float64</span>
<span style="color:#75715e">#  44  Symmetry_EX_multiply_Carat Weight_Power2          6000 non-null   float64</span>
<span style="color:#75715e">#  45  Report_GIA_multiply_Carat Weight_Power2           6000 non-null   float64</span>
<span style="color:#75715e">#  46  Clarity_VVS2_multiply_Carat Weight_Power2         6000 non-null   float64</span>
<span style="color:#75715e">#  47  Clarity_IF_multiply_Carat Weight_Power2           6000 non-null   float64</span>
<span style="color:#75715e">#  48  Clarity_SI1_multiply_Carat Weight_Power2          6000 non-null   float64</span>
<span style="color:#75715e">#  49  Carat Weight_Power2_multiply_data_cluster_0       6000 non-null   float64</span>
<span style="color:#75715e">#  50  Symmetry_EX_multiply_data_cluster_0               6000 non-null   float64</span>
<span style="color:#75715e">#  51  Report_GIA_multiply_data_cluster_0                6000 non-null   float64</span>
<span style="color:#75715e">#  52  Symmetry_VG_multiply_Carat Weight_Power2          6000 non-null   float64</span>
<span style="color:#75715e">#  53  Carat Weight_8.0_multiply_Carat Weight_Power2     6000 non-null   float64</span>
<span style="color:#75715e">#  54  Cut_Signature-Ideal_multiply_Carat Weight_Power2  6000 non-null   float64</span>
<span style="color:#75715e">#  55  data_cluster_0_multiply_Symmetry_EX               6000 non-null   float64</span>
<span style="color:#75715e">#  56  Color_E_multiply_Carat Weight_Power2              6000 non-null   float64</span>
<span style="color:#75715e">#  57  data_cluster_0_multiply_Cut_Ideal                 6000 non-null   float64</span>
<span style="color:#75715e">#  58  Carat Weight_Power2_multiply_Polish_EX            6000 non-null   float64</span>
<span style="color:#75715e">#  59  data_cluster_0_multiply_Report_GIA                6000 non-null   float64</span>
<span style="color:#75715e">#  60  Color_F_multiply_Carat Weight_Power2              6000 non-null   float64</span>
<span style="color:#75715e">#  61  Carat Weight_Power2_multiply_Carat Weight_8.0     6000 non-null   float64</span>
<span style="color:#75715e">#  62  Cut_Ideal_multiply_Carat Weight_Power2            6000 non-null   float64</span>
<span style="color:#75715e">#  63  Color_D_multiply_Carat Weight_Power2              6000 non-null   float64</span>
<span style="color:#75715e">#  64  data_cluster_0_multiply_Carat Weight_Power2       6000 non-null   float64</span>
<span style="color:#75715e">#  65  data_cluster_0_multiply_Polish_EX                 6000 non-null   float64</span>
<span style="color:#75715e">#  66  Color_I_multiply_Carat Weight_Power2              6000 non-null   float64</span>
<span style="color:#75715e">#  67  Polish_EX_multiply_data_cluster_0                 6000 non-null   float64</span>
<span style="color:#75715e">#  68  Color_H_multiply_Carat Weight_Power2              6000 non-null   float64</span>
<span style="color:#75715e">#  69  Carat Weight_Power2_multiply_Report_GIA           6000 non-null   float64</span>
<span style="color:#75715e">#  70  Clarity_VS2_multiply_Carat Weight_Power2          6000 non-null   float64</span>
<span style="color:#75715e">#  71  Carat Weight_Power2_multiply_Symmetry_VG          6000 non-null   float64</span>
<span style="color:#75715e"># dtypes: float64(72)</span>
<span style="color:#75715e"># memory usage: 3.3 MB</span>
</code></pre></div><p>returnされた前処理のパイプラインを確認すると下記の通りです。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">print</span>(prep_pipe)

<span style="color:#75715e">#実行結果</span>
<span style="color:#75715e"># Pipeline(memory=None,</span>
<span style="color:#75715e">#          steps=[(&#39;dtypes&#39;,#                  DataTypes_Auto_infer(categorical_features=[],</span>
<span style="color:#75715e">#                                       display_types=True, features_todrop=[],</span>
<span style="color:#75715e">#                                       ml_usecase=&#39;regression&#39;,</span>
<span style="color:#75715e">#                                       numerical_features=[], target=&#39;Price&#39;,</span>
<span style="color:#75715e">#                                       time_features=[])),</span>
<span style="color:#75715e">#                 (&#39;imputer&#39;,</span>
<span style="color:#75715e">#                  Simple_Imputer(categorical_strategy=&#39;not_available&#39;,</span>
<span style="color:#75715e">#                                 numeric_strategy=&#39;mean&#39;,</span>
<span style="color:#75715e">#                                 target_variable=None)),</span>
<span style="color:#75715e">#                 (&#39;new_levels1&#39;,</span>
<span style="color:#75715e">#                  New_Catagorical_Levels_i...</span>
<span style="color:#75715e">#                 (&#39;dummy&#39;, Dummify(target=&#39;Price&#39;)),</span>
<span style="color:#75715e">#                 (&#39;fix_perfect&#39;, Remove_100(target=&#39;Price&#39;)),</span>
<span style="color:#75715e">#                 (&#39;clean_names&#39;, Clean_Colum_Names()),</span>
<span style="color:#75715e">#                 (&#39;feature_select&#39;, Empty()), (&#39;fix_multi&#39;, Empty()),</span>
<span style="color:#75715e">#                 (&#39;dfs&#39;,</span>
<span style="color:#75715e">#                  DFS_Classic(interactions=[&#39;multiply&#39;, &#39;divide&#39;],</span>
<span style="color:#75715e">#                              ml_usecase=&#39;regression&#39;, random_state=123,</span>
<span style="color:#75715e">#                              subclass=&#39;binary&#39;, target=&#39;Price&#39;,</span>
<span style="color:#75715e">#                              top_features_to_pick_percentage=None)),</span>
<span style="color:#75715e">#                 (&#39;pca&#39;, Empty())],</span>
<span style="color:#75715e">#          verbose=False)</span>

</code></pre></div><h1 id="まとめ">まとめ</h1>
<p><strong>PyCaretは様々なデータクリーニングや特徴量の変換処理を簡潔なコードで実現できる</strong>
PyCaretは、パラメータの指定のみで様々な前処理を記述でき、大幅な時間短縮に繋がると感じました。また、コードがすっきりし統一的な記述となるため、チーム内や自分自身にとっての読みやすさや思考の効率も向上するように思いました。</p>
<p><strong>PyCaretでできる前処理を理解していくことは様々なテクニックの勉強にも繋がる</strong>
PyCaretは、コーディングが苦手な方にも比較的易しい作りです。これまでコーディングで躓いていた初学者の方にとっても、実際に動かしながら、理論の習得に注力できる良いツールになるのではないかと思いました。
（僕自身、今回調査を進める中で、今まで知らなかったテクニックを多く学ぶことができました。）</p>
<p><strong>一方で （現時点では）PyCaretはあくまで効率化のためのツールである</strong>
PyCaretは、あくまでユーザから入力されたデータに基づき、クリーニングや特徴量の変換処理を行うものであり、仮説立てやデータ収集、特徴量設計についてはやはり人の手で行う必要があるという点を実感できました。</p>

    </article>
  </div>

  
  
  
  <script>
  window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
  ga('create', 'UA-169005401-1', 'auto');
  ga('send', 'pageview');
  </script>
  <script async src='https://www.google-analytics.com/analytics.js'></script>
  

  

  
<link rel="stylesheet" type="text/css" href="//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.0/cookieconsent.min.css" />
<script src="//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.0/cookieconsent.min.js"></script>
<script>
window.addEventListener("load", function(){
window.cookieconsent.initialise({
  "palette": {
    "popup": {
      "background": "#216942",
      "text": "#b2d192"
    },
    "button": {
      "background": "#afed71"
    }
  }
})});
</script>

</body>

</html>
