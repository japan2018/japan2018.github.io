<!doctype html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  
  <meta name="generator" content="Hugo 0.72.0" />

  
  <meta name="description" content="Some description">
  

  
  <link rel="apple-touch-icon" sizes="180x180" href="https://memotut.com/apple-touch-icon.png">

  
  <link rel="icon" type="image/png" sizes="32x32" href="https://memotut.com/favicon-32x32.png">

  
  <link rel="icon" type="image/png" sizes="16x16" href="https://memotut.com/favicon-16x16.png">

  
  <link rel="manifest" href="https://memotut.com/site.webmanifest">

  
  <link rel="mask-icon" href="https://memotut.com/safari-pinned-tab.svg" color="#5bbad5">

  <meta name="msapplication-TileColor" content="#da532c">

  <meta name="theme-color" content="#ffffff">

  
  <link rel="stylesheet" href="https://memotut.com/css/bootstrap.min.css" />

  
  <title>Noise2Noise Explanation | Memo Tut</title>
  

  <style>
body {
  min-width: 300px;
}

.custom-navbar {
  margin-bottom: 1em;
  height: 60px;
}

.custom-navbar a {
  display: inline-block; 
  padding: 18px 0;
  margin-right: 1em; 
  font-weight: bold; 
}

.custom-navbar a:hover,
.custom-navbar a:focus {
  text-decoration: none; 
}

@media print {
  .custom-navbar {
    display: none;
  }
}

article {
  padding-bottom: 1em;
}

img {
  max-width: 100%;
}


body {
  background-color: #fff;
}



body {
  color: #212529;
}



a {
  color: #007bff;
}



a:hover,
a:focus {
  color: #0056b3;
}



.custom-navbar {
  background-color: #212529;
}



.custom-navbar a {
  color: rgba(255, 255, 255, 0.75);
}



.custom-navbar a:hover,
.custom-navbar a:focus {
  color: rgba(255, 255, 255, 1);
}



.container {
  max-width: 800px;
}



pre {
  display: block;
  padding: 9.5px;
  word-break: break-all;
  word-wrap: break-word;
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 4px;
}

pre code {
  padding: 0;
  font-size: inherit;
  color: inherit; 
  white-space: pre-wrap;
  background-color: transparent;
  border: none;
  border-radius: 0;
}

code {
  padding: 2px 4px;
  color: inherit; 
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 4px;
  font-size: .9em;
}



blockquote,
.blockquote {
  padding: 10px 20px;
  margin: 0 0 20px;
  font-size: 1em;
  border-left: 5px solid #6c757d;
}

</style>
</head>

<body>
  <nav class="custom-navbar">
  <div class="container">
    
    <a href="/">Posts</a>
    
    <a href="/tags/">Tags</a>
    
    <a href="/about/">About</a>
    
    <a href="/index.xml">RSS</a>
    
  </div>
</nav>
  
  <div class="container">
    <article>
      <h1>Noise2Noise Explanation</h1>
<p>
  <small class="text-secondary">
  
  
  Dec 7, 2019
  </small>
  

<small><code><a href="https://memotut.com/tags/python">Python</a></code></small>


<small><code><a href="https://memotut.com/tags/keras"> Keras</a></code></small>


<small><code><a href="https://memotut.com/tags/tensorflow"> TensorFlow</a></code></small>


<small><code><a href="https://memotut.com/tags/denoising"> denoising</a></code></small>


<small><code><a href="https://memotut.com/tags/noise2noise"> Noise2Noise</a></code></small>

</p>
<pre><code># Noise2Noise explanation (method to learn denoising network only with images with noise)
</code></pre>
<p>I would like to explain and summarize a paper on noise removal <a href="https://arxiv.org/abs/1803.04189">Noise2Noise: Learning Image Restoration without Clean Data</a>.
Since it is a 2018 paper and it has a big impact, I feel that it is old to write an introduction article, but there was an article on implementation in Qiita, but there was no article explaining the contents, so memorandum I will leave it as. Basically, it is written so that anyone can read it, with an emphasis on readability.</p>
<h2 id="overview">Overview</h2>
<p>In recent years, the approach (high resolution, De-JEPG, colorization, etc.) of restoring a correct signal from a damaged signal by deep learning has achieved high results. However, when using these approaches, you need to have a clean data set and a corrupted data set. If this is something that can be converted in a simple way, such as &ldquo;color image to black and white image&rdquo;, &ldquo;high resolution image to low resolution image&rdquo;, you can easily prepare a dataset, but for many tasks Not. Even if you are thinking of collecting clean images, you may continue to meet long exposures or subject standstill for all data. You can check images that contain noise when you take a picture by looking at ImageNet.</p>
<h3 id="formulation">formulation</h3>
<p>In this paper, in order to solve the problem of such a data set, we explain the method of learning the network for decoding the correct signal from the damaged signal given only the data set of the damaged signal. I will.</p>
<p>First, formulate the problem. The following formula is the input signal in which $ \hat{x}_i $ is corrupted, and if it is an image, it is a noise image. If $y_i$ is a clean output, it is a denoised image if it is an image.</p>
<pre><code class="language-math" data-lang="math">\underset{\theta}{argmin} \displaystyle \sum_i L(f_\theta(\hat{x}_i),y_i)
</code></pre><p>Basically, we want to learn the function $ f_{\theta} $ that adjusts the parameter $ \theta $ and converts from $ \hat{x}_i $ to $ y_i$. It is a problem setting of noise removal. The corrupted input $\hat{x}$ must then be a random variable generated according to a clean target ($ \hat{x} \thicksim p(\hat{x}|y_i) $).</p>
<h3 id="the-point">The point</h3>
<p>The three main points of the paper are</p>
<ul>
<li>Train clean image conversion from noise image only with noise image (corrupted signal) dataset</li>
<li>Can produce results that are almost the same as, or rather better than, using the noise image and clean image data sets.</li>
<li>Learning the conversion of noise image to noise image (Noise2Noise) is equivalent to learning the conversion of noise image to clean image (Noise2Clearn)</li>
</ul>
<p>I feel like I can really do it. Especially at the end I want to say that your head is okay. However, as mentioned in the title of the paper, noise to noise conversion is the key idea of this paper.</p>
<h2 id="theoretical-background">Theoretical background</h2>
<p>First, let&rsquo;s think about the regression model (regressor). In this theoretical background explanation, all models and classifications are assumed to be regressors, and classification models (classifications) are not considered.</p>
<p>So, let&rsquo;s start with a very simple example of a regression.</p>
<p>First, assume an unreliable room temperature dataset of measurements {$ { y_1,y_2,y_3 } $}
In other words, you can think of it as making multiple temperature measurements at several points in the room. I don&rsquo;t know if it&rsquo;s badly measured or the thermometer itself, but I&rsquo;m assuming there is an error between the true room temperature and the measured value.</p>
<p>At this time, the most common strategy for estimating the unknown true room temperature is to minimize the error from the measured value based on some loss function = $z$ with the smallest mean deviation. Is to ask.</p>
<pre><code class="language-math" data-lang="math">\underset{z}{argmin} \mathbb{E}_y\{L(z, y)\}
</code></pre><p>For example, if you try to minimize L2 loss (z-y)^2, z will be the simple arithmetic mean.</p>
<pre><code class="language-math" data-lang="math">z = \mathbb{E}_y\{y\}.
</code></pre><p>I think this is intuitively easy to understand</p>
<p>Similarly, when L1 loss is used, the median value of the observed data set is obtained as the optimum solution.</p>
<p>Training with neural regressors can be viewed as a generalization of the above method.
Here we formulate and consider the training task with input and target pairs as follows.</p>
<pre><code class="language-math" data-lang="math">\underset{\theta}{argmin} \mathbb{E}_{(x,y)} \{ L(f_\theta(x),y) \}
</code></pre><p>These formulas are common DNN formulas, but can be rewritten to the following conditional probabilities.</p>
<pre><code class="language-math" data-lang="math">\underset{\theta}{argmin} \mathbb{E}_x \{\mathbb{E}_{y|x} \{ L(f_\theta(x),y) \}\}
</code></pre><p>There is a significant point that neural regressors hide from these equations.</p>
<p>In other words, it seems that learning of the regressor is learning the conversion from x to y corresponding to 1:1, but in reality there are multiple y corresponding to x, so it can be said to be 1:n mapping.</p>
<p>It is easy to understand by giving a concrete example, but in the case of a high resolution task, for the input low resolution image $x$, the output high resolution image $ (at least humans think it is higher than x) $ It can be said that there are multiple y$.</p>
<p>Similarly, in the automatic coloring task, it can be said that there are multiple output color images for the input black and white images.</p>
<p>Therefore, many tasks that use neural regressors seem to connect points, but in reality it is thought that they are learning to connect points and regions.</p>
<p>These outputs will learn to output the average of all plausible explanations if they were trained on L2 loss, as explained in the room temperature dataset above.</p>
<p>As a result, the inferred output of NN contains spatial blurring.</p>
<p>This bokeh bothers researchers in many tasks. For example, there is a problem that a high-resolution image or a GAN generated result is output as an image that is smoothed like a Gaussian filter, and researchers are working on this improvement.</p>
<p>However, it can be said that the problem caused by this blur is creating an unexpected by-product in this case.</p>
<p>In other words, even if the target at the time of learning is contaminated with uniform random numbers (for example, Gaussian noise or salt and pepper), this averaging ability allows the learned network to output the same as the result of learning with a clean target. ..
Therefore, it can be said that the obtained $f_\theta$ is an equivalent function when optimizing the equation of 1 and the following equation.</p>
<pre><code class="language-math" data-lang="math">\underset{\theta}{argmin} \sum_i L(f_{\theta}(\hat{x}_i),\hat{y}_i)
</code></pre><p>This formula no longer requires the clean target $\hat{x}$ we needed earlier.</p>
<p>This theory is the most important and fundamental theory in Noise2Noise.</p>
<p>This is the end of the explanation of the theory, and then we move on to the experimental phase.</p>
<h2 id="experiment">Experiment</h2>
<p>coming soon&hellip;</p>

    </article>
  </div>

  
  
  
  <script>
  window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
  ga('create', 'UA-169005401-1', 'auto');
  ga('send', 'pageview');
  </script>
  <script async src='https://www.google-analytics.com/analytics.js'></script>
  

  

  
<link rel="stylesheet" type="text/css" href="//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.0/cookieconsent.min.css" />
<script src="//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.0/cookieconsent.min.js"></script>
<script>
window.addEventListener("load", function(){
window.cookieconsent.initialise({
  "palette": {
    "popup": {
      "background": "#216942",
      "text": "#b2d192"
    },
    "button": {
      "background": "#afed71"
    }
  }
})});
</script>

</body>

</html>
