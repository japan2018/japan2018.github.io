<!doctype html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  
  <meta name="generator" content="Hugo 0.72.0" />

  
  <meta name="description" content="Some description">
  

  
  <link rel="apple-touch-icon" sizes="180x180" href="https://memotut.com/apple-touch-icon.png">

  
  <link rel="icon" type="image/png" sizes="32x32" href="https://memotut.com/favicon-32x32.png">

  
  <link rel="icon" type="image/png" sizes="16x16" href="https://memotut.com/favicon-16x16.png">

  
  <link rel="manifest" href="https://memotut.com/site.webmanifest">

  
  <link rel="mask-icon" href="https://memotut.com/safari-pinned-tab.svg" color="#5bbad5">

  <meta name="msapplication-TileColor" content="#da532c">

  <meta name="theme-color" content="#ffffff">

  
  <link rel="stylesheet" href="https://memotut.com/css/bootstrap.min.css" />

  
  <title>[Python] I tried to compress the image using machine learning | Memo Tut</title>
  

  <style>
body {
  min-width: 300px;
}

.custom-navbar {
  margin-bottom: 1em;
  height: 60px;
}

.custom-navbar a {
  display: inline-block; 
  padding: 18px 0;
  margin-right: 1em; 
  font-weight: bold; 
}

.custom-navbar a:hover,
.custom-navbar a:focus {
  text-decoration: none; 
}

@media print {
  .custom-navbar {
    display: none;
  }
}

article {
  padding-bottom: 1em;
}

img {
  max-width: 100%;
}


body {
  background-color: #fff;
}



body {
  color: #212529;
}



a {
  color: #007bff;
}



a:hover,
a:focus {
  color: #0056b3;
}



.custom-navbar {
  background-color: #212529;
}



.custom-navbar a {
  color: rgba(255, 255, 255, 0.75);
}



.custom-navbar a:hover,
.custom-navbar a:focus {
  color: rgba(255, 255, 255, 1);
}



.container {
  max-width: 800px;
}



pre {
  display: block;
  padding: 9.5px;
  word-break: break-all;
  word-wrap: break-word;
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 4px;
}

pre code {
  padding: 0;
  font-size: inherit;
  color: inherit; 
  white-space: pre-wrap;
  background-color: transparent;
  border: none;
  border-radius: 0;
}

code {
  padding: 2px 4px;
  color: inherit; 
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 4px;
  font-size: .9em;
}



blockquote,
.blockquote {
  padding: 10px 20px;
  margin: 0 0 20px;
  font-size: 1em;
  border-left: 5px solid #6c757d;
}

</style>
</head>

<body>
  <nav class="custom-navbar">
  <div class="container">
    
    <a href="/">Posts</a>
    
    <a href="/tags/">Tags</a>
    
    <a href="/about/">About</a>
    
    <a href="/index.xml">RSS</a>
    
  </div>
</nav>
  
  <div class="container">
    <article>
      <h1>[Python] I tried to compress the image using machine learning</h1>
<p>
  <small class="text-secondary">
  
  
  Dec 4, 2015
  </small>
  

<small><code><a href="https://memotut.com/tags/python">Python</a></code></small>


<small><code><a href="https://memotut.com/tags/algorithm"> Algorithm</a></code></small>


<small><code><a href="https://memotut.com/tags/machine-learning"> Machine Learning</a></code></small>


<small><code><a href="https://memotut.com/tags/machinelearning"> MachineLearning</a></code></small>


<small><code><a href="https://memotut.com/tags/clustering"> Clustering</a></code></small>

</p>
<pre><code>![original_compressed.png](https://qiita-image-store.s3.amazonaws.com/0/77079/2f7e9fc7-d91a-bd40-dca9-bed85174e317.png)
</code></pre>
<p>#Introduction
Try compressing the image using <strong>K-Means</strong>, which is a typical clustering algorithm. First, I will explain the K-Means algorithm. After that, I will explain about image compression using K-Means. For details, refer to <a href="https://www.coursera.org/learn/machine-learning">Coursera Machine Learning</a>.</p>
<pre><code>What is clustering?
Dividing a collection of data into several groups (clusters) according to the similarity between the data.
</code></pre><h1 id="k-means-algorithm">K-Means algorithm</h1>
<h2 id="intuitive-description-of-algorithm">Intuitive description of algorithm</h2>
<p>The K-Means algorithm is roughly divided into the following three processes:</p>
<ul>
<li>Initialization of the center of gravity</li>
<li>Cluster allocation</li>
<li>Center of gravity recalculation</li>
</ul>
<p>Let&rsquo;s understand each step with an image.</p>
<h3 id="1-center-of-gravity-initialization">1. Center of gravity initialization</h3>
<p>First, determine the position of the point called the center of gravity. Each centroid is considered a standard pattern for each cluster. Therefore, the center of gravity corresponding to the number of clusters to divide the data is required. In the figure below, the star marks the center of gravity, and the circle marks the data. Since the number of centroids is three here, the data should be written as <font color="red"><strong>red</strong></font>, <font color="blue"><strong>blue</strong></font>, <font color="green"><strong>Green</strong></font> It will be divided into 3 clusters.</p>
<p>In this figure, the center of gravity is determined randomly, but we will introduce another method later.
<img src="https://qiita-image-store.s3.amazonaws.com/0/77079/44daf335-f7bb-b2ba-af37-1772f8de7a2d.png" alt="init.png"></p>
<h3 id="2-cluster-allocation">2. Cluster allocation</h3>
<p>After determining the position of the center of gravity, the next step is to <strong>assign a cluster to each data</strong>. Here, let&rsquo;s take a look at each data and see that <font color="red"><strong>red</strong></font>, <font color="blue"><strong>blue</strong></font>, <font color Assign the data to one of the clusters depending on which of the cluster centroids of ="green"><strong>green</strong></font> is close to.
<img src="https://qiita-image-store.s3.amazonaws.com/0/77079/c41ce0dc-f215-c1be-34c6-ff69e2b21754.png" alt="assignment0.png"></p>
<h3 id="3-recalculation-of-the-center-of-gravity">3. Recalculation of the center of gravity</h3>
<p>When the clusters have been assigned to all the data, <strong>the position of the center of gravity is recalculated</strong>. At this time, the average value of the data in each cluster is set as the position of the new center of gravity. For example, to calculate the center of gravity of <font color="green"><strong>green</strong></font>, add all the circles of <font color="green"><strong>green</strong></font>. Combine and divide by the number to find the center of gravity. You can confirm that the position of the center of gravity has changed by comparing the above figure and the below figure. Especially, it is easy to understand that <font color="blue"><strong>blue</strong></font> and <font color="green"><strong>green</strong></font> are working.
<img src="https://qiita-image-store.s3.amazonaws.com/0/77079/0b02ec91-17d3-d5cd-c7c1-a9586c7b4996.png" alt="assignment.png"></p>
<h3 id="4-repeat-steps-2-and-3">4. Repeat steps 2 and 3</h3>
<p>After that, cluster allocation and centroid recalculation will be <strong>repeatedly</strong>. You can check the situation with the animation below. Here, the color of the center of gravity is black to make it easier to see. At the end, you can see that the position of the center of gravity has not changed due to convergence.
<img src="https://qiita-image-store.s3.amazonaws.com/0/77079/eb68bb3c-ddfd-c93e-c8c9-cf0e79af510e.gif" alt="1449023850iBinuHaXtI5xPbA1449023805.gif"></p>
<p>Also, you can see that the center of gravity moves along the following trajectory when viewed in a still image.
<img src="https://qiita-image-store.s3.amazonaws.com/0/77079/e48bfba4-a3ae-6573-061d-4d687af3a26f.png" alt="result.png"></p>
<h2 id="algorithm-formalization">Algorithm formalization</h2>
<p>The K-Means algorithm described above can be summarized in code as follows.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py"><span style="color:#75715e">#Center of gravity initialization step</span>
centroids <span style="color:#f92672">=</span> kmeans_init_centroids(X, K)

<span style="color:#66d9ef">for</span> iter <span style="color:#f92672">in</span> range(iterations):

    <span style="color:#75715e"># Cluster assignment step: assign the cluster to which the centroid closest to each data belongs.</span>
    <span style="color:#75715e"># idx is a list, and idx[i] stores the index of the center of gravity assigned to the i-th data</span>
    idx <span style="color:#f92672">=</span> find_closest_centroids(X, centroids)

    <span style="color:#75715e"># Centroid Move Step: Calculates the average within the cluster based on the centroid assignment</span>
    centroids <span style="color:#f92672">=</span> compute_centroids(X, idx, K)
</code></pre></div><p>There are two inputs to the algorithm:</p>
<ul>
<li>Number of clusters $K$</li>
<li>Training set {$x^{(1)}, x^{(2)},&hellip;,x^{(m)}$} ,$x^{(i)} \in \mathbb{R} ^n$</li>
</ul>
<p>Since K-Means is unsupervised learning, the training set is unlabeled. Each training data $x^{(i)}$ is a $n$ dimensional vector. Each corresponds to the circle in the figure. Also, the number of clusters must be decided in advance.</p>
<p>Now that we know the input data, we will go through each step of the algorithm.</p>
<h3 id="center-of-gravity-initialization">Center of gravity initialization</h3>
<p>First, the center of gravity is initialized. The center of gravity is a $n$-dimensional vector represented by $u^{(j)}$, and there are $K$ pieces. In the figure, it corresponds to the points with an asterisk.</p>
<pre><code class="language-math" data-lang="math">u^{(1)}, u^{(2)},...,u^{(K)} \in \mathbb{R}^n
</code></pre><p>From the training set $x^{(1)}, x^{(2)},&hellip;,x^{(m)} \in \mathbb{R}^n$, we can initialize the center of gravity. There is a way to randomly select $K$ pieces. Specifically, it randomly shuffles the training set and then selects the first $K$ as the center of gravity.</p>
<p>When implemented as code, it looks like this:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py"><span style="color:#f92672">from</span> numpy <span style="color:#f92672">import</span> <span style="color:#f92672">*</span>


<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">kmeans_init_centroids</span>(X, K):
    <span style="color:#75715e"># Initialize the center of gravity to be random data</span>

    <span style="color:#75715e"># Randomly sort training data</span>
    rand_X <span style="color:#f92672">=</span> random<span style="color:#f92672">.</span>permutation(X)

    <span style="color:#75715e"># Adopt the first K as the center of gravity</span>
    centroids <span style="color:#f92672">=</span> rand_X[:K]

    <span style="color:#66d9ef">return</span> centroids
</code></pre></div><h3 id="cluster-allocation">Cluster allocation</h3>
<p>After initializing the center of gravity, assign a cluster to each data. Specifically, find the closest centroid $u_j$ to each training data $x^{(i)}$ and assign the cluster of that centroid.</p>
<p>To find the center of gravity closest to the data, find the square of the distance as follows: Assign the cluster of centroids that minimizes the square of the distance.</p>
<pre><code class="language-math" data-lang="math">idx^{(i)} := argmin_j ||x^{(i)} − μ_j||^2
</code></pre><p>Where $idx^{(i)}$ is the index of the centroid closest to $x^{(i)}$ and $u_j$ is the coordinate of the $j$th centroid.</p>
<p>A simple implementation using numpy is as follows.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py"><span style="color:#f92672">import</span> sys
<span style="color:#f92672">from</span> numpy <span style="color:#f92672">import</span> <span style="color:#f92672">*</span>


<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">find_closest_centroids</span>(X, centroids):
    K <span style="color:#f92672">=</span> centroids<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>]
    m <span style="color:#f92672">=</span> X<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>]
    idx <span style="color:#f92672">=</span> zeros((m, <span style="color:#ae81ff">1</span>))
    <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(m):
        lowest <span style="color:#f92672">=</span> sys<span style="color:#f92672">.</span>maxint
        lowest_index <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>

        <span style="color:#66d9ef">for</span> j <span style="color:#f92672">in</span> range(K):
            cost <span style="color:#f92672">=</span> X[i]<span style="color:#f92672">-</span>centroids[j]
            cost <span style="color:#f92672">=</span> cost<span style="color:#f92672">.</span>T<span style="color:#f92672">.</span>dot(cost)
            <span style="color:#66d9ef">if</span> cost <span style="color:#f92672">&lt;</span>lowest:
                lowest_index <span style="color:#f92672">=</span> j
                lowest <span style="color:#f92672">=</span> cost

        idx[i] <span style="color:#f92672">=</span> lowest_index

    <span style="color:#66d9ef">return</span> idx
</code></pre></div><p>Using scipy&rsquo;s scipy.spatial.distance.cdist, you can write it as follows. Various distance calculation methods are defined in cdist. Among them, this time I used the squared distance. Performance is also better than the naive code above.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py"><span style="color:#f92672">import</span> scipy.spatial.distance
<span style="color:#f92672">from</span> numpy <span style="color:#f92672">import</span> <span style="color:#f92672">*</span>


<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">find_closest_centroids</span>(X, centroids):
    sqdists <span style="color:#f92672">=</span> scipy<span style="color:#f92672">.</span>spatial<span style="color:#f92672">.</span>distance<span style="color:#f92672">.</span>cdist(centroids, X,<span style="color:#e6db74">&#39;sqeuclidean&#39;</span>) <span style="color:#75715e"># find the squared distance</span>
    idx <span style="color:#f92672">=</span> argmin(sqdists, axis<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>) <span style="color:#75715e"># index of the nearest centroid for each data</span>

    <span style="color:#66d9ef">return</span> idx
</code></pre></div><h3 id="center-of-gravity-recalculation">Center of gravity recalculation</h3>
<p>After assigning the data to the clusters, recalculate the centroids.</p>
<p>Once all the data has been assigned to a cluster, the next thing to do is recalculate the centroids. Here we calculate the average of the data assigned to the cluster. For every centroid $k$, execute the following formula.</p>
<pre><code class="language-math" data-lang="math">u_k := \frac{1}{|C_k|} \sum_{i \in C_k} x^{(i)}
</code></pre><p>Where $C_k$ is the dataset with the centroid $k$ assigned. Specifically, if two data $x^{(3)}$ and $x^{(5)}$ are assigned to the center of gravity $k=2$, $u_2=\frac{1}{2 }(x^{(3)} + x^{(5)})$.</p>
<p>The simple implementation is as follows.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py"><span style="color:#f92672">from</span> numpy <span style="color:#f92672">import</span> <span style="color:#f92672">*</span>


<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">compute_centroids</span>(X, idx, K):
    m, n <span style="color:#f92672">=</span> X<span style="color:#f92672">.</span>shape
    centroids <span style="color:#f92672">=</span> zeros((K, n))<span style="color:#66d9ef">for</span> k <span style="color:#f92672">in</span> range(K):
        temp <span style="color:#f92672">=</span> X[idx <span style="color:#f92672">==</span> k] <span style="color:#75715e"># retrieve data assigned to cluster k</span>
        count <span style="color:#f92672">=</span> temp<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>] <span style="color:#75715e"># number of data allocated to cluster k</span>

        <span style="color:#66d9ef">for</span> j <span style="color:#f92672">in</span> range(n):
            centroids[k, j] <span style="color:#f92672">=</span> sum(temp[:, j]) <span style="color:#f92672">/</span> count

    <span style="color:#66d9ef">return</span> centroids
</code></pre></div><p>If you write it short using the mean function of numpy, it will look like this.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py"><span style="color:#f92672">from</span> numpy <span style="color:#f92672">import</span> <span style="color:#f92672">*</span>


<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">compute_centroids</span>(X, idx, K):
    m, n <span style="color:#f92672">=</span> X<span style="color:#f92672">.</span>shape
    centroids <span style="color:#f92672">=</span> zeros((K, n))

    <span style="color:#66d9ef">for</span> k <span style="color:#f92672">in</span> range(K):
        centroids[k] <span style="color:#f92672">=</span> mean(X[idx <span style="color:#f92672">==</span> k], axis<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)

    <span style="color:#66d9ef">return</span> centroids
</code></pre></div><!--
## Supplement: How to determine the number of clusters K
How to decide $K$? It's a rule of thumb.
- ->

# Image compression
## Overview
Now that the explanation of the algorithm is over, we will move on to the main topic, image compression.
You can also compress images by applying the K-Means algorithm. Roughly speaking, we use K-Means to select the $K$ color used to represent a compressed image. The compression is done by replacing the pixels in the original image with this $K$ color. In the following, I will explain with $K=16$.

## How to express colors
If a pixel in an image has a 24-bit color representation, it can be decomposed into three 8-bit representations. Each 8bit is <font color="red">**Red**</font>, <font color="blue">**Blue**</font>, <font color="green">** It corresponds to green**</font>, so-called RGB. Since it is 8 bits, it can take values from 0 to 255 ($2^0 to 2^8-1$). So, the combination of these values causes the image to contain an infinite number of colors, but here we reduce that number to $K = 16$ colors. By reducing the number of colors to 16 colors, each pixel should be able to be expressed in 4 bits. ($16 = 2^4$)

The figure below shows that the RGB value of each pixel in the image on the left is mapped to a three-dimensional space to produce the image on the right.
![to3d.png](https://qiita-image-store.s3.amazonaws.com/0/77079/c4e80134-b769-4185-aac0-4d4ad14420bb.png)

## Applying K-Means
The K-Means algorithm is used to select the 16 colors used to represent the compressed image. Specifically, each pixel of the original image is input as data and clustered into 16 color groups using the K-Means algorithm. Once the center of gravity is calculated, the image can be represented with 16 colors by replacing the RGB value of the pixel of the original image with the RGB value of the center of gravity.

The image is as shown in the figure below. In the figure below, the stars represent the center of gravity, the circles represent each pixel, and the pixels that belong to the same cluster have the same color. At this time, the value of the pixel belonging to the same cluster is replaced with the value of the center of gravity of that cluster. Speaking of images, it is an image that values of circles of the same color are aggregated into values of cluster centroids. For convenience of calculation, RGB values are converted to 0.0 to 1.0.
![kmean3d.png](https://qiita-image-store.s3.amazonaws.com/0/77079/78875df7-217c-99bf-7107-0959b17b326f.png)

The animation below shows how K-Means is actually applied to the image. You can see that cluster allocation and centroid recalculation are repeated.
![optimize.gif](https://qiita-image-store.s3.amazonaws.com/0/77079/969a0b4d-d3fe-8262-cf9f-2d35fc8afb30.gif)

It is the original image and the compressed image that K-Means is actually applied. The left is the original image and the right is the compressed image. You can see that the compressed image has a reduced number of colors and has a rough appearance.
![original_compressed.png](https://qiita-image-store.s3.amazonaws.com/0/77079/2f7e9fc7-d91a-bd40-dca9-bed85174e317.png)

## Compressed size
When we find a representative 16 colors that represent the image, we replace each pixel value with the closest centroid value. This is equivalent to representing the original image with the centroid assigned to each pixel. Since there are only 16 centroids, the number of bits required to represent the image is greatly reduced. Let's look at the number of bits required for the original image and the compressed image.

The original image used this time is 128 x 128 pixels in size. We needed 24 bits of information for each pixel. As a result, in total

```
128 x 128 x 24 = 393216 bit
```

Was needed.

A compressed image requires the number of bits for storing 16 colors of information. Also, each color requires 24 bits, but the image itself only requires 4 bits per pixel. This is because it is enough to show which of 16 colors is used in 4 bits. As a result, in the end

```
16 x 24 + 128 x 128 x 4 = 65920 bit
```

Will be enough. This is about 1/6 the size of the original image.


#Sample code
* [Sample Code](https://github.com/Hironsan/CourseraMachineLearning/tree/master/ex7)

# in conclusion
This time, I tried compressing the image using the K-Means algorithm. The compression result image is not good, but I think it is an algorithm with a wide range of applications. Please try it together with the sample code.

#Reference
* [Coursera Machine Learning by Stanford University Week8](https://www.coursera.org/learn/machine-learning)
* [docs for scipy.spatial.distance.cdist](http://docs.scipy.org/doc/scipy-0.15.1/reference/generated/scipy.spatial.distance.cdist.html)
* [K-Mean with Numpy](http://codereview.stackexchange.com/questions/61598/k-mean-with-numpy)
* [NumPy v1.9 Manual numpy.mean](http://docs.scipy.org/doc/numpy-1.9.1/reference/generated/numpy.mean.html#numpy.mean)
    </article>
  </div>

  
  
  
  <script>
  window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
  ga('create', 'UA-169005401-1', 'auto');
  ga('send', 'pageview');
  </script>
  <script async src='https://www.google-analytics.com/analytics.js'></script>
  

  

  
<link rel="stylesheet" type="text/css" href="//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.0/cookieconsent.min.css" />
<script src="//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.0/cookieconsent.min.js"></script>
<script>
window.addEventListener("load", function(){
window.cookieconsent.initialise({
  "palette": {
    "popup": {
      "background": "#216942",
      "text": "#b2d192"
    },
    "button": {
      "background": "#afed71"
    }
  }
})});
</script>

</body>

</html>
