<!doctype html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  
  <meta name="generator" content="Hugo 0.72.0" />

  
  <meta name="description" content="Some description">
  

  
  <link rel="apple-touch-icon" sizes="180x180" href="https://memotut.com/apple-touch-icon.png">

  
  <link rel="icon" type="image/png" sizes="32x32" href="https://memotut.com/favicon-32x32.png">

  
  <link rel="icon" type="image/png" sizes="16x16" href="https://memotut.com/favicon-16x16.png">

  
  <link rel="manifest" href="https://memotut.com/site.webmanifest">

  
  <link rel="mask-icon" href="https://memotut.com/safari-pinned-tab.svg" color="#5bbad5">

  <meta name="msapplication-TileColor" content="#da532c">

  <meta name="theme-color" content="#ffffff">

  
  <link rel="stylesheet" href="https://memotut.com/css/bootstrap.min.css" />

  
  <title>[Python] Proposal of voice conversion and voice quality morphing among multiple speakers by RelGAN-VM | Memo Tut</title>
  

  <style>
body {
  min-width: 300px;
}

.custom-navbar {
  margin-bottom: 1em;
  height: 60px;
}

.custom-navbar a {
  display: inline-block; 
  padding: 18px 0;
  margin-right: 1em; 
  font-weight: bold; 
}

.custom-navbar a:hover,
.custom-navbar a:focus {
  text-decoration: none; 
}

@media print {
  .custom-navbar {
    display: none;
  }
}

article {
  padding-bottom: 1em;
}

img {
  max-width: 100%;
}


body {
  background-color: #fff;
}



body {
  color: #212529;
}



a {
  color: #007bff;
}



a:hover,
a:focus {
  color: #0056b3;
}



.custom-navbar {
  background-color: #212529;
}



.custom-navbar a {
  color: rgba(255, 255, 255, 0.75);
}



.custom-navbar a:hover,
.custom-navbar a:focus {
  color: rgba(255, 255, 255, 1);
}



.container {
  max-width: 800px;
}



pre {
  display: block;
  padding: 9.5px;
  word-break: break-all;
  word-wrap: break-word;
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 4px;
}

pre code {
  padding: 0;
  font-size: inherit;
  color: inherit; 
  white-space: pre-wrap;
  background-color: transparent;
  border: none;
  border-radius: 0;
}

code {
  padding: 2px 4px;
  color: inherit; 
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 4px;
  font-size: .9em;
}



blockquote,
.blockquote {
  padding: 10px 20px;
  margin: 0 0 20px;
  font-size: 1em;
  border-left: 5px solid #6c757d;
}

</style>
</head>

<body>
  <nav class="custom-navbar">
  <div class="container">
    
    <a href="/">Posts</a>
    
    <a href="/tags/">Tags</a>
    
    <a href="/about/">About</a>
    
    <a href="/index.xml">RSS</a>
    
  </div>
</nav>
  
  <div class="container">
    <article>
      <h1>[Python] Proposal of voice conversion and voice quality morphing among multiple speakers by RelGAN-VM</h1>
<p>
  <small class="text-secondary">
  
  
  Jan 28, 2020
  </small>
  

<small><code><a href="https://memotut.com/tags/python">Python</a></code></small>


<small><code><a href="https://memotut.com/tags/machine-learning"> machine learning</a></code></small>


<small><code><a href="https://memotut.com/tags/tensorflow"> TensorFlow</a></code></small>


<small><code><a href="https://memotut.com/tags/voice-quality-conversion"> voice quality conversion</a></code></small>


<small><code><a href="https://memotut.com/tags/relgan"> RelGAN</a></code></small>

</p>
<pre><code>#Imakita Sangyo
</code></pre>
<p>There is an image conversion model called RelGAN!
It&rsquo;s applied to voice quality conversion!
You can morph your voice!</p>
<p>#Introduction
The number of men who want to become beautiful girls is increasing. On the contrary, there are many women who want to become beautiful boys. In recent years, this trend has become particularly noticeable with the popularity of virtual YouTuber. Regarding appearance, it is becoming an era when MMD of CG technology and Live2D that can move illustrations have appeared and it can gradually transform into what you want to be, but you want to be the voice of a cute girl or the voice of a cool boy. Is also one of the major issues. In this paper, we propose RelGAN-VM model for voice quality conversion between multiple speakers and voice quality morphing to create an intermediate voice between two speakers.</p>
<p>#In reading this article
In this paper, words that are relatively difficult are scattered around. I will do some minimal supplements, but it requires some knowledge. Specifically, I am writing based on the following knowledge.</p>
<ul>
<li>Elementary level mathematics and statistics</li>
<li>Deep learning basics, image generation and conversion by Adversarial Generation Network (GAN)</li>
<li>Basics of speech processing, minimum sound programming ability, presence of Mel scale in speech processing</li>
</ul>
<p>If you search for a term or word that you don&rsquo;t understand, please do a search (circle-throw), because I think that a search will hit a lot of easier-to-understand articles than the author of this article explains.
#Preceding, related research or articles
<strong>Previous research</strong></p>
<ul>
<li><a href="http://www.kecl.ntt.co.jp/people/kaneko.takuhiro/projects/cyclegan-vc/">CycleGAN-VC Parallel-Data-Free Voice Conversion Using Cycle-Consistent Adversarial Networks</a></li>
<li><a href="http://www.kecl.ntt.co.jp/people/kaneko.takuhiro/projects/cyclegan-vc2/index.html">CycleGAN-VC2: Improved CycleGAN-based Non-parallel Voice Conversion</a></li>
<li><a href="http://www.kecl.ntt.co.jp/people/kameoka.hirokazu/Demos/stargan-vc/">StarGAN-VC Non-parallel many-to-many voice conversion with star generative adversarial networks</a></li>
<li><a href="http://www.kecl.ntt.co.jp/people/kaneko.takuhiro/projects/stargan-vc2/index.html">StarGAN-VC2: Rethinking Conditional Methods for StarGAN-Based Voice Conversion</a></li>
</ul>
<p>CycleGAN-VC converts voice quality between two speakers and CycleGAN-VC2 improves performance.
In StarGAN-VC, voice quality conversion between multiple speakers is performed, and in StarGAN-VC2, performance is further improved and voice quality morphing is also proposed.</p>
<p>__ Related articles (including external links) __</p>
<ul>
<li><a href="https://qiita.com/KSRG_Miyabi/items/2a3b5bdca464ec1154d7">Machine learning to switch the voices of Kizuna AI and Nekomasu</a></li>
</ul>
<p><a href="https://junyanz.github.io/CycleGAN/">CycleGAN</a>-based converter is used to convert the voice quality of virtual YouTuber. Although it is spectrogram-based, it shows amazing conversion performance by a careful method.</p>
<ul>
<li><a href="https://blog.hiroshiba.jp/became-yuduki-yukari-with-deep-learning-power/">I tried to become a voice related to Yuzuki with the power of deep learning (external link)</a></li>
</ul>
<p>Using the <a href="https://phillipi.github.io/pix2pix/">pix2pix</a>-based method, the voice of the author of the article is exchanged for the voice quality of &ldquo;VOICEROID Yukari Yuzuki&rdquo;. This is also high performance.</p>
<p>#Proposed method
In this paper, <a href="https://arxiv.org/abs/1908.07269">RelGAN: Multi-Domain Image-to-Image Translation via Relative Attributes</a>(hereinafterreferredtoasRelGAN)andRelGAN-VMbasedonCycleGAN-VC2areused.Iwillpropose.ForadetailedexplanationofRelGANandCycleGAN-VC2,pleaserefertosomeonewhohaswrittenaverycleararticle(theauthorofthesearticleswillbereferredtoasLento).</p>
<ul>
<li><a href="https://medium.com/@crosssceneofwindff/relgan%E3%82%92%E7%94%A8%E3%81%84%E3%81%9F%E6%AE%B5%E9%9A%8E%E7%9A%84%E5%A4%89%E5%8C%96%E3%82%92%E4%BC%B4%E3%81%86%E9%AB%AA%E8%89%B2%E5%A4%89%E6%8F%9B-bf7131a495bd">Hair color conversion with step change using RelGAN (external link)</a></li>
<li>[Beauty girl voice conversion and synthesis (external link)](<a href="https://medium.com/@crosssceneofwindff/%E7%BE%8E%E5%B0%91%E5%A5%B3%E5%A3%25(B0%E3%81%B8%E3%81%AE%E5%A4%89%E6%8F%9B%E3%81%A8%E5%90%88%E6%88%90-fe251a8e6933)">https://medium.com/@crosssceneofwindff/%E7%BE%8E%E5%B0%91%E5%A5%B3%E5%A3%(B0%E3%81%B8%E3%81%AE%E5%A4%89%E6%8F%9B%E3%81%A8%E5%90%88%E6%88%90-fe251a8e6933)</a></li>
</ul>
<h4 id="parallel-and-non-parallel-conversion">Parallel and non-parallel conversion</h4>
<h6 id="parallel-conversion">Parallel conversion</h6>
<p>The same pronunciation content, scale information, and vocalization timing data set are required. These will never be the same and must be aligned. Since the source and destination require the same data other than voice quality, it takes a huge amount of work to build the dataset, but the impression is that the amount of data is relatively small. &ldquo;I tried to become a voice related to Yuzuki with the power of deep learning&rdquo; seems to have adopted parallel conversion.</p>
<h6 id="non-parallel-conversion">Non-parallel conversion</h6>
<p>Learn with a data set with different utterance content, scale information, and utterance timing. It is relatively easy to build a data set, as long as the data does not require alignment and just reads the text. Non-parallel conversion is adopted in the method based on CycleGAN and StarGAN.
This implementation and the main family RelGAN also use non-parallel conversion.</p>
<h3 id="network-structure">Network structure</h3>
<p>The RelGAN-VM Generator and Discriminator are based on CycleGAN-VC2, the Generator has Relative attributes concatenated to Input, and Discriminator removes the convolution of the final layer $D_{real}$, $D_{interp}$, $ It is branched into three D_{match}$. See Implementation for details.</p>
<h3 id="loss-function">loss function</h3>
<p>The base RelGAN loss function is as follows. This implementation did not use orthogonal regularization.</p>
<p>$$
{\min_{D}}L_{D}=-L_{adv}+{\lambda_1}L_{match}^D+{\lambda_2}L_{interp}^D
$$
$$
{\min_{G}}L_{G}=L_{adv}+{\lambda_1}L_{match}^G+{\lambda_2}L_{interp}^G+{\lambda_3}L_{cycle}+{\lambda_4} L_{self}
$$
When learning with this loss function, Mode collapse occurred at about 30000 steps, so this implementation adds some constraints in addition to these.</p>
<h5 id="triangle-consistency-loss">Triangle consistency loss</h5>
<p>Select 3 from N domains and call them A, B, and C. Loss is the difference between the input and output when the domain is converted from A→B→C→A. When converting from domain A to B, we will write the input image as $x$, Relative attributes as $v_{ab}$, and Generator as $G(x, v_{ab})$. If you write it as an expression, it will be as follows.
$$
L_{tri}=||x-G(G(G(x, v_{ab}), v_{bc}), v_{ca})||_1
$$</p>
<h5 id="backward-consistency-loss">Backward consistency loss</h5>
<p>▽ Cycle consistency loss is also taken for the output converted by the interpolation rate ${\alpha}$. If you write it as an expression, it will be as follows.
$$
L_{back}=||x-G(G(x, {\alpha}v_{ab}), -{\alpha}v_{ab})||_1
$$</p>
<h5 id="mode-seeking-loss">Mode seeking loss</h5>
<p>The method proposed in <a href="https://arxiv.org/abs/1903.05628">Mode Seeking Generative Adversarial Networks for Diverse Image Synthesis</a>.Pleasereferto<a href="https://qiita.com/Phoeboooo/items/c2688a7f683925a3fdd6">[LatestPaper/cGANs]RegularizationTermthatMaySolveModeCollapse</a> because some Japanese commentators have written. Please give me.
For $I_a=G(z_a)$, $I_b=G(z_b)$ converted latent variables $z_a$, $z_b$
$$
\frac{d_I(I_a, I_b)}{d_z(z_a, z_b)}
$$
Seems to be a problem to maximize ($d_I(I_a,I_b)$ is the distance between generated images and $d_z(z_a,z_b)$ is the distance between latent variables).
Based on this, this implementation added a loss to minimize the following formula.
$$
L_{ms}=\frac{{||x_1-x_2||_ 1}}{||G(x_1,v_{ab})-G(x_2,v_{ab})||_1}
$$</p>
<p>As a result of adding up these losses, Loss on the Generator side is as follows.
$$
{\min_{G}}L_{G}=L_{adv}+{\lambda_1}L_{match}^G+{\lambda_2}L_{interp}^G+{\lambda_3}L_{cycle}+{\lambda_4} L_{self}+{\lambda_5}L_{tri}+{\lambda_6}L_{back}+{\lambda_7}L_{ms}
$$</p>
<p>#Experiment
<a href="https://github.com/itsuki8914/Voice-morphing-RelGAN">I have uploaded the implementation to Github. </a>
It is based on <a href="https://github.com/njellinas/GAN-Voice-Conversion">GAN-Voice-Conversion</a>,whichistheimplementationofCycleGAN-VC2bynjellinas.WhenrewritingtoRelGAN-like,Ireferredtotheofficialpaperand<a href="https://github.com/SerialLain3170/ImageStyleTransfer/tree/master/RelGAN">Lento&rsquo;simplementationofRelGAN</a>.
###data set
We borrowed <a href="https://sites.google.com/site/shinnosuketakamichi/research-topics/jvs_corpus">JVS (Japanese versatile speech) corpus</a> as a dataset.
We used parallel100 of jvs010, jvs016, jvs042, jvs054 as learning data. I also used about 5 files from each speaker&rsquo;s nonpara30 for validation.
In parallel100, the utterance content, scale information, and utterance timing are considerably saved, but this time it is treated as nonparallel data.
▽ Write the impression of each speaker subjectively.</p>
<ul>
<li>jvs010 (female, very high $f_o$ and voice like an anime character)</li>
<li>jvs016 (female, low $f_o$ and voice like announcer)</li>
<li>jvs042 (Male, low $f_o$, similar to the author&rsquo;s voice)</li>
<li>jvs054 (male, narrator-like voice with high $f_o$)</li>
</ul>
<p>###Preprocessing
For feature extraction, <a href="https://github.com/mmorise/World">World</a>Pythonwrapper,<a href="https://github.com/JeremyCCHsu/Python-Wrapper-for-World-Vocoder/blob/master/README.md">pyWorld</a>.
However, add some work before extracting features.</p>
<h5 id="silence-removal">Silence removal</h5>
<p>If there is a lot of silence in the learning data, it may be learned as a feature of itself. In addition, the program sometimes terminated without error when extracting the feature quantity, so I decided to remove the silence (there may be a division by 0 or divergence to infinity somewhere. ). There are various ways to remove silence, but this time I simply used <a href="https://librosa.github.io/librosa/#">librosa</a>. It is also possible to take $f_o$ and extract the $f_o&gt;0$ part.</p>
<h5 id="align-audio-data-length">Align audio data length</h5>
<p>To create a batch, first select one voice data at random and cut out feature data of the specified length from it at random. If the length of each audio data is different, each sample will not be selected with equal probability. To prevent this, all voices are combined once and then divided into voice data of equal length.</p>
<h5 id="feature-extraction">Feature extraction</h5>
<p>Features are extracted with pyWorld from the separated voice data. I honestly do not understand what I am doing (a big problem), but I asked for $f_o$ and MCEPs$sp$, and from there ${\mu_{f_o}}$,${\sigma_{f_o}} Perhaps you are calculating $,${\mu_{sp}}$,${\sigma_{sp}}$ and saving the normalized version of $sp$.</p>
<h3 id="learning">learning</h3>
<p>↑ You can finally go to deep learning. The models and networks used are as described above.
The normalized MCEPs are 36 dimensions, and 128 random frames are selected and input. In addition, there is a certain probability (0.0005%) to insert complete silence into the batch. The Batch size is set to 8. Therefore, the input size is (8, 36, 128) ((1, 36, 128) when inferring).
▽ GAN used <a href="https://arxiv.org/abs/1611.04076">LSGAN</a>. It has been adopted in CycleGAN-VC2 and StarGAN-VC2, so it went the royal road.
Optimizer uses Adam, learning is started at 0.0002 for Generator and 0.0001 for Discriminator for learning late, and each step is attenuated by 0.9999 times. It is ${\beta_1}$=0.5, ${\beta_2}$=0.999.
The loss function ${\lambda}$ is ${\lambda_1}=1, {\lambda_2}=10, {\lambda_3}=10, {\lambda_4}=10, {\lambda_5}=5,{\lambda_6 }=5, {\lambda_7}=1$. In addition, ${\lambda_5} and {\lambda_6}$ are attenuated 0.9 times every 10,000 steps.
Under this condition, I learned 100,000 steps (about 62 hours with RTX 2070). As a result, the conversion did not go well after 80,000 steps, so in this article, we will evaluate using a model trained after 80,000 steps. The figure below is the Loss graph by TensorBoard. It can be seen that the Adversarial loss on the Discriminator side sharply rises (the Generator side plunges) around 80,000 steps.
<img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/384153/a3cf22b2-e053-bce2-8c22-4531a91f95d4.png" alt="losses.png"></p>
<h1 id="voice-generation-and-evaluation">Voice generation and evaluation</h1>
<p><a href="https://github.com/itsuki8914/Voice-morphing-RelGAN/tree/master/result_examples">Put the generated audio on Github. </a>
<a href="https://youtu.be/y2AX6drB3RY">Also available on YouTube. </a></p>
<h3 id="audio-reconstruction">audio reconstruction</h3>
<p>The learned neural network is a model that converts only normalized MCEPs, and it is converted to speech again by pyWorld using this and other statistics. The conversion flow when converting from domain A to domain B with interpolation rate ${\alpha}$ is described below.</p>
<ol>
<li>Load wav.</li>
<li>Calculate $f_{o_A}$, $sp_A$, $ap_A$ (aperiodic index) from the read wav by pyWorld.</li>
<li>Transform $f_{o_A}$ with the following formula. $\mu_{f_{o_{\alpha}}}$ and ${\sigma_{f_{o_{\alpha}}}}$ are calculated using linear interpolation (standard deviation should be returned to variance once) Or verification required).
$$
\mu_{f_{o_{\alpha}}}=(1-{\alpha})\mu_{f_{o_A}}+{\alpha}\mu_{f_{o_B}}
$$
$$
{\sigma_{f_{o_{\alpha}}}}=(1-{\alpha})\sigma_{f_{o_A}}+{\alpha}\sigma_{f_{o_B}}
$$
$$
f_{o_\alpha}=\frac{f_{o_A}-\mu_{f_{o_A}}}{\sigma_{f_{o_A}}}\sigma_{f_{o_{\alpha}}}}+\ mu_{f_{o_{\alpha}}}
$$</li>
<li>Normalize $sp_A$.
$$
sp_{A_{norm}}=\frac{sp_A-\mu_{sp_A}}{\sigma_{sp_{A}}}
$$</li>
<li>Have the neural network infer $sp_{A_{norm}}$.
$$
sp_{{\alpha}_{norm}}=G({sp}_{A_{norm}}, {\alpha}v_{ab})
$$</li>
<li>Denormalize $sp_{{\alpha}_{norm}}$. By the mean, linear interpolation is used for the mean and standard deviation.</li>
</ol>
<p>$$
\mu_{sp_{{\alpha}}}=(1-{\alpha})\mu_{sp_{A}}+{\alpha}\mu_{sp_{B}}
$$
$$
{\sigma_{sp_{{\alpha}}}}=(1-{\alpha})\sigma_{sp_{A}}+{\alpha}\sigma_{sp_{B}}
$$
$$
{sp_{\alpha}}={\mu_{sp_{{\alpha}}}}+{\sigma_{sp_{{\alpha}}}}{sp_{{\alpha}_{norm}}}
$$</p>
<p>$$$$
7. World re-synthesizes the obtained $f_{o_\alpha}$,${sp_{\alpha}}$,$ap_A$. Note that the aperiodic index $ap_A$ uses the original one.
8. Normalize the volume and export if necessary.</p>
<p>That&rsquo;s it.</p>
<h3 id="subjective-evaluation">Subjective evaluation</h3>
<p>I do not know the method of quantitative evaluation, so I will evaluate it by myself (problem).
I think that the conversion to male speakers (jvs042, jvs054) has been done to some extent by all speakers. On the other hand, conversion to female speakers (jvs010, jvs016) is low in conversion accuracy between heterosexual groups aside from conversion between same sex, and is particularly remarkable in conversion to jvs010. Also, morphing is quite subtle and not only $f_o$ has changed? Some of the results are like this. You can see that they are morphing if they are of the opposite sex, but I feel that they are less natural.</p>
<p>#Summary
In this paper, we proposed RelGAN-VM and conducted experiments on voice quality conversion and voice quality morphing. I am proud that the conversion to a male speaker was comparable to the existing method, but the accuracy of the conversion from a male speaker to a female speaker, especially to a high voice, is not so high. did. Voice morphing was proposed in StarGAN-VC2 as a previous study, and it was a bit insufficient to write it in the paper, so I thought that it would be unavoidable to discard it and I decided to post it to Qiita. It&rsquo;s difficult to be a beautiful girl.</p>
<h1 id="future-works">Future works</h1>
<p>In this paper, only the voice quality between speakers was converted. For example, in <a href="https://voice-statistics.github.io/">Voice Actors Statistics Corpus</a>, 9 types of datasets, which are read out by 3 speakers with 3 types of emotions, are released. By learning this, it may be possible to morph not only the voice quality of the speaker but also the emotions. Also, the network structure, loss function, and hyperparameters of this implementation are not perfect, and there is room for improvement. I would like to continue to study higher-performance voice conversion models.
#Notes on voice conversion
Although not a good topic, <a href="https://acoustics.jp/qanda/answer/74.html">I have the right to individual voices. </a> For example, if you use voice conversion to spoof another person&rsquo;s voice and use it for commercial use or abuse without permission, it will be severely punished as a privacy infringement or copyright infringement. I will. Of course, you are free to use other voices by using this implementation, but when using audio data that is not copyrighted, please use it only for personal use. It is grayed out even if it is non-commercial and can be viewed by an unspecified number of people, so be careful. For example, the JVS corpus used in this experiment has the following usage conditions.__ The text data comes from the JSUT corpus, and its license information is described in the JSUT corpus. Tag information is licensed by CC-BY-SA 4.0. The voice data can be used only in the following cases.
Research at academic institutions
Research for non-commercial purposes (including research in commercial organizations)
Personal use (including blog etc.)
If you would like to use it for commercial purposes, please see the following. Redistribution of this voice data is not allowed, but it is possible to publish part of the corpus (for example, about 10 sentences) on your web page or blog. __</p>
<p>In addition, I do not take any responsibility for the accident caused by voice quality conversion using this implementation.
#Acknowledgement
Many people in the lab who gave me some advice in posting this article, especially <a href="https://twitter.com/lgeuwce">Rugiu-kun</a>, gave us many opinions regarding the technical aspects of the World, and provided incorrect information. I was able to fix it. It&rsquo;s still hard work to reach his required level, but it was a good study. We would also like to thank those who have uploaded clear articles, papers, implementations, and libraries, and those in the lab who have published useful datasets. Any deficiencies in this article are my fault.</p>
<p>#Reflection
What are you doing without writing a thesis I orz</p>

    </article>
  </div>

  
  
  
  <script>
  window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
  ga('create', 'UA-169005401-1', 'auto');
  ga('send', 'pageview');
  </script>
  <script async src='https://www.google-analytics.com/analytics.js'></script>
  

  

  
<link rel="stylesheet" type="text/css" href="//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.0/cookieconsent.min.css" />
<script src="//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.0/cookieconsent.min.js"></script>
<script>
window.addEventListener("load", function(){
window.cookieconsent.initialise({
  "palette": {
    "popup": {
      "background": "#216942",
      "text": "#b2d192"
    },
    "button": {
      "background": "#afed71"
    }
  }
})});
</script>

</body>

</html>
