<!doctype html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  
  <meta name="generator" content="Hugo 0.72.0" />

  
  <meta name="description" content="Some description">
  

  
  <link rel="apple-touch-icon" sizes="180x180" href="https://japan2018.github.io/apple-touch-icon.png">

  
  <link rel="icon" type="image/png" sizes="32x32" href="https://japan2018.github.io/favicon-32x32.png">

  
  <link rel="icon" type="image/png" sizes="16x16" href="https://japan2018.github.io/favicon-16x16.png">

  
  <link rel="manifest" href="https://japan2018.github.io/site.webmanifest">

  
  <link rel="mask-icon" href="https://japan2018.github.io/safari-pinned-tab.svg" color="#5bbad5">

  <meta name="msapplication-TileColor" content="#da532c">

  <meta name="theme-color" content="#ffffff">

  
  <link rel="stylesheet" href="https://japan2018.github.io/css/bootstrap.min.css" />

  
  <title>Natural Language : ChatBot Part1 - Twitter API Corpus | Some Title</title>
  

  <style>
body {
  min-width: 300px;
}

.custom-navbar {
  margin-bottom: 1em;
  height: 60px;
}

.custom-navbar a {
  display: inline-block; 
  padding: 18px 0;
  margin-right: 1em; 
  font-weight: bold; 
}

.custom-navbar a:hover,
.custom-navbar a:focus {
  text-decoration: none; 
}

@media print {
  .custom-navbar {
    display: none;
  }
}

article {
  padding-bottom: 1em;
}

img {
  max-width: 100%;
}


body {
  background-color: #fff;
}



body {
  color: #212529;
}



a {
  color: #007bff;
}



a:hover,
a:focus {
  color: #0056b3;
}



.custom-navbar {
  background-color: #212529;
}



.custom-navbar a {
  color: rgba(255, 255, 255, 0.75);
}



.custom-navbar a:hover,
.custom-navbar a:focus {
  color: rgba(255, 255, 255, 1);
}



.container {
  max-width: 800px;
}



pre {
  display: block;
  padding: 9.5px;
  word-break: break-all;
  word-wrap: break-word;
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 4px;
}

pre code {
  padding: 0;
  font-size: inherit;
  color: inherit; 
  white-space: pre-wrap;
  background-color: transparent;
  border: none;
  border-radius: 0;
}

code {
  padding: 2px 4px;
  color: inherit; 
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 4px;
  font-size: .9em;
}



blockquote,
.blockquote {
  padding: 10px 20px;
  margin: 0 0 20px;
  font-size: 1em;
  border-left: 5px solid #6c757d;
}

</style>
</head>

<body>
  <nav class="custom-navbar">
  <div class="container">
    
    <a href="/">Posts</a>
    
    <a href="/tags/">Tags</a>
    
    <a href="/about/">About</a>
    
    <a href="/index.xml">RSS</a>
    
  </div>
</nav>
  
  <div class="container">
    <article>
      <h1>Natural Language : ChatBot Part1 - Twitter API Corpus</h1>
<p>
  <small class="text-secondary">
  
  
  May 23, 2020
  </small>
  

<small><code><a href="https://japan2018.github.io/tags/python">Python</a></code></small>


<small><code><a href="https://japan2018.github.io/tags/nlp">NLP</a></code></small>


<small><code><a href="https://japan2018.github.io/tags/twitterapi">TwitterAPI</a></code></small>


<small><code><a href="https://japan2018.github.io/tags/chatbot">chatbot</a></code></small>


<small><code><a href="https://japan2018.github.io/tags/sentencepiece">SentencePiece</a></code></small>

</p>
<pre><code>#Target
</code></pre>
<p>I have summarized a chatbot using the Microsoft Cognitive Toolkit (CNTK).</p>
<p>Part 1 prepares you for training your chatbot with the Microsoft Cognitive Toolkit.</p>
<p>We will introduce them in the following order.</p>
<ol>
<li>Register Twitter Developer</li>
<li>Collecting conversation datasets using Twitter API</li>
<li>Pre-processing text data and creating Sentence Piece model</li>
<li>Create a file to be read by the built-in reader provided by CNTK</li>
</ol>
<p>#Introduction
Assuming you have a Twitter account and Linux is available.</p>
<h2 id="twitter-developer-registration">Twitter Developer Registration</h2>
<h3 id="apply-for-twitter-developer">Apply for Twitter Developer</h3>
<p><a href="https://developer.twitter.com/">Twitter-Developer</a></p>
<p>Access the page above and click Apply in the upper right corner.</p>
<p>Click Apply for a developer account and select your Twitter account.</p>
<p>Then select some basic items. As you proceed in order, you will be required to describe about 100 to 200 characters for each purpose such as what purpose you will use it, how to use Twitter API and data, but even in simple English it is a problem Since it does not seem to be true, let&rsquo;s honestly write the purpose of use.</p>
<p>Finally, after confirming the registration contents and applying to Twitter Developer, an email will be sent to the email address linked to the Twitter account used for registration, so let&rsquo;s wait for the examination result. If it is fast, you will receive an e-mail with the examination result in about one day.</p>
<p>In addition, at this stage, you may receive an email from Twitter Developer again, and you may be asked to use the Twitter API in Japanese, but it seems that there is no problem if you translate the content when written in English straight into Japanese. I will.</p>
<h3 id="get-api-key-and-access-token">Get API key and Access token</h3>
<p>Next, get the API key, API secret key, Access token, and Access token secret to get tweets and replies.</p>
<p>Click Apps -&gt; Create an app and you&rsquo;ll be asked for the name of the app to create, the website, and how to use it.</p>
<p>When the app is created, the API key and API secret key are displayed in Keys and tokens on the detail page of the created app.</p>
<p>Make a note of the Access token and Access token secret as they are only displayed once by pressing the generate button.</p>
<h2 id="collecting-conversation-datasets-using-twitter-api">Collecting conversation datasets using Twitter API</h2>
<p>If you register to Twitter Developer and get API key, API secret key, Access token, Access token secret, you will be able to collect tweets and replies.</p>
<p>Install tweepy to handle the Twitter API in Python. The Twitter API has various functions, but this time, we decided to collect posted tweets and one reply to them as one conversation and save it as a text file of up to 100 conversations.</p>
<p>The directory structure this time is as follows.</p>
<p><a href="https://qiita.com/sho_watari/items/c18355a76bc4bd825a3d">Doc2Vec</a>
STSA
|-twitter
|-20200401_000000.txt
――――&hellip;
Stsa_corpus.py
Stsa_twitter.py
<a href="https://qiita.com/sho_watari/items/59f5fab1b9f81fc9660a">Word2Vec</a></p>
<h2 id="preprocessing-text-data-and-creating-sentence-piece-model">Preprocessing text data and creating Sentence Piece model</h2>
<p>After collecting the tweet and reply conversation dataset, we preprocess the text data.</p>
<p>This time I did text cleaning using the Python standard module re and emoji.</p>
<p>In <a href="https://qiita.com/sho_watari/items/59f5fab1b9f81fc9660a">Word2Vec</a>and<a href="https://qiita.com/sho_watari/items/c18355a76bc4bd825a3d">Doc2Vec</a>,divideMeCabintowordsbasedonNEologddictionary.Iranitandcreatedaworddictionary,butthistimeIwillcreateasubwordmodelusing<a href="https://github.com/google/sentencepiece">sentencepiece</a>.</p>
<h2 id="create-a-file-to-be-read-by-the-built-in-reader-provided-by-cntk">Create a file to be read by the built-in reader provided by CNTK</h2>
<p>After converting it to a word ID using the Sentence Piece model trained with the training data, we are ready to create a text file for the CTFDeserializer used to train the chatbot.</p>
<p>CTFDeserializer is introduced in <a href="https://qiita.com/sho_watari/items/ad247ec4fb260b96ae6a">Computer Vision: Image Caption Part1-STAIR Captions</a>.</p>
<p>#Implementation</p>
<h2 id="execution-environment">Execution environment</h2>
<p>###hardware
・CPU Intel(R) Core(TM) i7-9750H 2.60GHz</p>
<p>###software
・Windows 10 Pro 1909
・Python 3.6.6
・Emoji 0.5.4
・Sentencepiece 0.1.86
・Tweepy 3.8.0</p>
<h2 id="program-to-execute">Program to execute</h2>
<p>The implemented program is published on <a href="https://github.com/sho-watari/NaturalLanguage/tree/master/STSA">GitHub</a>.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-Python:stsa_twitter.py" data-lang="Python:stsa_twitter.py"></code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-Python:stsa_corpus.py" data-lang="Python:stsa_corpus.py"></code></pre></div><h2 id="comment">Comment</h2>
<p>Some parts of the program to be executed are extracted and supplemented.</p>
<h3 id="text-cleaning">Text cleaning</h3>
<p>Text cleaning is important because the tweets and replies we collect contain a lot of noise.</p>
<h4 id="removal-of-twitter-specific-http-links-user-ids-and-tags">Removal of Twitter-specific http links, @user IDs, and #tags</h4>
<pre><code class="language-Python:text_cleaning" data-lang="Python:text_cleaning">s = re.sub(r&quot;http\S+&quot;, &quot;&quot;, s) # remove https
s = re.sub(r&quot;\@[a-z0-9-_][a-z0-9-_]*&quot;, &quot;&quot;, s) # remove @tweet
s = re.sub(r&quot;\#.+&quot;, &quot;&quot;, s) # remove #tag
</code></pre><h4 id="normalization-of-various-symbols">Normalization of various symbols</h4>
<pre><code class="language-Python:text_clearning" data-lang="Python:text_clearning">s = re.sub(&quot;[˗֊‐‑––⁃⁻₋−]+&quot;, &quot;-&quot;, s) # normalize hyphens
s = re.sub(&quot;[﹣－ー—――─━ー]+&quot;, &quot;ー&quot;, s) # normalize choonpus
s = re.sub(&quot;[~~∾~〰~]&quot;, &quot;&quot;, s) # remove tildes

s = s.lower() # normalize alphabet to lowercase
s = s.translate((ord(x): ord(y) for x, y in zip( # normalize half-width symbols to full-width symbols
    &quot;!\&quot;#$%&amp;'()*+,-./:;&lt;=&gt;?@[\]^_`{|}~., &quot;&quot;&quot;,
    &quot;!&quot; #$% &amp;'()*+,-. /:; &lt;=&gt;? @[\]^_`{|}~. ,・&quot;&quot;&quot;)})
</code></pre><h4 id="redundancy-reduction">Redundancy reduction</h4>
<pre><code class="language-Python:text_clearning" data-lang="Python:text_clearning">s = re.sub(r&quot;! +&quot;, &quot;!&quot;, s)
s = re.sub(r&quot;?? +&quot;, &quot;?&quot;, s)
s = re.sub(r &quot;…+&quot;, &quot;…&quot;, s)
s = re.sub(r&quot;w+w&quot;, &quot;.&quot;, s)
</code></pre><h4 id="delete-emoji-and-emoticons">Delete emoji and emoticons</h4>
<pre><code class="language-Python:text_clearning" data-lang="Python:text_clearning">s = &quot;&quot;.join([&quot;&quot; if c in emoji.UNICODE_EMOJI else c for c in s]) # remove emoji
s = re.sub(r&quot; (.*).*&quot;, &quot;&quot;, s) # remove kaomoji
</code></pre><p>This isn&rsquo;t quite perfect, but it&rsquo;s been reasonably good.</p>
<p>###Sentence Piece Model Training
First, clone SentencePiece from GitHub and build it. The following is running on a Linux distribution.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">$ git clone https://github.com/google/sentencepiece.git

$ sudo apt-get install cmake build-essential pkg-config libgoogle-perftools-dev

$ cd sentencepiece
$ mkdir build
$ cd build
$ cmake ..
$ make -j <span style="color:#66d9ef">$(</span>nproc<span style="color:#66d9ef">)</span>
$ sudo make install
$ sudo ldconfig -v
</code></pre></div><p>When the training is over, twitter.model and twitter.vocab will be created. By default, the Sentence Piece model is assigned 0, 1, 2 for unknown, start and end respectively.</p>
<h3 id="tweet-and-reply-conversation-data-set">Tweet and reply conversation data set</h3>
<p>The saved tweet and reply text file consists of two lines, one tweet and one reply.</p>
<p>You should ensure that there is at least one word, as the result of performing a text cleaning may be 0 words.</p>
<pre><code class="language-Python:stsa_preprocessing" data-lang="Python:stsa_preprocessing">for idx in range(len(corpus) // 2):
    tweet = text_cleaning(corpus[2 * idx])
    reply = text_cleaning(corpus[2 * idx + 1])

    if len(tweet) == 0 or len(reply) == 0:
        continue
    else:
        f.write(&quot;%s\n&quot;% tweet)
        f.write(&quot;%s\n&quot; %reply)
```In addition, since the sequence length of tweets and replies may be different, the length of tweets and replies is adjusted using zip_longest of Python standard module itertools to create a text file to be read by the built-in reader.

```Python:stsa_sentencepiece
with open(&quot;./tweet_reply_corpus.txt&quot;, &quot;w&quot;) as ctf_file:
    for i, (tweet, reply, target) in enumerate(tweet_reply):
        for (twt, rep) in zip_longest(tweet, reply, fillvalue=&quot;&quot;):
            if twt == &quot;&quot;:
                ctf_file.write(&quot;{} |reply {}:1\n&quot;.format(i, rep))
            elif rep == &quot;&quot;:
                ctf_file.write(&quot;{} | tweet {}:1\n&quot;.format(i, twt))
            else:
                ctf_file.write(&quot;{} |tweet {}:1\t|reply {}:1\n&quot;.format(i, twt, rep))
</code></pre><p>#result
First, run stsa_twitter.py to collect tweets and replies.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">  1 :@user_id ...
  2 :@user_id ...
...
</code></pre></div><p>The code published on GitHub does not completely handle exceptions, so if you execute it as it is, it will stop about every 24 hours, so please take a look every half a day.</p>
<p>After you have collected the required amount of data, running the function stsa_preprocessing produces a Twitter conversation corpus twitter.txt consisting of text-cleaned tweet and reply pairs.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">Number of tweet and reply: 1068294
</code></pre></div><p>Then train the Sentence Piece model. Training is started by setting the arguments as follows. I set the number of words to 32,000.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">$ spm_train --input<span style="color:#f92672">=</span>/mnt/c/.../twitter.txt --model_prefix<span style="color:#f92672">=</span>twitter --vocab_size<span style="color:#f92672">=</span><span style="color:#ae81ff">32000</span>
</code></pre></div><p>After the training, twitter.model and twitter.vocab will be created.</p>
<p>Finally, run the function stsa_sentencepiece to create a text file that CTFDeserializer reads.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">Now 10000 samples...
Now 20000 samples...
...
Now 970000 samples...

Number of samples 973124
</code></pre></div><p>We collected about 1 million conversations in a month, but the actual data we could use was 973,124 conversations.</p>
<p>Now that you&rsquo;re ready to train, in Part 2, you&rsquo;ll use CNTK to train your chatbot.</p>
<p>#reference
<a href="https://developer.twitter.com/">Twitter-Developer</a>
<a href="https://github.com/google/sentencepiece">sentencepiece</a></p>
<p><a href="https://qiita.com/sho_watari/items/ad247ec4fb260b96ae6a">Computer Vision :Image Caption Part1-STAIR Captions</a>
<a href="https://qiita.com/sho_watari/items/59f5fab1b9f81fc9660a">Natural Language: Word2Vec Part1-Japanese Corpus</a></p>

    </article>
  </div>

  
  
  
  <script>
  window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
  ga('create', 'UA-123456789-1', 'auto');
  ga('send', 'pageview');
  </script>
  <script async src='https://www.google-analytics.com/analytics.js'></script>
  

  

  
<link rel="stylesheet" type="text/css" href="//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.0/cookieconsent.min.css" />
<script src="//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.0/cookieconsent.min.js"></script>
<script>
window.addEventListener("load", function(){
window.cookieconsent.initialise({
  "palette": {
    "popup": {
      "background": "#216942",
      "text": "#b2d192"
    },
    "button": {
      "background": "#afed71"
    }
  }
})});
</script>

</body>

</html>
