<!doctype html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  
  <meta name="generator" content="Hugo 0.72.0" />

  
  <meta name="description" content="Some description">
  

  
  <link rel="apple-touch-icon" sizes="180x180" href="https://memotut.com/apple-touch-icon.png">

  
  <link rel="icon" type="image/png" sizes="32x32" href="https://memotut.com/favicon-32x32.png">

  
  <link rel="icon" type="image/png" sizes="16x16" href="https://memotut.com/favicon-16x16.png">

  
  <link rel="manifest" href="https://memotut.com/site.webmanifest">

  
  <link rel="mask-icon" href="https://memotut.com/safari-pinned-tab.svg" color="#5bbad5">

  <meta name="msapplication-TileColor" content="#da532c">

  <meta name="theme-color" content="#ffffff">

  
  <link rel="stylesheet" href="https://memotut.com/css/bootstrap.min.css" />

  
  <title>List of activation functions (2020) | Memo Tut</title>
  

  <style>
body {
  min-width: 300px;
}

.custom-navbar {
  margin-bottom: 1em;
  height: 60px;
}

.custom-navbar a {
  display: inline-block; 
  padding: 18px 0;
  margin-right: 1em; 
  font-weight: bold; 
}

.custom-navbar a:hover,
.custom-navbar a:focus {
  text-decoration: none; 
}

@media print {
  .custom-navbar {
    display: none;
  }
}

article {
  padding-bottom: 1em;
}

img {
  max-width: 100%;
}


body {
  background-color: #fff;
}



body {
  color: #212529;
}



a {
  color: #007bff;
}



a:hover,
a:focus {
  color: #0056b3;
}



.custom-navbar {
  background-color: #212529;
}



.custom-navbar a {
  color: rgba(255, 255, 255, 0.75);
}



.custom-navbar a:hover,
.custom-navbar a:focus {
  color: rgba(255, 255, 255, 1);
}



.container {
  max-width: 800px;
}



pre {
  display: block;
  padding: 9.5px;
  word-break: break-all;
  word-wrap: break-word;
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 4px;
}

pre code {
  padding: 0;
  font-size: inherit;
  color: inherit; 
  white-space: pre-wrap;
  background-color: transparent;
  border: none;
  border-radius: 0;
}

code {
  padding: 2px 4px;
  color: inherit; 
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 4px;
  font-size: .9em;
}



blockquote,
.blockquote {
  padding: 10px 20px;
  margin: 0 0 20px;
  font-size: 1em;
  border-left: 5px solid #6c757d;
}

</style>
</head>

<body>
  <nav class="custom-navbar">
  <div class="container">
    
    <a href="/">Posts</a>
    
    <a href="/tags/">Tags</a>
    
    <a href="/about/">About</a>
    
    <a href="/index.xml">RSS</a>
    
  </div>
</nav>
  
  <div class="container">
    <article>
      <h1>List of activation functions (2020)</h1>
<p>
  <small class="text-secondary">
  
  
  May 30, 2020
  </small>
  

<small><code><a href="https://memotut.com/tags/python">Python</a></code></small>


<small><code><a href="https://memotut.com/tags/deep-learning"> deep learning</a></code></small>


<small><code><a href="https://memotut.com/tags/activation-function"> activation function</a></code></small>

</p>
<pre><code># Target audience
</code></pre>
<p>I summarized what kind of activation function there are.
**Latest Swish and Mish, plus tanhExp! **
I&rsquo;m targeting a layer where I can&rsquo;t find something good even if I search the list.
New ones will be added as soon as they are found.
If you have information on new functions or functions on the TODO list below, please let us know!</p>
<p>#TODO list</p>
<ul>
<li>Look up additional information for the hardShrink function</li>
<li>Look up supplementary information of softShrink function</li>
<li>Check supplementary information of Threshold function</li>
<li>Look up supplementary information of logSigmoid function</li>
<li>Look up additional information for the tanhShrink function</li>
<li>Look up additional information for hardtanh function</li>
<li>Check supplementary information of ReLU6 function</li>
<li>Look up additional information for CELU function</li>
<li>Check the supplementary information of the softmin function</li>
<li>Look up additional information for the logSoftmax function</li>
<li>Examine some unimplemented functions
-Pytorch
-What is the MultiheadAttention function
-Learning method of PReLU function
-Implementation of RReLU function
-What is the cumulative distribution function that appears in the GELU function?
-Implementation of GELU function
-Implementation of Softmax2d function</li>
</ul>
<p>#table of contents
-<a href="#stepfunctionstep">Step function (step)</a>
-<a href="#Identityfunctionidentity">Identity function (identity)</a>
-<a href="#bent-identityfunction">Bent Identity function</a>
-<a href="#hardshrinkfunction">hardShrink function</a>
-<a href="#softshrinkfunction">softShrink function</a>
-<a href="#thresholdfunction">Threshold function</a>
-<a href="#sigmoidfunctionsigmoid">Sigmoid function (sigmoid)</a>
-<a href="#hardsigmoidfunction">hardSigmoid function</a>
-<a href="#logsigmoidfunction">logSigmoid function</a>
-<a href="#tanhfunction">tanh function</a>
-<a href="#tanhshrinkfunction">tanhShrink function</a>
-<a href="#hardtanhfunction">hardtanh function</a>
-<a href="#relufunction">ReLU function</a>
-<a href="#relu6function">ReLU6 function</a>
-<a href="#leaky-relufunction">leaky-ReLU function</a>
-<a href="#elufunction">ELU function</a>
-<a href="#selufunction">SELU function</a>
-<a href="#celufunction">CELU function</a>
-<a href="#softmaxfunctionsoftmax">Softmax function (softmax)</a>
-<a href="#softminfunction">softmin function</a>
-<a href="#logsoftmaxfunction">logSoftmax function</a>
-<a href="#softplusfunction">softplus function</a>
-<a href="#softsignfunction">softsign function</a>
-<a href="#swishfunction">Swish function</a>
-<a href="#mishfunction">Mish function</a>
-<a href="#tanhexpfunction">tanhExp function</a>
-<a href="#codeexample">Code example</a></p>
<p>#Step function (step)
First, from the step function. Probably the oldest activation function.
<img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/640911/c5c5949e-14ef-6275-4ed7-0cc55f05af91.png" alt="step.png">
It was used to implement the perceptron at that time, but it is rarely seen in deep learning these days.
The reason is that the derivative is $0$ for all real numbers ($x \ne 0$), so the parameters cannot be optimized by backpropagation.</p>
<p>The forward propagation formula is</p>
<pre><code class="language-math" data-lang="math">y = \left\{
  \begin{array}{cc}
    1 &amp; (x \gt 0) \\
    0 &amp; (x \le 0)
  \end{array}
\right.
</code></pre><p>With such a feeling, the back propagation formula is natural</p>
<pre><code class="language-math" data-lang="math">\cfrac{\partial y}{\partial x} = 0
</code></pre><p>And it is multiplied by the error that has flowed. That is, nothing is flown.
For this reason, the error backpropagation method cannot be applied, and in deep learning, it is forced into the shade.</p>
<p>#Identity function (Identity)
The identity function outputs its input unchanged. It is used for the activation function of the output layer of regression analysis. There is no turn in the middle tier.
The purpose of using such an activation function is <strong>to implement it uniquely</strong>. Unique implementation is intended not to divide processing by conditional branching, etc. here.
<img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/640911/57b57530-fdbc-c6ff-f8f5-d1e9cdbb3dc2.png" alt="identity.png">
Since the differential value is $1$, the error propagates to the previous layer as it is. The error calculation uses the squared error, so the propagation to the next layer is $y-t$ ~</p>
<p>The forward propagation formula is</p>
<pre><code class="language-math" data-lang="math">y = x
</code></pre><p>And backpropagation is</p>
<pre><code class="language-math" data-lang="math">\cfrac{\partial y}{\partial x} = 1
</code></pre><p>Will be. You can see that it will flow as it is!</p>
<p>#Bent Identity function
It is a function similar to <a href="#identityfunctionidentity">Identity function</a>.
However, the feature is that it is not straight but slightly bent.
<img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/640911/eb57ee1b-0b10-294a-2373-9566ee2db1a3.png" alt="bent-identity.png"></p>
<p>The forward propagation formula is</p>
<pre><code class="language-math" data-lang="math">y = \cfrac{1}{2}(\sqrt{x^2 + 1}-1) + x
</code></pre><p>Back propagation is like this</p>
<pre><code class="language-math" data-lang="math">\cfrac{\partial y}{\partial x} = \cfrac{x}{2 \sqrt{x^2 + 1}} + 1
</code></pre><p>Will be. Somehow it looks like <a href="#relufunction">ReLU function</a> (personal impression).
I couldn&rsquo;t find the article introduced in Japanese at first glance, so I think it&rsquo;s a minor activation function.</p>
<p>#hardShrink function
From Pytorch, it&rsquo;s just an introduction.
<strong>Find TODO supplementary information</strong>
<img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/640911/f68a484a-8c4e-5e31-35bf-556305f99868.png" alt="hard-shrink.png">
The forward propagation formula is</p>
<pre><code class="language-math" data-lang="math">y = \left\{
  \begin{array}{cc}
    x &amp; (x \lt -\lambda \quad \textrm{or} \quad \lambda \lt x) \\
    0 &amp; (\textrm{otherwise})
  \end{array}
\right.
</code></pre><p>And backpropagation is</p>
<pre><code class="language-math" data-lang="math">\cfrac{\partial y}{\partial x} = \left\{
  \begin{array}{cc}
    1 &amp; (x \lt -\lambda \quad \textrm{or} \quad \lambda \lt x) \\
    0 &amp; (\textrm{otherwise})
  \end{array}
\right.
</code></pre><p>Will be. The default value for $\lambda$ is $0.5$.</p>
<p>#softShrink function
This is just an introduction from Pytorch.
<strong>Find TODO supplementary information</strong>*
<img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/640911/9a1e48e1-f093-fafb-d6a3-8a048a601091.png" alt="soft-shrink.png">
The forward propagation formula is</p>
<pre><code class="language-math" data-lang="math">y = \left\{
  \begin{array}{cc}
    x + \lambda &amp; (x \lt -\lambda) \\
    x-\lambda &amp; (x \gt \lambda) \\
    0 &amp; (\textrm{otherwise})
  \end{array}
\right.
</code></pre><p>And backpropagation is</p>
<pre><code class="language-math" data-lang="math">\cfrac{\partial y}{\partial x} = \left\{
  \begin{array}{cc}
    1 &amp; (x \lt -\lambda \quad \textrm{or} \quad \lambda \lt x) \\
    0 &amp; (\textrm{otherwise})
  \end{array}
\right.
</code></pre><p>Will be. The initial value of $\lambda$ here is also $0.5$.</p>
<p>#Threshold function
This is just an introduction from Pytorch.
<strong>Find TODO supplementary information</strong>
<img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/640911/b59e611d-501c-546d-44b2-ae8a4acfb696.png" alt="threshold.png">
The forward propagation formula is</p>
<pre><code class="language-math" data-lang="math">y = \left\{
  \begin{array}{cc}
    x &amp; (x \gt threshold) \\
    value &amp; (\textrm{otherwise})
  \end{array}
\right.
</code></pre><p>And backpropagation is</p>
<pre><code class="language-math" data-lang="math">y = \left\{
  \begin{array}{cc}
    1 &amp; (x \gt threshold) \\
    0 &amp; (\textrm{otherwise})
  \end{array}
\right.
</code></pre><p>Will be. Here, the variables threshold and value are values that should be given in advance. In the graph for the time being</p>
<pre><code class="language-math" data-lang="math">threshold = -1 \\
value = -2
</code></pre><p>I am.</p>
<p>#Sigmoid function
The sigmoid function is an activation function that was often used when the backpropagation method appeared. However, nowadays it is rarely used in the middle layer, and is often used in the output layer of binary classification problems.
The reason is because of the disadvantages described below.
<img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/640911/54bbccf4-25f3-c25c-9b95-334dde1c53dd.png" alt="sigmoid.png">
Forward propagation</p>
<pre><code class="language-math" data-lang="math">y = \cfrac{1}{1 + e^{-x}}
</code></pre><p>Backpropagation</p>
<pre><code class="language-math" data-lang="math">\cfrac{\partial y}{\partial x} = y(1-y)
</code></pre><p>Can be written asThe biggest feature is that the derivative can be easily obtained from the output, but the response is extremely poor for extremely large and small inputs, and the maximum value of the derivative is $0.25$. There are also disadvantages such as the problem of disappearance**.
Since there is also exponential calculation and division, the calculation load is inevitably higher than with simple functions such as <a href="#relufunction">ReLU function</a>.</p>
<p>#hardSigmoid function
The hardSigmoid function is a linear approximation of the sigmoid function.
<img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/640911/45c44607-6a8d-0729-cb1b-4d2990950800.png" alt="hard-sigmoid.png">
Mathematically, forward propagation is</p>
<pre><code class="language-math" data-lang="math">y = \left\{
  \begin{array}{cc}
    1 &amp; (x \gt 2.5) \\
    0.2x + 0.5 &amp; (-2.5 \le x \le 2.5) \\
    0 &amp; (x \lt -2.5)
  \end{array}
\right.
</code></pre><p>Backpropagation</p>
<pre><code class="language-math" data-lang="math">\cfrac{\partial y}{\partial x} = \left\{
  \begin{array}{cc}
    0.2 &amp; (-2.5 \le x \le 2.5) \\
    0 &amp; (\textrm{otherwise})
  \end{array}
\right.
</code></pre><p>It looks like.
<a href="https://qiita.com/hsjoihs/items/88d1569aaef01659bbd5">Detailed article</a>, so if you want to know more, please go there!
There is such a complicated theoretical reason that the coefficient of the linear function is $0.2$ &hellip;
~~ I read it, but I didn&rsquo;t understand it at all ~~</p>
<p>#logSigmoid function
This is just an introduction from Pytorch. Takes the logarithm of <a href="#sigmoidfunctionsigmoid">sigmoid function</a>.
<strong>Find TODO supplementary information</strong>
<img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/640911/072bd12c-b7bf-5ed4-af1a-214a3837328c.png" alt="log-sigmoid.png">
The forward propagation formula is</p>
<pre><code class="language-math" data-lang="math">y = \log \left( \cfrac{1}{1 + e^{-x}} \right)
</code></pre><p>And backpropagation is</p>
<pre><code class="language-math" data-lang="math">\cfrac{\partial y}{\partial x} = \cfrac{1}{1 + e^x}
</code></pre><p>Will be. Note that the denominator of backpropagation is not the $-x$ power.</p>
<p>#tanhfunction
The tanh function, which is one of the hyperbolic functions, has been proposed as one of the functions to solve the weak point that the maximum value of the differential of the <a href="#sigmoidfunctionsigmoid">sigmoid function</a> is $0.25$.
<img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/640911/169ed9e9-7b6d-2bae-c3ca-42f9372479c4.png" alt="tanh.png">
As you can see, the maximum value of the differential is $1$, and the cause of the vanishing gradient can be removed.
However, there still remains the problem that the derivative with extremely large and small inputs becomes $0$.</p>
<pre><code class="language-math" data-lang="math">y = \tanh x = \cfrac{e^x-e^{-x}}{e^x + e^{-x}}
</code></pre><p>Backpropagation</p>
<pre><code class="language-math" data-lang="math">\cfrac{\partial y}{\partial x} = \textrm{sech}^2 x = \cfrac{1}{\cosh^2 x} = \cfrac{4}{(e^x + e^{- x})^2}
</code></pre><p>Will be.
Recently, it has been used in some of the expected new stars such as <a href="#mishfunction">Mish function</a> and <a href="#tanhexpfunction">tanhExp function</a>.</p>
<p>#tanhShrink function
This is also from Pytorch. Just an introduction.
<strong>Find TODO supplementary information</strong>
<img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/640911/d1774d43-642c-23ba-a667-8125740a4767.png" alt="tanh-shrink.png">
The forward propagation formula is</p>
<pre><code class="language-math" data-lang="math">y = x-\tanh x
</code></pre><p>And backpropagation is</p>
<pre><code class="language-math" data-lang="math">\cfrac{\partial y}{\partial x} = \tanh^2 x
</code></pre><p>Will be.</p>
<p>#hardtanhfunction
This is also Pytorch. Just an introduction&hellip;
<strong>Find TODO supplementary information</strong>
<img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/640911/47c88f81-5e58-8f1e-3325-3183152ae1fd.png" alt="hard-tanh.png">
The forward propagation formula is</p>
<pre><code class="language-math" data-lang="math">y = \left\{
  \begin{array}{cc}
    1 &amp; (x \gt 1) \\
    -1 &amp; (x \lt -1) \\
    x &amp; (\textrm{otherwise})
  \end{array}
\right.
</code></pre><p>And backpropagation is</p>
<pre><code class="language-math" data-lang="math">\cfrac{\partial y}{\partial x} = \left\{
  \begin{array}{cc}
    0 &amp; (x \lt -1 \quad \textrm{or} \quad 1 \le x) \\
    1 &amp; (\textrm{otherwise})
  \end{array}
\right.
</code></pre><p>Will be.</p>
<p>#ReLU function
The ReLU function (generally called the ramp function) is an activation function that has been fairly recently proposed and holds the hegemony. Its feature is its simple and high-speed operation.
<img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/640911/830b7112-2936-91b2-9989-4f328192e79e.png" alt="ReLU.png">
The forward propagation formula is</p>
<pre><code class="language-math" data-lang="math">y = \left\{
  \begin{array}{cc}
    x &amp; (x \gt 0) \\
    0 &amp; (x \le 0)
  \end{array}
\right.
</code></pre><p>Backpropagation</p>
<pre><code class="language-math" data-lang="math">\cfrac{\partial y}{\partial x} = \left\{
  \begin{array}{cc}
    1 &amp; (x \gt 0) \\
    0 &amp; (x \le 0)
  \end{array}
\right.
</code></pre><p>Will be. If the input is a positive value, the gradient will always be $1$, so there is the advantage that gradient disappearance is unlikely to occur and layers can be stacked easily, but the disadvantage is that learning will not proceed at all for negative inputs.
Also, we basically ignore the discontinuity at $x=0$.
In the error backpropagation method, learning proceeds based on the gradient propagation using the chain rule, so the activation function should be differentiable with all real numbers, but in reality it is exactly $x=0$ There are few cases where &amp; becomes $0$ anyway, so it is not a problem.</p>
<p>#ReLU6 function
From Pytorch, just an introduction.
<strong>Find TODO supplementary information</strong>
<img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/640911/80db9449-a39d-1c92-193b-c5f199991469.png" alt="ReLU6.png">
The forward propagation formula is</p>
<pre><code class="language-math" data-lang="math">y = \left\{
  \begin{array}{cc}
    0 &amp; (x \le 0) \\
    6 &amp; (x \ge 6) \\
    x &amp; (\textrm{otherwise})
  \end{array}
\right.
</code></pre><p>And backpropagation is</p>
<pre><code class="language-math" data-lang="math">\cfrac{\partial y}{\partial x} = \left\{
  \begin{array}{cc}
    0 &amp; (x \le 0 \quad \textrm{or} \quad 6 \le x) \\
    1 &amp; (\textrm{otherwise})
  \end{array}
\right.
</code></pre><p>Will be.</p>
<p>#leaky-ReLU function
The leaky-ReLU function outputs a linear function with a very small slope when using a negative input, in order to compensate the &ldquo;ReLU function&rdquo; (#relufunction) that &ldquo;learning does not progress for a negative input&rdquo;. It was done like this.
<img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/640911/8b96e34a-0c66-ec1c-6ac3-0976bef05c94.png" alt="leaky-ReLU.png">
I can hardly understand it in the graph, but in the formula</p>
<pre><code class="language-math" data-lang="math">y = \left\{
  \begin{array}{cc}
    x &amp; (x \gt 0) \\
    0.01x &amp; (x \le 0)
  \end{array}
\right.
</code></pre><p>The output at the time of negative input is different like this. So backpropagation is</p>
<pre><code class="language-math" data-lang="math">\cfrac{\partial y}{\partial x} = \left\{
  \begin{array}{cc}
    1 &amp; (x \gt 0) \\
    0.01 &amp; (x \le 0)
  \end{array}
\right.
</code></pre><p>Will be. Again, this is $x=0$ and discontinuous.
Also, while looking through it, I saw it in various places, but apparently the name of this function says &ldquo;it didn&rsquo;t make any sense to use it&rdquo;. That&rsquo;s a bit surprising. Although it seems to be improved a little&hellip;</p>
<p>#ELU function
ELU function is one of the smoother functions when $x=0$, which is similar to <a href="#relufunction">ReLU function</a>.
<img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/640911/e3c5264f-4d0f-628b-ee84-a084d3314a7a.png" alt="ELU.png">
As you can see from the graph, a negative input does not result in a slope of $0$ ($x\to-\infty$ at $0$). In the formula</p>
<pre><code class="language-math" data-lang="math">y = \left\{
  \begin{array}{cc}
    x &amp; (x \ge 0) \\
    \alpha (e^x-1) &amp; (x \lt 0)
  \end{array}
\right.
</code></pre><p>And backpropagation is</p>
<pre><code class="language-math" data-lang="math">\cfrac{\partial y}{\partial x} = \left\{
  \begin{array}{cc}
    1 &amp; (x \ge 0) \\\alpha e^x &amp; (x \lt 0)
  \end{array}
\right.
</code></pre><p>Will be.
The value of ~~$\alpha$ seems to be theoretically appropriate in the following <a href="#selufunction">SELU function</a> (maybe). ~~
** 2020/6/2 modified **
The default value for $\alpha$ is likely to be $1$. I will replace it with the graph of $\alpha=1$.
I am sorry that I sent wrong information&hellip;</p>
<p>#SELU function
The SELU function is the output of <a href="#elufunction">ELU function</a> multiplied by $\lambda$.
<img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/640911/9609aba5-feda-7716-3f8b-233d4ccd81da.png" alt="SeLU.png">
In the formula</p>
<pre><code class="language-math" data-lang="math">y = \left\{
  \begin{array}{cc}
    \lambda x &amp; (x \ge 0) \\
    \lambda \alpha (e^x-1) &amp; (x \lt 0)
  \end{array}
\right.
</code></pre><p>And backpropagation also</p>
<pre><code class="language-math" data-lang="math">\cfrac{\partial y}{\partial x} = \left\{
  \begin{array}{cc}
    \lambda &amp; (x \ge 0) \\
    \lambda \alpha e^x &amp; (x \lt 0)
  \end{array}
\right.
</code></pre><p>Will be multiplied by $\lambda$.
It seems that the theoretically optimum parameter value can be found, and that value is</p>
<pre><code class="language-math" data-lang="math">\alpha = 1.67326\ldots, \quad \lambda = 1.0507\ldots
</code></pre><p>It seems that I may read the paper soon&hellip; I will supplement when I read it.</p>
<p>#CELU function
This is also an introduction only from Pytorch.
<strong>Find TODO supplementary information</strong>
<img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/640911/7b38ed09-a2c5-0c8e-a071-ef9ae275260c.png" alt="CeLU.png">
The forward propagation formula is</p>
<pre><code class="language-math" data-lang="math">y = \left\{
  \begin{array}{cc}
    x &amp; (x \ge 0) \\
    \alpha \left( e^{\frac{x}{\alpha}}-1 \right) &amp; (\textrm{otherwise})
  \end{array}
\right.
</code></pre><p>And the backpropagation formula is</p>
<pre><code class="language-math" data-lang="math">y = \left\{
  \begin{array}{cc}
    1 &amp; (x \ge 0) \\
    e^{\frac{x}{\alpha}} &amp; (\textrm{otherwise})
  \end{array}
\right.
</code></pre><p>Will be.</p>
<h1 id="softmax-function-softmax">Softmax function (softmax)</h1>
<p>The softmax function is used as the activation function of the output layer of the multivalued classification problem.
Due to the characteristics of the calculation, it is possible to regard the output as probability.
<img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/640911/96a3cf1b-1d83-bed0-1254-dc2f780f6023.png" alt="softmax.png">
Don&rsquo;t worry too much about the vertical axis of the graph. All that matters is that integrating (since the computer is discrete, the sum is taken) yields $1$.
Mathematically</p>
<pre><code class="language-math" data-lang="math">y_i = \cfrac{e^{x_i}}{\displaystyle\sum_{k=1}^{n}{e^{x_k}}} \quad (i = 1, 2, \ldots, n)
</code></pre><p>It&rsquo;s like that. Back propagation is temporary</p>
<pre><code class="language-math" data-lang="math">\left( \cfrac{\partial y}{\partial x} \right)_i = e^{x_i} \cfrac{\displaystyle\sum_{k=1}^{n}{e^{x_k}}-e ^{x_i}}{\left( \displaystyle\sum_{k=1}^{n}{e^{x_k}} \right)^2}
</code></pre><p>However, <strong>cross entropy error</strong></p>
<pre><code class="language-math" data-lang="math">Error = t \log y
</code></pre><p>By taking back propagation from the output layer to the middle layer</p>
<pre><code class="language-math" data-lang="math">y-t
</code></pre><p>It will be very simple.
By the way, this is not a coincidence, and the cross entropy error is a function designed to fit the softmax function so that the gradient becomes $y-t$.
I may introduce it someday in a calculation graph.</p>
<p>#softminfunction
This is also from Pytorch. The inverse of <a href="#softmaxfunctionsoftmax">softmax function</a>, the probability of small value is large.
<strong>Find TODO supplementary information</strong>
<img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/640911/0769571e-6c62-dba8-8904-e598ad0cbbdd.png" alt="softmin.png">
The forward propagation formula is</p>
<pre><code class="language-math" data-lang="math">y_i = \cfrac{e^{-x_i}}{\displaystyle\sum_{k=1}^{n}{e^{-x_k}}} \quad (i = 1, 2, \ldots, n)
</code></pre><p>And the backpropagation formula is</p>
<pre><code class="language-math" data-lang="math">\left( \cfrac{\partial y}{\partial x} \right)_i = e^{-x_i} \cfrac{\displaystyle\sum_{k=1}^{n}{e^{-x_k}} -e^{-x_i}}{\left( \displaystyle\sum_{k=1}^{n}{e^{-x_k}} \right)^2}
</code></pre><p>Will be. If the cross entropy error is also used, is the error backpropagated neatly? I will investigate again.</p>
<p>#logSoftmax function
From Pytorch, this is the logarithm of the <a href="#softmaxfunctionsoftmax">softmax function</a>.
<strong>Find TODO supplementary information</strong>
<img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/640911/1810a4ac-81b4-f190-6a4d-6858ce803f1d.png" alt="log-softmax.png">
It looks almost straight. Is it correct &hellip; I think the code is correct for the time being.
The forward propagation formula is</p>
<pre><code class="language-math" data-lang="math">y_i = \log \left( \cfrac{e^{x_i}}{\displaystyle\sum_{k=1}^{n}{e^{x_k}}} \right)
</code></pre><p>And backpropagation is</p>
<pre><code class="language-math" data-lang="math">\left( \cfrac{\partial y}{\partial x} \right)_i = \cfrac{\displaystyle\sum_{k=1}^{n}{e^{x_k}}-e^{x_i}} {\displaystyle\sum_{k=1}^{n}{e^{x_k}}}
</code></pre><p>Will be.</p>
<p>#softplusfunction
The softplus function has a name similar to <a href="#softmaxfunctionsoftmax">softmax function</a>, but its essence is similar to <a href="#relufunction">ReLU function</a>.
<img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/640911/792ca998-f9b8-7088-946f-2388b227677a.png" alt="softplus.png"></p>
<p>In the formula</p>
<pre><code class="language-math" data-lang="math">y = \log{(1 + e^x)} = \ln{(1 + e^x)}
</code></pre><p>And backpropagation is</p>
<pre><code class="language-math" data-lang="math">\cfrac{\partial y}{\partial x} = \cfrac{e^x}{1 + e^x} = \cfrac{1}{1 + e^{-x}}
</code></pre><p>It looks like. It looks exactly like <a href="#relufunction">ReLU function</a> and differentiation.</p>
<p>By the way, $\ln x$ is to make it clear that the base is a logarithmic function of Napier&rsquo;s number. That is</p>
<pre><code class="language-math" data-lang="math">\ln x = \log_ex
</code></pre><p>It is that.</p>
<p>#softsignfunction
This is also similar in name to <a href="#softmaxfunctionsoftmax">softmax function</a>, but actually it is similar to <a href="#tanhfunction">tanh function</a> (forward propagation).
<img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/640911/3c5b7eed-e816-51c3-ad42-34366373d1d3.png" alt="softsign.png"></p>
<p>Forward propagation looks just like the tanh function, but back propagation is completely different. It&rsquo;s very sharp.
Looking at forward propagation with a mathematical formula</p>
<pre><code class="language-math" data-lang="math">y = \cfrac{x}{1 + |x|}
</code></pre><p>And backpropagation is</p>
<pre><code class="language-math" data-lang="math">\cfrac{\partial y}{\partial x} = \cfrac{1}{(1 + |x|)^2}
</code></pre><p>Has become. The derivative becomes a discontinuous function at ~~$x=0$. ~~
** 2020/6/2 modified **
I misunderstood the continuity of the function. Correctly, it is not discontinuous. Not differentiable.</p>
<pre><code class="language-math" data-lang="math">\lim_{x \to \pm 0}{\cfrac{1}{(1 + |x|)^2}} = 1
\Leftrightarrow
\lim_{x \to 0}{\cfrac{1}{(1 + |x|)^2}} = 1
</code></pre><p>And</p>
<pre><code class="language-math" data-lang="math">\cfrac{\partial y}{\partial x} = \cfrac{1}{(1 + |0|)^2} = 1 \quad (\because x = 0)
</code></pre><p>So</p>
<pre><code class="language-math" data-lang="math">\lim_{x \to 0}{\cfrac{1}{(1 + |x|)^2}} = \cfrac{1}{(1 + |0|)^2}
</code></pre><p>And is shown to be continuous.</p>
<p>#Swish function
This is the Swish function that appeared in 2017 and is expected to succeed the <a href="#relufunction">ReLU function</a>.
<img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/640911/de3d273a-5f5b-f8e1-58e2-f5d230eaf389.png" alt="Swish.png">
The appearance is similar to <a href="#relufunction">ReLU function</a>, but unlike <a href="#elufunction">ELU function</a> and <a href="#selufunction">SELU function</a>, $x=0$ is a continuous function. I am.
Another feature is that it is a $C^{\infty}$ class function.
Furthermore, you can see that it takes a small negative value for a negative input. The nice thing is that there is a minimum value and no maximum value.
Expressing forward propagation as a mathematical expression</p>
<pre><code class="language-mathy" data-lang="mathy"></code></pre><p>It&rsquo;s like that. In the graph above, we have set $\beta=1$.
By the way, $\beta$ can be optimized by the error backpropagation method (not implemented).
Backpropagation</p>
<pre><code class="language-math" data-lang="math">\cfrac{\partial y}{\partial x} = \beta y + \sigma_{sigmoid}(\beta x)(1-\beta y) = \beta y + \cfrac{1-\beta y}{1 + e^{-\beta x}}
</code></pre><p>Can be written as <a href="#sigmoidfunctionsigmoid">Sigmoid function</a> is a glimpse of the image.</p>
<p>#Mish function
Mish function is the successor to <a href="#relufunction">ReLU function</a> proposed in 2019, which is more recent than <a href="#swishfunction">Swish function</a>. The paper shows that it often performs better than the Swish function. (I haven&rsquo;t read the paper yet, but he wrote that.)
<img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/640911/ba47ce55-6679-ba7e-8ce5-0755b579d711.png" alt="Mish.png">
It looks almost the same as the Swish function, but is slightly different.
<img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/640911/6b31d54c-d803-f3aa-f3ba-79f6426a0027.png" alt="swish_vs_mish.png">
The graph on the far right shows the most difference. This graph is the second derivative of each.
In other words, it represents the degree of change in the gradient.
What can be read from the graph is that the error is transmitted more prominently in the $\Rightarrow$ gradient calculation in which the Mish function changes dynamically around $x=0$.
The formula for forward propagation is</p>
<pre><code class="language-math" data-lang="math">y = x \tanh{(\varsigma(x))} = x \tanh{(\ln{(1 + e^x)})}
</code></pre><p>And backpropagation is a little complicated</p>
<pre><code class="language-math" data-lang="math">\cfrac{\partial y}{\partial x} = \cfrac{e^x \omega}{\delta^2}\\
\omega = 4(x + 1) + 4e^{2x} + e^{3x} + (4x + 6)e^x \\
\delta = 2e^x + e^{2x} + 2
</code></pre><p>Is calculated as Therefore, learning takes longer than ReLU function.
However, in terms of accuracy, it is often better than using the ReLU function, so let&rsquo;s select the activation function considering the trade-off between learning time and accuracy.</p>
<p>#tanhExp function
This is the tanhExp function that got information from @reppy4620!
According to <a href="https://arxiv.org/pdf/2003.09855.pdf">Papers</a>, it&rsquo;s from March 2020 ~ It&rsquo;s really late.
<img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/640911/73db18a2-c8fb-874b-5487-338a7b74bbdf.png" alt="tanhExp.png">
As you can see in the paper, it&rsquo;s a member of <a href="#relufunction">ReLU function</a> (Is it called ReLU family?).
Apparently, it seems to perform better than <a href="#mishfunction">Mish function</a> on famous datasets such as MNIST, CIFER-10, CIFER-100 (I have not read it properly yet).
Compared with <a href="#swishfunction">Swish function</a>
<img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/640911/6faf3809-0f0d-1dd2-4150-6f433f3b28ba.png" alt="Swish_vs_tanhExp.png">
The output of forward propagation looks almost the same, but the tanhExp function has a steeper back propagation and a smaller range over the derivative value $1$. Gradients are very delicate. If the absolute value of the derivative is smaller than $1$, the gradient disappears immediately, and if it is larger than $1$, the phenomenon of gradient explosion occurs. In that respect, the tanhExp function looks excellent.
Then compare with <a href="#mishfunction">Mish function</a>.
<img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/640911/4801b52f-52f1-2b09-c85c-fa834b27aff2.png" alt="Mish_vs_tanhExp.png">
Mish function follows tanhExp function rather than Swish function. However, the tanhExp function still excels at steep slopes around $0$.
Now let&rsquo;s look at forward propagation with a mathematical formula.</p>
<pre><code class="language-math" data-lang="math">y = x \tanh(e^x)
</code></pre><p>We use the tanh function as well as the Mish function. Isn&rsquo;t the tanh function getting more attention recently?
Backpropagation</p>
<pre><code class="language-math" data-lang="math">\begin{align}
  \cfrac{\partial y}{\partial x} &amp;= \tanh(e^x) + xe^x\textrm{sech}^2(e^x) \\
  &amp;= \tanh(e^x)-xe^x(\tanh^2(e^x)-1)
\end{align}
</code></pre><p>Will be. It&rsquo;s good that it can be calculated much simpler than the Mish function.</p>
<p>#Code example
Here is the code example used when writing the graph. Please use it as a reference when implementing it.
I am using a jupyter notebook.</p>
<details>
<summary>activators.py</summary>
<div>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-activators.py" data-lang="activators.py"><span style="color:#f92672">import</span> numpy <span style="color:#f92672">as</span> np


<span style="color:#66d9ef">class</span> <span style="color:#a6e22e">Activator</span>():
    <span style="color:#66d9ef">def</span> __init__(self, <span style="color:#f92672">*</span>args,<span style="color:#f92672">**</span>kwds):
        <span style="color:#66d9ef">pass</span>
    

    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, <span style="color:#f92672">*</span>args,<span style="color:#f92672">**</span>kwds):
        <span style="color:#66d9ef">raise</span> <span style="color:#a6e22e">Exception</span>(<span style="color:#e6db74">&#34;Not Implemented&#34;</span>)
    
    
    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">backward</span>(self, <span style="color:#f92672">*</span>args,<span style="color:#f92672">**</span>kwds):
        <span style="color:#66d9ef">raise</span> <span style="color:#a6e22e">Exception</span>(<span style="color:#e6db74">&#34;Not Implemented&#34;</span>)
    
    
    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">update</span>(self, <span style="color:#f92672">*</span>args,<span style="color:#f92672">**</span>kwds):
        <span style="color:#66d9ef">pass</span>


<span style="color:#66d9ef">class</span> <span style="color:#a6e22e">step</span>(Activator):
    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x, <span style="color:#f92672">*</span>args,<span style="color:#f92672">**</span>kwds):
        <span style="color:#66d9ef">return</span> np<span style="color:#f92672">.</span>where(x<span style="color:#f92672">&gt;</span> <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">0</span>)
    
    
    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">backward</span>(self, x, <span style="color:#f92672">*</span>args,<span style="color:#f92672">**</span>kwds):
        <span style="color:#66d9ef">return</span> np<span style="color:#f92672">.</span>zeros_like(x)


<span style="color:#66d9ef">class</span> <span style="color:#a6e22e">identity</span>(Activator):
    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x, <span style="color:#f92672">*</span>args,<span style="color:#f92672">**</span>kwds):
        <span style="color:#66d9ef">return</span> x
    
    
    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">backward</span>(self, x, <span style="color:#f92672">*</span>args,<span style="color:#f92672">**</span>kwds):
        <span style="color:#66d9ef">return</span> np<span style="color:#f92672">.</span>ones_like(x)


<span style="color:#66d9ef">class</span> <span style="color:#a6e22e">bentIdentity</span>(Activator):
    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x, <span style="color:#f92672">*</span>args,<span style="color:#f92672">**</span>kwds):
        <span style="color:#66d9ef">return</span> <span style="color:#ae81ff">0.5</span><span style="color:#f92672">*</span>(np<span style="color:#f92672">.</span>sqrt(x<span style="color:#f92672">**</span><span style="color:#ae81ff">2</span> <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span>)<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>) <span style="color:#f92672">+</span> x
    
    
    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">backward</span>(self, x, <span style="color:#f92672">*</span>args,<span style="color:#f92672">**</span>kwds):
        <span style="color:#66d9ef">return</span> <span style="color:#ae81ff">0.5</span><span style="color:#f92672">*</span>x<span style="color:#f92672">/</span>np<span style="color:#f92672">.</span>sqrt(x<span style="color:#f92672">**</span><span style="color:#ae81ff">2</span> <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span>) <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span>


<span style="color:#66d9ef">class</span> <span style="color:#a6e22e">hardShrink</span>(Activator):
    <span style="color:#66d9ef">def</span> __init__(self, lambda_<span style="color:#f92672">=</span><span style="color:#ae81ff">0.5</span>, <span style="color:#f92672">*</span>args,<span style="color:#f92672">**</span>kwds):
        self<span style="color:#f92672">.</span>lambda_ <span style="color:#f92672">=</span> lambda_
        super()<span style="color:#f92672">.</span>__init__(<span style="color:#f92672">*</span>args,<span style="color:#f92672">**</span>kwds)
    
    
    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x, <span style="color:#f92672">*</span>args,<span style="color:#f92672">**</span>kwds):
        <span style="color:#66d9ef">return</span> np<span style="color:#f92672">.</span>where((<span style="color:#f92672">-</span>self<span style="color:#f92672">.</span>lambda_ <span style="color:#f92672">&lt;=</span> x) <span style="color:#f92672">&amp;</span> (x <span style="color:#f92672">&lt;=</span> self<span style="color:#f92672">.</span>lambda_),
                        <span style="color:#ae81ff">0</span>, x)
    
    
    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">backward</span>(self, x, <span style="color:#f92672">*</span>args,<span style="color:#f92672">**</span>kwds):
        <span style="color:#66d9ef">return</span> np<span style="color:#f92672">.</span>where((<span style="color:#f92672">-</span>self<span style="color:#f92672">.</span>lambda_ <span style="color:#f92672">&lt;=</span> x) <span style="color:#f92672">&amp;</span> (x <span style="color:#f92672">&lt;=</span> self<span style="color:#f92672">.</span>lambda_),
                        <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1</span>)


<span style="color:#66d9ef">class</span> <span style="color:#a6e22e">softShrink</span>(Activator):
    <span style="color:#66d9ef">def</span> __init__(self, lambda_<span style="color:#f92672">=</span><span style="color:#ae81ff">0.5</span>, <span style="color:#f92672">*</span>args,<span style="color:#f92672">**</span>kwds):
        self<span style="color:#f92672">.</span>lambda_ <span style="color:#f92672">=</span> lambda_
        super()<span style="color:#f92672">.</span>__init__(<span style="color:#f92672">*</span>args,<span style="color:#f92672">**</span>kwds)
    
    
    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x, <span style="color:#f92672">*</span>args,<span style="color:#f92672">**</span>kwds):
        <span style="color:#66d9ef">return</span> np<span style="color:#f92672">.</span>where(x <span style="color:#f92672">&lt;-</span>self<span style="color:#f92672">.</span>lambda_, x <span style="color:#f92672">+</span> self<span style="color:#f92672">.</span>lambda_,
                        np<span style="color:#f92672">.</span>where(x<span style="color:#f92672">&gt;</span> self<span style="color:#f92672">.</span>lambda_, x<span style="color:#f92672">-</span>self<span style="color:#f92672">.</span>lambda_, <span style="color:#ae81ff">0</span>))
    
    
    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">backward</span>(self, x, <span style="color:#f92672">*</span>args,<span style="color:#f92672">**</span>kwds):
        <span style="color:#66d9ef">return</span> np<span style="color:#f92672">.</span>where((<span style="color:#f92672">-</span>self<span style="color:#f92672">.</span>lambda_ <span style="color:#f92672">&lt;=</span> x) <span style="color:#f92672">&amp;</span> (x <span style="color:#f92672">&lt;=</span> self<span style="color:#f92672">.</span>lambda_),
                        <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1</span>)


<span style="color:#66d9ef">class</span> <span style="color:#a6e22e">threshold</span>(Activator):
    <span style="color:#66d9ef">def</span> __init__(self, threshold, value, <span style="color:#f92672">*</span>args,<span style="color:#f92672">**</span>kwds):
        self<span style="color:#f92672">.</span>threshold <span style="color:#f92672">=</span> threshold
        self<span style="color:#f92672">.</span>value <span style="color:#f92672">=</span> value
        super()<span style="color:#f92672">.</span>__init__(<span style="color:#f92672">*</span>args,<span style="color:#f92672">**</span>kwds)
    
    
    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x, <span style="color:#f92672">*</span>args,<span style="color:#f92672">**</span>kwds):
        <span style="color:#66d9ef">return</span> np<span style="color:#f92672">.</span>where(x<span style="color:#f92672">&gt;</span> self<span style="color:#f92672">.</span>threshold, x, self<span style="color:#f92672">.</span>value)
    
    
    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">backward</span>(self, x, <span style="color:#f92672">*</span>args,<span style="color:#f92672">**</span>kwds):
        <span style="color:#66d9ef">return</span> np<span style="color:#f92672">.</span>where(x<span style="color:#f92672">&gt;</span> self<span style="color:#f92672">.</span>threshold, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">0</span>)


<span style="color:#66d9ef">class</span> <span style="color:#a6e22e">sigmoid</span>(Activator):<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x, <span style="color:#f92672">*</span>args,<span style="color:#f92672">**</span>kwds):
        <span style="color:#66d9ef">return</span> <span style="color:#ae81ff">1</span><span style="color:#f92672">/</span>(<span style="color:#ae81ff">1</span> <span style="color:#f92672">+</span> np<span style="color:#f92672">.</span>exp(<span style="color:#f92672">-</span>x))
    
    
    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">backward</span>(self, x, y, <span style="color:#f92672">*</span>args,<span style="color:#f92672">**</span>kwds):
        <span style="color:#66d9ef">return</span> y<span style="color:#f92672">*</span>(<span style="color:#ae81ff">1</span> <span style="color:#f92672">-</span> y)


<span style="color:#66d9ef">class</span> <span style="color:#a6e22e">hardSigmoid</span>(Activator):
    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x, <span style="color:#f92672">*</span>args,<span style="color:#f92672">**</span>kwds):
        <span style="color:#66d9ef">return</span> np<span style="color:#f92672">.</span>clip(<span style="color:#ae81ff">0.2</span><span style="color:#f92672">*</span>x <span style="color:#f92672">+</span> <span style="color:#ae81ff">0.5</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1</span>)
    
    
    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">backward</span>(self, x, <span style="color:#f92672">*</span>args,<span style="color:#f92672">**</span>kwds):
        <span style="color:#66d9ef">return</span> np<span style="color:#f92672">.</span>where((x <span style="color:#f92672">&gt;</span> <span style="color:#ae81ff">2.5</span>) <span style="color:#f92672">|</span> (x <span style="color:#f92672">&lt;</span> <span style="color:#f92672">-</span><span style="color:#ae81ff">2.5</span>), <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0.2</span>)


<span style="color:#66d9ef">class</span> <span style="color:#a6e22e">logSigmoid</span>(Activator):
    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x, <span style="color:#f92672">*</span>args,<span style="color:#f92672">**</span>kwds):
        <span style="color:#66d9ef">return</span> <span style="color:#f92672">-</span>np<span style="color:#f92672">.</span>log(<span style="color:#ae81ff">1</span> <span style="color:#f92672">+</span> np<span style="color:#f92672">.</span>exp(<span style="color:#f92672">-</span>x))
    
    
    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">backward</span>(self, x, <span style="color:#f92672">*</span>args,<span style="color:#f92672">**</span>kwds):
        <span style="color:#66d9ef">return</span> <span style="color:#ae81ff">1</span><span style="color:#f92672">/</span>(<span style="color:#ae81ff">1</span> <span style="color:#f92672">+</span> np<span style="color:#f92672">.</span>exp(x))


<span style="color:#66d9ef">class</span> <span style="color:#a6e22e">act_tanh</span>(Activator):
    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x, <span style="color:#f92672">*</span>args,<span style="color:#f92672">**</span>kwds):
        <span style="color:#66d9ef">return</span> np<span style="color:#f92672">.</span>tanh(x)
    
    
    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">backward</span>(self, x, <span style="color:#f92672">*</span>args,<span style="color:#f92672">**</span>kwds):
        <span style="color:#66d9ef">return</span> <span style="color:#ae81ff">1</span> <span style="color:#f92672">-</span> np<span style="color:#f92672">.</span>tanh(x)<span style="color:#f92672">**</span><span style="color:#ae81ff">2</span>


<span style="color:#66d9ef">class</span> <span style="color:#a6e22e">hardtanh</span>(Activator):
    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x, <span style="color:#f92672">*</span>args,<span style="color:#f92672">**</span>kwds):
        <span style="color:#66d9ef">return</span> np<span style="color:#f92672">.</span>clip(x, <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>)
    
    
    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">backward</span>(self, x, <span style="color:#f92672">*</span>args,<span style="color:#f92672">**</span>kwds):
        <span style="color:#66d9ef">return</span> np<span style="color:#f92672">.</span>where((<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span> <span style="color:#f92672">&lt;=</span> x) <span style="color:#f92672">&amp;</span> (x <span style="color:#f92672">&lt;=</span> <span style="color:#ae81ff">1</span>), <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">0</span>)


<span style="color:#66d9ef">class</span> <span style="color:#a6e22e">tanhShrink</span>(Activator):
    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x, <span style="color:#f92672">*</span>args,<span style="color:#f92672">**</span>kwds):
        <span style="color:#66d9ef">return</span> x <span style="color:#f92672">-</span> np<span style="color:#f92672">.</span>tanh(x)
    
    
    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">backward</span>(self, x, <span style="color:#f92672">*</span>args,<span style="color:#f92672">**</span>kwds):
        <span style="color:#66d9ef">return</span> np<span style="color:#f92672">.</span>tanh(x)<span style="color:#f92672">**</span><span style="color:#ae81ff">2</span>


<span style="color:#66d9ef">class</span> <span style="color:#a6e22e">ReLU</span>(Activator):
    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x, <span style="color:#f92672">*</span>args,<span style="color:#f92672">**</span>kwds):
        <span style="color:#66d9ef">return</span> np<span style="color:#f92672">.</span>maximum(<span style="color:#ae81ff">0</span>, x)
    
    
    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">backward</span>(self, x, <span style="color:#f92672">*</span>args,<span style="color:#f92672">**</span>kwds):
        <span style="color:#66d9ef">return</span> np<span style="color:#f92672">.</span>where(x <span style="color:#f92672">&gt;</span> <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">0</span>)


<span style="color:#66d9ef">class</span> <span style="color:#a6e22e">ReLU6</span>(Activator):
    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x, <span style="color:#f92672">*</span>args,<span style="color:#f92672">**</span>kwds):
        <span style="color:#66d9ef">return</span> np<span style="color:#f92672">.</span>clip(x, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">6</span>)
    
    
    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">backward</span>(self, x, <span style="color:#f92672">*</span>args,<span style="color:#f92672">**</span>kwds):
        <span style="color:#66d9ef">return</span> np<span style="color:#f92672">.</span>where((<span style="color:#ae81ff">0</span> <span style="color:#f92672">&lt;</span> x) <span style="color:#f92672">&amp;</span> (x <span style="color:#f92672">&lt;</span> <span style="color:#ae81ff">6</span>), <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">0</span>)


<span style="color:#66d9ef">class</span> <span style="color:#a6e22e">leakyReLU</span>(Activator):
    <span style="color:#66d9ef">def</span> __init__(self, alpha<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-2</span>, <span style="color:#f92672">*</span>args,<span style="color:#f92672">**</span>kwds):
        self<span style="color:#f92672">.</span>alpha <span style="color:#f92672">=</span> alpha
        super()<span style="color:#f92672">.</span>__init__(<span style="color:#f92672">*</span>args,<span style="color:#f92672">**</span>kwds)
    
    
    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x, <span style="color:#f92672">*</span>args,<span style="color:#f92672">**</span>kwds):
        <span style="color:#66d9ef">return</span> np<span style="color:#f92672">.</span>maximum(self<span style="color:#f92672">.</span>alpha <span style="color:#f92672">*</span> x, x)
    
    
    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">backward</span>(self, x, <span style="color:#f92672">*</span>args,<span style="color:#f92672">**</span>kwds):
        <span style="color:#66d9ef">return</span> np<span style="color:#f92672">.</span>where(x <span style="color:#f92672">&lt;</span> <span style="color:#ae81ff">0</span>, self<span style="color:#f92672">.</span>alpha, <span style="color:#ae81ff">1</span>)


<span style="color:#66d9ef">class</span> <span style="color:#a6e22e">ELU</span>(Activator):
    <span style="color:#66d9ef">def</span> __init__(self, alpha<span style="color:#f92672">=</span><span style="color:#ae81ff">1.</span>, <span style="color:#f92672">*</span>args,<span style="color:#f92672">**</span>kwds):
        self<span style="color:#f92672">.</span>alpha <span style="color:#f92672">=</span> alpha
        super()<span style="color:#f92672">.</span>__init__(<span style="color:#f92672">*</span>args,<span style="color:#f92672">**</span>kwds)
    
    
    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x, <span style="color:#f92672">*</span>args,<span style="color:#f92672">**</span>kwds):
        <span style="color:#66d9ef">return</span> np<span style="color:#f92672">.</span>where(x <span style="color:#f92672">&gt;=</span> <span style="color:#ae81ff">0</span>, x, self<span style="color:#f92672">.</span>alpha<span style="color:#f92672">*</span>(np<span style="color:#f92672">.</span>exp(x) <span style="color:#f92672">-</span> <span style="color:#ae81ff">1</span>))
    
    
    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">backward</span>(self, x, <span style="color:#f92672">*</span>args,<span style="color:#f92672">**</span>kwds):
        <span style="color:#66d9ef">return</span> np<span style="color:#f92672">.</span>where(x <span style="color:#f92672">&gt;=</span> <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1</span>, self<span style="color:#f92672">.</span>alpha<span style="color:#f92672">*</span>np<span style="color:#f92672">.</span>exp(x))


<span style="color:#66d9ef">class</span> <span style="color:#a6e22e">SELU</span>(Activator):
    <span style="color:#66d9ef">def</span> __init__(self, lambda_<span style="color:#f92672">=</span><span style="color:#ae81ff">1.0507</span>, alpha<span style="color:#f92672">=</span><span style="color:#ae81ff">1.67326</span>, <span style="color:#f92672">*</span>args,<span style="color:#f92672">**</span>kwds):
        self<span style="color:#f92672">.</span>lambda_ <span style="color:#f92672">=</span> lambda_
        self<span style="color:#f92672">.</span>alpha <span style="color:#f92672">=</span> alpha
        super()<span style="color:#f92672">.</span>__init__(<span style="color:#f92672">*</span>args,<span style="color:#f92672">**</span>kwds)
    
    
    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x, <span style="color:#f92672">*</span>args,<span style="color:#f92672">**</span>kwds):
        <span style="color:#66d9ef">return</span> np<span style="color:#f92672">.</span>where(x <span style="color:#f92672">&gt;=</span> <span style="color:#ae81ff">0</span>,
                        self<span style="color:#f92672">.</span>lambda_<span style="color:#f92672">*</span>x,
                        self<span style="color:#f92672">.</span>lambda_<span style="color:#f92672">*</span>self<span style="color:#f92672">.</span>alpha<span style="color:#f92672">*</span>(np<span style="color:#f92672">.</span>exp(x) <span style="color:#f92672">-</span> <span style="color:#ae81ff">1</span>))
    
    
    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">backward</span>(self, x, <span style="color:#f92672">*</span>args,<span style="color:#f92672">**</span>kwds):
        <span style="color:#66d9ef">return</span> np<span style="color:#f92672">.</span>where(x <span style="color:#f92672">&gt;=</span> <span style="color:#ae81ff">0</span>, 
                        self<span style="color:#f92672">.</span>lambda_,
                        self<span style="color:#f92672">.</span>lambda_<span style="color:#f92672">*</span>self<span style="color:#f92672">.</span>alpha<span style="color:#f92672">*</span>np<span style="color:#f92672">.</span>exp(x))


<span style="color:#66d9ef">class</span> <span style="color:#a6e22e">CELU</span>(Activator):
    <span style="color:#66d9ef">def</span> __init__(self, alpha<span style="color:#f92672">=</span><span style="color:#ae81ff">1.</span>, <span style="color:#f92672">*</span>args,<span style="color:#f92672">**</span>kwds):
        self<span style="color:#f92672">.</span>alpha <span style="color:#f92672">=</span> alpha
        super()<span style="color:#f92672">.</span>__init__(<span style="color:#f92672">*</span>args,<span style="color:#f92672">**</span>kwds)
    
    
    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x, <span style="color:#f92672">*</span>args,<span style="color:#f92672">**</span>kwds):
        <span style="color:#66d9ef">return</span> np<span style="color:#f92672">.</span>where(x <span style="color:#f92672">&gt;=</span> <span style="color:#ae81ff">0</span>,
                        x,
                        self<span style="color:#f92672">.</span>alpha<span style="color:#f92672">*</span>(np<span style="color:#f92672">.</span>exp(x<span style="color:#f92672">/</span>self<span style="color:#f92672">.</span>alpha) <span style="color:#f92672">-</span> <span style="color:#ae81ff">1</span>))
    
    
    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">backward</span>(self, x, <span style="color:#f92672">*</span>args,<span style="color:#f92672">**</span>kwds):
        <span style="color:#66d9ef">return</span> np<span style="color:#f92672">.</span>where(x <span style="color:#f92672">&gt;=</span> <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1</span>, np<span style="color:#f92672">.</span>exp(x<span style="color:#f92672">/</span>self<span style="color:#f92672">.</span>alpha))


<span style="color:#66d9ef">class</span> <span style="color:#a6e22e">softmax</span>(Activator):
    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x, <span style="color:#f92672">*</span>args,<span style="color:#f92672">**</span>kwds):
        <span style="color:#66d9ef">return</span> np<span style="color:#f92672">.</span>exp(x)<span style="color:#f92672">/</span>np<span style="color:#f92672">.</span>sum(np<span style="color:#f92672">.</span>exp(x))
    
    
    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">backward</span>(self, x, <span style="color:#f92672">*</span>args,<span style="color:#f92672">**</span>kwds):
        <span style="color:#66d9ef">return</span> np<span style="color:#f92672">.</span>exp(x)<span style="color:#f92672">*</span>(np<span style="color:#f92672">.</span>sum(np<span style="color:#f92672">.</span>exp(x)) 
                          <span style="color:#f92672">-</span> np<span style="color:#f92672">.</span>exp(x))<span style="color:#f92672">/</span>np<span style="color:#f92672">.</span>sum(np<span style="color:#f92672">.</span>exp(x))<span style="color:#f92672">**</span><span style="color:#ae81ff">2</span>


<span style="color:#66d9ef">class</span> <span style="color:#a6e22e">softmin</span>(Activator):
    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x, <span style="color:#f92672">*</span>args,<span style="color:#f92672">**</span>kwds):
        <span style="color:#66d9ef">return</span> np<span style="color:#f92672">.</span>exp(<span style="color:#f92672">-</span>x)<span style="color:#f92672">/</span>np<span style="color:#f92672">.</span>sum(np<span style="color:#f92672">.</span>exp(<span style="color:#f92672">-</span>x))
    
    
    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">backward</span>(self, x, <span style="color:#f92672">*</span>args,<span style="color:#f92672">**</span>kwds):
        <span style="color:#66d9ef">return</span> <span style="color:#f92672">-</span>(np<span style="color:#f92672">.</span>exp(x)<span style="color:#f92672">*</span>(np<span style="color:#f92672">.</span>sum(np<span style="color:#f92672">.</span>exp(<span style="color:#f92672">-</span>x)) <span style="color:#f92672">-</span> np<span style="color:#f92672">.</span>exp(x))
                 <span style="color:#f92672">/</span>np<span style="color:#f92672">.</span>sum(np<span style="color:#f92672">.</span>exp(<span style="color:#f92672">-</span>x))<span style="color:#f92672">**</span><span style="color:#ae81ff">2</span>)


<span style="color:#66d9ef">class</span> <span style="color:#a6e22e">logSoftmax</span>(Activator):
    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x, <span style="color:#f92672">*</span>args,<span style="color:#f92672">**</span>kwds):
        <span style="color:#66d9ef">return</span> np<span style="color:#f92672">.</span>log(np<span style="color:#f92672">.</span>exp(x)<span style="color:#f92672">/</span>np<span style="color:#f92672">.</span>sum(np<span style="color:#f92672">.</span>exp(x)))
    
    
    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">backward</span>(self, x, <span style="color:#f92672">*</span>args,<span style="color:#f92672">**</span>kwds):
        y <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>sum(np<span style="color:#f92672">.</span>exp(x))
        <span style="color:#66d9ef">return</span> (y <span style="color:#f92672">-</span> np<span style="color:#f92672">.</span>exp(x))<span style="color:#f92672">/</span>y


<span style="color:#66d9ef">class</span> <span style="color:#a6e22e">softplus</span>(Activator):
    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x, <span style="color:#f92672">*</span>args,<span style="color:#f92672">**</span>kwds):
        <span style="color:#66d9ef">return</span> np<span style="color:#f92672">.</span>logaddexp(x, <span style="color:#ae81ff">0</span>)
    
    
    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">backward</span>(self, x, <span style="color:#f92672">*</span>args,<span style="color:#f92672">**</span>kwds):
        <span style="color:#66d9ef">return</span> <span style="color:#ae81ff">1</span><span style="color:#f92672">/</span>(<span style="color:#ae81ff">1</span> <span style="color:#f92672">+</span> np<span style="color:#f92672">.</span>exp(<span style="color:#f92672">-</span>x))


<span style="color:#66d9ef">class</span> <span style="color:#a6e22e">softsign</span>(Activator):
    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x, <span style="color:#f92672">*</span>args,<span style="color:#f92672">**</span>kwds):
        <span style="color:#66d9ef">return</span> x<span style="color:#f92672">/</span>(<span style="color:#ae81ff">1</span> <span style="color:#f92672">+</span> np<span style="color:#f92672">.</span>abs(x))
    
    
    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">backward</span>(self, x, <span style="color:#f92672">*</span>args,<span style="color:#f92672">**</span>kwds):
        <span style="color:#66d9ef">return</span> <span style="color:#ae81ff">1</span><span style="color:#f92672">/</span>(<span style="color:#ae81ff">1</span> <span style="color:#f92672">+</span> np<span style="color:#f92672">.</span>abs(x)) <span style="color:#f92672">**</span> <span style="color:#ae81ff">2</span>


<span style="color:#66d9ef">class</span> <span style="color:#a6e22e">Swish</span>(Activator):
    <span style="color:#66d9ef">def</span> __init__(self, beta<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, <span style="color:#f92672">*</span>args,<span style="color:#f92672">**</span>kwds):
        self<span style="color:#f92672">.</span>beta <span style="color:#f92672">=</span> beta
        super()<span style="color:#f92672">.</span>__init__(<span style="color:#f92672">*</span>args,<span style="color:#f92672">**</span>kwds)
    
    
    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x, <span style="color:#f92672">*</span>args,<span style="color:#f92672">**</span>kwds):
        <span style="color:#66d9ef">return</span> x<span style="color:#f92672">/</span>(<span style="color:#ae81ff">1</span> <span style="color:#f92672">+</span> np<span style="color:#f92672">.</span>exp(<span style="color:#f92672">-</span>self<span style="color:#f92672">.</span>beta<span style="color:#f92672">*</span>x))
    
    
    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">backward</span>(self, x, y, <span style="color:#f92672">*</span>args,<span style="color:#f92672">**</span>kwds):
        <span style="color:#66d9ef">return</span> self<span style="color:#f92672">.</span>beta<span style="color:#f92672">*</span>y <span style="color:#f92672">+</span> (<span style="color:#ae81ff">1</span> <span style="color:#f92672">-</span> self<span style="color:#f92672">.</span>beta<span style="color:#f92672">*</span>y)<span style="color:#f92672">/</span>(<span style="color:#ae81ff">1</span> <span style="color:#f92672">+</span> np<span style="color:#f92672">.</span>exp(<span style="color:#f92672">-</span>self<span style="color:#f92672">.</span>beta<span style="color:#f92672">*</span>x))
    
    
    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">d2y</span>(self, x, <span style="color:#f92672">*</span>args,<span style="color:#f92672">**</span>kwds):
        <span style="color:#66d9ef">return</span> (<span style="color:#f92672">-</span><span style="color:#ae81ff">0.25</span><span style="color:#f92672">*</span>self<span style="color:#f92672">.</span>beta<span style="color:#f92672">*</span>(self<span style="color:#f92672">.</span>beta<span style="color:#f92672">*</span>x<span style="color:#f92672">*</span>np<span style="color:#f92672">.</span>tanh(<span style="color:#ae81ff">0.5</span><span style="color:#f92672">*</span>self<span style="color:#f92672">.</span>beta<span style="color:#f92672">*</span>x) <span style="color:#f92672">-</span> <span style="color:#ae81ff">2</span>)
                               <span style="color:#f92672">*</span>(<span style="color:#ae81ff">1</span> <span style="color:#f92672">-</span> np<span style="color:#f92672">.</span>tanh(<span style="color:#ae81ff">0.5</span><span style="color:#f92672">*</span>self<span style="color:#f92672">.</span>beta<span style="color:#f92672">*</span>x)<span style="color:#f92672">**</span><span style="color:#ae81ff">2</span>))


<span style="color:#66d9ef">class</span> <span style="color:#a6e22e">Mish</span>(Activator):
    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x, <span style="color:#f92672">*</span>args,<span style="color:#f92672">**</span>kwds):<span style="color:#66d9ef">return</span> x<span style="color:#f92672">*</span>np<span style="color:#f92672">.</span>tanh(np<span style="color:#f92672">.</span>logaddexp(x, <span style="color:#ae81ff">0</span>))
    
    
    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">backward</span>(self, x, <span style="color:#f92672">*</span>args,<span style="color:#f92672">**</span>kwds):
        omega <span style="color:#f92672">=</span> (<span style="color:#ae81ff">4</span><span style="color:#f92672">*</span>(x <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span>) <span style="color:#f92672">+</span> <span style="color:#ae81ff">4</span><span style="color:#f92672">*</span>np<span style="color:#f92672">.</span>exp(<span style="color:#ae81ff">2</span><span style="color:#f92672">*</span>x) 
                 <span style="color:#f92672">+</span> np<span style="color:#f92672">.</span>exp(<span style="color:#ae81ff">3</span><span style="color:#f92672">*</span>x) <span style="color:#f92672">+</span> (<span style="color:#ae81ff">4</span><span style="color:#f92672">*</span>x <span style="color:#f92672">+</span> <span style="color:#ae81ff">6</span>)<span style="color:#f92672">*</span>np<span style="color:#f92672">.</span>exp(x))
        delta <span style="color:#f92672">=</span> <span style="color:#ae81ff">2</span><span style="color:#f92672">*</span>np<span style="color:#f92672">.</span>exp(x) <span style="color:#f92672">+</span> np<span style="color:#f92672">.</span>exp(<span style="color:#ae81ff">2</span><span style="color:#f92672">*</span>x) <span style="color:#f92672">+</span> <span style="color:#ae81ff">2</span>
        <span style="color:#66d9ef">return</span> np<span style="color:#f92672">.</span>exp(x)<span style="color:#f92672">*</span>omega<span style="color:#f92672">/</span>delta<span style="color:#f92672">**</span><span style="color:#ae81ff">2</span>
    
    
    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">d2y</span>(self, x, <span style="color:#f92672">*</span>args,<span style="color:#f92672">**</span>kwds):
        omega <span style="color:#f92672">=</span> (<span style="color:#ae81ff">2</span><span style="color:#f92672">*</span>(x <span style="color:#f92672">+</span> <span style="color:#ae81ff">2</span>) 
                 <span style="color:#f92672">+</span> np<span style="color:#f92672">.</span>exp(x)<span style="color:#f92672">*</span>(np<span style="color:#f92672">.</span>exp(x)<span style="color:#f92672">*</span>(<span style="color:#f92672">-</span><span style="color:#ae81ff">2</span><span style="color:#f92672">*</span>np<span style="color:#f92672">.</span>exp(x)<span style="color:#f92672">*</span>(x <span style="color:#f92672">-</span> <span style="color:#ae81ff">1</span>) <span style="color:#f92672">-</span> <span style="color:#ae81ff">3</span><span style="color:#f92672">*</span>x <span style="color:#f92672">+</span> <span style="color:#ae81ff">6</span>)
                              <span style="color:#f92672">+</span> <span style="color:#ae81ff">2</span><span style="color:#f92672">*</span>(x <span style="color:#f92672">+</span> <span style="color:#ae81ff">4</span>)))
        delta <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>exp(x)<span style="color:#f92672">*</span>(np<span style="color:#f92672">.</span>exp(x) <span style="color:#f92672">+</span> <span style="color:#ae81ff">2</span>) <span style="color:#f92672">+</span> <span style="color:#ae81ff">2</span>
        <span style="color:#66d9ef">return</span> <span style="color:#ae81ff">4</span><span style="color:#f92672">*</span>np<span style="color:#f92672">.</span>exp(x)<span style="color:#f92672">*</span>omega<span style="color:#f92672">/</span>delta<span style="color:#f92672">**</span><span style="color:#ae81ff">3</span>


<span style="color:#66d9ef">class</span> <span style="color:#a6e22e">tanhExp</span>(Activator):
    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x, <span style="color:#f92672">*</span>args,<span style="color:#f92672">**</span>kwds):
        <span style="color:#66d9ef">return</span> x<span style="color:#f92672">*</span>np<span style="color:#f92672">.</span>tanh(np<span style="color:#f92672">.</span>exp(x))
    
    
    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">backward</span>(self, x, <span style="color:#f92672">*</span>args,<span style="color:#f92672">**</span>kwds):
        tanh_exp <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>tanh(np<span style="color:#f92672">.</span>exp(x))
        <span style="color:#66d9ef">return</span> tanh_exp <span style="color:#f92672">-</span> x<span style="color:#f92672">*</span>np<span style="color:#f92672">.</span>exp(x)<span style="color:#f92672">*</span>(tanh_exp<span style="color:#f92672">**</span><span style="color:#ae81ff">2</span> <span style="color:#f92672">-</span> <span style="color:#ae81ff">1</span>)
    
    
    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">d2y</span>(self, x, <span style="color:#f92672">*</span>args,<span style="color:#f92672">**</span>kwds):
        tanh_exp <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>tanh(np<span style="color:#f92672">.</span>exp(x))
        <span style="color:#66d9ef">return</span> (np<span style="color:#f92672">.</span>exp(x)<span style="color:#f92672">*</span>(<span style="color:#f92672">-</span>x <span style="color:#f92672">+</span> <span style="color:#ae81ff">2</span><span style="color:#f92672">*</span>np<span style="color:#f92672">.</span>exp(x)<span style="color:#f92672">*</span>x<span style="color:#f92672">*</span>tanh_exp <span style="color:#f92672">-</span> <span style="color:#ae81ff">2</span>)
                         <span style="color:#f92672">*</span>(tanh_exp<span style="color:#f92672">**</span><span style="color:#ae81ff">2</span> <span style="color:#f92672">-</span> <span style="color:#ae81ff">1</span>))


<span style="color:#66d9ef">class</span> <span style="color:#a6e22e">maxout</span>(Activator):
    <span style="color:#66d9ef">def</span> __init__(self, n_prev, n, k, wb_width<span style="color:#f92672">=</span><span style="color:#ae81ff">5e-2</span>, <span style="color:#f92672">*</span>args,<span style="color:#f92672">**</span>kwds):
        self<span style="color:#f92672">.</span>n_prev <span style="color:#f92672">=</span> n_prev
        self<span style="color:#f92672">.</span>n <span style="color:#f92672">=</span> n
        self<span style="color:#f92672">.</span>k <span style="color:#f92672">=</span> k
        self<span style="color:#f92672">.</span>w <span style="color:#f92672">=</span> wb_width<span style="color:#f92672">*</span>np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>rand((n_prev, n<span style="color:#f92672">*</span>k))
        self<span style="color:#f92672">.</span>b <span style="color:#f92672">=</span> wb_width<span style="color:#f92672">*</span>np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>rand(n<span style="color:#f92672">*</span>k)
        
        super()<span style="color:#f92672">.</span>__init__(<span style="color:#f92672">*</span>args,<span style="color:#f92672">**</span>kwds)
    
    
    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x, <span style="color:#f92672">*</span>args,<span style="color:#f92672">**</span>kwds):
        self<span style="color:#f92672">.</span>x <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>copy()
        self<span style="color:#f92672">.</span>z <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>dot(self<span style="color:#f92672">.</span>w<span style="color:#f92672">.</span>T, x) <span style="color:#f92672">+</span> self<span style="color:#f92672">.</span>b
        self<span style="color:#f92672">.</span>z <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>z<span style="color:#f92672">.</span>reshape(self<span style="color:#f92672">.</span>n, self<span style="color:#f92672">.</span>k)
        self<span style="color:#f92672">.</span>y <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>max(self<span style="color:#f92672">.</span>z, axis<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
        <span style="color:#66d9ef">return</span> self<span style="color:#f92672">.</span>y
    
    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">backward</span>(self, g, <span style="color:#f92672">*</span>args,<span style="color:#f92672">**</span>kwds):
        self<span style="color:#f92672">.</span>dw <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>sum(np<span style="color:#f92672">.</span>dot(self<span style="color:#f92672">.</span>w, self<span style="color:#f92672">.</span>x))
</code></pre></div></div></details>
<details>
<summary>test_activators.py</summary>
<div>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-test_activators.py" data-lang="test_activators.py"><span style="color:#f92672">import</span> numpy <span style="color:#f92672">as</span> np
<span style="color:#f92672">import</span> matplotlib.pyplot <span style="color:#f92672">as</span> plt


_act_dic <span style="color:#f92672">=</span> {<span style="color:#e6db74">&#34;step&#34;</span>: step,
            <span style="color:#e6db74">&#34;identity&#34;</span>: identity,
            <span style="color:#e6db74">&#34;bent-identity&#34;</span>: bentIdentity,
            <span style="color:#e6db74">&#34;hard-shrink&#34;</span>: hardShrink,
            <span style="color:#e6db74">&#34;soft-shrink&#34;</span>: softShrink,
            <span style="color:#e6db74">&#34;threshold&#34;</span>: threshold,
            <span style="color:#e6db74">&#34;sigmoid&#34;</span>: sigmoid,
            <span style="color:#e6db74">&#34;hard-sigmoid&#34;</span>: hardSigmoid,
            <span style="color:#e6db74">&#34;log-sigmoid&#34;</span>: logSigmoid,
            <span style="color:#e6db74">&#34;tanh&#34;</span>: act_tanh,
            <span style="color:#e6db74">&#34;tanh-shrink&#34;</span>: tanhShrink,
            <span style="color:#e6db74">&#34;hard-tanh&#34;</span>:hardtanh,
            <span style="color:#e6db74">&#34;ReLU&#34;</span>: ReLU,
            <span style="color:#e6db74">&#34;ReLU6&#34;</span>: ReLU6,
            <span style="color:#e6db74">&#34;leaky-ReLU&#34;</span>: leakyReLU,
            <span style="color:#e6db74">&#34;ELU&#34;</span>: ELU,
            <span style="color:#e6db74">&#34;SELU&#34;</span>: SELU,
            <span style="color:#e6db74">&#34;CELU&#34;</span>: CELU,
            <span style="color:#e6db74">&#34;softmax&#34;</span>: softmax,
            <span style="color:#e6db74">&#34;softmin&#34;</span>: softmin,
            <span style="color:#e6db74">&#34;log-softmax&#34;</span>: logSoftmax,
            <span style="color:#e6db74">&#34;softplus&#34;</span>: softplus,
            <span style="color:#e6db74">&#34;softsign&#34;</span>: softsign,
            <span style="color:#e6db74">&#34;Swish&#34;</span>: Swish,
            <span style="color:#e6db74">&#34;Mish&#34;</span>: Mish,
            <span style="color:#e6db74">&#34;tanhExp&#34;</span>: tanhExp,
           }


<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">get_act</span>(name, <span style="color:#f92672">*</span>args,<span style="color:#f92672">**</span>kwds):
    <span style="color:#66d9ef">for</span> act <span style="color:#f92672">in</span> _act_dic:
        <span style="color:#66d9ef">if</span> name <span style="color:#f92672">==</span> act:
            activator <span style="color:#f92672">=</span> _act_dic[name](<span style="color:#f92672">*</span>args,<span style="color:#f92672">**</span>kwds)
            <span style="color:#66d9ef">break</span>
    <span style="color:#66d9ef">else</span>:
        <span style="color:#66d9ef">raise</span> <span style="color:#a6e22e">ValueError</span>(name, <span style="color:#e6db74">&#34;: Unknown activator&#34;</span>)
    
    <span style="color:#66d9ef">return</span> activator


<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">plot_graph</span>(x, name, <span style="color:#f92672">*</span>args,<span style="color:#f92672">**</span>kwds):
    activator <span style="color:#f92672">=</span> get_act(name, <span style="color:#f92672">*</span>args,<span style="color:#f92672">**</span>kwds)
    
    y <span style="color:#f92672">=</span> activator<span style="color:#f92672">.</span>forward(x, <span style="color:#f92672">*</span>args,<span style="color:#f92672">**</span>kwds)
    dx <span style="color:#f92672">=</span> activator<span style="color:#f92672">.</span>backward(x, y, <span style="color:#f92672">*</span>args,<span style="color:#f92672">**</span>kwds)
    
    plt<span style="color:#f92672">.</span>plot(x, y, label<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;forward&#34;</span>)
    plt<span style="color:#f92672">.</span>plot(x, dx, label<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;backward&#34;</span>)
    plt<span style="color:#f92672">.</span>title(name)
    plt<span style="color:#f92672">.</span>xlabel(<span style="color:#e6db74">&#34;x&#34;</span>)
    plt<span style="color:#f92672">.</span>ylabel(<span style="color:#e6db74">&#34;y&#34;</span>)
    plt<span style="color:#f92672">.</span>grid()
    plt<span style="color:#f92672">.</span>legend(loc<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;best&#34;</span>)
    plt<span style="color:#f92672">.</span>savefig(<span style="color:#e6db74">&#34;{}.png&#34;</span><span style="color:#f92672">.</span>format(name))
    plt<span style="color:#f92672">.</span>show()


<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">vs_plot</span>(x, A, B):
    A_activator <span style="color:#f92672">=</span> get_act(A)
    B_activator <span style="color:#f92672">=</span> get_act(B)
    
    y_A <span style="color:#f92672">=</span> {}
    y_B <span style="color:#f92672">=</span> {}
    
    y_A[<span style="color:#e6db74">&#34;{} y&#34;</span><span style="color:#f92672">.</span>format(A)] <span style="color:#f92672">=</span> A_activator<span style="color:#f92672">.</span>forward(x)
    y_B[<span style="color:#e6db74">&#34;{} y&#34;</span><span style="color:#f92672">.</span>format(B)] <span style="color:#f92672">=</span> B_activator<span style="color:#f92672">.</span>forward(x)
    y_A[<span style="color:#e6db74">&#34;{} dy&#34;</span><span style="color:#f92672">.</span>format(A)] <span style="color:#f92672">=</span> A_activator<span style="color:#f92672">.</span>backward(x, 
                                                  y_A[<span style="color:#e6db74">&#34;{} y&#34;</span><span style="color:#f92672">.</span>format(A)])
    y_B[<span style="color:#e6db74">&#34;{} dy&#34;</span><span style="color:#f92672">.</span>format(B)] <span style="color:#f92672">=</span> B_activator<span style="color:#f92672">.</span>backward(x,
                                                  y_B[<span style="color:#e6db74">&#34;{} y&#34;</span><span style="color:#f92672">.</span>format(B)])
    y_A[<span style="color:#e6db74">&#34;{} d2y&#34;</span><span style="color:#f92672">.</span>format(A)] <span style="color:#f92672">=</span> A_activator<span style="color:#f92672">.</span>d2y(x, y_A[<span style="color:#e6db74">&#34;{} y&#34;</span><span style="color:#f92672">.</span>format(A)])
    y_B[<span style="color:#e6db74">&#34;{} d2y&#34;</span><span style="color:#f92672">.</span>format(B)] <span style="color:#f92672">=</span> B_activator<span style="color:#f92672">.</span>d2y(x, y_B[<span style="color:#e6db74">&#34;{} y&#34;</span><span style="color:#f92672">.</span>format(B)])
    
    fig, ax <span style="color:#f92672">=</span> plt<span style="color:#f92672">.</span>subplots(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">3</span>, figsize<span style="color:#f92672">=</span>(<span style="color:#ae81ff">18</span>, <span style="color:#ae81ff">6</span>))
    <span style="color:#66d9ef">for</span> i, key <span style="color:#f92672">in</span> enumerate(y_A):
        ax[i]<span style="color:#f92672">.</span>plot(x, y_A[key], label<span style="color:#f92672">=</span>key)
        ax[i]<span style="color:#f92672">.</span>set_xlabel(<span style="color:#e6db74">&#34;x&#34;</span>)
        ax[i]<span style="color:#f92672">.</span>set_ylabel(<span style="color:#e6db74">&#34;y&#34;</span>)
        ax[i]<span style="color:#f92672">.</span>grid()
    <span style="color:#66d9ef">for</span> i, key <span style="color:#f92672">in</span> enumerate(y_B):
        ax[i]<span style="color:#f92672">.</span>plot(x, y_B[key], label<span style="color:#f92672">=</span>key)
        ax[i]<span style="color:#f92672">.</span>legend(loc<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;best&#34;</span>)
    ax[<span style="color:#ae81ff">0</span>]<span style="color:#f92672">.</span>set_title(<span style="color:#e6db74">&#34;forward&#34;</span>)
    ax[<span style="color:#ae81ff">1</span>]<span style="color:#f92672">.</span>set_title(<span style="color:#e6db74">&#34;backward&#34;</span>)
    ax[<span style="color:#ae81ff">2</span>]<span style="color:#f92672">.</span>set_title(<span style="color:#e6db74">&#34;second-order derivative&#34;</span>)
    fig<span style="color:#f92672">.</span>tight_layout()
    fig<span style="color:#f92672">.</span>savefig(<span style="color:#e6db74">&#34;{}_vs_{}.png&#34;</span><span style="color:#f92672">.</span>format(A, B))
    plt<span style="color:#f92672">.</span>show()


x <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>arange(<span style="color:#f92672">-</span><span style="color:#ae81ff">5</span>, <span style="color:#ae81ff">5</span>, <span style="color:#ae81ff">5e-2</span>)

plot_graph(x, <span style="color:#e6db74">&#34;step&#34;</span>)
plot_graph(x, <span style="color:#e6db74">&#34;identity&#34;</span>)
plot_graph(x, <span style="color:#e6db74">&#34;bent-identity&#34;</span>)
plot_graph(x, <span style="color:#e6db74">&#34;hard-shrink&#34;</span>)
plot_graph(x, <span style="color:#e6db74">&#34;soft-shrink&#34;</span>)
plot_graph(x, <span style="color:#e6db74">&#34;threshold&#34;</span>, <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, <span style="color:#f92672">-</span><span style="color:#ae81ff">2</span>)
plot_graph(x, <span style="color:#e6db74">&#34;sigmoid&#34;</span>)
plot_graph(x, <span style="color:#e6db74">&#34;hard-sigmoid&#34;</span>)
plot_graph(x, <span style="color:#e6db74">&#34;log-sigmoid&#34;</span>)
plot_graph(x, <span style="color:#e6db74">&#34;tanh&#34;</span>)
plot_graph(x, <span style="color:#e6db74">&#34;tanh-shrink&#34;</span>)
plot_graph(x, <span style="color:#e6db74">&#34;hard-tanh&#34;</span>)
plot_graph(x, <span style="color:#e6db74">&#34;ReLU&#34;</span>)
plot_graph(x <span style="color:#f92672">+</span> <span style="color:#ae81ff">2</span>, <span style="color:#e6db74">&#34;ReLU6&#34;</span>)
plot_graph(x, <span style="color:#e6db74">&#34;leaky-ReLU&#34;</span>)
plot_graph(x, <span style="color:#e6db74">&#34;ELU&#34;</span>)
plot_graph(x, <span style="color:#e6db74">&#34;SELU&#34;</span>)
plot_graph(x, <span style="color:#e6db74">&#34;CELU&#34;</span>)
plot_graph(x, <span style="color:#e6db74">&#34;softmax&#34;</span>)plot_graph(x, <span style="color:#e6db74">&#34;softmin&#34;</span>)
plot_graph(x, <span style="color:#e6db74">&#34;log-softmax&#34;</span>)
plot_graph(x, <span style="color:#e6db74">&#34;softplus&#34;</span>)
plot_graph(x, <span style="color:#e6db74">&#34;softsign&#34;</span>)
plot_graph(x, <span style="color:#e6db74">&#34;Swish&#34;</span>)
plot_graph(x, <span style="color:#e6db74">&#34;Mish&#34;</span>)
plot_graph(x, <span style="color:#e6db74">&#34;tanhExp&#34;</span>)

vs_plot(x, <span style="color:#e6db74">&#34;Swish&#34;</span>, <span style="color:#e6db74">&#34;Mish&#34;</span>)
vs_plot(x, <span style="color:#e6db74">&#34;Swish&#34;</span>, <span style="color:#e6db74">&#34;tanhExp&#34;</span>)
vs_plot(x, <span style="color:#e6db74">&#34;Mish&#34;</span>, <span style="color:#e6db74">&#34;tanhExp&#34;</span>)
</code></pre></div></div></details>
<p>There are some other implementations. I think I&rsquo;ll add it&hellip;</p>
<p>#reference
-<a href="https://fresopiya.com/2019/05/14/activefunc/">[Artificial Intelligence] Different from the types of activation functions. merit and demerit. </a>
-<a href="https://www.gabormelli.com/RKB/Bent_Identity_Activation_Function">Bent Identity Activation Function</a>
-<a href="https://qiita.com/hsjoihs/items/88d1569aaef01659bbd5">Keras&rsquo;s hard_sigmoid is max(0, min(1, (0.2 * x) + 0.5))</a>
-<a href="https://www.renom.jp/en/notebooks/tutorial/basic_algorithm/activation_swish/notebook.html">Introduction to the Swish activation function</a>
-<a href="https://ai-scholar.tech/articles/treatise/mish-ai-374">Finally born! Expected new activation function &ldquo;Mish&rdquo; commentary</a>
-<a href="https://medium.com/lsc-psd/AboutExpectedRookieinActivationFunctionIndustry-mish-b1982782e186">About Expected Rookie &ldquo;Mish&rdquo; in Activation Function Industry</a>
-<a href="https://pytorch.org/docs/stable/nn.html">Pytorch</a></p>
<p>#Additional list &amp; acknowledgments
-@reppy4620-san gave me the information about tanhExp function! Thank you very much for posting the paper link carefully!</p>
<p>#Deep learning series
-<a href="https://qiita.com/kuroitu/items/221e8c477ffdd0774b6b">Introduction to Deep Learning ~Basics~</a>
-<a href="https://qiita.com/kuroitu/items/884c62c48c2daa3def08">Introduction to Deep Learning ~Coding Preparation Edition~</a>
-<a href="https://qiita.com/kuroitu/items/d22c8750e34d5d75fb6c">Introduction to Deep Learning-Forward Propagation Edition</a>
-<a href="https://qiita.com/kuroitu/items/35d7b5a4bde470f69570">im2col thorough understanding</a></p>

    </article>
  </div>

  
  
  
  <script>
  window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
  ga('create', 'UA-169005401-1', 'auto');
  ga('send', 'pageview');
  </script>
  <script async src='https://www.google-analytics.com/analytics.js'></script>
  

  

  
<link rel="stylesheet" type="text/css" href="//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.0/cookieconsent.min.css" />
<script src="//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.0/cookieconsent.min.js"></script>
<script>
window.addEventListener("load", function(){
window.cookieconsent.initialise({
  "palette": {
    "popup": {
      "background": "#216942",
      "text": "#b2d192"
    },
    "button": {
      "background": "#afed71"
    }
  }
})});
</script>

</body>

</html>
