<!doctype html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  
  <meta name="generator" content="Hugo 0.72.0" />

  
  <meta name="description" content="Some description">
  

  
  <link rel="apple-touch-icon" sizes="180x180" href="https://memotut.com/apple-touch-icon.png">

  
  <link rel="icon" type="image/png" sizes="32x32" href="https://memotut.com/favicon-32x32.png">

  
  <link rel="icon" type="image/png" sizes="16x16" href="https://memotut.com/favicon-16x16.png">

  
  <link rel="manifest" href="https://memotut.com/site.webmanifest">

  
  <link rel="mask-icon" href="https://memotut.com/safari-pinned-tab.svg" color="#5bbad5">

  <meta name="msapplication-TileColor" content="#da532c">

  <meta name="theme-color" content="#ffffff">

  
  <link rel="stylesheet" href="https://memotut.com/css/bootstrap.min.css" />

  
  <title>[Python] PyTorch super introduction PyTorch basics | Memo Tut</title>
  

  <style>
body {
  min-width: 300px;
}

.custom-navbar {
  margin-bottom: 1em;
  height: 60px;
}

.custom-navbar a {
  display: inline-block; 
  padding: 18px 0;
  margin-right: 1em; 
  font-weight: bold; 
}

.custom-navbar a:hover,
.custom-navbar a:focus {
  text-decoration: none; 
}

@media print {
  .custom-navbar {
    display: none;
  }
}

article {
  padding-bottom: 1em;
}

img {
  max-width: 100%;
}


body {
  background-color: #fff;
}



body {
  color: #212529;
}



a {
  color: #007bff;
}



a:hover,
a:focus {
  color: #0056b3;
}



.custom-navbar {
  background-color: #212529;
}



.custom-navbar a {
  color: rgba(255, 255, 255, 0.75);
}



.custom-navbar a:hover,
.custom-navbar a:focus {
  color: rgba(255, 255, 255, 1);
}



.container {
  max-width: 800px;
}



pre {
  display: block;
  padding: 9.5px;
  word-break: break-all;
  word-wrap: break-word;
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 4px;
}

pre code {
  padding: 0;
  font-size: inherit;
  color: inherit; 
  white-space: pre-wrap;
  background-color: transparent;
  border: none;
  border-radius: 0;
}

code {
  padding: 2px 4px;
  color: inherit; 
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 4px;
  font-size: .9em;
}



blockquote,
.blockquote {
  padding: 10px 20px;
  margin: 0 0 20px;
  font-size: 1em;
  border-left: 5px solid #6c757d;
}

</style>
</head>

<body>
  <nav class="custom-navbar">
  <div class="container">
    
    <a href="/">Posts</a>
    
    <a href="/tags/">Tags</a>
    
    <a href="/about/">About</a>
    
    <a href="/index.xml">RSS</a>
    
  </div>
</nav>
  
  <div class="container">
    <article>
      <h1>[Python] PyTorch super introduction PyTorch basics</h1>
<p>
  <small class="text-secondary">
  
  
  Nov 21, 2019
  </small>
  

<small><code><a href="https://memotut.com/tags/python">Python</a></code></small>


<small><code><a href="https://memotut.com/tags/deeplearning">DeepLearning</a></code></small>


<small><code><a href="https://memotut.com/tags/%E5%85%A5%E9%96%80">入門</a></code></small>


<small><code><a href="https://memotut.com/tags/pytorch">PyTorch</a></code></small>


<small><code><a href="https://memotut.com/tags/colaboratory">colaboratory</a></code></small>

</p>
<pre><code>#Introduction
</code></pre>
<p>This series is described as a summary of learning of PyTorch which is a machine learning framework of Python.
This time, I have written a summary of the basics of PyTorch. It may be difficult to read as a reading material because I try to write down the main points rather than to convey it, but I hope you can use it to understand the points.</p>
<p>What is #PyTorch
As I mentioned earlier, PyTorch is a Python machine learning framework. Speaking of machine learning frameworks, there are TensorFlow developed by Google, Keras or chainer, etc.
Personal personal subjectivity is included, but if you roughly divide these, the impression is as follows.</p>
<table>
<thead>
<tr>
<th align="left">Framework</th>
<th align="left">Main features</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">TensorFlow, Keras</td>
<td align="left">Industrial</td>
</tr>
<tr>
<td align="left">PyTorch, Chainer</td>
<td align="left">For research (highly customizable)</td>
</tr>
</tbody>
</table>
<p>Also, I think most people will enter from TensorFlow when they try to start machine learning, but a slightly strange rule to describe the definition and execution of the data structure <code>define and run</code> separately Because of this, there may be some people who are not familiar with it and fade out.
On the other hand, because PyTorch and Chainer use <code>define by run</code>, they can be coded in the same way as the Python code that is usually coded, so it is easy to use (although I like it). Is a feature.</p>
<p>However, it seems that the <code>define and run</code> mechanism, which is a bit uncomfortable, has some merits such as easy optimization and faster calculation speed because it is defined earlier. There are merits and demerits.</p>
<p>Then, let&rsquo;s actually take a look at the contents of PyTorch.</p>
<h1 id="pytorch-configuration">PyTorch configuration</h1>
<p>PyTorch has the following structure.</p>
<table>
<thead>
<tr>
<th align="left">Content</th>
<th align="left">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">torch</td>
<td align="left">The main namespace includes Tensor and various mathematical functions in this package. <strong>Imitation of NumPy structure</strong></td>
</tr>
<tr>
<td align="left">torch.autograd</td>
<td align="left">Contains functions for automatic differentiation. Includes enable_grad/no_grad of the context manager that controls on/off of automatic differentiation and Function that is the base class used when defining your own differentiable function</td>
</tr>
<tr>
<td align="left">torch.nn</td>
<td align="left">Various data structures and layers for building neural networks are defined. For example, activation functions such as Convolution, LSTM, and ReLU, and loss functions such as MSELoss are included.</td>
</tr>
<tr>
<td align="left">torch.optim</td>
<td align="left">A parameter optimization algorithm centered on stochastic gradient descent (SGD) is implemented.</td>
</tr>
<tr>
<td align="left">torch.utils.data</td>
<td align="left">Contains utility functions for creating mini-batches when iterating SGD iterations</td>
</tr>
<tr>
<td align="left">Used to export the model in the format of</td>
<td align="left">torch.onnx</td>
</tr>
</tbody>
</table>
<p>#Tensor generation and conversion</p>
<h2 id="tensor-generation">Tensor generation</h2>
<p>There are several ways to generate Tensor.</p>
<pre><code class="language-python:Tensor" data-lang="python:Tensor">import numpy as np
import torch

# Create by passing a nested list
t = torch.tensor([[1, 2], [3, 4]])

Create Tensor on GPU by specifying # device
t = torch.tensor([[1, 2], [3, 4]], device = &quot;cuda:0&quot;)

Create a double-precision Tensor by specifying #dtype
t = torch.tensor([[1, 2], [3, 4]], dtype = &quot;torch.float64&quot;)

A one-dimensional Tensor initialized with the numbers from 0 to 9
t = torch.arange(0, 10)

#Create 100x100 Tensor with all values 0 and transfer to GPU with to method
t = torch.zeros(100,100).to(&quot;cuda:0&quot;)

#100x100 random Tensor
t = torch.random(100, 100)
</code></pre><h2 id="how-to-convert-tensor">How to convert Tensor</h2>
<p>TENSOR can be easily converted to NumPy ndarray. However, Tensor on GPU cannot be converted as it is and must be moved to CPU once.</p>
<pre><code class="language-python:tensor" data-lang="python:tensor">Convert to ndarray using #numpy method
t = torch.tensor([[1, 2], [3, 4]])
nd_arr = t.numpy()

# Move Tensor on GPU to CPU
t = torch.tensor([[1, 2], [3, 4]], device = &quot;cuda: 0&quot;)
t_cpu = t.to(&quot;cpu&quot;)
nd_arr = t_cpu.numpy()

# Fill in the above
t = torch.tensor([[1, 2], [3, 4]], device = &quot;cuda: 0&quot;)
nd_arr = t.to(&quot;cpu&quot;).numpy()
</code></pre><h1 id="tensor-indexing-operations">Tensor indexing operations</h1>
<p>TENsor also supports indexing operation <sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup> like ndarray. The methods that can be specified are as follows.</p>
<ul>
<li>Specify by scalar</li>
<li>Subscript list</li>
<li>Specification by mask array <sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup> by ByteTensor</li>
</ul>
<pre><code class="language-python:Tensor" data-lang="python:Tensor">
t = torch.tensor([[1, 2, 3], [4, 5, 6]])

#Specify with a scalar subscript
t[0, 2] # tensor(3)

#Specify by slice
t[:, :2] # tensor([[1, 2], [4, 5]])

#Specified by a list of subscripts
t[:, [1, 2]] # tensor([[2, 3], [5, 6]])

# Select only areas greater than 3 using mask array
t[t &gt;3] # tensor([4, 5, 6])

# Replace [0, 1] element with 100
t[0, 1] = 100 # tensor([[1, 100, 3], [4, 5, 6]])

Bulk assignment using # slices
t[:, 1] = 200 # tensor([[1, 200, 3], [4, 200, 6]])

# Use mask array to replace only specific condition elements
t[t&gt; 10] = 20 # tensor([[1, 2, 3], [4, 5, 6]])

</code></pre><p>#Tensor operation
Tensor can perform arithmetic operations, mathematical functions, linear algebra, etc. Although it can be done with ndarray, especially for linear algebra calculations such as matrix multiplication and singular value decomposition <sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup>, GPU can be used, so better performance than using NumPy/SciPy for large-scale data. There are many.</p>
<h2 id="tensors-four-arithmetic-operations">Tensor&rsquo;s four arithmetic operations</h2>
<p>The availability of Tensor&rsquo;s four arithmetic operations is as follows.</p>
<table>
<thead>
<tr>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>Tensor</td>
<td>Tensor (same type)</td>
<td>〇</td>
</tr>
<tr>
<td>Tensor</td>
<td>Tensor (different types)</td>
<td>✕</td>
</tr>
<tr>
<td>Tensor</td>
<td>scalar of Python</td>
<td>〇</td>
</tr>
<tr>
<td>Tensor</td>
<td>ndarray</td>
<td>✕</td>
</tr>
</tbody>
</table>
<p>It is important to note that the four arithmetic operations between Tensors must have the same type.
When performing the four arithmetic operations, the dimension is automatically complemented even in the operation between vectors and scalars or between matrices and vectors (<a href="https://docs.scipi.org/doc/numpy-1.13.0/user/basics.broadcasting.html">Broadcast</a>).</p>
<pre><code class="language-python:" data-lang="python:"># vector
v = torch.tensor([1, 2, 3])
w = torch.tensor([4, 5, 6])

# queue
m = torch.tensor([[0, 1, 2], [10, 20, 30]])
n = torch.tensor([[3, 4, 5], [40, 50, 60]])

# Vector-Scalar
v_pl = v + 10 # tensor([11, 12, 13])
v_mi = v-10 # tensor([-9, -8, -7])
v_mu = v * 10 # tensor([10, 20, 30])
v_di = v / 10 # tensor([0, 0, 0])

# Vector-Vector
v_pl = v + w #tensor([5, 7, 9])
v_mi = v-w # tensor([-3, -3, -3])
v_mu = v * w # tensor([4, 10, 18])
v_di = v / w #tensor([0, 0, 0])

# Matrix and vector
v_pl = m + v #tensor([[ 1, 3, 5], [11, 22, 33]])
v_mi = m-v # tensor([[-1, -1, -1], [9, 18, 27]])
v_mu = m * v # tensor([[ 0, 2, 6], [10, 40, 90]])
v_di = m / v # tensor([[ 0, 0, 0], [10, 10, 10]])

#Matrix and matrix
v_pl = m + n # tensor([[3, 5, 7], [50, 70, 90]])
v_mi = m-n # tensor([[-3, -3, -3], [-30, -30, -30]])
v_mu = m * n # tensor([[0, 4, 10], [400, 1000, 1800]])
v_di = m / n # tensor([[0, 0, 0], [0, 0, 0]])


</code></pre><p>There are places where the output becomes 0 when dividing. As you can see, it is because the type of tensor is int even if the result of the operation shows the value after the decimal point. If you want to avoid it, specify float etc. in advance.</p>
<h1 id="math-functions-available-in-pytorch">Math functions available in PyTorch</h1>
<p>PyTorch provides various mathematical functions for Tensor.</p>
<p><strong>A mathematical function that acts on each value of Tensor</strong></p>
<table>
<thead>
<tr>
<th align="left">Function name</th>
<th align="left">Overview</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">abs</td>
<td align="left">absolute value</td>
</tr>
<tr>
<td align="left">sin</td>
<td align="left">sine</td>
</tr>
<tr>
<td align="left">cos</td>
<td align="left">cosine</td>
</tr>
<tr>
<td align="left">exp</td>
<td align="left">Exponential function</td>
</tr>
<tr>
<td align="left">log</td>
<td align="left">Log function</td>
</tr>
<tr>
<td align="left">sqrt</td>
<td align="left">square root</td>
</tr>
</tbody>
</table>
<p><strong>Aggregation function</strong></p>
<table>
<thead>
<tr>
<th align="left">Function</th>
<th align="left">Overview</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">sum of values in</td>
<td align="left">sum</td>
</tr>
<tr>
<td align="left">Maximum value in</td>
<td align="left">max</td>
</tr>
<tr>
<td align="left">minimum value in</td>
<td align="left">min</td>
</tr>
<tr>
<td align="left">standard deviation of values in</td>
<td align="left">std</td>
</tr>
</tbody>
</table>
<p>#Linear algebra operators</p>
<table>
<thead>
<tr>
<th align="left">Function</th>
<th align="left">Overview</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">dot</td>
<td align="left">Vector dot product</td>
</tr>
<tr>
<td align="left">mv</td>
<td align="left">product of matrix and vector</td>
</tr>
<tr>
<td align="left">mm</td>
<td align="left">Matrix-matrix product</td>
</tr>
<tr>
<td align="left">matmul</td>
<td align="left">Automatically selects and executes dot, mv, mm depending on the argument type</td>
</tr>
<tr>
<td align="left">gesv</td>
<td align="left">LU solution of simultaneous equations by decomposition</td>
</tr>
<tr>
<td align="left">eig, symeig</td>
<td align="left">Eigenvalue decomposition. symeig is for the target matrix</td>
</tr>
<tr>
<td align="left">svd</td>
<td align="left">singular value decomposition</td>
</tr>
</tbody>
</table>
<p>Let&rsquo;s check what kind of results will be obtained (the results are quite long and are not shown).</p>
<pre><code class="language-python:" data-lang="python:">m = torch.randn(100, 10) # create 100x10 matrix test data
v = torch.randn(10)

# Dot product
torch.dot(v, v)

# Product of matrix and vector
torch.mv(m, v)

# Matrix product
torch.mm(m.t(), m)

# Singular value decomposition
u, s, v = torch.svd(m)
</code></pre><h1 id="other-frequently-used-functions">Other frequently used functions</h1>
<table>
<thead>
<tr>
<th align="left">Function name</th>
<th align="left">Overview</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">Changing the dimension of</td>
<td align="left">view</td>
</tr>
<tr>
<td align="left">cat, stack</td>
<td align="left">Connecting Tensors</td>
</tr>
<tr>
<td align="left">transpose</td>
<td align="left">Transpose dimensions</td>
</tr>
</tbody>
</table>
<h1 id="automatic-differentiation">Automatic differentiation</h1>
<p>If you set <code>requires_grad</code> of Tensor to <code>True</code>, the flag that performs automatic differentiation is enabled. This flag is valid for all parameters and data when dealing with neural networks.
A calculation graph is constructed by stacking various operations on a Tensor with valid <code>requires_grad</code>, and it is automatically differentiated from that information by calling the backward method.</p>
<h1 id="in-conclusion">in conclusion</h1>
<p>This time, I have summarized Tensor, which is essential for handling PyTorch configuration and machine learning. Thank you to everyone who read through to the end.
I will build a neural network little by little from the next time, so if you are interested, please check it out.</p>
<h4 id="reference">Reference</h4>
<p>Toyohashi (2018) “<a href="https://www.shoeisha.co.jp/book/detail/9784798157184">Can be used on site! Introduction to PyTorch Development Creating a Deep Learning Model and Implementing it in an Application</a>”</p>
<section class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1" role="doc-endnote">
<p>Get or change array value by specifying subscript <a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2" role="doc-endnote">
<p>An array of the same size as the original array, where each element in the array is True/False. <a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3" role="doc-endnote">
<p>Singular Value Decomposition (SVD) is a calculation that is often used in linear algebra, and decomposes matrix A into the product of three matrices like USV. <br /> U and V are orthogonal matrices, and S is a square matrix with only diagonal elements. It is used for solving the least squares method and for matrix approximation and compression. <a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</section>

    </article>
  </div>

  
  
  
  <script>
  window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
  ga('create', 'UA-169005401-1', 'auto');
  ga('send', 'pageview');
  </script>
  <script async src='https://www.google-analytics.com/analytics.js'></script>
  

  

  
<link rel="stylesheet" type="text/css" href="//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.0/cookieconsent.min.css" />
<script src="//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.0/cookieconsent.min.js"></script>
<script>
window.addEventListener("load", function(){
window.cookieconsent.initialise({
  "palette": {
    "popup": {
      "background": "#216942",
      "text": "#b2d192"
    },
    "button": {
      "background": "#afed71"
    }
  }
})});
</script>

</body>

</html>
