<!doctype html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  
  <meta name="generator" content="Hugo 0.72.0" />

  
  <meta name="description" content="Some description">
  

  
  <link rel="apple-touch-icon" sizes="180x180" href="https://memotut.com/apple-touch-icon.png">

  
  <link rel="icon" type="image/png" sizes="32x32" href="https://memotut.com/favicon-32x32.png">

  
  <link rel="icon" type="image/png" sizes="16x16" href="https://memotut.com/favicon-16x16.png">

  
  <link rel="manifest" href="https://memotut.com/site.webmanifest">

  
  <link rel="mask-icon" href="https://memotut.com/safari-pinned-tab.svg" color="#5bbad5">

  <meta name="msapplication-TileColor" content="#da532c">

  <meta name="theme-color" content="#ffffff">

  
  <link rel="stylesheet" href="https://memotut.com/css/bootstrap.min.css" />

  
  <title>I made a library konoha to switch the tokenizer to a nice one. | Memo Tut</title>
  

  <style>
body {
  min-width: 300px;
}

.custom-navbar {
  margin-bottom: 1em;
  height: 60px;
}

.custom-navbar a {
  display: inline-block; 
  padding: 18px 0;
  margin-right: 1em; 
  font-weight: bold; 
}

.custom-navbar a:hover,
.custom-navbar a:focus {
  text-decoration: none; 
}

@media print {
  .custom-navbar {
    display: none;
  }
}

article {
  padding-bottom: 1em;
}

img {
  max-width: 100%;
}


body {
  background-color: #fff;
}



body {
  color: #212529;
}



a {
  color: #007bff;
}



a:hover,
a:focus {
  color: #0056b3;
}



.custom-navbar {
  background-color: #212529;
}



.custom-navbar a {
  color: rgba(255, 255, 255, 0.75);
}



.custom-navbar a:hover,
.custom-navbar a:focus {
  color: rgba(255, 255, 255, 1);
}



.container {
  max-width: 800px;
}



pre {
  display: block;
  padding: 9.5px;
  word-break: break-all;
  word-wrap: break-word;
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 4px;
}

pre code {
  padding: 0;
  font-size: inherit;
  color: inherit; 
  white-space: pre-wrap;
  background-color: transparent;
  border: none;
  border-radius: 0;
}

code {
  padding: 2px 4px;
  color: inherit; 
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 4px;
  font-size: .9em;
}



blockquote,
.blockquote {
  padding: 10px 20px;
  margin: 0 0 20px;
  font-size: 1em;
  border-left: 5px solid #6c757d;
}

</style>
</head>

<body>
  <nav class="custom-navbar">
  <div class="container">
    
    <a href="/">Posts</a>
    
    <a href="/tags/">Tags</a>
    
    <a href="/about/">About</a>
    
    <a href="/index.xml">RSS</a>
    
  </div>
</nav>
  
  <div class="container">
    <article>
      <h1>I made a library konoha to switch the tokenizer to a nice one.</h1>
<p>
  <small class="text-secondary">
  
  
  Nov 14, 2019
  </small>
  

<small><code><a href="https://memotut.com/tags/python">Python</a></code></small>


<small><code><a href="https://memotut.com/tags/natural-language-processing"> natural language processing</a></code></small>


<small><code><a href="https://memotut.com/tags/nlp"> NLP</a></code></small>

</p>
<pre><code># TL; DR
</code></pre>
<p>I will introduce <a href="https://github.com/himkt/konoha">konoha</a>, a library for tokenizing sentences.
(Formerly tiny_tokenizer)
You can use it like â†“. What?</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">from</span> konoha <span style="color:#f92672">import</span> WordTokenizer

sentence <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;I am studying natural language processing&#34;</span>

tokenizer <span style="color:#f92672">=</span> WordTokenizer(<span style="color:#e6db74">&#39;MeCab&#39;</span>)
<span style="color:#66d9ef">print</span>(tokenizer<span style="color:#f92672">.</span>tokenize(sentence)) <span style="color:#75715e"># -&gt; [natural, language, processing, study, study, do, do, masu]</span>

tokenizer <span style="color:#f92672">=</span> WordTokenizer(<span style="color:#e6db74">&#39;Kytea&#39;</span>)
<span style="color:#66d9ef">print</span>(tokenizer<span style="color:#f92672">.</span>tokenize(sentence)) <span style="color:#75715e"># -&gt; [natural, language, processing, study, study, te, i, ma, su]</span>

tokenizer <span style="color:#f92672">=</span> WordTokenizer(<span style="color:#e6db74">&#39;Sentencepiece&#39;</span>, model_path<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;data/model.spm&#34;</span>)
<span style="color:#66d9ef">print</span>(tokenizer<span style="color:#f92672">.</span>tokenize(sentence)) <span style="color:#75715e"># -&gt; [, natural, language, processing, studying, doing, doing]</span>
</code></pre></div><h1 id="introduction-what-is-a-tokenizer">Introduction: What is a tokenizer?</h1>
<p>Unlike languages such as English, Japanese has no clear delimiter at word boundaries.
Therefore, when analyzing Japanese, the sentence must first be divided into some units (eg words).
The process of this division is to divide it into word units, to divide the word into sub-word units,
And there are things that divide the character string in the sentence into each character.
In this paper, the substrings divided by the units listed above are called tokens.
There are various methods for tokenization.
The morphological analyzer, which is widely used for analyzing Japanese text, also performs word segmentation.
(In morphological analysis, in addition to word segmentation, word lemmatization and part-of-speech tag estimation are performed.)</p>
<p>For word-by-word division, a lattice is used to build a lattice, and then MeCab, which determines the optimal word string,
There are algorithms such as Kytea that determine word boundaries at the character level.
These algorithms may return the same split result or different split results.
In addition, even if the division algorithm is the same, the word unit also changes if the part-of-speech system is different.</p>
<ul>
<li><a href="https://hayashibe.jp/tr/mecab/dictionary/ipadic">IPADic&rsquo;s part-of-speech system</a></li>
<li><a href="https://hayashibe.jp/tr/mecab/dictionary/unidic/pos">UniDic&rsquo;s part-of-speech system</a></li>
</ul>
<p>A subword is a subdivided word.
Effectiveness has been confirmed by neural machine translation.
As a representative subword tokenizer used in Japanese text analysis
<a href="https://github.com/google/sentencepiece">Sentencepiece</a> is famous.</p>
<ul>
<li><a href="https://techlife.cookpad.com/entry/2016/05/11/170000">Detailed description of MeCab</a>(worddivisionisalsomentioned)</li>
<li><a href="http://www.phontron.com/kytea/method-ja.html">Explanation of Kytea&rsquo;s word division method</a></li>
<li><a href="https://www.anlp.jp/proceedings/annual_meeting/2016/pdf_dir/D6-5.pdf">Comparison of various parts-of-speech systems and tokenizers</a></li>
<li><a href="https://qiita.com/taku910/items/7e52f1e58d0ea6e7859c">Explanation of Sentencepiece</a></li>
</ul>
<h1 id="tokenizer-what-to-do">Tokenizer What to do</h1>
<p>Those who do Japanese text mining
Do you often use MeCab + NEologd?
There may be cases where Kytea is often used in research.
In addition, the dependency analysis library <a href="https://megagonlabs.github.io/ginza/">Ginza</a>, which has become a hot topic recently,
Using a morphological analyzer called SudachiPy (Python implementation of Sudachi)
Various analyzers are used for word-level analysis.
It is difficult to determine which analyzer is most suitable to use.</p>
<p>Moreover, in recent years mainly in the context of machine translation
&ldquo;The task performance is better if the token is divided into subword units rather than using the result of the morphological analyzer.&rdquo;
It has also been reported that subword-based division is often adopted.</p>
<p>Character-level tokenization is characterized by a small number of character types compared to the number of word types.
The number of word types is generally much larger than the number of character types, and tokenization at the character level has the effect of suppressing the vocabulary size.
For example, in the study of proper expression extraction, there is the <a href="https://arxiv.org/abs/1603.01360">Add character level features to LSTM features</a> approach,
Many recent studies have also adopted this approach.
(The paper listed in the link is old, but my favorite paper)</p>
<p>In which situation, in what unit should we tokenize sentences?
In general, I understand that the answer to this question is &ldquo;<strong>task-dependent</strong>&rdquo; and there is no single answer.
For this reason, &ldquo;I use MeCab+NEologd because many people use it.&rdquo;
&ldquo;Because I use subwords in my paper, I will use subwords for the time being.&rdquo;
It is easy to choose options such as.</p>
<p>Recent natural language processing tasks (especially when using neural networks)
There are a number of things that you should be careful with, even more than tokenizing.
(Eg. architecture, hidden layer dimension, optimizer, learning rate&hellip;etc),
Against this background, which tokenization method to use at the beginning of tackling the problem
I think the current situation is that there are many cases where it is decided to be &ldquo;Eiya&rdquo;.
But aren&rsquo;t other tokenization methods really worth trying?
I think it&rsquo;s worth trying out various tokenization schemes, so
We have developed a library to easily switch the tokenization method.
This is the reason why konoha was developed.</p>
<h1 id="various-tokenizers-and-various-apis">Various tokenizers and various APIs</h1>
<p>Switching tokenizers often costs some money.
All of the above morphological analyzers and tokenizers have Python wrappers,
Users can use these tools from Python by installing the wrapper library.</p>
<p>However, each wrapper library provides an API with different idioms.
(It&rsquo;s natural because each analyzer and the author of their wrapper libraries are different)
Therefore, if you want to switch and use the output of multiple analyzers according to the situation,
You have to implement the layer that absorbs the idiom difference of those APIs.</p>
<h1 id="precedent-case">Precedent case</h1>
<p>There is a library called <a href="https://qiita.com/Kensuke-Mitsuzawa/items/1d2837b374152c9758c9">JapaneseTokenizer</a>.
(GitHub repository: <a href="https://github.com/Kensuke-Mitsuzawa/JapaneseTokenizers">Kensuke-Mitsuzawa/JapaneseTokenizers</a>)
JapaneseTokenizer also provides wrappers for multiple tokenizers, similar to konoha.
JapaneseTokenizer provides an interface to handle multiple morphological analyzers.
JapaneseTokenizer can be used to filter sentence analysis results by specific part-of-speech tags.
Many practical functions that are useful for text analysis are implemented.
It is a very useful tool for natural language processing that utilizes the results of multiple morphological analyzers.</p>
<h1 id="konoha">konoha</h1>
<p>On the other hand, tiny tokenizer does not provide any functions such as part-of-speech filtering at this time.
tiny tokenizer is a library that abstracts the tokenization process of each analyzer.
It provides subword-based division and character-level division functions that JapaneseTokenizer does not target.</p>
<p>The position of this library is a wrapper for the Python wrapper.
Thank you to everyone who provided a Python wrapper for the parser,
The purpose of this library is to absorb the differences in the interfaces of those libraries.
By using konoha, users can use multiple analyzers with a unified API.</p>
<h2 id="tokenization">Tokenization</h2>
<p>First, I will show an example using MeCab.
In this example, the dictionary uses mecab-ipadic.
If you are using macOS, <code>mecab</code>, <code>mecab-ipadic</code>,
If you are using Ubuntu, install <code>libmecab-dev</code> in addition to the above.
Operation has not been verified for other distributions.
(I think that it works if you can execute <code>mecab</code>, <code>mecab-config</code> and the dictionary is installed)
If you build the environment by building the Dockerfile in the GitHub repository, you are all set.</p>
<ul>
<li>Code</li>
</ul>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">from</span> konoha <span style="color:#f92672">import</span> WordTokenizer

sentence <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;I am studying natural language processing&#34;</span>

tokenizer <span style="color:#f92672">=</span> WordTokenizer(<span style="color:#e6db74">&#39;MeCab&#39;</span>)
<span style="color:#66d9ef">print</span>(tokenizer<span style="color:#f92672">.</span>tokenize(sentence))
</code></pre></div><ul>
<li>Output</li>
</ul>
<pre><code>[Nature, language, processing, study, study, do, do, masu]
</code></pre><p>Next, try using Kytea.
Again, you&rsquo;ll need to build Kytea.
(I hope you can refer to the Dockerfile of the repository as well)</p>
<ul>
<li>Code</li>
</ul>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">from</span> konoha <span style="color:#f92672">import</span> WordTokenizer

sentence <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;I am studying natural language processing&#34;</span>

tokenizer <span style="color:#f92672">=</span> WordTokenizer(<span style="color:#e6db74">&#39;Kytea&#39;</span>)
<span style="color:#66d9ef">print</span>(tokenizer<span style="color:#f92672">.</span>tokenize(sentence))
</code></pre></div><ul>
<li>Output</li>
</ul>
<pre><code>[Nature, language, processing, study, study, do, do, ma]
</code></pre><p>If you want to divide a sentence into subwords, you can use Sentencepiece.
When using Sentencepiece, it is necessary to specify the model file.
Pass the path to the model file in <code>model_path</code>.</p>
<ul>
<li>Code</li>
</ul>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">from</span> konoha <span style="color:#f92672">import</span> WordTokenizer

sentence <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;I am studying natural language processing&#34;</span>tokenizer <span style="color:#f92672">=</span> WordTokenizer(<span style="color:#e6db74">&#39;Sentencepiece&#39;</span>, model_path<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;data/model.spm&#34;</span>)
<span style="color:#66d9ef">print</span>(tokenizer<span style="color:#f92672">.</span>tokenize(sentence))
</code></pre></div><ul>
<li>Output</li>
</ul>
<pre><code>[, nature, language, processing, study, studying,]
</code></pre><p>In this way, multiple analyzers can be used uniformly by simply changing the value of the argument passed to <code>WordTokenizer</code>.
This makes it easy to experiment with different tokenizers at the experimental stage.</p>
<h2 id="part-of-speech-estimation">Part of speech estimation</h2>
<p>A morphological analyzer is also included in the tokenizer.
Of the tokenizers currently supported by konoha,
The morphological analyzers are <code>MeCab</code>, <code>Kytea</code> and <code>Sudachi (SudachiPy)</code>.
For these, whether or not to obtain information given by a morphological analyzer such as a part-of-speech tag when tokenized
It can be controlled by options.</p>
<p>An example of using <code>SudachiPy</code> is shown below.</p>
<ul>
<li>Code</li>
</ul>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">from</span> konoha <span style="color:#f92672">import</span> WordTokenizer

sentence <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;I am studying natural language processing&#34;</span>

tokenizer <span style="color:#f92672">=</span> WordTokenizer(<span style="color:#e6db74">&#39;Sudachi&#39;</span>, mode<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;A&#39;</span>, with_postag<span style="color:#f92672">=</span>True)
<span style="color:#66d9ef">print</span>(tokenizer<span style="color:#f92672">.</span>tokenize(sentence))
</code></pre></div><ul>
<li>Output</li>
</ul>
<pre><code>[Natural (noun), Language (noun), Processing (noun), s (particle), Study (noun), Shi (verb), te (
Particle), i (verb), mas (auxiliary verb)]
</code></pre><p>The output of <code>tokenizer.tokenize</code> is an instance of the <code>Token</code> class.
The following instance variables are defined in the <code>Token</code> class.
(Excerpt from docstring of <code>Token</code> class)</p>
<p>It is <code>None</code> for information not returned by the parser.
For example, <code>token.normalized_form</code> uses <code>SudachiPy</code>,
And only when <code>with_postag</code> is <code>True</code> is a value that is not <code>None</code>.
(<code>token</code> is one element of the token string array output by <code>tokenizer.tokenize</code>)</p>
<pre><code>&quot;&quot;&quot;
surface (str)
    surface (original form) of a word
postag (str, default: None)
    part-of-speech tag of a word (optional)
postag2 (str, default: None)
    detailed part-of-speech tag of a word (optional)
postag3 (str, default: None)
    detailed part-of-speech tag of a word (optional)
postag4 (str, default: None)
    detailed part-of-speech tag of a word (optional)
inflection (str, default: None)
    conjugate type of word (optional)
conjugation (str, default: None)
    conjugate type of word (optional)
base_form (str, default: None)
    base form of a word
yomi (str, default: None)
    yomi of a word (optional)
pron (str, default: None)
    pronounciation of a word (optional)
normalized_form (str, default: None)
    normalized form of a word (optional)
    Note that normalized_form is only
    available on SudachiPy
&quot;&quot;&quot;
</code></pre><h2 id="use-your-own-user-dictionary-mecab">Use your own user dictionary (MeCab)</h2>
<p>If you want to use user dictionary, <code>user_dictionary_path</code> of <code>WordTokenizer</code>
Pass the path to the user dictionary in the argument named.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">from</span> konoha <span style="color:#f92672">import</span> WordTokenizer

sentence <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;I am studying natural language processing&#34;</span>

tokenizer <span style="color:#f92672">=</span> WordTokenizer(<span style="color:#e6db74">&#39;MeCab&#39;</span>, user_dictionary_path<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;path/to/user_dict&#34;</span>)
<span style="color:#66d9ef">print</span>(tokenizer<span style="color:#f92672">.</span>tokenize())
</code></pre></div><h2 id="use-your-own-system-dictionary-mecab">Use your own system dictionary (MeCab)</h2>
<p>I want to use <code>mecab-ipadic-NEologd</code>,
Or if you want to use the system dictionary that you have re-learned using the corpus,
It is possible to generate a tokenizer by specifying a system dictionary.
Since the argument <code>system_dictionary_path</code> grows in <code>WordTokenizer</code>,
Give it the path to the system dictionary you want to use.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">from</span> konoha <span style="color:#f92672">import</span> WordTokenizer

sentence <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;I am studying natural language processing&#34;</span>

tokenizer <span style="color:#f92672">=</span> WordTokenizer(<span style="color:#e6db74">&#39;MeCab&#39;</span>, system_dictionary_path<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;path/to/system_dict&#34;</span>)
<span style="color:#66d9ef">print</span>(tokenizer<span style="color:#f92672">.</span>tokenize())
</code></pre></div><p>#Summary</p>
<p>This article is a library to use multiple tokenizers with the same interface.
We introduced konoha.
By using this library when you are wondering which analyzer to use at the beginning of text analysis
It is possible to easily switch the analyzer.
Also, I plan to do experiments using MeCab, but the previous research uses other analyzers.
Even in the case where you have to write code to use another analyzer for comparison
By inserting konoha, you can perform experiments with the same code without trouble.
I would be happy if it would be helpful to those who do natural language processing in the field and those who do natural language processing in research.
If you like it, please try using it. Thank you.</p>
<hr>
<p>In order to build Kytea on Ubuntu 18.04, <a href="https://github.com/neubig/kytea/pull/24">this pull request</a>
Should be imported. I hope you can refer to the Dockerfile in the konoha repository.</p>

    </article>
  </div>

  
  
  
  <script>
  window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
  ga('create', 'UA-169005401-1', 'auto');
  ga('send', 'pageview');
  </script>
  <script async src='https://www.google-analytics.com/analytics.js'></script>
  

  

  
<link rel="stylesheet" type="text/css" href="//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.0/cookieconsent.min.css" />
<script src="//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.0/cookieconsent.min.js"></script>
<script>
window.addEventListener("load", function(){
window.cookieconsent.initialise({
  "palette": {
    "popup": {
      "background": "#216942",
      "text": "#b2d192"
    },
    "button": {
      "background": "#afed71"
    }
  }
})});
</script>

</body>

</html>
