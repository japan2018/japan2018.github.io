<!doctype html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  
  <meta name="generator" content="Hugo 0.72.0" />

  
  <meta name="description" content="Some description">
  

  
  <link rel="apple-touch-icon" sizes="180x180" href="https://memotut.com/apple-touch-icon.png">

  
  <link rel="icon" type="image/png" sizes="32x32" href="https://memotut.com/favicon-32x32.png">

  
  <link rel="icon" type="image/png" sizes="16x16" href="https://memotut.com/favicon-16x16.png">

  
  <link rel="manifest" href="https://memotut.com/site.webmanifest">

  
  <link rel="mask-icon" href="https://memotut.com/safari-pinned-tab.svg" color="#5bbad5">

  <meta name="msapplication-TileColor" content="#da532c">

  <meta name="theme-color" content="#ffffff">

  
  <link rel="stylesheet" href="https://memotut.com/css/bootstrap.min.css" />

  
  <title>History of major GAN studies (as of November 2019) | Memo Tut</title>
  

  <style>
body {
  min-width: 300px;
}

.custom-navbar {
  margin-bottom: 1em;
  height: 60px;
}

.custom-navbar a {
  display: inline-block; 
  padding: 18px 0;
  margin-right: 1em; 
  font-weight: bold; 
}

.custom-navbar a:hover,
.custom-navbar a:focus {
  text-decoration: none; 
}

@media print {
  .custom-navbar {
    display: none;
  }
}

article {
  padding-bottom: 1em;
}

img {
  max-width: 100%;
}


body {
  background-color: #fff;
}



body {
  color: #212529;
}



a {
  color: #007bff;
}



a:hover,
a:focus {
  color: #0056b3;
}



.custom-navbar {
  background-color: #212529;
}



.custom-navbar a {
  color: rgba(255, 255, 255, 0.75);
}



.custom-navbar a:hover,
.custom-navbar a:focus {
  color: rgba(255, 255, 255, 1);
}



.container {
  max-width: 800px;
}



pre {
  display: block;
  padding: 9.5px;
  word-break: break-all;
  word-wrap: break-word;
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 4px;
}

pre code {
  padding: 0;
  font-size: inherit;
  color: inherit; 
  white-space: pre-wrap;
  background-color: transparent;
  border: none;
  border-radius: 0;
}

code {
  padding: 2px 4px;
  color: inherit; 
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 4px;
  font-size: .9em;
}



blockquote,
.blockquote {
  padding: 10px 20px;
  margin: 0 0 20px;
  font-size: 1em;
  border-left: 5px solid #6c757d;
}

</style>
</head>

<body>
  <nav class="custom-navbar">
  <div class="container">
    
    <a href="/">Posts</a>
    
    <a href="/tags/">Tags</a>
    
    <a href="/about/">About</a>
    
    <a href="/index.xml">RSS</a>
    
  </div>
</nav>
  
  <div class="container">
    <article>
      <h1>History of major GAN studies (as of November 2019)</h1>
<p>
  <small class="text-secondary">
  
  
  Nov 29, 2019
  </small>
  

<small><code><a href="https://memotut.com/tags/python">Python</a></code></small>


<small><code><a href="https://memotut.com/tags/gans"> GANs</a></code></small>

</p>
<pre><code>### For people like this: point_up_tone1:
</code></pre>
<p>GAN (Generative Adversarial Network) was announced in 2014, but due to epoch-making research, there are many derivative researches recently, and what is it? .. I myself made full use of Google professor to do various research, but I couldn&rsquo;t find any Japanese literature that summarizes major related research in an easy-to-understand way, sorry! So, this article is for those who want to get an outline of the flow of GAN-related research, or for those who want to jump over each research ~~ one by one with quick and one-click jumping to the paper and code. I am. By the way, this article is written mainly with reference to <a href="https://blog.floydhub.com/gans-story-so-far/">Generative Adversarial Networks-The Story So Far</a>.</p>
<h5 id="what-is-gan-to-those-who">What is GAN? To those who</h5>
<ul>
<li>@triwave33&rsquo;s <a href="https://qiita.com/triwave33/items/1890ccc71fab6cbca87e">GAN(1) that can not be heard now</a></li>
<li>SONY&rsquo;s Neural Network Console <a href="https://www.youtube.com/watch?v=2rC2_-HtpsQ">Official YouTube channel</a>(Overview)</li>
<li>@typecprint&rsquo;s <a href="https://qiita.com/typecprint/items/035478748a811dd29721">Objective function of GAN that can not be heard now</a>(Explanationoflearningprocessandobjectivefunction)</li>
</ul>
<p>Please refer to the following.</p>
<h3 id="gan-family-introduction-family_mwbb">GAN family introduction: family_mwbb:</h3>
<ol>
<li>GAN (Generative Adversarial Network)</li>
<li>CGAN (Conditional Generative Adversarial Network)</li>
<li>DCGAN (Deep Convolutional Generative Adversarial Network)</li>
<li>CoGAN (Coupled Generative Adversarial Networks)</li>
<li>Pix2pix</li>
<li>WGAN (Wasserstein Generative Adversarial Network)</li>
<li>CycleGAN</li>
<li>StackGAN (Stack Generative Adversarial Network)</li>
<li>ProGAN (Progressive Growing of Generative Adversarial Network)</li>
<li>SAGAN (Self-Attention Generative Adversarial Network)</li>
<li>BigGAN (Big Generative Adversarial Network)</li>
<li>StyleGAN (Style-based Generative Adversarial Network)
<img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/496974/eb2761e6-3e59-6b6c-8dab-8e2cf5ac2e7c.png" alt="GAN (1).png"></li>
</ol>
<p>It&rsquo;s a big family. (Actually, it is a much larger family.) From here on</p>
<ul>
<li>Research paper</li>
<li>Code</li>
<li>Points</li>
<li>Recommended references (there are many kind and smart people in the world)</li>
</ul>
<p>I will introduce.</p>
<h4 id="1-gan-generative-adversarial-network">1. GAN (Generative Adversarial Network)</h4>
<ul>
<li><a href="https://arxiv.org/abs/1406.2661">Generative Adversarial Networks</a></li>
<li><a href="https://github.com/goodfeli/adversarial">Code</a></li>
<li>Points:
-Using two neural networks (Generator and Discriminator)
-Generator creates &ldquo;realistic&rdquo; images (first image is from random noise)
-Discriminator distinguishes between the fake image generated by Generator and the input real image
-Generator and Discriminator share the following objective function. Generator aims to minimize this value, Discriminator maximizes this value, and proceed with learning.
<img width="484" alt="Screen Shot 2019-11-27 at 6.00.18 PM.png" src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/496974/9c8db07f-1f81-359e-89a7-3697d5956920.png"></li>
</ul>
<img width="1303" alt="Screen Shot 2019-11-28 at 8.33.52 PM.png" src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/496974/4d90b4ba-1b5d-67c4-439d-0a1d466b4013.png">
<p><a href="https://mc.ai/a-tutorial-on-conditional-generative-adversarial-nets-keras-implementation/">Reference</a></p>
<h4 id="2-cgan-conditional-generative-adversarial-network">2. CGAN (Conditional Generative Adversarial Network)</h4>
<ul>
<li><a href="https://arxiv.org/abs/1411.1784">Conditional Generative Adversarial Nets</a></li>
<li><a href="https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Keras">Code</a></li>
<li>Point: By adding the class condition vector (y in the figure below) to Generator and Discriminator, you can write different images for each class!</li>
<li>Recommended reference: <a href="https://qiita.com/triwave33/items/f6352a40bcfbfdea0476">GAN (6) that I can not listen to now</a></li>
</ul>
<img width="1306" alt="Screen Shot 2019-11-28 at 8.33.59 PM.png" src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/496974/87c1f17f-7336-6d9c-0b3c-2de5b9d51b7e.png">
<h4 id="3-dcgan-deep-convolutional-generative-adversarial-network">3. DCGAN (Deep Convolutional Generative Adversarial Network)</h4>
<ul>
<li><a href="https://arxiv.org/abs/1511.06434">Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks</a></li>
<li><a href="https://github.com/floydhub/dcgan">Code</a></li>
<li>Point: Improved image quality using the following methods
-Replace the pooling layer with the Convolution layer with stride (see the figure below)
-Unsampling uses deconvolution layer
-Eliminate all bonds
-Perform batch normalization for all layers (except Generator output layer and Discriminator input layer)
-In Generator, use Relu except for output layer (use tanh for output layer)
-Use LeakyReLU in Discriminator</li>
<li>Recommended reference: <a href="https://elix-tech.github.io/en/2017/02/06/gan.html">First time GAN</a>(Althoughitisthetitle,detailedexplanationaboutDCGANisalsoincluded.)
<img width="805" alt="Screen Shot 2019-11-27 at 6.28.38 PM.png" src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/496974/abe7ff4f-7af9-08f1-92c5-d3cf1194eaa2.png"></li>
</ul>
<h4 id="4-cogan-coupled-generative-adversarial-networks">4. CoGAN (Coupled Generative Adversarial Networks)</h4>
<ul>
<li><a href="https://arxiv.org/abs/1606.07536">Coupled Generative Adversarial Networks</a></li>
<li><a href="https://github.com/mingyuliutw/CoGAN">Code</a></li>
<li>Point: I used two GANs side by side (not so hot these days)</li>
<li>Recommended reference: <a href="https://wiseodd.github.io/techblog/2017/02/18/coupled_gan/">Agustinus Kristiadi&rsquo;s Blog</a>
<img width="1150" alt="Screen Shot 2019-11-27 at 7.37.21 PM.png" src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/496974/0cd587fc-1d32-5188-335e-d71e32f5ea5b.png"></li>
</ul>
<h4 id="5-pix2pix">5. Pix2pix</h4>
<ul>
<li><a href="https://arxiv.org/pdf/1612.03242.pdf">Image-to-Image Translation with Conditional Adversarial Networks</a></li>
<li><a href="https://github.com/phillipi/pix2pix">Code</a></li>
<li>Point: Image to image conversion can now be performed with high accuracy
-Source image and destination image must be paired (such as a map of the same location in Google Maps and an aerial photo)</li>
<li>Recommended reference: <a href="https://qiita.com/mine820/items/36ffc3c0aea0b98027fd">I want to understand pix2pix</a>
<img width="1346" alt="Screenshots 2019-11-28 20.18.57.png" src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/496974/c7eb3f7e-17be-f41f-9bce-1f0ed19b5b06.png"></li>
</ul>
<h4 id="6-wgan-wasserstein-generative-adversarial-networks">6. WGAN (Wasserstein Generative Adversarial Networks)</h4>
<ul>
<li><a href="https://arxiv.org/abs/1701.07875v3">Wasserstein GAN</a>-<a href="https://github.com/eriklindernoren/Keras-GAN">Code</a></li>
<li>Point: Stabilization of learning and avoiding mode collapse by the following methods
-Design loss function using Wasserstein distance instead of Jensen-Shannon divergence (which causes vanishing gradient problem) used in normal GAN</li>
<li>Recommended references: <a href="https://qiita.com/mittyantest/items/0fdc9ce7624dbd2ee134">Wasserstein GAN and Kantorovich-Rubinstein duality</a>(explainsWassersteindistanceindetail)</li>
</ul>
<h4 id="7-cyclegan-cycle-generative-adversarial-network">7. CycleGAN (Cycle Generative Adversarial Network)</h4>
<ul>
<li><a href="https://arxiv.org/abs/1703.10593v6">Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks</a></li>
<li><a href="https://github.com/junyanz/CycleGAN">Code</a></li>
<li>Point: <strong>Unpaired</strong> When two images are given as training data, one image can be converted to the other
-It became possible by using &ldquo;cycle consistency loss&rdquo; in addition to &ldquo;hostile loss&rdquo; (loss function used in normal GAN)
-5. Although it is similar to Pix2pix in terms of &ldquo;image → image conversion&rdquo;, CycleGAN does not need to pair the training data images unlike Pix2pix!
-Strong in texture and color conversion, but difficult in structural conversion (ex. dog → cat)</li>
<li>Recommended reference: <a href="https://qiita.com/hikaru-light/items/98d06b21b4f3e2bb6ca4">Supplement to &ldquo;GAN&rdquo;</a>(explainskeycycleconsistencyloss)
<img width="724" alt="Screen Shot 2019-11-28 at 11.09.46 AM.png" src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/496974/1259d8ab-8e3d-2263-ab65-5331ddafc599.png"></li>
</ul>
<p>The image of what you can do is like this. (See <a href="https://arxiv.org/abs/1703.10593v6">Papers</a>forimages)</p>
<h4 id="8-stackgan-stack-generative-adversarial-networks">8. StackGAN (Stack Generative Adversarial Networks)</h4>
<ul>
<li><a href="https://arxiv.org/pdf/1612.03242.pdf">StackGAN: Text to Photo-realistic Image Synthesis
with Stacked Generative Adversarial Networks</a></li>
<li>Point: You can now generate high resolution images using only textual representations</li>
<li>Recommended references: <a href="https://www.nogawanogawa.com/entry/stackgan">[Notes on paper: StackGAN]</a></li>
</ul>
<img width="912" alt="Screen Shot 2019-11-28 at 10.42.25 PM.png" src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/496974/1246b9e6-2979-7a84-50ac-e6bfe832208e.png">
<p>See the paper. The text &ldquo;This bird has black wings and white abdomen&rdquo; produces this image.</p>
<h4 id="9-progan-progressive-growing-of-generative-adversarial-networks">9. ProGAN (Progressive growing of Generative Adversarial Networks)</h4>
<ul>
<li><a href="https://arxiv.org/abs/1710.10196">Progressive Growing of GANs for Improved Quality, Stability, and Variation</a></li>
<li><a href="https://github.com/tkarras/progressive_growing_of_gans">Code</a></li>
<li>Point: We gradually learned from low resolution images and succeeded in gradually increasing the resolution.</li>
<li>Recommended References: <a href="https://www.st-hakky-blog.com/entry/2017/11/22/173112">I summarized because I read [Progressive Growing of GANs for Improved Quality, Stability, and Variation]</a>
<img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/496974/17d66a1a-6a89-f15a-c8bc-fed97a76cc71.jpeg" alt="ProGAN.jpg"></li>
</ul>
<h4 id="10-sagan-self-attention-generative-adversarial-networks">10. SAGAN (Self-Attention Generative Adversarial Networks)</h4>
<ul>
<li><a href="https://arxiv.org/abs/1805.08318v1">Self-Attention Generative Adversarial Networks</a></li>
<li><a href="https://github.com/heykeetae/Self-Attention-GAN">Code</a></li>
<li>Point: The accuracy of the generated image was improved by using the following method
-Considering the processing status of the entire image by introducing Self Attention
-Apply Spectral Normalization to both Discriminator and Generator</li>
<li>Recommended reference: <a href="https://urusulambda.wordpress.com/2018/07/15/saganself-attention-generative-adversarial-network%E3%81%AEself-attention%E6%A9%9F%E6%A7%8B%E3%82%92%E3%81%96%E3%81%A3%E3%81%8F%E3%82%8A%E7%90%86%E8%A7%A3%E3%81%97%E3%81%9F/">A thorough understanding of the Self Attention mechanism of SAGAN (Self Attention Generative Adversarial Network)</a>(Asthenameimplies,thewholepictureiseasytograsp)</li>
</ul>
<img width="981" alt="Screen Shot 2019-11-29 at 2.02.29 AM.png" src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/496974/ef7a86e7-83fe-167c-7803-da9554e2bd17.png">
<p>See the paper. The structure of the Self Attention mechanism is like this.</p>
<h4 id="11-biggan-big-generative-adversarial-networks">11. BigGAN (Big Generative Adversarial Networks)</h4>
<ul>
<li><a href="https://arxiv.org/abs/1809.11096v2">Large Scale GAN Training for High Fidelity Natural Image Synthesis</a></li>
<li><a href="https://github.com/huggingface/pytorch-pretrained-BigGAN">Code</a></li>
<li>Point: An improved version of SAGAN. Preventing over-learning by using orthogonal regularization in the Generator, it is now possible to generate high resolution images up to 512 × 512 pixels</li>
<li>Recommended reference: <a href="https://medium.com/@crosssceneofwindff/large-scale-gan-training-for-high-fidelity-natural-image-synthesis-about-biggan-b1f7c02228a4">“Large Scale GAN Training for High Fidelity Natural Image Synthesis” -About BigGAN</a>(Japanese)</li>
</ul>
<img width="941" alt="Screen Shot 2019-11-29 at 6.19.47 PM.png" src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/496974/14040ddd-990b-2597-6b94-52f453cac12c.png">
<p>It is an image generated by BigGAN in the paper. You can see it&rsquo;s high quality!</p>
<h4 id="12-stylegan-style-based-generative-adversarial-networks">12. StyleGAN (Style-based Generative Adversarial Networks)</h4>
<ul>
<li><a href="https://arxiv.org/abs/1812.04948">
A Style-Based Generator Architecture for Generative Adversarial Networks</a></li>
<li><a href="https://github.com/NVlabs/stylegan">Code</a></li>
<li>Point: We succeeded in generating a high-resolution image with a size of 1024 x 1024 by devising a new generator that incorporates style conversion.</li>
<li>Recommended references:
[StyleGAN &ldquo;The days when photographs become evidence are over.&quot;](<a href="https://qiita.com/Phoeboooo/items/7be15acb960837adab21#%E5%85%A8%E4%BD%93%E3%81%BE%E3(%81%A8%E3%82%81)">https://qiita.com/Phoeboooo/items/7be15acb960837adab21#%E5%85%A8%E4%BD%93%E3%81%BE%E3(%81%A8%E3%82%81)</a></li>
</ul>
<img width="458" alt="Screen Shot 2019-11-29 at 1.51.26 AM.png" src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/496974/fe544fbd-a0d8-9547-7b41-7c5261a5c774.png">
<h3 id="at-the-end-upside_down">At the end: upside_down:</h3>
<p>Thank you for reading to the end. If you want to know more about GAN and want to know more about it and you can say it in English, please read this <a href="https://arxiv.org/pdf/1701.00160.pdf">NIPS 2016 Tutorial: Generative Adversarial Networks</a>.
I hope you helped give you an overall picture of GAN. (Please let me know if you have any corrections!)</p>

    </article>
  </div>

  
  
  
  <script>
  window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
  ga('create', 'UA-169005401-1', 'auto');
  ga('send', 'pageview');
  </script>
  <script async src='https://www.google-analytics.com/analytics.js'></script>
  

  

  
<link rel="stylesheet" type="text/css" href="//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.0/cookieconsent.min.css" />
<script src="//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.0/cookieconsent.min.js"></script>
<script>
window.addEventListener("load", function(){
window.cookieconsent.initialise({
  "palette": {
    "popup": {
      "background": "#216942",
      "text": "#b2d192"
    },
    "button": {
      "background": "#afed71"
    }
  }
})});
</script>

</body>

</html>
