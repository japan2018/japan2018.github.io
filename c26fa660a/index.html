<!doctype html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  
  <meta name="generator" content="Hugo 0.72.0" />

  
  <meta name="description" content="Some description">
  

  
  <link rel="apple-touch-icon" sizes="180x180" href="https://memotut.com/apple-touch-icon.png">

  
  <link rel="icon" type="image/png" sizes="32x32" href="https://memotut.com/favicon-32x32.png">

  
  <link rel="icon" type="image/png" sizes="16x16" href="https://memotut.com/favicon-16x16.png">

  
  <link rel="manifest" href="https://memotut.com/site.webmanifest">

  
  <link rel="mask-icon" href="https://memotut.com/safari-pinned-tab.svg" color="#5bbad5">

  <meta name="msapplication-TileColor" content="#da532c">

  <meta name="theme-color" content="#ffffff">

  
  <link rel="stylesheet" href="https://memotut.com/css/bootstrap.min.css" />

  
  <title>[Python] Let&#39;s use the distributed expression of words quickly with fastText! | Memo Tut</title>
  

  <style>
body {
  min-width: 300px;
}

.custom-navbar {
  margin-bottom: 1em;
  height: 60px;
}

.custom-navbar a {
  display: inline-block; 
  padding: 18px 0;
  margin-right: 1em; 
  font-weight: bold; 
}

.custom-navbar a:hover,
.custom-navbar a:focus {
  text-decoration: none; 
}

@media print {
  .custom-navbar {
    display: none;
  }
}

article {
  padding-bottom: 1em;
}

img {
  max-width: 100%;
}


body {
  background-color: #fff;
}



body {
  color: #212529;
}



a {
  color: #007bff;
}



a:hover,
a:focus {
  color: #0056b3;
}



.custom-navbar {
  background-color: #212529;
}



.custom-navbar a {
  color: rgba(255, 255, 255, 0.75);
}



.custom-navbar a:hover,
.custom-navbar a:focus {
  color: rgba(255, 255, 255, 1);
}



.container {
  max-width: 800px;
}



pre {
  display: block;
  padding: 9.5px;
  word-break: break-all;
  word-wrap: break-word;
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 4px;
}

pre code {
  padding: 0;
  font-size: inherit;
  color: inherit; 
  white-space: pre-wrap;
  background-color: transparent;
  border: none;
  border-radius: 0;
}

code {
  padding: 2px 4px;
  color: inherit; 
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 4px;
  font-size: .9em;
}



blockquote,
.blockquote {
  padding: 10px 20px;
  margin: 0 0 20px;
  font-size: 1em;
  border-left: 5px solid #6c757d;
}

</style>
</head>

<body>
  <nav class="custom-navbar">
  <div class="container">
    
    <a href="/">Posts</a>
    
    <a href="/tags/">Tags</a>
    
    <a href="/about/">About</a>
    
    <a href="/index.xml">RSS</a>
    
  </div>
</nav>
  
  <div class="container">
    <article>
      <h1>[Python] Let&rsquo;s use the distributed expression of words quickly with fastText!</h1>
<p>
  <small class="text-secondary">
  
  
  Dec 5, 2019
  </small>
  

<small><code><a href="https://memotut.com/tags/python">Python</a></code></small>


<small><code><a href="https://memotut.com/tags/natural-language-processing"> natural language processing</a></code></small>


<small><code><a href="https://memotut.com/tags/gensim"> gensim</a></code></small>


<small><code><a href="https://memotut.com/tags/fasttext"> fastText</a></code></small>


<small><code><a href="https://memotut.com/tags/torchtext"> torchtext</a></code></small>

</p>
<pre><code>In this article, we'll look at how to use fastText to get a distributed word representation. [Article on the previous day](https://qiita.com/BlueRayi/items/eb7949594839771c1b70) I thought it would be nice if I could talk about a little link.
</code></pre>
<p>What is #fastText</p>
<p><a href="https://github.com/facebookresearch/fastText#text-classification">fastText</a>isamethodtoacquiredistributedexpressions(wordsexpressedasnumbers)announcedbyFacebook.ItisbasedonthefamiliarWord2Vec(CBOW/skip-gram). As for Word2Vec, it doesn&rsquo;t need to be explained because it has been changed.</p>
<p>Paper: <a href="https://arxiv.org/pdf/1607.04606v1.pdf">Enriching Word Vectors with Subword Information</a></p>
<p>The difference between Word2Vec and fastText is how to take a vector. By incorporating a subword system, we are drawing words that are close to each other in usage.</p>
<p>In Word2Vec, go and goes were completely different words. But fastText takes this into account and makes go and goes meaningful as the same component. Naturally, it will be strong against unknown words too!</p>
<p>For details, we recommend the papers and the following materials.</p>
<ul>
<li><a href="https://qiita.com/icoxfog417/items/42a95b279c0b7ad26589">Getting distributed words in Fast with Facebook fastText-Qiita</a></li>
<li><a href="https://qiita.com/n_kats_/items/2691c08639468e30abdd">Mechanism of fast and rumored fastText-Qiita</a></li>
</ul>
<p>Play with #FastText</p>
<p>The purpose of this time is to &ldquo;play with distributed expressions of words quickly using Python and FastText&rdquo;.</p>
<h2 id="environment">Environment</h2>
<p>e? Can&rsquo;t you even introduce Python? Then use <a href="https://colab.research.google.com/notebooks/welcome.ipynb?hl=ja">Google Colaboratory</a>.(Hereinaftercolab)</p>
<p>colab is the strongest free environment where anyone with a Google account can easily use Python and GPU.</p>
<h2 id="crisp-fasttext">Crisp fastText</h2>
<p>A learning model is required to use fastText. You can collect and train the data yourself, but here we will use the trained model.</p>
<p>Trained model: <a href="https://qiita.com/Hironsan/items/513b9f93752ecee9e670">FastText trained model released-Qiita</a></p>
<p>Just drop the model into Google Drive. Since it is a big deal, let&rsquo;s unzip it from the drive, including instructions on how to use colab!</p>
<p>A mount is required to access the drive with colab. To do this, execute the following code, but if you press the &ldquo;Mount Drive&rdquo; button, the same code will appear, so execute it.</p>
<pre><code>from google.colab import drive
drive.mount('/content/drive')
</code></pre><p>After that, I think that the authorization code will appear when I access the URL and log in to the account, so I just copy and paste it with colab. Easy.</p>
<p>All you have to do is unzip the previous model!</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">%</span>cd <span style="color:#f92672">/</span>content<span style="color:#f92672">/</span>drive<span style="color:#f92672">/</span>My Drive<span style="color:#f92672">/</span>data
<span style="color:#960050;background-color:#1e0010">!</span>unzip vector_neologd<span style="color:#f92672">.</span>zip
</code></pre></div><p>For path, specify the location of the trained model that you raised to the drive.</p>
<p>Let&rsquo;s look for similar words.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">
<span style="color:#f92672">import</span> gensim

model <span style="color:#f92672">=</span> gensim<span style="color:#f92672">.</span>models<span style="color:#f92672">.</span>KeyedVectors<span style="color:#f92672">.</span>load_word2vec_format(<span style="color:#e6db74">&#39;model.vec&#39;</span>, binary<span style="color:#f92672">=</span>False)

model<span style="color:#f92672">.</span>most_similar(positive<span style="color:#f92672">=</span>[<span style="color:#e6db74">&#39;natural language processing&#39;</span>])
</code></pre></div><p>result.</p>
<pre><code>[('Natural language understanding', 0.7600098848342896),
 ('Natural language', 0.7503659725189209),
 ('Computational Linguistics', 0.7258570194244385),
 ('Automatic programming', 0.6848069429397583),
 ('Text mining', 0.6811494827270508),
 ('Computer language', 0.6618390083312988),
 ('Metaprogramming', 0.658093273639679),
 ('Web programming', 0.6488876342773438),
 ('Morphological analysis', 0.6479052901268005),
 ('Corpus Linguistics', 0.6465639472007751)]]
</code></pre><p>Surprisingly fun to play with.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">
model<span style="color:#f92672">.</span>most_similar(positive<span style="color:#f92672">=</span>[<span style="color:#e6db74">&#39;friend&#39;</span>], negative<span style="color:#f92672">=</span>[<span style="color:#e6db74">&#39;friendship&#39;</span>])
</code></pre></div><pre><code>[('Acquaintance', 0.4586910605430603),
 ('Home', 0.35488438606262207),
 ('Acquaintance', 0.329221248626709),
 ('Dip', 0.3212822675704956),
 ('Relatives', 0.31865695118904114),
 ('Meeting', 0.3158203959465027),
 ('Home', 0.31503963470458984),
 ('Invitation', 0.302945077419281),
 ('Prosperous', 0.30250048637390137),
 ('Coworkers', 0.29792869091033936)]]
</code></pre><h2 id="crisp-fasttext-1">Crisp? fastText</h2>
<p>It&rsquo;s too crunchy, so let&rsquo;s limit the vocabulary and try the same thing. It is an option that there is also such a method.</p>
<p>This time, we will deal with Japanese text, but unlike English, each word is not divided by spaces, so you must first perform preprocessing (dividing).</p>
<p>The implementation is based on <a href="https://github.com/YutaroOgawa/pytorch_advanced">Book &ldquo;Learning While Making! Advanced Deep Learning with PyTorch</a>.</p>
<p>We use janome here.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#960050;background-color:#1e0010">!</span>pip install janome
</code></pre></div><p>Let&rsquo;s make a book.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">from</span> janome.tokenizer <span style="color:#f92672">import</span> Tokenizer

j_t <span style="color:#f92672">=</span> Tokenizer()

<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">tokenizer_janome</span>(text):
    <span style="color:#66d9ef">return</span> [tok <span style="color:#66d9ef">for</span> tok <span style="color:#f92672">in</span> j_t<span style="color:#f92672">.</span>tokenize(text, wakati<span style="color:#f92672">=</span>True)]
</code></pre></div><p>You can criss-cross with janome alone, but let&rsquo;s do some simple pre-processing.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> re
<span style="color:#f92672">import</span> unicodedata
<span style="color:#f92672">import</span> string


<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">format_text</span>(text):
    text <span style="color:#f92672">=</span> unicodedata<span style="color:#f92672">.</span>normalize(<span style="color:#e6db74">&#34;NFKC&#34;</span>, text)
    table <span style="color:#f92672">=</span> str<span style="color:#f92672">.</span>maketrans(<span style="color:#e6db74">&#34;&#34;</span>, <span style="color:#e6db74">&#34;&#34;</span>, string<span style="color:#f92672">.</span>punctuation <span style="color:#f92672">+</span> <span style="color:#e6db74">&#34;&#34;</span> <span style="color:#e6db74">&#34;,.・&#34;</span>)
    text <span style="color:#f92672">=</span> text<span style="color:#f92672">.</span>translate(table)

    <span style="color:#66d9ef">return</span> text


<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">preprocessing</span>(text):
    <span style="color:#e6db74">&#34;&#34;&#34;
</span><span style="color:#e6db74">    Preprocessing function
</span><span style="color:#e6db74">    &#34;&#34;&#34;</span>
    text<span style="color:#f92672">.</span>translate(str<span style="color:#f92672">.</span>maketrans((chr(<span style="color:#ae81ff">0xFF01</span> <span style="color:#f92672">+</span> i): chr(<span style="color:#ae81ff">0x21</span> <span style="color:#f92672">+</span> i) <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">94</span>)}))) <span style="color:#75715e"># full width → half width</span>

    text <span style="color:#f92672">=</span> text<span style="color:#f92672">.</span>lower() <span style="color:#75715e"># uppercase → lowercase</span>

    text <span style="color:#f92672">=</span> re<span style="color:#f92672">.</span>sub(<span style="color:#e6db74">&#39;</span><span style="color:#ae81ff">\r</span><span style="color:#e6db74">&#39;</span>, <span style="color:#e6db74">``</span>, text)
    text <span style="color:#f92672">=</span> re<span style="color:#f92672">.</span>sub(<span style="color:#e6db74">&#39;</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&#39;</span>, <span style="color:#e6db74">``</span>, text)
    text <span style="color:#f92672">=</span> re<span style="color:#f92672">.</span>sub(<span style="color:#e6db74">&#39;&#39;</span>, <span style="color:#e6db74">``</span>, text)
    text <span style="color:#f92672">=</span> re<span style="color:#f92672">.</span>sub(<span style="color:#e6db74">&#39;,&#39;</span>, <span style="color:#e6db74">``</span>, text)

    text <span style="color:#f92672">=</span> re<span style="color:#f92672">.</span>sub(<span style="color:#e6db74">r</span><span style="color:#e6db74">&#39;[0-9 0-9]&#39;</span>, <span style="color:#e6db74">``</span>, text) <span style="color:#75715e"># remove numbers</span>
    text <span style="color:#f92672">=</span> re<span style="color:#f92672">.</span>sub(<span style="color:#e6db74">r</span><span style="color:#e6db74">&#39;[!-/:-@[-`{-~]&#39;</span>, <span style="color:#e6db74">``</span>, text) <span style="color:#75715e"># Remove single-byte symbols</span>
    text <span style="color:#f92672">=</span> re<span style="color:#f92672">.</span>sub(<span style="color:#e6db74">r</span><span style="color:#e6db74">&#39;/[!-/:-@[-｀{-~,-~&#34;&#39;</span><span style="color:#960050;background-color:#1e0010">・</span>]<span style="color:#e6db74">&#39;, ``, text) # Remove full-width symbols</span>
    text <span style="color:#f92672">=</span> format_text(text) <span style="color:#75715e"># Remove full-width symbol</span>

    <span style="color:#66d9ef">return</span> text


<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">tokenizer_with_preprocessing</span>(text):
    text <span style="color:#f92672">=</span> preprocessing(text)
    text <span style="color:#f92672">=</span> tokenizer_janome(text)
    
    <span style="color:#66d9ef">return</span> text
</code></pre></div><p>As I wrote in the comments, I&rsquo;ve done some preprocessing. Of course, what you need to do in the pre-processing should change depending on the task and usage.</p>
<p>Now, let&rsquo;s try inserting a suitable sentence.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">
text <span style="color:#f92672">=</span><span style="color:#e6db74">&#39;I want to use fastText to acquire distributed expressions this time! ! ! ?&#39;</span>
<span style="color:#66d9ef">print</span>(tokenizer_with_preprocessing(text))
</code></pre></div><p>result.</p>
<pre><code>['This time','is','fasttext','is','use','te','dispersion','expression','is','acquire','shi','tai']
</code></pre><p>Sounds good.</p>
<p>Then use <a href="https://torchtext.readthedocs.io/en/latest/vocab.html#fasttext">torchtext</a>tomakethingseasier.Fortorchtext,<a href="https://qiita.com/itok_msi/items/1f3746f7e89a19dafac5">EasyDeepNaturalLanguageProcessingwithtorchtext-Qiita</a> is detailed.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">
<span style="color:#f92672">import</span> torchtext

max_length <span style="color:#f92672">=</span> <span style="color:#ae81ff">25</span>
TEXT <span style="color:#f92672">=</span> torchtext<span style="color:#f92672">.</span>data<span style="color:#f92672">.</span>Field(sequential<span style="color:#f92672">=</span>True, tokenize<span style="color:#f92672">=</span>tokenizer_with_preprocessing,
                            use_vocab<span style="color:#f92672">=</span>True, lower<span style="color:#f92672">=</span>True, include_lengths<span style="color:#f92672">=</span>True,
                            batch_first<span style="color:#f92672">=</span>True, fix_length<span style="color:#f92672">=</span>max_length)
LABEL <span style="color:#f92672">=</span> torchtext<span style="color:#f92672">.</span>data<span style="color:#f92672">.</span>Field(sequential<span style="color:#f92672">=</span>False, use_vocab<span style="color:#f92672">=</span>False)train_ds, val_ds, test_ds <span style="color:#f92672">=</span> torchtext<span style="color:#f92672">.</span>data<span style="color:#f92672">.</span>TabularDataset<span style="color:#f92672">.</span>splits(
    path<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;./tsv/&#39;</span>, train<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;train.tsv&#39;</span>,
    validation<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;val.tsv&#39;</span>, test<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;test.tsv&#39;</span>, format<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;tsv&#39;</span>,
    fields<span style="color:#f92672">=</span>[(<span style="color:#e6db74">&#39;Text&#39;</span>, TEXT), (<span style="color:#e6db74">&#39;Label&#39;</span>, LABEL)])
</code></pre></div><p>I prepared train.tsv / test.tsv / val.tsv. This is a tsv file that is made by dividing the text of &ldquo;What is fastText&rdquo; in this paper into three parts. Again, I recommend the link above because it is detailed.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">from</span> torchtext.vocab <span style="color:#f92672">import</span> Vectors

vectors <span style="color:#f92672">=</span> Vectors(name<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;model.vec&#39;</span>)

<span style="color:#75715e"># Create a vectorized version of the vocabulary</span>
TEXT<span style="color:#f92672">.</span>build_vocab(train_ds, vectors<span style="color:#f92672">=</span>vectors, min_freq<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)

<span style="color:#75715e"># Check vocabulary vector</span>
<span style="color:#66d9ef">print</span>(TEXT<span style="color:#f92672">.</span>vocab<span style="color:#f92672">.</span>vectors<span style="color:#f92672">.</span>shape) <span style="color:#75715e"># 52 words are represented by a 300-dimensional vector</span>
TEXT<span style="color:#f92672">.</span>vocab<span style="color:#f92672">.</span>vectors

<span style="color:#75715e">#Check the order of words in the vocabulary</span>
TEXT<span style="color:#f92672">.</span>vocab<span style="color:#f92672">.</span>stoi
</code></pre></div><p>Now that we are ready, let&rsquo;s calculate the similarity of each word in the vocabulary. Let&rsquo;s see if the three words are similar to the word.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">
<span style="color:#f92672">import</span> torch.nn.functional <span style="color:#f92672">as</span> F

tensor_calc <span style="color:#f92672">=</span> TEXT<span style="color:#f92672">.</span>vocab<span style="color:#f92672">.</span>vectors[TEXT<span style="color:#f92672">.</span>vocab<span style="color:#f92672">.</span>stoi[<span style="color:#e6db74">&#39;word&#39;</span>]]

<span style="color:#75715e">#Cosine similarity</span>
<span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;Paper&#34;</span>, F<span style="color:#f92672">.</span>cosine_similarity(tensor_calc, TEXT<span style="color:#f92672">.</span>vocab<span style="color:#f92672">.</span>vectors[TEXT<span style="color:#f92672">.</span>vocab<span style="color:#f92672">.</span>stoi[<span style="color:#e6db74">&#39;Paper&#39;</span>]], dim<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>))
<span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;word&#34;</span>, F<span style="color:#f92672">.</span>cosine_similarity(tensor_calc, TEXT<span style="color:#f92672">.</span>vocab<span style="color:#f92672">.</span>vectors[TEXT<span style="color:#f92672">.</span>vocab<span style="color:#f92672">.</span>stoi[<span style="color:#e6db74">&#39;word&#39;</span>]], dim<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>))
<span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;vector&#34;</span>, F<span style="color:#f92672">.</span>cosine_similarity(tensor_calc, TEXT<span style="color:#f92672">.</span>vocab<span style="color:#f92672">.</span>vectors[TEXT<span style="color:#f92672">.</span>vocab<span style="color:#f92672">.</span>stoi[<span style="color:#e6db74">&#39;vector&#39;</span>]], dim<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>))
</code></pre></div><p>result.</p>
<pre><code>Paper tensor (0.3089)
Word tensor(0.3704)
Vector tensor(0.3265)
</code></pre><p>#If you want to know more</p>
<p>FastText is not as good as Word2Vec, but there are various articles, so I would like to conclude with a particularly recommended one (references).</p>
<h2 id="document-classification-with-fasttext">Document classification with fastText</h2>
<p>Additions. I have introduced various articles before, but the following articles that have been recently posted are the most recommended. There is a comparison with Watson in the title, but since it has the code of fastText that works with Google colab, it just pops up. The explanation of the format of the learning data is also polite, and it would be nice to glance at the article and code.</p>
<ul>
<li><a href="https://www.cresco.co.jp/blog/entry/11577/">Document classification comparison between fasttext and Watson Natural Language Classifier</a></li>
</ul>
<h2 id="embedding-words-other-than-fasttext">Embedding words other than fastText</h2>
<ul>
<li><a href="https://qiita.com/Hironsan/items/8f7d35f0a36e0f99752c">List of word embedding vectors that can be used immediately-Qiita</a></li>
</ul>
<p>If you want to know more recent word embedding, I think there will be many if you googling with ELMo or BERT. Of course, the more recent models, the harder it gets.</p>
<h1 id="next-time">next time?</h1>
<p>This is @takashi1029!</p>
<ul>
<li><a href="http://acro-engineer.hatenablog.com/entry/2019/12/06/120000">The first step of dependency search with GiNZA+Elasticsearch-Taste of Tech Topics</a></li>
</ul>

    </article>
  </div>

  
  
  
  <script>
  window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
  ga('create', 'UA-169005401-1', 'auto');
  ga('send', 'pageview');
  </script>
  <script async src='https://www.google-analytics.com/analytics.js'></script>
  

  

  
<link rel="stylesheet" type="text/css" href="//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.0/cookieconsent.min.css" />
<script src="//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.0/cookieconsent.min.js"></script>
<script>
window.addEventListener("load", function(){
window.cookieconsent.initialise({
  "palette": {
    "popup": {
      "background": "#216942",
      "text": "#b2d192"
    },
    "button": {
      "background": "#afed71"
    }
  }
})});
</script>

</body>

</html>
