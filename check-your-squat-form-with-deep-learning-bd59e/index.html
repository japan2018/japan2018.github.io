<!doctype html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  
  <meta name="generator" content="Hugo 0.72.0" />

  
  <meta name="description" content="Some description">
  

  
  <link rel="apple-touch-icon" sizes="180x180" href="https://memotut.com/apple-touch-icon.png">

  
  <link rel="icon" type="image/png" sizes="32x32" href="https://memotut.com/favicon-32x32.png">

  
  <link rel="icon" type="image/png" sizes="16x16" href="https://memotut.com/favicon-16x16.png">

  
  <link rel="manifest" href="https://memotut.com/site.webmanifest">

  
  <link rel="mask-icon" href="https://memotut.com/safari-pinned-tab.svg" color="#5bbad5">

  <meta name="msapplication-TileColor" content="#da532c">

  <meta name="theme-color" content="#ffffff">

  
  <link rel="stylesheet" href="https://memotut.com/css/bootstrap.min.css" />

  
  <title>Check your squat form with deep learning | Memo Tut</title>
  

  <style>
body {
  min-width: 300px;
}

.custom-navbar {
  margin-bottom: 1em;
  height: 60px;
}

.custom-navbar a {
  display: inline-block; 
  padding: 18px 0;
  margin-right: 1em; 
  font-weight: bold; 
}

.custom-navbar a:hover,
.custom-navbar a:focus {
  text-decoration: none; 
}

@media print {
  .custom-navbar {
    display: none;
  }
}

article {
  padding-bottom: 1em;
}

img {
  max-width: 100%;
}


body {
  background-color: #fff;
}



body {
  color: #212529;
}



a {
  color: #007bff;
}



a:hover,
a:focus {
  color: #0056b3;
}



.custom-navbar {
  background-color: #212529;
}



.custom-navbar a {
  color: rgba(255, 255, 255, 0.75);
}



.custom-navbar a:hover,
.custom-navbar a:focus {
  color: rgba(255, 255, 255, 1);
}



.container {
  max-width: 800px;
}



pre {
  display: block;
  padding: 9.5px;
  word-break: break-all;
  word-wrap: break-word;
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 4px;
}

pre code {
  padding: 0;
  font-size: inherit;
  color: inherit; 
  white-space: pre-wrap;
  background-color: transparent;
  border: none;
  border-radius: 0;
}

code {
  padding: 2px 4px;
  color: inherit; 
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 4px;
  font-size: .9em;
}



blockquote,
.blockquote {
  padding: 10px 20px;
  margin: 0 0 20px;
  font-size: 1em;
  border-left: 5px solid #6c757d;
}

</style>
</head>

<body>
  <nav class="custom-navbar">
  <div class="container">
    
    <a href="/">Posts</a>
    
    <a href="/tags/">Tags</a>
    
    <a href="/about/">About</a>
    
    <a href="/index.xml">RSS</a>
    
  </div>
</nav>
  
  <div class="container">
    <article>
      <h1>Check your squat form with deep learning</h1>
<p>
  <small class="text-secondary">
  
  
  Apr 11, 2020
  </small>
  

<small><code><a href="https://memotut.com/tags/python">Python</a></code></small>


<small><code><a href="https://memotut.com/tags/deeplearning"> DeepLearning</a></code></small>


<small><code><a href="https://memotut.com/tags/muscle-training"> Muscle training</a></code></small>


<small><code><a href="https://memotut.com/tags/deep-learning"> Deep learning</a></code></small>


<small><code><a href="https://memotut.com/tags/pytorch"> PyTorch</a></code></small>

</p>
<pre><code>**The video used in this article was recorded a long time ago, and I currently refrain from exercising at the gym. **
</code></pre>
<p>#Introduction
This is the second edition of &ldquo;Muscle Training x Deep Learning&rdquo;. The first is here (<a href="https://qiita.com/shuheilocale/items/f0e60416da3bdde38cfc">Deep learning made it easier to see the time-lapse of physical changes</a>).
&ldquo;Form check&rdquo; that many trainees (people who love muscle training) have become habitual. You can fix your smartphone during the training, take a selfie, and look back later to know the habit of your form and improve the quality of your training afterwards!
This article writes about the story of checking the form using deep learning by focusing on &ldquo;squat (king of muscle training)&rdquo; among muscle training.</p>
<h2 id="first-from-the-result">First from the result</h2>
<p><img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/22648/631e2623-998a-bfda-6f01-d183540e1655.gif" alt="ezgif.com-optimize.gif">
*Compressed for publication.</p>
<p>The source code is available on Google Colab&rsquo;s notebook.
<a href="https://colab.research.google.com/drive/18bFHGZ415T6emBoPHacrMyEckPGW9VCv">https://colab.research.google.com/drive/18bFHGZ415T6emBoPHacrMyEckPGW9VCv</a>
You can also try using your own video, so please give it a try.</p>
<h1 id="table-of-contents">table of contents</h1>
<ul>
<li><a href="#1Attitudeestimation">1. Attitude estimation</a>
<ul>
<li><a href="#1-1Trytomove">1-1. Try to move</a></li>
<li><a href="#1-2HeatMap">1-2. Heat Map</a></li>
<li><a href="#1-3Attitudeestimation">1-3. Attitude estimation</a></li>
</ul>
</li>
<li><a href="#2Applicationtosquatformcheck">2. Application to squat form check</a>
<ul>
<li><a href="#2-1Importantfactorsinsquats">2-1. Important factors in squats</a></li>
<li><a href="#2-2Makeformchecklike">2-2. Make form check like</a></li>
</ul>
</li>
<li><a href="#Summary">Summary</a></li>
</ul>
<h1 id="1-posture-estimation">1. Posture estimation</h1>
<p>Posture estimation is a technology that estimates the posture of a human body or animal. For example, in the case of a human body, the posture can be expressed by detecting the neck, hips, and knees and connecting them. Well-known one is <a href="https://github.com/CMU-Perceptual-Computing-Lab/openpose">OpenPose</a>.</p>
<p><img src="https://github.com/CMU-Perceptual-Computing-Lab/openpose/blob/master/doc/media/dance_foot.gif?raw=true" alt="https://github.com/CMU-Perceptual-Computing-Lab/openpose/blob/master/doc/media/dance_foot.gif?raw=true">
<em>Quoted from github on openpose</em></p>
<h2 id="1-1-try-to-move">1-1. Try to move</h2>
<p>This time, I referred to “[Learn while making! Development deep learning by PyTorch]” (<a href="https://www.amazon.co.jp/%E3%81%A4%E3%81%8F%E3%82%8A">https://www.amazon.co.jp/%E3%81%A4%E3%81%8F%E3%82%8A</a> %E3%81%AA%E3%81%8C%E3%82%89%E5%AD%A6%E3%81%B6%EF%BC%81PyTorch%E3%81%AB%E3%82%88%E3 %82%8B%E7%99%BA%E5%B1%95%E3%83%87%E3%82%A3%E3%83%BC%E3%83%97%E3%83%A9%E3%83 %BC%E3%83%8B%E3%83%B3%E3%82%B0-%E5%B0%8F%E5%B7%9D-%E9%9B%84%E5%A4%AA%E9%83 %8E-ebook/dp/B07VPDVNKW/)” <a href="https://github.com/YutaroOgawa/pytorch_advanced">GitHub page</a>. The above implementation was very helpful as I wanted to implement it on PyTorch x Google Colab.</p>
<p>For the time being, I will try posture estimation from a single image using free materials. The source is a rough version. See the Google Colab notebook for more information.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">create_model</span>(weight_path):
  <span style="color:#e6db74">&#34;&#34;&#34;
</span><span style="color:#e6db74">  Create a model.
</span><span style="color:#e6db74">  Network layer names differ between trained model and OpenPoseNet
</span><span style="color:#e6db74">  (For example, module.model0.0.weight and model0.model.0.weight)
</span><span style="color:#e6db74">  Corresponding and loading
</span><span style="color:#e6db74">  &#34;&#34;&#34;</span>

  <span style="color:#75715e">#Model definition</span>
  model <span style="color:#f92672">=</span> OpenPoseNet()

  <span style="color:#75715e"># Load learned parameters</span>
  net_weights <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>load(
      weights_path, map_location<span style="color:#f92672">=</span>{<span style="color:#e6db74">&#39;cuda:0&#39;</span>:<span style="color:#e6db74">&#39;cpu&#39;</span>})
  keys <span style="color:#f92672">=</span> list(net_weights<span style="color:#f92672">.</span>keys())

  weights_load <span style="color:#f92672">=</span> {}

  <span style="color:#75715e"># Load the contents of OpenPoseNet</span>
  <span style="color:#75715e"># Copy to parameter name model.state_dict().keys()</span>
  <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(len(keys)):
    weights_load[list(model<span style="color:#f92672">.</span>state_dict()<span style="color:#f92672">.</span>keys())[i]
                 ] <span style="color:#f92672">=</span> net_weights[list(keys)[i]]

  <span style="color:#75715e"># Give the copied contents to the model</span>
  state <span style="color:#f92672">=</span> model<span style="color:#f92672">.</span>state_dict()
  state<span style="color:#f92672">.</span>update(weights_load)
  model<span style="color:#f92672">.</span>load_state_dict(state)
  <span style="color:#66d9ef">return</span> model

model <span style="color:#f92672">=</span> create_model(weights_path)
model<span style="color:#f92672">.</span>to(device)

model<span style="color:#f92672">.</span>eval()
<span style="color:#66d9ef">with</span> torch<span style="color:#f92672">.</span>no_grad():
  predicted_outputs, _ <span style="color:#f92672">=</span> model(img<span style="color:#f92672">.</span>to(device))
  pafs <span style="color:#f92672">=</span> predicted_outputs[<span style="color:#ae81ff">0</span>][<span style="color:#ae81ff">0</span>]<span style="color:#f92672">.</span>cpu()<span style="color:#f92672">.</span>detach()<span style="color:#f92672">.</span>numpy()<span style="color:#f92672">.</span>transpose(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">0</span>)
  heatmaps <span style="color:#f92672">=</span> predicted_outputs[<span style="color:#ae81ff">1</span>][<span style="color:#ae81ff">0</span>]<span style="color:#f92672">.</span>cpu()<span style="color:#f92672">.</span>detach()<span style="color:#f92672">.</span>numpy()<span style="color:#f92672">.</span>transpose(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">0</span>)

pafs <span style="color:#f92672">=</span> cv2<span style="color:#f92672">.</span>resize(
    pafs, (test_img<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">1</span>], test_img<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>]), interpolation<span style="color:#f92672">=</span>cv2<span style="color:#f92672">.</span>INTER_CUBIC)
heatmaps <span style="color:#f92672">=</span> cv2<span style="color:#f92672">.</span>resize(
    heatmaps, (test_img<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">1</span>], test_img<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>]), interpolation<span style="color:#f92672">=</span>cv2<span style="color:#f92672">.</span>INTER_CUBIC)

_, result_img, _, _ <span style="color:#f92672">=</span> decode_pose(test_img, heatmaps, pafs)

cv2_imshow(result_img)
</code></pre></div><p>The results are as follows.
<img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/22648/0cbec73b-4420-db81-5168-0d4c09d5b8b0.png" alt="image.png">
<em><a href="https://jp.freepik.com/free-photo/full-length-of-young-concentrated-sportsman-doing-squats-during-useworkout-outdoors_2593727.htm#page=1&amp;query=squat&amp;position=17">Freepic.diller-Photos created by jp.freepik.com</a></em></p>
<p>It feels quite good. If you do this for each frame of the video, you can check the form. However, I don&rsquo;t want all of these joints, just the minimum necessary parts. This time,</p>
<ul>
<li>Neck</li>
<li>Buttocks (right)</li>
<li>Knee (right)</li>
<li>Ankle (right)</li>
</ul>
<p>I will narrow it down to just</p>
<h2 id="1-2-heat-map">1-2. Heat map</h2>
<p>In the model used, a heat map is output and postures are estimated by extracting each part from it. First, draw a heat map of the target part.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e">#1: Neck, 8: Butt (right), 9: Knee (right), 10: Ankle (right)</span>
necessary_parts<span style="color:#f92672">=</span>[<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">8</span>,<span style="color:#ae81ff">9</span>,<span style="color:#ae81ff">10</span>]
fig, ax <span style="color:#f92672">=</span> plt<span style="color:#f92672">.</span>subplots(<span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">2</span>, figsize<span style="color:#f92672">=</span>(<span style="color:#ae81ff">16</span>, <span style="color:#ae81ff">10</span>))

<span style="color:#66d9ef">for</span> i, part <span style="color:#f92672">in</span> enumerate(necessary_parts):
    heat_map <span style="color:#f92672">=</span> heatmaps[:, :, part]
    heat_map <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>uint8(cm<span style="color:#f92672">.</span>jet(heat_map)<span style="color:#f92672">*</span><span style="color:#ae81ff">255</span>)
    heat_map <span style="color:#f92672">=</span> cv2<span style="color:#f92672">.</span>cvtColor(heat_map, cv2<span style="color:#f92672">.</span>COLOR_RGBA2RGB)
    blend_img <span style="color:#f92672">=</span> cv2<span style="color:#f92672">.</span>addWeighted(test_img, <span style="color:#ae81ff">0.5</span>, heat_map, <span style="color:#ae81ff">0.5</span>, <span style="color:#ae81ff">0</span>)
    ax[int(i<span style="color:#f92672">/</span><span style="color:#ae81ff">2</span>), i<span style="color:#f92672">%</span><span style="color:#ae81ff">2</span>]<span style="color:#f92672">.</span>imshow(blend_img)

plt<span style="color:#f92672">.</span>show()

</code></pre></div><p>The results are as follows. I have been able to properly extract a specific part.</p>
<p><img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/22648/c8b13052-6b71-0a94-2b00-52bfee9c6a0a.png" alt="image.png"></p>
<h2 id="1-3-posture-estimation">1-3. Posture estimation</h2>
<p>Identify the joint position from the output heat map of the specific part. This time, on the assumption that only one person can be photographed per sheet, we will perform processing to leave only one point for each part and connect the specified parts.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">find_joint_coords</span>(heatmaps, necessary_parts, param <span style="color:#f92672">=</span> (<span style="color:#e6db74">&#39;thre1&#39;</span>: <span style="color:#ae81ff">0.1</span>,<span style="color:#e6db74">&#39;thre2&#39;</span>: <span style="color:#ae81ff">0.05</span>,<span style="color:#e6db74">&#39;thre3&#39;</span>: <span style="color:#ae81ff">0.5</span>}):
  <span style="color:#e6db74">&#34;&#34;&#34;
</span><span style="color:#e6db74">  Detect joint coordinates
</span><span style="color:#e6db74">  &#34;&#34;&#34;</span>
  
  joints <span style="color:#f92672">=</span> []
  <span style="color:#66d9ef">for</span> part <span style="color:#f92672">in</span> necessary_parts:
      heat_map <span style="color:#f92672">=</span> heatmaps[:, :, part]
      peaks <span style="color:#f92672">=</span> find_peaks(param, heat_map)
      <span style="color:#66d9ef">if</span> len(peaks) <span style="color:#f92672">==</span> <span style="color:#ae81ff">0</span>:
        joints<span style="color:#f92672">.</span>append(img, [np<span style="color:#f92672">.</span>nan, np<span style="color:#f92672">.</span>nan], [np<span style="color:#f92672">.</span>nan, np<span style="color:#f92672">.</span>nan])
      <span style="color:#75715e">#If there are two or more peaks, leave only the strongest place</span>
      <span style="color:#66d9ef">if</span> peaks<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>]<span style="color:#f92672">&gt;</span><span style="color:#ae81ff">1</span>:
        max_peak <span style="color:#f92672">=</span> None
        <span style="color:#66d9ef">for</span> peak <span style="color:#f92672">in</span> peaks:
          val <span style="color:#f92672">=</span> heat_map[peak[<span style="color:#ae81ff">1</span>], peak[<span style="color:#ae81ff">0</span>]]<span style="color:#66d9ef">if</span> max_peak <span style="color:#f92672">is</span> None <span style="color:#f92672">or</span> heat_map[max_peak[<span style="color:#ae81ff">1</span>], max_peak[<span style="color:#ae81ff">0</span>]] <span style="color:#f92672">&lt;</span>val:
            max_peak <span style="color:#f92672">=</span> peak
      <span style="color:#66d9ef">else</span>:
        max_peak <span style="color:#f92672">=</span> peaks[<span style="color:#ae81ff">0</span>]
      joints<span style="color:#f92672">.</span>append(max_peak)

  <span style="color:#66d9ef">return</span> joints

img <span style="color:#f92672">=</span> test_img<span style="color:#f92672">.</span>copy()
joints <span style="color:#f92672">=</span> find_joint_coords(heatmaps, necessary_parts)
<span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(len(joints)<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>):
  img <span style="color:#f92672">=</span> cv2<span style="color:#f92672">.</span>line(img,tuple(joints[i]<span style="color:#f92672">.</span>astype(int)),tuple(joints[i<span style="color:#f92672">+</span><span style="color:#ae81ff">1</span>]<span style="color:#f92672">.</span>astype(int)),(<span style="color:#ae81ff">255</span>,<span style="color:#ae81ff">0</span>,<span style="color:#ae81ff">0</span>),<span style="color:#ae81ff">3</span>)

cv2_imshow(img)
</code></pre></div><p>The results are as follows. feel well.</p>
<p><img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/22648/309b57c5-a240-273e-7445-1607566c7a3b.png" alt="image.png"></p>
<h1 id="2-application-to-squat-form-check">2. Application to squat form check</h1>
<p>I knew it would work, so I will apply it to my squat video.</p>
<h2 id="2-1-important-factors-in-squats">2-1. Important factors in squats</h2>
<p>The skeleton that forms the shin, thigh, and trunk differs from person to person, so the optimal form will naturally differ from person to person. I think it will be easier to imagine in the figure below.</p>
<p><img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/22648/eab56609-f6c0-1b06-57ba-62c51616cb29.png" alt="image.png"></p>
<p>In other words, the form that suits the person depends more on the skeletal proportion than on the flexibility and strength balance. Therefore, if you can grasp this skeletal proportion by yourself, you can prevent injuries and safely increase the weight of the squat. See YouTube and many other great web pages below for more information.</p>
<p><a href="http://www.youtube.com/watch?v=Av3LO2GwpAk"><img src="http://img.youtube.com/vi/Av3LO2GwpAk/0.jpg" alt="IMAGE ALT TEXT HERE"></a>
<a href="https://www.youtube.com/watch?v=Av3LO2GwpAk">Squats Part 1: Fold-Ability and Proportions</a></p>
<h2 id="2-2-make-it-like-a-form-check">2-2. Make it like a form check</h2>
<p>Since the joint coordinates can be obtained, find the joint angle and moment arm and arrange them in time series. The joint angle is the angle between two vectors that sandwich the joint. For the moment arm, the ideal center of gravity (midfoot) is obtained in a pseudo manner, and the distance is the perpendicular line passing through the joint with respect to the vertical straight line. This is calculated for each frame and connected to complete.
Finally, plot the angle and moment arm produced in each frame with matplotlib, paste it on the top of the screen, and add a nice graph that is kindly analyzed.</p>
<p>** * I will omit the source code because it will be long. Please refer to the notebook. **</p>
<p>The completed example is as follows (the frame is selected appropriately).</p>
<p><img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/22648/9747f06a-933d-3eb8-27d3-02d427085ffc.png" alt="image.png"></p>
<p>#Summary</p>
<p>Applying posture estimation using deep learning, we performed a squat form check. I think that the joint could be detected with high accuracy. From this result, you may be able to further improve the quality of squat by finding a player with a skeleton close to you and comparing the form with that player.</p>
<p>The issues include the following.</p>
<ul>
<li>Hard coded midfoot (ideal center of gravity)</li>
<li>Heavy processing
<ul>
<li>Uselessly detecting parts</li>
<li>Introduce a device such as Pointwise convolution</li>
<li>Learning also needs to be redone</li>
</ul>
</li>
<li>Cannot be detected when the plate is attached
<ul>
<li>This is a fatal issue, but I didn&rsquo;t like it because I didn&rsquo;t like it, so I&rsquo;m sorry.</li>
<li>As a response, train the data in the state of carrying the plate. It is also possible to detect the tip of the bar or face and estimate the neck from it.</li>
</ul>
</li>
</ul>
<p>↑ Above, muscle training you can do at home is not just about moving your body! Have a great muscle training hack life!</p>

    </article>
  </div>

  
  
  
  <script>
  window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
  ga('create', 'UA-169005401-1', 'auto');
  ga('send', 'pageview');
  </script>
  <script async src='https://www.google-analytics.com/analytics.js'></script>
  

  

  
<link rel="stylesheet" type="text/css" href="//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.0/cookieconsent.min.css" />
<script src="//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.0/cookieconsent.min.js"></script>
<script>
window.addEventListener("load", function(){
window.cookieconsent.initialise({
  "palette": {
    "popup": {
      "background": "#216942",
      "text": "#b2d192"
    },
    "button": {
      "background": "#afed71"
    }
  }
})});
</script>

</body>

</html>
