<!doctype html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  
  <meta name="generator" content="Hugo 0.72.0" />

  
  <meta name="description" content="Some description">
  

  
  <link rel="apple-touch-icon" sizes="180x180" href="https://memotut.com/apple-touch-icon.png">

  
  <link rel="icon" type="image/png" sizes="32x32" href="https://memotut.com/favicon-32x32.png">

  
  <link rel="icon" type="image/png" sizes="16x16" href="https://memotut.com/favicon-16x16.png">

  
  <link rel="manifest" href="https://memotut.com/site.webmanifest">

  
  <link rel="mask-icon" href="https://memotut.com/safari-pinned-tab.svg" color="#5bbad5">

  <meta name="msapplication-TileColor" content="#da532c">

  <meta name="theme-color" content="#ffffff">

  
  <link rel="stylesheet" href="https://memotut.com/css/bootstrap.min.css" />

  
  <title>[Python] I tried to make Othello AI with tensorflow without understanding the theory of machine learning-Implementation- | Memo Tut</title>
  

  <style>
body {
  min-width: 300px;
}

.custom-navbar {
  margin-bottom: 1em;
  height: 60px;
}

.custom-navbar a {
  display: inline-block; 
  padding: 18px 0;
  margin-right: 1em; 
  font-weight: bold; 
}

.custom-navbar a:hover,
.custom-navbar a:focus {
  text-decoration: none; 
}

@media print {
  .custom-navbar {
    display: none;
  }
}

article {
  padding-bottom: 1em;
}

img {
  max-width: 100%;
}


body {
  background-color: #fff;
}



body {
  color: #212529;
}



a {
  color: #007bff;
}



a:hover,
a:focus {
  color: #0056b3;
}



.custom-navbar {
  background-color: #212529;
}



.custom-navbar a {
  color: rgba(255, 255, 255, 0.75);
}



.custom-navbar a:hover,
.custom-navbar a:focus {
  color: rgba(255, 255, 255, 1);
}



.container {
  max-width: 800px;
}



pre {
  display: block;
  padding: 9.5px;
  word-break: break-all;
  word-wrap: break-word;
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 4px;
}

pre code {
  padding: 0;
  font-size: inherit;
  color: inherit; 
  white-space: pre-wrap;
  background-color: transparent;
  border: none;
  border-radius: 0;
}

code {
  padding: 2px 4px;
  color: inherit; 
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 4px;
  font-size: .9em;
}



blockquote,
.blockquote {
  padding: 10px 20px;
  margin: 0 0 20px;
  font-size: 1em;
  border-left: 5px solid #6c757d;
}

</style>
</head>

<body>
  <nav class="custom-navbar">
  <div class="container">
    
    <a href="/">Posts</a>
    
    <a href="/tags/">Tags</a>
    
    <a href="/about/">About</a>
    
    <a href="/index.xml">RSS</a>
    
  </div>
</nav>
  
  <div class="container">
    <article>
      <h1>[Python] I tried to make Othello AI with tensorflow without understanding the theory of machine learning-Implementation-</h1>
<p>
  <small class="text-secondary">
  
  
  Oct 10, 2016
  </small>
  

<small><code><a href="https://memotut.com/tags/python">Python</a></code></small>


<small><code><a href="https://memotut.com/tags/deeplearning"> DeepLearning</a></code></small>


<small><code><a href="https://memotut.com/tags/reinforcement-learning"> Reinforcement Learning</a></code></small>


<small><code><a href="https://memotut.com/tags/dqn"> DQN</a></code></small>


<small><code><a href="https://memotut.com/tags/tensorflow"> TensorFlow</a></code></small>

</p>
<pre><code># Series table of contents
</code></pre>
<ul>
<li><a href="http://qiita.com/sasaco/items/3b0b8565d6aa2a640caf">I tried to make Othello AI with tensorflow without understanding the theory of machine learning-Introduction-</a></li>
<li>I tried to make Othello AI with tensorflow without understanding the theory of machine learning-Implementation-</li>
<li><a href="http://qiita.com/sasaco/items/f9aa608860eebb3026c1">I tried to make Othello AI with tensorflow without understanding the theory of machine learning-Izou battle version-</a></li>
<li><a href="https://qiita.com/sasaco/items/5e102063c256bf56e396">I tried to make Othello AI after trying to understand the theory of machine learning ~ Restart‼ ~</a></li>
<li><a href="https://qiita.com/sasaco/items/d249ee3493b5b85c6eb5">I tried to make Othello AI after trying to understand the theory of machine learning ~ What this is Alpha Zero ~</a></li>
<li><a href="https://qiita.com/sasaco/items/a70ea2940380f7f8e1a1">I tried to create a neutral network in Excel to understand the theory of machine learning-Image recognition mnist-</a></li>
</ul>
<p><a href="http://qiita.com/sasaco/items/3b0b8565d6aa2a640caf">Last time</a> continued&hellip;
In this field, I am an outsider, without studying &ldquo;the theory of machine learning&rdquo; at all.
I will try to make an Othello AI.
Click here for the referenced site
・<a href="https://elix-tech.github.io/en/2016/06/29/dqn-en.html">Implement DQN on Keras, TensorFlow and OpenAI Gym</a>
・<a href="https://github.com/3cky/tensorflow-rl-tictactoe">Training TensorFlow neural network to play Tic-Tac-Toe game using one-step Q-learning algorithm.</a></p>
<p>#Reinforcement learning basics
I made Othello&rsquo;s AI without studying &ldquo;machine learning theory&rdquo; at all.
Summarize the minimum knowledge you need to implement.</p>
<h2 id="file-structure-and-role">File structure and role</h2>
<p>The file structure and role are like this.
<img src="https://qiita-image-store.s3.amazonaws.com/0/142847/240b3f5e-2b60-2c2b-f09d-ecc2f14a794f.png" alt="Configuration.png"></p>
<ul>
<li>train.py &mdash; perform AI training</li>
<li>Reversi.py &mdash; <a href="http://qiita.com/Tsutomu-KKE@github/items/5824eb00250bf08f9197">Othello game management</a></li>
<li>dqn_agent.py &mdash; Management of AI training</li>
<li>FightWithAI.py &mdash; Fight against the user</li>
</ul>
<h2 id="overall-algorithm">Overall algorithm</h2>
<p>The DQN algorithm implemented this time is like this.</p>
<p><img src="https://qiita-image-store.s3.amazonaws.com/0/142847/afa03c7b-1cbb-d58f-955a-9552108d45be.png" alt="algorithm.png">
If you keep this in mind, you will understand what and what is going to be explained.</p>
<h1 id="othello-game-specifications">Othello game specifications</h1>
<p>The board used for the Othello game and AI training is
It is performed using a two-dimensional array with the No assigned in the figure below.
<img src="https://qiita-image-store.s3.amazonaws.com/0/142847/203d1eb0-729c-0f23-962b-afb8adfe5aa2.png" alt="screen.png"></p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python3:Reversi.py" data-lang="python3:Reversi.py">self<span style="color:#f92672">.</span>screen[<span style="color:#ae81ff">0</span><span style="color:#f92672">-</span><span style="color:#ae81ff">7</span>][<span style="color:#ae81ff">0</span><span style="color:#f92672">-</span><span style="color:#ae81ff">7</span>]
</code></pre></div><p>The operation that AI can select is to select the number from 0 to 63 in the above figure.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python3:Reversi.py" data-lang="python3:Reversi.py">self<span style="color:#f92672">.</span>enable_actions[<span style="color:#ae81ff">0</span><span style="color:#f92672">-</span><span style="color:#ae81ff">63</span>]
</code></pre></div><p>#AI training</p>
<p>In AI training, players[0] and players[1] have played Othello n_epochs = 1000 times,
Finally, save the AI of the second player players[1].</p>
<h2 id="reward-for-ai">Reward for AI</h2>
<ul>
<li>If you win the game, set reward = 1</li>
<li>Other than that, reward = 0</li>
</ul>
<h2 id="training-method">Training method</h2>
<p>I&rsquo;m fighting with two AIs, but I have to act on the opponent&rsquo;s turn
Because the story until the end is not connected (Q value is not transmitted)</p>
<p><img src="https://qiita-image-store.s3.amazonaws.com/0/142847/f721dbe2-b13a-c9c8-56b0-223a13ef6474.png" alt="8151b82b-2d1a-7b02-4282-1e98a5a9a265.png"></p>
<p>Both act on every turn
This time, I decided to &ldquo;save transitions in D&rdquo; for all numbers that can be kept separately from the progress of the game.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python3:train.py" data-lang="python3:train.py">
<span style="color:#75715e">#targets contains all the numbers you can put this turn</span>
<span style="color:#66d9ef">for</span> tr <span style="color:#f92672">in</span> targets:
    <span style="color:#75715e"># Duplicate status</span>
    tmp <span style="color:#f92672">=</span> copy<span style="color:#f92672">.</span>deepcopy(env)
    <span style="color:#75715e">#Action</span>
    tmp<span style="color:#f92672">.</span>update(tr, playerID[i])
    <span style="color:#75715e">#End judgment</span>
    win <span style="color:#f92672">=</span> tmp<span style="color:#f92672">.</span>winner()
    end <span style="color:#f92672">=</span> tmp<span style="color:#f92672">.</span>isEnd()
    <span style="color:#75715e"># Board after action</span>
    state_X <span style="color:#f92672">=</span> tmp<span style="color:#f92672">.</span>screen
    <span style="color:#75715e">#The number you can leave after you act</span>
    target_X <span style="color:#f92672">=</span> tmp<span style="color:#f92672">.</span>get_enables(playerID[i<span style="color:#f92672">+</span><span style="color:#ae81ff">1</span>])
                       
    <span style="color:#75715e">#Both actions</span>
    <span style="color:#66d9ef">for</span> j <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">0</span>, len(players)):
        reword <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
        <span style="color:#66d9ef">if</span> end <span style="color:#f92672">==</span> True:
            <span style="color:#66d9ef">if</span> win <span style="color:#f92672">==</span> playerID[j]:
                <span style="color:#75715e"># Get 1 reward if you win</span>
                reword <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span>
        <span style="color:#75715e"># Both &#34;save transition in D&#34;</span>
        players[j]<span style="color:#f92672">.</span>store_experience(state, targets, tr, reword, state_X, target_X, end)
        players[j]<span style="color:#f92672">.</span>experience_replay()
</code></pre></div><blockquote>
<p>The following parts of the DQN algorithm are done by dqn_agent.py.</p>
</blockquote>
<ul>
<li>Save transition (si,ai,ri,si+1,tarminal) in D</li>
<li>Sample mini patch (si,ai,ri,si+1,tarminal) that changes randomly from D</li>
<li>Teacher signal yi=ri+γmax Q(si+1, a:θ)</li>
<li>Perform gradient method with (yi-Q(si, ai; θ))^2 for parameter θ of Q Network</li>
<li>Reset Target Network periodically Q=Q</li>
</ul>
<blockquote>
</blockquote>
<p>I don&rsquo;t know because it&rsquo;s the whole site I referred to</p>
<blockquote>
<blockquote>
</blockquote>
</blockquote>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python3:dqn_agent.py" data-lang="python3:dqn_agent.py">    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">store_experience</span>(self, state, targets, action, reward, state_1, targets_1, terminal):
        self<span style="color:#f92672">.</span>D<span style="color:#f92672">.</span>append((state, targets, action, reward, state_1, targets_1, terminal))
<span style="color:#f92672">&gt;&gt;</span>
    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">experience_replay</span>(self):
        state_minibatch <span style="color:#f92672">=</span> []
        y_minibatch <span style="color:#f92672">=</span> []
<span style="color:#f92672">&gt;&gt;</span>
        <span style="color:#75715e"># sample random minibatch</span>
        minibatch_size <span style="color:#f92672">=</span> min(len(self<span style="color:#f92672">.</span>D), self<span style="color:#f92672">.</span>minibatch_size)
        minibatch_indexes <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>randint(<span style="color:#ae81ff">0</span>, len(self<span style="color:#f92672">.</span>D), minibatch_size)
<span style="color:#f92672">&gt;&gt;</span>
        <span style="color:#66d9ef">for</span> j <span style="color:#f92672">in</span> minibatch_indexes:
            state_j, targets_j, action_j, reward_j, state_j_1, targets_j_1, terminal <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>D[j]
            action_j_index <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>enable_actions<span style="color:#f92672">.</span>index(action_j)
<span style="color:#f92672">&gt;&gt;</span>
            y_j <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>Q_values(state_j)
<span style="color:#f92672">&gt;&gt;</span>
            <span style="color:#66d9ef">if</span> terminal:
                y_j[action_j_index] <span style="color:#f92672">=</span> reward_j
            <span style="color:#66d9ef">else</span>:
                <span style="color:#75715e"># reward_j + gamma * max_action&#39; Q(state&#39;, action&#39;)</span>
                qvalue, action <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>select_enable_action(state_j_1, targets_j_1)
                y_j[action_j_index] <span style="color:#f92672">=</span> reward_j <span style="color:#f92672">+</span> self<span style="color:#f92672">.</span>discount_factor <span style="color:#f92672">*</span> qvalue
<span style="color:#f92672">&gt;&gt;</span>
            state_minibatch<span style="color:#f92672">.</span>append(state_j)
            y_minibatch<span style="color:#f92672">.</span>append(y_j)
<span style="color:#f92672">&gt;&gt;</span>
        <span style="color:#75715e"># training</span>
        self<span style="color:#f92672">.</span>sess<span style="color:#f92672">.</span>run(self<span style="color:#f92672">.</span>training, feed_dict<span style="color:#f92672">=</span>{self<span style="color:#f92672">.</span>x: state_minibatch, self<span style="color:#f92672">.</span>y_: y_minibatch})
<span style="color:#f92672">&gt;&gt;</span>
        <span style="color:#75715e"># for log</span>
        self<span style="color:#f92672">.</span>current_loss <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>sess<span style="color:#f92672">.</span>run(self<span style="color:#f92672">.</span>loss, feed_dict<span style="color:#f92672">=</span>{self<span style="color:#f92672">.</span>x: state_minibatch, self<span style="color:#f92672">.</span>y_: y_minibatch})
<span style="color:#f92672">&gt;&gt;</span>
</code></pre></div><blockquote>
<blockquote>
</blockquote>
</blockquote>
<table>
<thead>
<tr>
<th align="center">Variable name</th>
<th align="center">Content</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center">state</td>
<td align="center">Board surface (= Reversi.screen[0-7][0-7])</td>
</tr>
<tr>
<td align="center">targets</td>
<td align="center">Numbers to place</td>
</tr>
<tr>
<td align="center">action</td>
<td align="center">Selected action</td>
</tr>
<tr>
<td align="center">reward</td>
<td align="center">Reward for actions 0-1</td>
</tr>
<tr>
<td align="center">state_1</td>
<td align="center">Board after action</td>
</tr>
<tr>
<td align="center">targets_1</td>
<td align="center">Numbers you can leave after taking action</td>
</tr>
<tr>
<td align="center">terminal</td>
<td align="center">Game ends = True</td>
</tr>
</tbody>
</table>
<p>#Implementation</p>
<p>In AI training, players[0] and players[1] have played Othello n_epochs = 1000 times,
Finally, save the AI of the second player players[1].```python3:train.py</p>
<h1 id="parameters">parameters</h1>
<pre><code>n_epochs = 1000
# environment, agent
env = Reversi()

# playerID
playerID = [env.Black, env.White, env.Black]

# player agent
players = []
# player[0]= env.Black
players.append(DQNAgent(env.enable_actions, env.name, env.screen_n_rows, env.screen_n_cols))
# player[1]= env.White
players.append(DQNAgent(env.enable_actions, env.name, env.screen_n_rows, env.screen_n_cols))
</code></pre>
<pre><code>&gt;This `DQNAgent(env.enable_actions, env.name, env.screen_n_rows, env.screen_n_cols)` part is
&gt;&gt;- Initialization of Replay Memory D
- Initialize Q NetworkQ with random weight θ
- Initialize Target NetworkQ θ^=θ

&gt;, done by dqn_agent.py.
&gt;&gt;
```python3:dqn_agent.py
class DQNAgent:
&gt;&gt;
    def __init__(self, enable_actions, environment_name, rows, cols):
        ... omitted ...
        # Replay Memory D initialization
        self.D = deque(maxlen=self.replay_memory_size)
        ... omitted ...
&gt;&gt;
    def init_model(self):
        # input layer (rows x cols)
        self.x = tf.placeholder(tf.float32, [None, self.rows, self.cols])
&gt;&gt;
        # flatten (rows x cols)
        size = self.rows * self.cols
        x_flat = tf.reshape(self.x, [-1, size])
&gt;&gt;
        # Q Network Q is initialized with random weight θ
        W_fc1 = tf.Variable(tf.truncated_normal([size, size], stddev=0.01))
        b_fc1 = tf.Variable(tf.zeros([size]))
        h_fc1 = tf.nn.relu(tf.matmul(x_flat, W_fc1) + b_fc1)
&gt;&gt;
        # Initialize Target Network Q θ^=θ
        W_out = tf.Variable(tf.truncated_normal([size, self.n_actions], stddev=0.01))
        b_out = tf.Variable(tf.zeros([self.n_actions]))
        self.y = tf.matmul(h_fc1, W_out) + b_out
&gt;&gt;
        # loss function
        self.y_ = tf.placeholder(tf.float32, [None, self.n_actions])
        self.loss = tf.reduce_mean(tf.square(self.y_-self.y))
&gt;&gt;
        # train operation
        optimizer = tf.train.RMSPropOptimizer(self.learning_rate)
        self.training = optimizer.minimize(self.loss)
&gt;&gt;
        # saver
        self.saver = tf.train.Saver()
&gt;&gt;
        # session
        self.sess = tf.Session()
        self.sess.run(tf.initialize_all_variables())
</code></pre><pre><code class="language-python3:" data-lang="python3:">    for e in range(n_epochs):
        # reset
        env.reset()
        terminal = False
</code></pre><blockquote>
<ul>
<li>for episode =1, M do
-Initial screen x1, pre-process and create initial state s1</li>
</ul>
</blockquote>
<pre><code class="language-python3:" data-lang="python3:">        while terminal == False: #1 Loop until the end of the episode

            for i in range(0, len(players)):
                
                state = env.screen
                targets = env.get_enables(playerID[i])
                
                if len(targets)&gt; 0:
                    # If there is a place to put somewhere

#← Here, all the aforementioned hands are &quot;saved in D&quot;

                    # Choose action
                    action = players[i].select_action(state, targets, players[i].exploration)
                    # Take action
                    env.update(action, playerID[i])
</code></pre><blockquote>
<ul>
<li>while not terminal
-Action selection</li>
</ul>
</blockquote>
<blockquote>
<blockquote>
<p>Action selection <code>agent.select_action(state_t, targets, agent.exploration)</code> is
This is done by dqn_agent.py.</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<ul>
<li>Action selection
-Random action ai
-Or ai= argmax Q(s1, a:θ)</li>
</ul>
</blockquote>
</blockquote>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python3:dqn_agent.py" data-lang="python3:dqn_agent.py">    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">Q_values</span>(self, state):
        <span style="color:#75715e"># Q(state, action) of all actions</span>
        <span style="color:#66d9ef">return</span> self<span style="color:#f92672">.</span>sess<span style="color:#f92672">.</span>run(self<span style="color:#f92672">.</span>y, feed_dict<span style="color:#f92672">=</span>{self<span style="color:#f92672">.</span>x: [state]})[<span style="color:#ae81ff">0</span>]
<span style="color:#f92672">&gt;&gt;</span>
    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">select_action</span>(self, state, targets, epsilon):
<span style="color:#f92672">&gt;&gt;</span>
        <span style="color:#66d9ef">if</span> np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>rand() <span style="color:#f92672">&lt;=</span> epsilon:
            <span style="color:#75715e"># random</span>
            <span style="color:#66d9ef">return</span> np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>choice(targets)
        <span style="color:#66d9ef">else</span>:
            <span style="color:#75715e"># max_action Q(state, action)</span>
            qvalue, action <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>select_enable_action(state, targets)
            <span style="color:#66d9ef">return</span> action
<span style="color:#f92672">&gt;&gt;</span>
    <span style="color:#75715e">#Return the Q value and the number that maximizes the Q value from the places (targets) where you can put it on the board (state)</span>
    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">select_enable_action</span>(self, state, targets):
        Qs <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>Q_values(state)
        <span style="color:#75715e">#descend = np.sort(Qs)</span>
        index <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>argsort(Qs)
        <span style="color:#66d9ef">for</span> action <span style="color:#f92672">in</span> reversed(index):
            <span style="color:#66d9ef">if</span> action <span style="color:#f92672">in</span> targets:
                <span style="color:#66d9ef">break</span>
        <span style="color:#75715e"># max_action Q(state, action)</span>
        qvalue <span style="color:#f92672">=</span> Qs[action]
<span style="color:#f92672">&gt;&gt;</span>
        <span style="color:#66d9ef">return</span> qvalue, action
</code></pre></div><blockquote>
<ul>
<li>Execute action ai, observe reward ri, next screen xi+1 and end judgment tarminal</li>
</ul>
</blockquote>
<ul>
<li>Preprocess and create next state si+1</li>
</ul>
<p>Finally save the back AI</p>
<pre><code class="language-python3:" data-lang="python3:">
                #Result of executing action
                terminal = env.isEnd()
                              
        w = env.winner()
        print(&quot;EPOCH: {:03d}/{:03d} | WIN: player{:1d}&quot;.format(
                         e, n_epochs, w))


    #Save saves player2, which is the second player.
    players[1].save_model()

</code></pre><p>The source is here.
<a href="https://github.com/sasaco/tf-dqn-reversi.git"><code>$ git clone https://github.com/sasaco/tf-dqn-reversi.git</code></a></p>
<p><a href="http://qiita.com/sasaco/items/f9aa608860eebb3026c1">Next time</a> is about the battle version.</p>

    </article>
  </div>

  
  
  
  <script>
  window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
  ga('create', 'UA-169005401-1', 'auto');
  ga('send', 'pageview');
  </script>
  <script async src='https://www.google-analytics.com/analytics.js'></script>
  

  

  
<link rel="stylesheet" type="text/css" href="//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.0/cookieconsent.min.css" />
<script src="//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.0/cookieconsent.min.js"></script>
<script>
window.addEventListener("load", function(){
window.cookieconsent.initialise({
  "palette": {
    "popup": {
      "background": "#216942",
      "text": "#b2d192"
    },
    "button": {
      "background": "#afed71"
    }
  }
})});
</script>

</body>

</html>
