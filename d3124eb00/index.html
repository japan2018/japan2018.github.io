<!doctype html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  
  <meta name="generator" content="Hugo 0.72.0" />

  
  <meta name="description" content="Some description">
  

  
  <link rel="apple-touch-icon" sizes="180x180" href="https://memotut.com/apple-touch-icon.png">

  
  <link rel="icon" type="image/png" sizes="32x32" href="https://memotut.com/favicon-32x32.png">

  
  <link rel="icon" type="image/png" sizes="16x16" href="https://memotut.com/favicon-16x16.png">

  
  <link rel="manifest" href="https://memotut.com/site.webmanifest">

  
  <link rel="mask-icon" href="https://memotut.com/safari-pinned-tab.svg" color="#5bbad5">

  <meta name="msapplication-TileColor" content="#da532c">

  <meta name="theme-color" content="#ffffff">

  
  <link rel="stylesheet" href="https://memotut.com/css/bootstrap.min.css" />

  
  <title>[Python] Understand and implement ridge regression (L2 regularization) | Memo Tut</title>
  

  <style>
body {
  min-width: 300px;
}

.custom-navbar {
  margin-bottom: 1em;
  height: 60px;
}

.custom-navbar a {
  display: inline-block; 
  padding: 18px 0;
  margin-right: 1em; 
  font-weight: bold; 
}

.custom-navbar a:hover,
.custom-navbar a:focus {
  text-decoration: none; 
}

@media print {
  .custom-navbar {
    display: none;
  }
}

article {
  padding-bottom: 1em;
}

img {
  max-width: 100%;
}


body {
  background-color: #fff;
}



body {
  color: #212529;
}



a {
  color: #007bff;
}



a:hover,
a:focus {
  color: #0056b3;
}



.custom-navbar {
  background-color: #212529;
}



.custom-navbar a {
  color: rgba(255, 255, 255, 0.75);
}



.custom-navbar a:hover,
.custom-navbar a:focus {
  color: rgba(255, 255, 255, 1);
}



.container {
  max-width: 800px;
}



pre {
  display: block;
  padding: 9.5px;
  word-break: break-all;
  word-wrap: break-word;
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 4px;
}

pre code {
  padding: 0;
  font-size: inherit;
  color: inherit; 
  white-space: pre-wrap;
  background-color: transparent;
  border: none;
  border-radius: 0;
}

code {
  padding: 2px 4px;
  color: inherit; 
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 4px;
  font-size: .9em;
}



blockquote,
.blockquote {
  padding: 10px 20px;
  margin: 0 0 20px;
  font-size: 1em;
  border-left: 5px solid #6c757d;
}

</style>
</head>

<body>
  <nav class="custom-navbar">
  <div class="container">
    
    <a href="/">Posts</a>
    
    <a href="/tags/">Tags</a>
    
    <a href="/about/">About</a>
    
    <a href="/index.xml">RSS</a>
    
  </div>
</nav>
  
  <div class="container">
    <article>
      <h1>[Python] Understand and implement ridge regression (L2 regularization)</h1>
<p>
  <small class="text-secondary">
  
  
  Nov 30, 2019
  </small>
  

<small><code><a href="https://memotut.com/tags/python">Python</a></code></small>


<small><code><a href="https://memotut.com/tags/data-analysis"> data analysis</a></code></small>


<small><code><a href="https://memotut.com/tags/regularization"> regularization</a></code></small>


<small><code><a href="https://memotut.com/tags/ridge-regression"> ridge regression</a></code></small>

</p>
<pre><code>#Introduction
</code></pre>
<p>I studied regularization as a development of multiple regression analysis.
This time I have summarized Ridge regression (L2 regularization).</p>
<p>##reference
In understanding Ridge regression (L2 regularization), I have referred to the following.</p>
<ul>
<li><a href="https://www.amazon.co.jp/dp/4797393963/ref=cm_sw_r_tw_dp_U_x_UHqVDbP9A69TP">Essence of machine learning</a>KoichiKato(Author) Publisher; SB Creative Co., Ltd.</li>
<li><a href="https://to-kei.net/neural-network/regularization/">Type and purpose of regularization L1 regularization About L2 regularization</a></li>
<li><a href="https://qiita.com/Takayoshi_Makabe/items/8f6dcb25124b9dcb1ae8">Carefully the theory and implementation of Ridge regression and Lasso regression from the beginning</a></li>
</ul>
<p>#Ridge regression (L2 regularization) overview</p>
<h2 id="multiple-regression-analysis-review">Multiple regression analysis review</h2>
<p>Ridge regression is a regularization term added to the loss function used in multiple regression analysis.
The multiple regression analysis derives the optimal regression equation by finding the weights that minimize the loss function as shown below.</p>
<p>$$L = \sum_{n=1}^{n} (y_{n} -\hat{y}_{n} )^2$$</p>
<ul>
<li>$y_{n}$ is the measured value</li>
<li>$\hat{y}_{n}$ predicted value</li>
</ul>
<p>When expressed in vector format, it looks like this.</p>
<p>$$L = (\boldsymbol{y}-X\boldsymbol{w})^T(\boldsymbol{y}-X\boldsymbol{w})$$</p>
<ul>
<li>$\boldsymbol{y}$ is a vectorization of the measured value of the objective variable</li>
<li>$\boldsymbol{w}$ is a vectorization of regression coefficient when multiple regression equation is created</li>
<li>$X$ is a matrix of actual values of explanatory variables of $n$ samples and $m$ variables</li>
</ul>
<p>It is OK if the weight $\boldsymbol{w}$ that minimizes the above $L$ is required.
Differentiating $L$ above with $\boldsymbol{w}$ and putting it as $0$ gives the following.</p>
<p>$$-2X^T\boldsymbol{y}+2\boldsymbol{w}X^TX = 0$$</p>
<p>By solving this, the weight $\boldsymbol{w}$ can be obtained.</p>
<h2 id="ridge-regression-l2-regularization">Ridge regression (L2 regularization)</h2>
<p>$$L = (\boldsymbol{y}-X\boldsymbol{w})^T (\boldsymbol{y}-X\boldsymbol{w}) + \lambda|| \boldsymbol{w} ||_{2} $$</p>
<p>The above is the formula for the loss function of ridge regression (L2 regularization). It is a form in which the regularization term $\lambda|| \boldsymbol{w} ||_{2} $ is added to the loss function of multiple regression analysis.
In ridge regression (L2 regularization), regularization is performed by adding the square of the L2 norm of the weight $\boldsymbol{w}$ as described above.</p>
<h3 id="what-is-the-l2-norm">What is the L2 norm?</h3>
<p>The L2 norm is the square root of the sum of squared differences of vector components (so-called &ldquo;normal distance&rdquo;, called the Euclidean distance). The norm is an index that represents &ldquo;magnitude&rdquo;, and the L1 norm and Lâˆž norm are also used.</p>
<h3 id="l2-regularization-effect">L2 regularization effect</h3>
<p>Adding a regularization term to the loss function has the effect of reducing the magnitude of the value of the weight $\boldsymbol{w}$.
In the case of normal multiple regression analysis, the only terms to be minimized are:
$$(\boldsymbol{y}-X\boldsymbol{w})^T (\boldsymbol{y}-X\boldsymbol{w})$$
If regularization is added here, it is necessary to perform minimization including the following items.
$$\lambda|| \boldsymbol{w} ||_{2}$$
Since the weight $\boldsymbol{w}$ has a stronger influence on the loss function, the value of the weight $\boldsymbol{w}$ will be further reduced. The degree is controlled by the size of $\lambda$.</p>
<p>The following figure is often used to explain ridge regression (L2 regularization).
Since it is difficult to understand if it is left as it is, explanation is added in the figure. The following is a two-dimensional plot of contours of the values of the loss function when there are two types of weight parameters. You can see in the figure how the weight value is reduced by adding the regularization term.</p>
<p><img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/395230/0a6a3f82-8f5d-f326-949b-b2a036349d82.png" alt="Figure 1.png"></p>
<h3 id="derivation-of-weights-for-ridge-regression-l2-regularization">Derivation of weights for ridge regression (L2 regularization)</h3>
<p>$$L = (\boldsymbol{y}-X\boldsymbol{w})^T (\boldsymbol{y}-X\boldsymbol{w}) + \lambda|| \boldsymbol{w} ||_{2} $$</p>
<p>The above loss function is differentiated by the weight $\boldsymbol{w}$ and put as $0$ for calculation.</p>
<pre><code class="language-math" data-lang="math">- 2X^T\boldsymbol{y}+2\boldsymbol{w}X^TX+2\lambda\boldsymbol{w} = 0 \\
(X^TX+\lambda I)\boldsymbol{w}-X^T\boldsymbol{y} = 0 \\
(X^TX+\lambda I)\boldsymbol{w} = X^T\boldsymbol{y} \\
\boldsymbol{w} = (X^TX+\lambda I)^{-1}X^T\boldsymbol{y}

</code></pre><p>We were able to derive the weight $\boldsymbol{w}$ here.</p>
<h1 id="implement-ridge-regression-l2-regularization">Implement ridge regression (L2 regularization)</h1>
<p>##Implementation
Below is the result of self-implementing the model of ridge regression (L2 regularization).</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">
<span style="color:#f92672">import</span> numpy <span style="color:#f92672">as</span> np

<span style="color:#66d9ef">class</span> <span style="color:#a6e22e">RidgeReg</span>:

    <span style="color:#66d9ef">def</span> __init__(self, lambda_ <span style="color:#f92672">=</span> <span style="color:#ae81ff">1.0</span>):
        self<span style="color:#f92672">.</span>lambda_ <span style="color:#f92672">=</span> lambda_
        self<span style="color:#f92672">.</span>coef_ <span style="color:#f92672">=</span> None
        self<span style="color:#f92672">.</span>intercept_ <span style="color:#f92672">=</span> None

    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">fit</span>(self, X, y):
        Add a column <span style="color:#66d9ef">with</span> all <span style="color:#ae81ff">1</span>s to the first row of the matrix of explanatory variables to include <span style="color:#75715e">#intercept calculations</span>
        X <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>insert(X, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1</span>, axis<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
        <span style="color:#75715e">#Create identity matrix</span>
        i <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>eye(X<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">1</span>])
        <span style="color:#75715e">#Calculation formula for weight</span>
        temp <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>linalg<span style="color:#f92672">.</span>inv(X<span style="color:#f92672">.</span>T <span style="color:#960050;background-color:#1e0010">@</span> X <span style="color:#f92672">+</span> self<span style="color:#f92672">.</span>lambda_ <span style="color:#f92672">*</span> i) <span style="color:#960050;background-color:#1e0010">@</span> X<span style="color:#f92672">.</span>T <span style="color:#960050;background-color:#1e0010">@</span> y
        <span style="color:#75715e"># This is the value of the regression coefficient</span>
        self<span style="color:#f92672">.</span>coef_ <span style="color:#f92672">=</span> temp[<span style="color:#ae81ff">1</span>:]
        <span style="color:#75715e"># This is the intercept value</span>
        self<span style="color:#f92672">.</span>intercept_ <span style="color:#f92672">=</span> temp[<span style="color:#ae81ff">0</span>]
        
    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">predict</span>(self, X):
        <span style="color:#75715e"># Return the predicted value by the ridge regression model</span>
        <span style="color:#66d9ef">return</span> (X <span style="color:#960050;background-color:#1e0010">@</span> self<span style="color:#f92672">.</span>coef_ <span style="color:#f92672">+</span> self<span style="color:#f92672">.</span>intercept_)
</code></pre></div><h2 id="verification">Verification</h2>
<p>Verify whether the above self-implementation model matches the sklearn result.
This time, we will use the Boston house price data set for verification. For details on the contents of the data set, see <a href="https://qiita.com/g-k/items/c32137e60c2d0d7ef987">Articles that verified multiple regression analysis</a>.</p>
<p>###sklearn&rsquo;s model</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">from</span> sklearn.datasets <span style="color:#f92672">import</span> load_boston
<span style="color:#f92672">import</span> pandas <span style="color:#f92672">as</span> pd
<span style="color:#f92672">from</span> sklearn.preprocessing <span style="color:#f92672">import</span> StandardScaler

<span style="color:#75715e">#Read data</span>
boston <span style="color:#f92672">=</span> load_boston()

<span style="color:#75715e"># Once converted to pandas data frame format</span>
df <span style="color:#f92672">=</span> pd<span style="color:#f92672">.</span>DataFrame(boston<span style="color:#f92672">.</span>data, columns<span style="color:#f92672">=</span>boston<span style="color:#f92672">.</span>feature_names)

<span style="color:#75715e">#Get the target variable (value you want to predict)</span>
target <span style="color:#f92672">=</span> boston<span style="color:#f92672">.</span>target

df[<span style="color:#e6db74">&#39;target&#39;</span>] <span style="color:#f92672">=</span> target

<span style="color:#f92672">from</span> sklearn.linear_model <span style="color:#f92672">import</span> Ridge

X <span style="color:#f92672">=</span> df[[<span style="color:#e6db74">&#39;INDUS&#39;</span>,<span style="color:#e6db74">&#39;CRIM&#39;</span>]]<span style="color:#f92672">.</span>values
X <span style="color:#f92672">=</span> StandardScaler()<span style="color:#f92672">.</span>fit_transform(X)
y <span style="color:#f92672">=</span> df[<span style="color:#e6db74">&#39;target&#39;</span>]<span style="color:#f92672">.</span>values

clf <span style="color:#f92672">=</span> Ridge(alpha<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)

clf<span style="color:#f92672">.</span>fit(X, y)

<span style="color:#66d9ef">print</span>(clf<span style="color:#f92672">.</span>coef_)
<span style="color:#66d9ef">print</span>(clf<span style="color:#f92672">.</span>intercept_)

</code></pre></div><p>The output is here. From above is the regression coefficient and intercept of the model.</p>
<pre><code>[-3.58037552 -2.1078602]
22.532806324110677
</code></pre><h3 id="self-implementation-model">Self-implementation model</h3>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">X <span style="color:#f92672">=</span> df[[<span style="color:#e6db74">&#39;INDUS&#39;</span>,<span style="color:#e6db74">&#39;CRIM&#39;</span>]]<span style="color:#f92672">.</span>values
X <span style="color:#f92672">=</span> StandardScaler()<span style="color:#f92672">.</span>fit_transform(X)
y <span style="color:#f92672">=</span> df[<span style="color:#e6db74">&#39;target&#39;</span>]<span style="color:#f92672">.</span>values

linear <span style="color:#f92672">=</span> RidgeReg(lambda_ <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span>)

linear<span style="color:#f92672">.</span>fit(X,y)

<span style="color:#66d9ef">print</span>(linear<span style="color:#f92672">.</span>coef_)
<span style="color:#66d9ef">print</span>(linear<span style="color:#f92672">.</span>intercept_)
</code></pre></div><p>The output is here. From above is the regression coefficient and intercept of the model.</p>
<pre><code>[-3.58037552 -2.1078602]
22.532806324110677
</code></pre><p>The regression coefficients were exactly the same, but for some reason there was a subtle difference in the intercept.
I investigated it, but I did not understand the cause, so I will post it as it is.
If you understand, please point out&hellip;
#Next
Next, we will try to understand Lasso regression (L1 regularization) and self-implement.</p>

    </article>
  </div>

  
  
  
  <script>
  window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
  ga('create', 'UA-169005401-1', 'auto');
  ga('send', 'pageview');
  </script>
  <script async src='https://www.google-analytics.com/analytics.js'></script>
  

  

  
<link rel="stylesheet" type="text/css" href="//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.0/cookieconsent.min.css" />
<script src="//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.0/cookieconsent.min.js"></script>
<script>
window.addEventListener("load", function(){
window.cookieconsent.initialise({
  "palette": {
    "popup": {
      "background": "#216942",
      "text": "#b2d192"
    },
    "button": {
      "background": "#afed71"
    }
  }
})});
</script>

</body>

</html>
