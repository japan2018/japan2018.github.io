<!doctype html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  
  <meta name="generator" content="Hugo 0.72.0" />

  
  <meta name="description" content="Some description">
  

  
  <link rel="apple-touch-icon" sizes="180x180" href="https://memotut.com/apple-touch-icon.png">

  
  <link rel="icon" type="image/png" sizes="32x32" href="https://memotut.com/favicon-32x32.png">

  
  <link rel="icon" type="image/png" sizes="16x16" href="https://memotut.com/favicon-16x16.png">

  
  <link rel="manifest" href="https://memotut.com/site.webmanifest">

  
  <link rel="mask-icon" href="https://memotut.com/safari-pinned-tab.svg" color="#5bbad5">

  <meta name="msapplication-TileColor" content="#da532c">

  <meta name="theme-color" content="#ffffff">

  
  <link rel="stylesheet" href="https://memotut.com/css/bootstrap.min.css" />

  
  <title>AI Edge Contest (Implementation Contest) Tutorial [10: Controlling HW with Python... But...] | Memo Tut</title>
  

  <style>
body {
  min-width: 300px;
}

.custom-navbar {
  margin-bottom: 1em;
  height: 60px;
}

.custom-navbar a {
  display: inline-block; 
  padding: 18px 0;
  margin-right: 1em; 
  font-weight: bold; 
}

.custom-navbar a:hover,
.custom-navbar a:focus {
  text-decoration: none; 
}

@media print {
  .custom-navbar {
    display: none;
  }
}

article {
  padding-bottom: 1em;
}

img {
  max-width: 100%;
}


body {
  background-color: #fff;
}



body {
  color: #212529;
}



a {
  color: #007bff;
}



a:hover,
a:focus {
  color: #0056b3;
}



.custom-navbar {
  background-color: #212529;
}



.custom-navbar a {
  color: rgba(255, 255, 255, 0.75);
}



.custom-navbar a:hover,
.custom-navbar a:focus {
  color: rgba(255, 255, 255, 1);
}



.container {
  max-width: 800px;
}



pre {
  display: block;
  padding: 9.5px;
  word-break: break-all;
  word-wrap: break-word;
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 4px;
}

pre code {
  padding: 0;
  font-size: inherit;
  color: inherit; 
  white-space: pre-wrap;
  background-color: transparent;
  border: none;
  border-radius: 0;
}

code {
  padding: 2px 4px;
  color: inherit; 
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 4px;
  font-size: .9em;
}



blockquote,
.blockquote {
  padding: 10px 20px;
  margin: 0 0 20px;
  font-size: 1em;
  border-left: 5px solid #6c757d;
}

</style>
</head>

<body>
  <nav class="custom-navbar">
  <div class="container">
    
    <a href="/">Posts</a>
    
    <a href="/tags/">Tags</a>
    
    <a href="/about/">About</a>
    
    <a href="/index.xml">RSS</a>
    
  </div>
</nav>
  
  <div class="container">
    <article>
      <h1>AI Edge Contest (Implementation Contest) Tutorial [10: Controlling HW with Python&hellip; But&hellip;]</h1>
<p>
  <small class="text-secondary">
  
  
  Mar 20, 2020
  </small>
  

<small><code><a href="https://memotut.com/tags/python">Python</a></code></small>


<small><code><a href="https://memotut.com/tags/fpga"> FPGA</a></code></small>


<small><code><a href="https://memotut.com/tags/pynq"> PYNQ</a></code></small>


<small><code><a href="https://memotut.com/tags/ultra96"> ultra96</a></code></small>


<small><code><a href="https://memotut.com/tags/ai-edge-contest"> AI edge contest</a></code></small>

</p>
<pre><code>Finally, after a long road, the preparation is over. Let's move the convolution circuit designed with the Ultra96V2 board!
</code></pre>
<h3 id="transfer-necessary-files-to-ultra96v2">Transfer necessary files to Ultra96V2</h3>
<p>Created the following files last time (<a href="https://qiita.com/HirokiNakahara/items/7de1099717db960e5251">AI Edge Contest (Implementation Contest) Tutorial [9: Until HW synthesis and bitstream generation]</a>) must.</p>
<ul>
<li>pynq_ultra96_conv_l0_r1.bit</li>
<li>pynq_ultra96_conv_l0_r1.tcl</li>
<li>pynq_ultra96_conv_l0_r1.hdf</li>
<li>pynq_ultra96_conv_l0_r1.hwh</li>
</ul>
<p>These files by referring to the 3rd <a href="https://qiita.com/HirokiNakahara/items/20742dc7d2a5be209b23">(AI Edge Contest (Implementation Contest) Tutorial [3: Inference with the CPU of Ultra96 board]</a>) On your Ultra96V2 board in <code>/home/xilinx/pynq/overlays/base</code>.</p>
<p>Also, the test bench I have used so far</p>
<ul>
<li>testbench_input.txt</li>
<li>testbench_output.txt</li>
</ul>
<p>Should be placed in <code>/home/xilinx/data</code>.</p>
<p><a href="https://github.com/HirokiNakahara/FPGA_AI_Edge_Contest_2019/blob/master/Inference_PYNQ_1/ultra96v2_pynq_convolution_layer0.ipynb">Notebook controlling hardware</a>tutorialrepository(<code>https://github.com/HirokiNakahara/FPGA_AIGA/FPGA_AIGA/FPGA_AIGA/FPGA_AIGA/FPGA_AIGA/FPGA_AIGA/FPGA_AIGA/FPGA_AIGA/FPGA_AIGA/FPGA_AIGA/FPGA_AI______________/blob/master/Inference_PYNQ_1</code>). Clone it and place it in the Ultra96V2 home <code>/home/xilinx</code>. Read it later with Ultra96V2 Jupyter Notebook.</p>
<h3 id="finally-inference-is-executed-by-hardware">Finally, inference is executed by hardware</h3>
<p>Connect to the Ultra96V2 Jupyter Notebook from your browser. Please refer to Part 3.
<img width="836" alt="Screenshot 2020-03-20 11.21.39.png" src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/589480/f9726bdb-5aa7-5e81-8433-d08c9ed1e3b6.png">
Click Upload to load and execute the notebook (<code>ultra96v2_pynq_convolution_layer0.ipynb</code>). After that, when you execute from the top, preparation, inference execution (but slow), inference on the CPU for comparison are performed respectively.</p>
<p>Below, I will explain the points.</p>
<pre><code class="language-python:ultra96v2_pynq_convolution_layer0.ipynb" data-lang="python:ultra96v2_pynq_convolution_layer0.ipynb">from pynq import Overlay
import pynq

overlay = Overlay('/home/xilinx/pynq/overlays/base/pynq_ultra96_conv_l0_r1.bit')
dir(overlay)
</code></pre><p>PYNQ abstracts the hardware with the concept of overlay. As you can see from the display, I think there are some core names (kernel_0 or axi_dma_0) used in the previous IP connection. You can access it and perform operations.</p>
<pre><code class="language-python:ultra96v2_pynq_convolution_layer0.ipynb" data-lang="python:ultra96v2_pynq_convolution_layer0.ipynb">registers = overlay.kernel_0.register_map
</code></pre><p>For example, you can control your IP core with Python by accessing register_map. This time, the pragma specifies the AXI stream, so that operation is possible easily! This is great (for those who have written an AXI bus in RTL).</p>
<p>DMA settings,</p>
<pre><code class="language-python:ultra96v2_pynq_convolution_layer0.ipynb" data-lang="python:ultra96v2_pynq_convolution_layer0.ipynb">import pynq.lib.dma

dma = overlay.axi_dma_0
</code></pre><p>And access the overlay</p>
<pre><code class="language-python:ultra96v2_pynq_convolution_layer0.ipynb" data-lang="python:ultra96v2_pynq_convolution_layer0.ipynb">from pynq import Xlnk

inimg_size = 416*11*3
outfmap_size = 102*64+1

xlnk = Xlnk()

send_buf = xlnk.cma_array(shape=(inimg_size),dtype=np.int32)
recv_buf = xlnk.cma_array(shape=(outfmap_size),dtype=np.int32)
</code></pre><p>And read <code>Xlnk()</code> (a DMA control middleware wrapper designed by Xilinx) and secured the array. Easy victory.</p>
<p>For hardware data transfer and reception</p>
<pre><code class="language-python:ultra96v2_pynq_convolution_layer0.ipynb" data-lang="python:ultra96v2_pynq_convolution_layer0.ipynb">%%time
for line in range(102):
    # load input image
    for i in range(11):
        inimg_buf[i] = inimg[i+line*4]
    
    tmp = inimg_buf.copy().transpose((2,0,1)).reshape(-1,) # CH,Y,X
    send_buf[0:inimg_size] = tmp[0:inimg_size]

    # activate DMA
    registers.CTRL.AP_START = 1

    # DMA access
    dma.sendchannel.transfer(send_buf)
    dma.recvchannel.transfer(recv_buf)

    # wait DMA
    dma.sendchannel.wait()
    dma.recvchannel.wait()
    
    # store output buffer
    tmp2 = recv_buf[0:outfmap_size-1]
    tmp2 = tmp2.reshape((64,102)) # CH, X
    outfmap_buf[line] = tmp2
</code></pre><p>Pass the numpy array (inimg here) and set the transfer start register to ON (<code>AP_START</code>). After that, pass the buffer to <code>transfer</code> and wait until the transfer (that is, the processing of the convolution operation) is completed (<code>wait</code>). After that, pass the corresponding data to the numpy array and you&rsquo;re done. This is repeated for output lines.</p>
<p>Time was measured with <code>%%time</code>. It is a method to call an external command with Jupyter Notebook. And which</p>
<pre><code>CPU times: user 22.5 s, sys: 6.85 ms, total: 22.5 s
Wall time: 22.5 s
</code></pre><p>Late. .. .. After all, the estimate of 22 seconds and HLS was accurate. .. ..
I am also verifying after this. Check that the HW is working properly by moving it.</p>
<h3 id="bonus-what-is-the-inference-time-on-the-cpu">bonus. What is the inference time on the CPU?</h3>
<p>Incidentally, Pytorch should be installed, so let&rsquo;s check the CPU inference time.</p>
<pre><code class="language-python:ultra96v2_pynq_convolution_layer0.ipynb" data-lang="python:ultra96v2_pynq_convolution_layer0.ipynb">import torch
x = torch.randn(1,3,416,416)
conv = torch.nn.Conv2d(in_channels=3, out_channels=64, kernel_size=11,stride=4,bias=False)
y = conv(x)
</code></pre><pre><code>CPU times: user 259 ms, sys: 7.96 ms, total: 267 ms
Wall time: 93.2 ms
</code></pre><p>Well, about 100 times faster. .. .. This is this.
(That often happens&gt; FPGA design)</p>
<h3 id="what-are-you-doing">What are you doing?</h3>
<p>So I tried Pytorch learning → software design → hardware design → actual operation with FPGA, but the results were messy. .. I think you can understand how high the hurdles of hardware design are, and if it is deep learning.</p>
<p>It&rsquo;s not good at this condition, so let&rsquo;s try harder and make it faster.
So let&rsquo;s continue for a while.</p>

    </article>
  </div>

  
  
  
  <script>
  window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
  ga('create', 'UA-169005401-1', 'auto');
  ga('send', 'pageview');
  </script>
  <script async src='https://www.google-analytics.com/analytics.js'></script>
  

  

  
<link rel="stylesheet" type="text/css" href="//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.0/cookieconsent.min.css" />
<script src="//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.0/cookieconsent.min.js"></script>
<script>
window.addEventListener("load", function(){
window.cookieconsent.initialise({
  "palette": {
    "popup": {
      "background": "#216942",
      "text": "#b2d192"
    },
    "button": {
      "background": "#afed71"
    }
  }
})});
</script>

</body>

</html>
