<!doctype html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  
  <meta name="generator" content="Hugo 0.72.0" />

  
  <meta name="description" content="Some description">
  

  
  <link rel="apple-touch-icon" sizes="180x180" href="https://memotut.com/apple-touch-icon.png">

  
  <link rel="icon" type="image/png" sizes="32x32" href="https://memotut.com/favicon-32x32.png">

  
  <link rel="icon" type="image/png" sizes="16x16" href="https://memotut.com/favicon-16x16.png">

  
  <link rel="manifest" href="https://memotut.com/site.webmanifest">

  
  <link rel="mask-icon" href="https://memotut.com/safari-pinned-tab.svg" color="#5bbad5">

  <meta name="msapplication-TileColor" content="#da532c">

  <meta name="theme-color" content="#ffffff">

  
  <link rel="stylesheet" href="https://memotut.com/css/bootstrap.min.css" />

  
  <title>[Python] TensorFlow Deep MNIST for Experts 翻訳 | Memo Tut</title>
  

  <style>
body {
  min-width: 300px;
}

.custom-navbar {
  margin-bottom: 1em;
  height: 60px;
}

.custom-navbar a {
  display: inline-block; 
  padding: 18px 0;
  margin-right: 1em; 
  font-weight: bold; 
}

.custom-navbar a:hover,
.custom-navbar a:focus {
  text-decoration: none; 
}

@media print {
  .custom-navbar {
    display: none;
  }
}

article {
  padding-bottom: 1em;
}

img {
  max-width: 100%;
}


body {
  background-color: #fff;
}



body {
  color: #212529;
}



a {
  color: #007bff;
}



a:hover,
a:focus {
  color: #0056b3;
}



.custom-navbar {
  background-color: #212529;
}



.custom-navbar a {
  color: rgba(255, 255, 255, 0.75);
}



.custom-navbar a:hover,
.custom-navbar a:focus {
  color: rgba(255, 255, 255, 1);
}



.container {
  max-width: 800px;
}



pre {
  display: block;
  padding: 9.5px;
  word-break: break-all;
  word-wrap: break-word;
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 4px;
}

pre code {
  padding: 0;
  font-size: inherit;
  color: inherit; 
  white-space: pre-wrap;
  background-color: transparent;
  border: none;
  border-radius: 0;
}

code {
  padding: 2px 4px;
  color: inherit; 
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 4px;
  font-size: .9em;
}



blockquote,
.blockquote {
  padding: 10px 20px;
  margin: 0 0 20px;
  font-size: 1em;
  border-left: 5px solid #6c757d;
}

</style>
</head>

<body>
  <nav class="custom-navbar">
  <div class="container">
    
    <a href="/">Posts</a>
    
    <a href="/tags/">Tags</a>
    
    <a href="/about/">About</a>
    
    <a href="/index.xml">RSS</a>
    
  </div>
</nav>
  
  <div class="container">
    <article>
      <h1>[Python] TensorFlow Deep MNIST for Experts 翻訳</h1>
<p>
  <small class="text-secondary">
  
  
  Dec 31, 2015
  </small>
  

<small><code><a href="https://memotut.com/tags/python">Python</a></code></small>


<small><code><a href="https://memotut.com/tags/machine-learning"> Machine Learning</a></code></small>


<small><code><a href="https://memotut.com/tags/machinelearning"> MachineLearning</a></code></small>


<small><code><a href="https://memotut.com/tags/translation"> Translation</a></code></small>


<small><code><a href="https://memotut.com/tags/tensorflow"> TensorFlow</a></code></small>

</p>
<pre><code>#Introduction
</code></pre>
<p><a href="http://qiita.com/uramonk/items/13bb92e9c6a1b548ac4b">Before</a>and<a href="http://qiita.com/uramonk/items/c207c948ccb6cd0a1346">previous</a> using Beginner tutorial translation and actually using TensorFlow I did machine learning.
This time I translated the tutorial for Expert.</p>
<h1 id="deep-mnist-for-experts">Deep MNIST for Experts</h1>
<p>TensorFlow is a powerful library for performing large-scale numerical calculations.
One of the best tasks is the training and execution of deep neural networks.
In this tutorial we will learn the basic building blocks of a TensorFlow model while building a deep convolutional MNIST classifier.</p>
<p>This introduction assumes that you are familiar with neural networks and the MNIST dataset.
If you can&rsquo;t have those backgrounds, check out the &ldquo;Introduction for Beginners&rdquo; (<a href="https://www.tensorflow.org/versions/master/tutorials/mnist/beginners/index.html)">https://www.tensorflow.org/versions/master/tutorials/mnist/beginners/index.html)</a>.
Be sure to install TensorFlow before you start.</p>
<h2 id="setup">Setup</h2>
<p>Before building our model, we first load the MNIST dataset and then start a TensorFlow session.</p>
<h3 id="load-mnist-data">Load MNIST Data</h3>
<p>For your convenience, we will automatically download and import the MNIST dataset <a href="https://tensorflow.googlesource.com/tensorflow/+/master/tensorflow/examples/tutorials/mnist/input_data.py">script</a> included.
This will create a&rsquo;MNIST_data&rsquo; directory to store the data files.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py"><span style="color:#f92672">import</span> input_data
mnist <span style="color:#f92672">=</span> input_data<span style="color:#f92672">.</span>read_data_sets(<span style="color:#e6db74">&#39;MNIST_data&#39;</span>, one_hot<span style="color:#f92672">=</span>True)
</code></pre></div><p>This mnist is a lightweight class that stores a set of training, validation and tests as an array of NumPy.
It also provides a function that iterates through a mini-batch of data, which we will use below.</p>
<h3 id="start-tensorflow-interactive-session">Start TensorFlow Interactive Session</h3>
<p>TensorFlow relies on a highly effective C++ backend to do its calculations.
The connection to this backend is called a session.
The usual use of TensorFlow programs is to first create a graph and then start it in a session.</p>
<p>Here we instead use the convenient InteractiveSession class, which makes TensorFlow more flexible in how you build your code.
This allows you to plug in the process of starting a graph and building a <a href="https://www.tensorflow.org/versions/master/get_started/basic_usage.html#the-computation-graph">calculation graph</a> To
This is especially useful when working in a bidirectional context like iPython.
If you are not using InteractiveSession, you start a session and <a href="https://www.tensorflow.org/versions/master/get_started/basic_usage.html#launching-the-graph-in-a-session">start graph</a> should build the whole computational graph before.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py"><span style="color:#f92672">import</span> tensorflow <span style="color:#f92672">as</span> tf
sess <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>InteractiveSession()
</code></pre></div><h4 id="computation-graph">Computation Graph</h4>
<p>In order to do efficient numerical computations in Python, we generally use very effective code implemented in other languages, like NumPy, which does high processing like matrix multiplication outside Python. Use a different library.
Unfortunately, there will still be a lot of overhead in switching to all Python processing.
This overhead is especially bad if you want to compute on a GPU or in a distributed way that would have a high cost of transferring the data.</p>
<p>TensorFlow also does its hard work outside Python, but takes further steps to avoid this overhead.
Instead of running one expensive process that is Python independent, TensorFlow describes a graph of interacting processes that runs entirely outside of Python.
This approach is similar to using Theano or Torch.</p>
<p>The role of the Python code is thus to build this external computational graph, and to indicate which part of the computational graph to execute.
For more details, see <a href="https://www.tensorflow.org/versions/master">Calculation Graph</a>in<a href="https://www.tensorflow.org/versions/master/get_started/basic_usage.html">BasicUsage</a>Lookatthe/get_started/basic_usage.html#the-computation-graph) section.</p>
<h2 id="build-a-softmax-regression-model">Build a Softmax Regression Model</h2>
<p>In this section, we build a softmax regression model for one linear layer.
In the next section, we extend this to the case of softmax regression of multilayer convolutional networks.</p>
<h3 id="placeholders">Placeholders</h3>
<p>We start building a computational graph by creating nodes for image inputs and output classes of interest.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py">x <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>placeholder(<span style="color:#e6db74">&#34;float&#34;</span>, shape<span style="color:#f92672">=</span>[None, <span style="color:#ae81ff">784</span>])
y_ <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>placeholder(<span style="color:#e6db74">&#34;float&#34;</span>, shape<span style="color:#f92672">=</span>[None, <span style="color:#ae81ff">10</span>])
</code></pre></div><p>The <code>x</code> and <code>y_</code> are not special values.
Rather, they are each a placeholder, the value we enter when we ask TensorFlow to run a calculation.</p>
<p>The input image <code>x</code> consists of a 2D tensor of floating point numbers.
Here we assign it the form [None, 784]. 783 is the number of dimensions of the MNIST image parallelized to one row, and indicates that the first dimension None can be any size, corresponding to the batch size.
The target output class <code>y_</code> also consists of a 2D tensor. Each column is a one-hot 10-dimensional vector showing the numbers corresponding to the MNIST image.</p>
<p>The shape parameter of placeholder is optional, but it allows TensorFlow to automatically catch bugs that originate from inconsistent tensor shapes.</p>
<h3 id="variables">Variables</h3>
<p>We now define a weight <code>W</code> and a bias <code>b</code> for our model.
We can imagine dealing with these like additional inputs, but TensorFlow has a more sophisticated way to deal with them. Variable.
Variable is a value that exists in the calculation graph of TensorFlow.
In machine learning applications, most model parameters are Variable.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py">W <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>Variable(tf<span style="color:#f92672">.</span>zeros([<span style="color:#ae81ff">784</span>,<span style="color:#ae81ff">10</span>]))
b <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>Variable(tf<span style="color:#f92672">.</span>zeros([<span style="color:#ae81ff">10</span>]))
</code></pre></div><p>We pass initial values for those parameters by calling <code>tf.Variable</code>.
In this case we initialize both <code>W</code> and <code>b</code> as a tensor of all zeros.
<code>W</code> is a 784x10 matrix (because we have 784 input features and 10 outputs), and <code>b</code> is a 10-dimensional vector (because we have 10 classes).</p>
<p>Before Variables will be used in a session, they must be initialized using the session.
In this step, we take the already defined initial values (for an all-zero tensor) and assign them their respective Variables.
This can be done once for all Variables.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py">sess<span style="color:#f92672">.</span>run(tf<span style="color:#f92672">.</span>initialize_all_variables())
</code></pre></div><h3 id="predicted-class-and-cost-function">Predicted Class and Cost Function</h3>
<p>We can now implement our regression model.
It&rsquo;s only one line!
We multiply the vectorized input image <code>x</code> by the weight matrix <code>W</code>, add the bias <code>b</code>, and calculate the softmax probability assigned to each class.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py">y <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>softmax(tf<span style="color:#f92672">.</span>matmul(x,W) <span style="color:#f92672">+</span> b)
</code></pre></div><p>The cost function that is minimized during training is easily described.
Our cost function is the cross entropy between the target and the model prediction.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py">cross_entropy <span style="color:#f92672">=</span> <span style="color:#f92672">-</span>tf<span style="color:#f92672">.</span>reduce_sum(y_<span style="color:#f92672">*</span>tf<span style="color:#f92672">.</span>log(y))
</code></pre></div><p>Note that <code>tf.reduce_sum</code> sums over all images in the mini-batch as well as all classes.
We are computing the cross entropy for the whole mini-batch.</p>
<h2 id="train-the-model">Train the Model</h2>
<p>Now that we have defined our model and the training cost function, training with TensorFlow is simple.
Because TensorFlow knows all computational graphs and uses auto-discrimination to find the cost gradient for each variable.
TensorFlow has various <a href="https://www.tensorflow.org/versions/master/api_docs/python/train.html#optimizers">built-in optimization algorithms</a>.
For example, we take a step of length 0.01 and use the steepest descent to lower the cross entropy.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py">train_step <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>train<span style="color:#f92672">.</span>GradientDescentOptimizer(<span style="color:#ae81ff">0.01</span>)<span style="color:#f92672">.</span>minimize(cross_entropy)
</code></pre></div><p>What this TensorFlow actually did in one line is to add new processing to the calculation graph.
The process, including those that compute these gradients, computes the parameter update steps and applies the updated steps to the parameters.</p>
<p>The returned process, <code>train_step</code>, applies the gradient descent updating parameters as it is executed.
Model training will thus be trained by executing <code>train_step</code> multiple times.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py"><span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">1000</span>):
  batch <span style="color:#f92672">=</span> mnist<span style="color:#f92672">.</span>train<span style="color:#f92672">.</span>next_batch(<span style="color:#ae81ff">50</span>)
  train_step<span style="color:#f92672">.</span>run(feed_dict<span style="color:#f92672">=</span>{x: batch[<span style="color:#ae81ff">0</span>], y_: batch[<span style="color:#ae81ff">1</span>]})
</code></pre></div><p>At each training iteration, we load 50 training samples.We then use <code>feed_dict</code> to replace the <code>x</code> and <code>_y</code> of the <code>placeholder</code> tensor with the training swatches and perform the <code>train_step</code> process.
Note that you can replace any tensor in Atana&rsquo;s calculation graph using <code>feed_dict</code>. Not just a <code>placeholder</code>.</p>
<h3 id="evaluate-the-model">Evaluate the Model</h3>
<p>How do we improve our model?</p>
<p>First we calculate where we predicted the correct label.
<code>tf.argmax</code> is a very handy function that gives the index of the highest input in a tensor along an axis.
For example, <code>tf.argmax(_y,1)</code> is the true label, whereas <code>tf.argmax(y,1)</code> is the closest label for each input that our model considers.
We can use <code>tf.equal</code> to see if our prediction matches the truth.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py">correct_prediction <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>equal(tf<span style="color:#f92672">.</span>argmax(y,<span style="color:#ae81ff">1</span>), tf<span style="color:#f92672">.</span>argmax(y_,<span style="color:#ae81ff">1</span>))
</code></pre></div><p>This gives a list of booleans.
To determine if the parts are correct, we cast to floating point and then take the average.
For example, <code>[True, False, True, True]</code> becomes <code>[1,0,1,1]</code> and becomes <code>0.75</code>.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py">accuracy <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>reduce_mean(tf<span style="color:#f92672">.</span>cast(correct_prediction, <span style="color:#e6db74">&#34;float&#34;</span>))
</code></pre></div><p>Finally, we evaluate our accuracy with test data.
This should be about 91% correct.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py"><span style="color:#66d9ef">print</span>(accuracy<span style="color:#f92672">.</span>eval(feed_dict<span style="color:#f92672">=</span>{x: mnist<span style="color:#f92672">.</span>test<span style="color:#f92672">.</span>images, y_: mnist<span style="color:#f92672">.</span>test<span style="color:#f92672">.</span>labels}))
</code></pre></div><h2 id="build-a-multilayer-convolutional-network">Build a Multilayer Convolutional Network</h2>
<p>It&rsquo;s not good to get about 91% accuracy with MNIST.
Almost embarrassingly bad.
In this section, we modify it and jump from a very simple model to something reasonably sophisticated, a small convolutional neural network.
This gives an accuracy of about 99.2%. Not cutting edge, but straightforward.</p>
<h3 id="weight-initialization">Weight initialization</h3>
<p>To create this model, we need to create many weights and biases.
One generally initializes the weights with a small amount of noise due to symmetry breaking, preventing the gradient from going to zero.
Since we have used Normalized Linear Function (ReLU) neurons, it is a good practice to initialize them with an initial bias that is slightly positive to prevent dying neurons.
Instead of doing this iteratively while we build the model, let&rsquo;s create two easy-to-use functions that do it for us.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py"><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">weight_variable</span>(shape):
  initial <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>truncated_normal(shape, stddev<span style="color:#f92672">=</span><span style="color:#ae81ff">0.1</span>)
  <span style="color:#66d9ef">return</span> tf<span style="color:#f92672">.</span>Variable(initial)

<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">bias_variable</span>(shape):
  initial <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>constant(<span style="color:#ae81ff">0.1</span>, shape<span style="color:#f92672">=</span>shape)
  <span style="color:#66d9ef">return</span> tf<span style="color:#f92672">.</span>Variable(initial)
</code></pre></div><h3 id="convolution-and-pooling">Convolution and pooling</h3>
<p>TensorFlow also gives us a lot of flexibility in the convolution and subsampling process.
How to deal with boundaries?
What is the size of our stride?
In this example, we always choose the mediocre version.
Our convolution uses one step and is padded to 0 so that the output is the same size as the input.
Our subsampling is an old maximum subsampling (tier) scheme that exceeds 2x2 blocks.
To keep our code clean, let&rsquo;s also make these abstract functions functions.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py"><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">conv2d</span>(x, W):
  <span style="color:#66d9ef">return</span> tf<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>conv2d(x, W, strides<span style="color:#f92672">=</span>[<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>], padding<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;SAME&#39;</span>)

<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">max_pool_2x2</span>(x):
  <span style="color:#66d9ef">return</span> tf<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>max_pool(x, ksize<span style="color:#f92672">=</span>[<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">1</span>],
                        strides<span style="color:#f92672">=</span>[<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">1</span>], padding<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;SAME&#39;</span>)
</code></pre></div><h3 id="first-convolutional-layer">First Convolutional Layer</h3>
<p>We can now implement our first layer.
It consists of convolutions and occurs after max-pooling.
The convolution computes 32 features for a 5x5 patch.
The weight tensor has the form <code>[5, 5, 1, 32]</code>.
The first two dimensions are the patch size, the second is the number of input channels, and the last is the output channel.
We also have a bias vector with an element for each output channel.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py">W_conv1 <span style="color:#f92672">=</span> weight_variable([<span style="color:#ae81ff">5</span>, <span style="color:#ae81ff">5</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">32</span>])
b_conv1 <span style="color:#f92672">=</span> bias_variable([<span style="color:#ae81ff">32</span>])
</code></pre></div><p>To apply to the layers, we first transform the <code>x</code> into a 4D tensor with the second and third dimensions corresponding to the width and height of the image, and the last dimension corresponding to the number of color channels. Remake.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py">x_image <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>reshape(x, [<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">28</span>,<span style="color:#ae81ff">28</span>,<span style="color:#ae81ff">1</span>])
</code></pre></div><p>We then make a convolution of the <code>x_image</code> and the weight tensor, add a bias, apply the ReLU function, and finally compute the max pool.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py">h_conv1 <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>relu(conv2d(x_image, W_conv1) <span style="color:#f92672">+</span> b_conv1)
h_pool1 <span style="color:#f92672">=</span> max_pool_2x2(h_conv1)
</code></pre></div><h3 id="second-convolutional-layer">Second Convolutional Layer</h3>
<p>Instead of building a deep network, we stack several layers of this type.
The second layer has 64 features for each 5x5 patch.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py">W_conv2 <span style="color:#f92672">=</span> weight_variable([<span style="color:#ae81ff">5</span>, <span style="color:#ae81ff">5</span>, <span style="color:#ae81ff">32</span>, <span style="color:#ae81ff">64</span>])
b_conv2 <span style="color:#f92672">=</span> bias_variable([<span style="color:#ae81ff">64</span>])

h_conv2 <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>relu(conv2d(h_pool1, W_conv2) <span style="color:#f92672">+</span> b_conv2)
h_pool2 <span style="color:#f92672">=</span> max_pool_2x2(h_conv2)
</code></pre></div><h3 id="densely-connected-layer">Densely Connected Layer</h3>
<p>Now that the image size has been reduced to 7x7, we add a fully connected layer with 1024 neurons allowing the processing of the entire image.
We transform the tensor from a subsampling layer into a batch of vectors, multiply by the weight matrix, add a bias, and apply the ReLU.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py">W_fc1 <span style="color:#f92672">=</span> weight_variable([<span style="color:#ae81ff">7</span> <span style="color:#f92672">*</span> <span style="color:#ae81ff">7</span> <span style="color:#f92672">*</span> <span style="color:#ae81ff">64</span>, <span style="color:#ae81ff">1024</span>])
b_fc1 <span style="color:#f92672">=</span> bias_variable([<span style="color:#ae81ff">1024</span>])

h_pool2_flat <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>reshape(h_pool2, [<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">7</span><span style="color:#f92672">*</span><span style="color:#ae81ff">7</span><span style="color:#f92672">*</span><span style="color:#ae81ff">64</span>])
h_fc1 <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>relu(tf<span style="color:#f92672">.</span>matmul(h_pool2_flat, W_fc1) <span style="color:#f92672">+</span> b_fc1)
</code></pre></div><h4 id="dropout">Dropout</h4>
<p>To reduce overapplication, we apply dropout before the read layer.
We create a <code>placeholder</code> for the probability of holding the neuron&rsquo;s output during dropout.
This allows us to drop out during training, and quit running during testing.
TensorFlow&rsquo;s <code>tf.nn.dropout</code> operation automatically scales neuron outputs in addition to masking them, and the dropout executes without any additional scaling.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py">keep_prob <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>placeholder(<span style="color:#e6db74">&#34;float&#34;</span>)
h_fc1_drop <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>dropout(h_fc1, keep_prob)
</code></pre></div><h3 id="readout-layer">Readout Layer</h3>
<p>Finally, we add a softmax layer, similar to the one layer of softmax regression above.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py">W_fc2 <span style="color:#f92672">=</span> weight_variable([<span style="color:#ae81ff">1024</span>, <span style="color:#ae81ff">10</span>])
b_fc2 <span style="color:#f92672">=</span> bias_variable([<span style="color:#ae81ff">10</span>])

y_conv<span style="color:#f92672">=</span>tf<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>softmax(tf<span style="color:#f92672">.</span>matmul(h_fc1_drop, W_fc2) <span style="color:#f92672">+</span> b_fc2)
</code></pre></div><h3 id="train-and-evaluate-the-model">Train and Evaluate the Model</h3>
<p>How good is this model?
For training and evaluation, we use almost the same code as the one simple softmax network layer above.
This is the difference.
We transform the steepest descent optimization program into a more sophisticated ADAM optimization program.
We include an extra parameter <code>keep_prob</code> in the <code>deed_dict</code> to control the dropout rate.
And we add logging to all 100 iterations in the training process.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py">cross_entropy <span style="color:#f92672">=</span> <span style="color:#f92672">-</span>tf<span style="color:#f92672">.</span>reduce_sum(y_<span style="color:#f92672">*</span>tf<span style="color:#f92672">.</span>log(y_conv))
train_step <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>train<span style="color:#f92672">.</span>AdamOptimizer(<span style="color:#ae81ff">1e-4</span>)<span style="color:#f92672">.</span>minimize(cross_entropy)
correct_prediction <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>equal(tf<span style="color:#f92672">.</span>argmax(y_conv,<span style="color:#ae81ff">1</span>), tf<span style="color:#f92672">.</span>argmax(y_,<span style="color:#ae81ff">1</span>))
accuracy <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>reduce_mean(tf<span style="color:#f92672">.</span>cast(correct_prediction, <span style="color:#e6db74">&#34;float&#34;</span>))
sess<span style="color:#f92672">.</span>run(tf<span style="color:#f92672">.</span>initialize_all_variables())
<span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">20000</span>):
  batch <span style="color:#f92672">=</span> mnist<span style="color:#f92672">.</span>train<span style="color:#f92672">.</span>next_batch(<span style="color:#ae81ff">50</span>)
  <span style="color:#66d9ef">if</span> i<span style="color:#f92672">%</span><span style="color:#ae81ff">100</span> <span style="color:#f92672">==</span> <span style="color:#ae81ff">0</span>:
    train_accuracy <span style="color:#f92672">=</span> accuracy<span style="color:#f92672">.</span>eval(feed_dict<span style="color:#f92672">=</span>{
        x:batch[<span style="color:#ae81ff">0</span>], y_: batch[<span style="color:#ae81ff">1</span>], keep_prob: <span style="color:#ae81ff">1.0</span>})
    <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;step </span><span style="color:#e6db74">%d</span><span style="color:#e6db74">, training accuracy </span><span style="color:#e6db74">%g</span><span style="color:#e6db74">&#34;</span><span style="color:#f92672">%</span>(i, train_accuracy))
  train_step<span style="color:#f92672">.</span>run(feed_dict<span style="color:#f92672">=</span>{x: batch[<span style="color:#ae81ff">0</span>], y_: batch[<span style="color:#ae81ff">1</span>], keep_prob: <span style="color:#ae81ff">0.5</span>})

<span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;test accuracy </span><span style="color:#e6db74">%g</span><span style="color:#e6db74">&#34;</span><span style="color:#f92672">%</span>accuracy<span style="color:#f92672">.</span>eval(feed_dict<span style="color:#f92672">=</span>{
    x: mnist<span style="color:#f92672">.</span>test<span style="color:#f92672">.</span>images, y_: mnist<span style="color:#f92672">.</span>test<span style="color:#f92672">.</span>labels, keep_prob: <span style="color:#ae81ff">1.0</span>}))
</code></pre></div><p>The accuracy of the final test set after running this code should be about 99.2%.</p>
<p>We learned how to quickly and easily build, train, and evaluate fairly sophisticated models of deep learning with TensorFlow.</p>
<h1 id="in-conclusion">in conclusion</h1>
<p>The translation is over.It&rsquo;s more complicated than the beginner&rsquo;s tutorial, but it&rsquo;s similar in that it applies weights and biases in each layer before applying the function.
Next, I would like to actually execute the code while understanding this content more.</p>

    </article>
  </div>

  
  
  
  <script>
  window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
  ga('create', 'UA-169005401-1', 'auto');
  ga('send', 'pageview');
  </script>
  <script async src='https://www.google-analytics.com/analytics.js'></script>
  

  

  
<link rel="stylesheet" type="text/css" href="//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.0/cookieconsent.min.css" />
<script src="//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.0/cookieconsent.min.js"></script>
<script>
window.addEventListener("load", function(){
window.cookieconsent.initialise({
  "palette": {
    "popup": {
      "background": "#216942",
      "text": "#b2d192"
    },
    "button": {
      "background": "#afed71"
    }
  }
})});
</script>

</body>

</html>
