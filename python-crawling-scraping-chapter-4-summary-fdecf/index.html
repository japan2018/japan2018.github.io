<!doctype html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  
  <meta name="generator" content="Hugo 0.72.0" />

  
  <meta name="description" content="Some description">
  

  
  <link rel="apple-touch-icon" sizes="180x180" href="https://japan2018.github.io/apple-touch-icon.png">

  
  <link rel="icon" type="image/png" sizes="32x32" href="https://japan2018.github.io/favicon-32x32.png">

  
  <link rel="icon" type="image/png" sizes="16x16" href="https://japan2018.github.io/favicon-16x16.png">

  
  <link rel="manifest" href="https://japan2018.github.io/site.webmanifest">

  
  <link rel="mask-icon" href="https://japan2018.github.io/safari-pinned-tab.svg" color="#5bbad5">

  <meta name="msapplication-TileColor" content="#da532c">

  <meta name="theme-color" content="#ffffff">

  
  <link rel="stylesheet" href="https://japan2018.github.io/css/bootstrap.min.css" />

  
  <title>Python Crawling &amp; Scraping Chapter 4 Summary | Some Title</title>
  

  <style>
body {
  min-width: 300px;
}

.custom-navbar {
  margin-bottom: 1em;
  height: 60px;
}

.custom-navbar a {
  display: inline-block; 
  padding: 18px 0;
  margin-right: 1em; 
  font-weight: bold; 
}

.custom-navbar a:hover,
.custom-navbar a:focus {
  text-decoration: none; 
}

@media print {
  .custom-navbar {
    display: none;
  }
}

article {
  padding-bottom: 1em;
}

img {
  max-width: 100%;
}


body {
  background-color: #fff;
}



body {
  color: #212529;
}



a {
  color: #007bff;
}



a:hover,
a:focus {
  color: #0056b3;
}



.custom-navbar {
  background-color: #212529;
}



.custom-navbar a {
  color: rgba(255, 255, 255, 0.75);
}



.custom-navbar a:hover,
.custom-navbar a:focus {
  color: rgba(255, 255, 255, 1);
}



.container {
  max-width: 800px;
}



pre {
  display: block;
  padding: 9.5px;
  word-break: break-all;
  word-wrap: break-word;
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 4px;
}

pre code {
  padding: 0;
  font-size: inherit;
  color: inherit; 
  white-space: pre-wrap;
  background-color: transparent;
  border: none;
  border-radius: 0;
}

code {
  padding: 2px 4px;
  color: inherit; 
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 4px;
  font-size: .9em;
}



blockquote,
.blockquote {
  padding: 10px 20px;
  margin: 0 0 20px;
  font-size: 1em;
  border-left: 5px solid #6c757d;
}

</style>
</head>

<body>
  <nav class="custom-navbar">
  <div class="container">
    
    <a href="/">Posts</a>
    
    <a href="/tags/">Tags</a>
    
    <a href="/about/">About</a>
    
    <a href="/index.xml">RSS</a>
    
  </div>
</nav>
  
  <div class="container">
    <article>
      <h1>Python Crawling &amp; Scraping Chapter 4 Summary</h1>
<p>
  <small class="text-secondary">
  
  
  Dec 16, 2019
  </small>
  

<small><code><a href="https://japan2018.github.io/tags/python">Python</a></code></small>


<small><code><a href="https://japan2018.github.io/tags/scraping"> scraping</a></code></small>


<small><code><a href="https://japan2018.github.io/tags/python3"> Python3</a></code></small>


<small><code><a href="https://japan2018.github.io/tags/crawling"> crawling</a></code></small>

</p>
<pre><code>#Introduction
</code></pre>
<p>Learning summary of &ldquo;Python crawling &amp; scraping [enhanced and revised version] Practical development guide for data collection and analysis&rdquo;
The fourth chapter of this time was mainly about the points to note when making a crawler under the theme of &ldquo;methods for practical use&rdquo;.</p>
<p>#4.1 Crawler characteristics
##4.1.1 Crawler with state</p>
<ul>
<li>
<p>Create a crawler that supports cookies when crawling sites that require login
-Python&rsquo;s Requests library automatically sends cookies to the server by using Session object</p>
</li>
<li>
<p>Referer
-HTTP header to send the URL of the previous page viewed to the server
-(Example) When you access Qiita from Google search results, it looks like this when you check with Chrome&rsquo;s verification tool
<img width="778" alt="referrer.png" src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/253705/769bbe79-1ebd-483f-d1ee-611beed070b1.png"></p>
</li>
</ul>
<h2 id="412-crawler-that-interprets-javascript">4.1.2 Crawler that interprets JavaScript</h2>
<p>JavaScript to crawl sites created as SPA (Single Page Application)
Need to be interpreted. To do this, use tools such as Selenium and Puppeteer to operate browsers automatically.
Also, browsers such as Chrome and FireFox have a headless mode that can be run without a GUI, which can be useful for creating crawlers.</p>
<h2 id="413-crawler-for-unspecified-number-of-websites">4.1.3 Crawler for unspecified number of websites</h2>
<p>Things like Googlebot. More difficult than a crawler that targets a specific site.
A mechanism that does not depend on the page structure is required.</p>
<p>#4.2 Notes on using collected data
##4.2.1 Copyright
Copyrights to be careful of when creating crawlers → Duplication rights, adaptation rights, public transmission rights
<code>Revision of copyright law in 2009 allows duplication for information analysis and duplication/adaptation/automatic public transmission for the purpose of providing search engine service without permission of the copyright holder</code>
##4.2.2 Terms of use and personal information
A story about keeping the terms of the site.
Personal information will be managed based on the Personal Information Protection Law.</p>
<p>#4.3 Notes on the load at the crawl destination
How to avoid overloading the crawl destination.
<a href="https://en.wikipedia.org/wiki/OkazakiCityCentralLibraryCase">Okazaki City Central Library Case-Wikipedia</a>
There was such an incident
##4.3.1 Number of simultaneous connections and crawl interval</p>
<ul>
<li>Number of simultaneous connections
-Recent browsers have a maximum of 6 simultaneous connections per host, but the crawler gets multiple pages for a long time, so it should be less than this.</li>
<li>Crawl interval
-It is customary to provide an interval of 1 second or more. Example: Crawler operated by the Diet Library
-If there is a method to acquire information other than HTML such as RSS and XML, use that.</li>
</ul>
<p>##4.3.2 Instructions to the crawler with robots.txt</p>
<ul>
<li>robots.txt
-Write instructions (directives) to the crawler in a standardized format called Robots Exclusion Protocol
-RobotFile for parsing robots.txt in Python standard library urllib.robotparser
There is a Parser class</li>
<li>robots meta tag
-Describe the instructions to the crawler in the HTML meta tag <code>&lt;meta name=&quot;robots&quot; content=&quot;&lt;attribute value&gt;&quot;&gt;</code>
-Attribute values include <code>nofollow</code> not allowed to follow links on the page, <code>noarchive</code> not allowed to be saved, <code>noindex</code> not allowed to index by search engines</li>
</ul>
<p>Regarding netkeiba who is always scraping, there seems to be no particular instructions in robots.txt or meta tag.</p>
<p>##4.3.3 XML site map
XML file that tells the crawler what URL you want it to crawl
More efficient than following links and crawling
Describe in Sitemap directive of robots.txt</p>
<p>##4.3.4 Clear contact information
You can write contact information such as email address and URL in User-Agent header of request sent by crawler</p>
<p>##4.3.5 Status code and error handling
Error handling is important to prevent extra load on the crawl destination
When retrying when an error occurs, take measures such as increasing the retry interval exponentially.
Error handling is often described in a standard way, but it can be described simply by using the library called tenacity.</p>
<p>#4.4 Design assuming repeated execution
##4.4.1 Get only updated data</p>
<ul>
<li>HTTP cache policy
-HTTP server can specify cache policy in detail by adding cache related header to response
-These headers are divided into two types: &ldquo;strong cache&rdquo; and &ldquo;weak cache&rdquo;.
-Strong cache → Cache-Control (detailed specification such as whether to cache, etc.), Expires (expiration date of content) The client side does not send a request while the cache is valid. Use the cached response during the expiration.
-Weak cache → Last-Modified (last modified date), ETag (identifier) The client sends a request each time, but if it has not been updated, it uses the cached response.
-In Python, a library called CacheControl allows you to handle cache related operations in a simple way <code>pip install &quot;CacheControl[filecache]&quot;</code></li>
</ul>
<p>##4.4.2 Detect changes in crawling destination</p>
<ul>
<li>Validate with regular expression</li>
<li>Validate with JSON Schema
-Python can use jsonschema library to describe validation rules in JSON format called JSON Schema <code>pip install jsonschema</code></li>
</ul>
<p>When a change can be detected by such a method, the crawler is terminated by sending an email or the like.
#4.5 Summary
Omitted</p>
<p>#in conclusion
I was not motivated and the interval between posts was open, but for the time being it is an article of confirmation of survival (?)</p>

    </article>
  </div>

  
  
  
  <script>
  window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
  ga('create', 'UA-123456789-1', 'auto');
  ga('send', 'pageview');
  </script>
  <script async src='https://www.google-analytics.com/analytics.js'></script>
  

  

  
<link rel="stylesheet" type="text/css" href="//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.0/cookieconsent.min.css" />
<script src="//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.0/cookieconsent.min.js"></script>
<script>
window.addEventListener("load", function(){
window.cookieconsent.initialise({
  "palette": {
    "popup": {
      "background": "#216942",
      "text": "#b2d192"
    },
    "button": {
      "background": "#afed71"
    }
  }
})});
</script>

</body>

</html>
