<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title> statistics on Some Title</title>
    <link>https://japan2018.github.io/tags/statistics/</link>
    <description>Recent content in  statistics on Some Title</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 18 Apr 2020 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://japan2018.github.io/tags/statistics/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>[Linear regression] Number of explanatory variables and coefficient of determination (adjusted for degrees of freedom)</title>
      <link>https://japan2018.github.io/linear-regression-number-of-explanatory-variables-and-coefficient-of-determination-adjusted-for-degrees-of-freedom-fab7d/</link>
      <pubDate>Sat, 18 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>https://japan2018.github.io/linear-regression-number-of-explanatory-variables-and-coefficient-of-determination-adjusted-for-degrees-of-freedom-fab7d/</guid>
      <description># at first  For the explanatory variables of linear regression, &amp;ldquo;It is better not to use variables unrelated to the objective variable for learning&amp;rdquo; (*1) I have the image. However, while studying linear regression analysis, I have learned that the coefficient of determination has the following properties.
&amp;ldquo;When an explanatory variable is added, the coefficient of determination cannot fall below that before addition. (So, for accuracy comparison of models with different numbers of explanatory variables, use the &amp;ldquo;coefficient of determination with adjusted degrees of freedom&amp;rdquo;.</description>
    </item>
    
    <item>
      <title>Regression analysis in Python</title>
      <link>https://japan2018.github.io/regression-analysis-in-python-f9028/</link>
      <pubDate>Mon, 06 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>https://japan2018.github.io/regression-analysis-in-python-f9028/</guid>
      <description>Hello, this is Motty.  This time, I wrote about regression analysis using Python.
#Regression
Regression analysis is a method of predicting target data using the data at hand. At that time, the structure of quantitative relationships is applied to the data (regression model). If the regression model is a straight line, it is called a regression line, and if an n-order function is applied by polynomial regression, it is called a regression curve.</description>
    </item>
    
    <item>
      <title>Mathematical statistics from the basics Random variables</title>
      <link>https://japan2018.github.io/mathematical-statistics-from-the-basics-random-variables-fb901/</link>
      <pubDate>Fri, 31 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>https://japan2018.github.io/mathematical-statistics-from-the-basics-random-variables-fb901/</guid>
      <description>#Random variables and dice  First of all, let&amp;rsquo;s consider an example of 1 to 6 dice: game_die: which has no distortion.
The dice rolls from 1 to 6 are equally likely (there is no bias in their appearance), so the appearance of each die can be given with the following probabilities.
$ P (probability of getting 1) = \frac{1}{6}\qquad P (probability of getting 2) = \frac{1}{6}\qquad P (probability of getting 3) = \frac{1}{6}\</description>
    </item>
    
    <item>
      <title>Calculation of mutual information (continuous value) with numpy</title>
      <link>https://japan2018.github.io/calculation-of-mutual-information-continuous-value-with-numpy-ffaab/</link>
      <pubDate>Mon, 11 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>https://japan2018.github.io/calculation-of-mutual-information-continuous-value-with-numpy-ffaab/</guid>
      <description>#Motivation  I want to calculate the mutual information $I(X;Y)$ of continuous random variables $X$ and $Y$ with Python. $$ I(X;Y) = \int_Y \int_X p(x, y) \log \frac{p(x,y)}{p(x)p(y)} dx dy $$
#Code
import numpy def mutual_information(X, Y, bins=10): # Compute joint probability distribution p(x,y) p_xy, xedges, yedges = np.histogram2d(X, Y, bins=bins, density=True) #p(x)p(y) calculation p_x, _ = np.histogram(X, bins=xedges, density=True) p_y, _ = np.histogram(Y, bins=yedges, density=True) p_x_y = p_x[:, np.newaxis] * p_y #dx and dy dx = xedges[1]-xedges[0] dy = yedges[1]-yedges[0] # Integral element elem = p_xy * np.</description>
    </item>
    
  </channel>
</rss>