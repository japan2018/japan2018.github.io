<!doctype html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  
  <meta name="generator" content="Hugo 0.72.0" />

  
  <meta name="description" content="Some description">
  

  
  <link rel="apple-touch-icon" sizes="180x180" href="https://memotut.com/apple-touch-icon.png">

  
  <link rel="icon" type="image/png" sizes="32x32" href="https://memotut.com/favicon-32x32.png">

  
  <link rel="icon" type="image/png" sizes="16x16" href="https://memotut.com/favicon-16x16.png">

  
  <link rel="manifest" href="https://memotut.com/site.webmanifest">

  
  <link rel="mask-icon" href="https://memotut.com/safari-pinned-tab.svg" color="#5bbad5">

  <meta name="msapplication-TileColor" content="#da532c">

  <meta name="theme-color" content="#ffffff">

  
  <link rel="stylesheet" href="https://memotut.com/css/bootstrap.min.css" />

  
  <title>[Python] Let&#39;s recognize emotions with Azure Face | Memo Tut</title>
  

  <style>
body {
  min-width: 300px;
}

.custom-navbar {
  margin-bottom: 1em;
  height: 60px;
}

.custom-navbar a {
  display: inline-block; 
  padding: 18px 0;
  margin-right: 1em; 
  font-weight: bold; 
}

.custom-navbar a:hover,
.custom-navbar a:focus {
  text-decoration: none; 
}

@media print {
  .custom-navbar {
    display: none;
  }
}

article {
  padding-bottom: 1em;
}

img {
  max-width: 100%;
}


body {
  background-color: #fff;
}



body {
  color: #212529;
}



a {
  color: #007bff;
}



a:hover,
a:focus {
  color: #0056b3;
}



.custom-navbar {
  background-color: #212529;
}



.custom-navbar a {
  color: rgba(255, 255, 255, 0.75);
}



.custom-navbar a:hover,
.custom-navbar a:focus {
  color: rgba(255, 255, 255, 1);
}



.container {
  max-width: 800px;
}



pre {
  display: block;
  padding: 9.5px;
  word-break: break-all;
  word-wrap: break-word;
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 4px;
}

pre code {
  padding: 0;
  font-size: inherit;
  color: inherit; 
  white-space: pre-wrap;
  background-color: transparent;
  border: none;
  border-radius: 0;
}

code {
  padding: 2px 4px;
  color: inherit; 
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 4px;
  font-size: .9em;
}



blockquote,
.blockquote {
  padding: 10px 20px;
  margin: 0 0 20px;
  font-size: 1em;
  border-left: 5px solid #6c757d;
}

</style>
</head>

<body>
  <nav class="custom-navbar">
  <div class="container">
    
    <a href="/">Posts</a>
    
    <a href="/tags/">Tags</a>
    
    <a href="/about/">About</a>
    
    <a href="/index.xml">RSS</a>
    
  </div>
</nav>
  
  <div class="container">
    <article>
      <h1>[Python] Let&rsquo;s recognize emotions with Azure Face</h1>
<p>
  <small class="text-secondary">
  
  
  Dec 20, 2019
  </small>
  

<small><code><a href="https://memotut.com/tags/python">Python</a></code></small>


<small><code><a href="https://memotut.com/tags/depression"> Depression</a></code></small>


<small><code><a href="https://memotut.com/tags/faceapi"> FaceAPI</a></code></small>


<small><code><a href="https://memotut.com/tags/azurefaceapi"> AzureFaceAPI</a></code></small>

</p>
<pre><code>I've used it recently, so I'll write a little.
</code></pre>
<p>Since this is my first post, there may be many mistakes and missing parts.
I would appreciate it if you could kindly teach me :girl_tone1:</p>
<p>Azure Face had a strong impression of face authentication,
I thought it was interesting because I could recognize emotions, so I tried using it for emotion recognition.</p>
<p>#<a href="https://azure.microsoft.com/en-us/services/cognitive-services/face/">Azure Face</a>
*I used the python SDK.</p>
<p>What you can do: face verification, face detection, emotion recognition
It seems that there are other ways to find similar faces, group with the same face, etc.
In this article, we use face detection and emotion recognition.</p>
<p>##1. Create variables for Azure endpoint and key
You can use it by writing this: ok_woman_tone1:</p>
<pre><code>KEY ='your_key'
ENDPOINT ='your_endpoint'
face_client = FaceClient(ENDPOINT, CognitiveServicesCredentials(KEY))
</code></pre><p>##2. Call Face Detection Method</p>
<pre><code>params = ['age','emotion']
result = face_client.face.detect_with_stream(
             image
             , return_face_id=True
             , return_face_landmarks=False
             , return_face_attributes=params
         )
</code></pre><p>Basically, leaving the default, I changed only return_face_attributes from False.
image: This time pass a stream of images
return_face_id: If set to True, the face ID will be returned.
return_face_landmarks: When set to True, it will return. Face landmark
return_face_attributes: Returns the attribute information passed by parameter.</p>
<p>This time I passed age, emotion to return_face_attributes
The subject&rsquo;s age and emotions will be returned.</p>
<h1 id="make-ui">Make UI</h1>
<p>I want to be able to take a picture by pressing a button on the camera → emotion recognition → display the result… :upside_down:
I will make it using kivy.</p>
<h2 id="make-a-simple-kivy-file">make a simple kivy file</h2>
<p>[Emotion.py]</p>
<pre><code># -*- coding: utf-8 -*-
from kivy.app import App
from kivy.uix.boxlayout import BoxLayout
from kivy.lang import Builder

Builder.load_string(&quot;&quot;&quot;
&lt;MainWidget&gt;:

&quot;&quot;&quot;)

class MainWidget(BoxLayout):
    pass
class EmotionApp(App):
    title = &quot;E.A.&quot;
    def build(self):
        return MainWidget()

if __name__ == &quot;__main__&quot;:
    EmotionApp().run()
</code></pre><p>Screen ↓
<img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/364550/33fc473e-a8e7-5697-04ef-d7c4057ccb16.png" alt="E.A1.png"></p>
<p>Black</p>
<p>##2. Attach the camera and buttons.
[Emotion.py]</p>
<pre><code>(Omitted)

Builder.load_string(&quot;&quot;&quot;
&lt;MainWidget&gt;:
    orientation:'vertical'
    Camera:
        id: camera
        resolution: (640, 480)
        play: True
    Button:
        text:'Capture'
        size_hint_y: None
        height: '48dp'
        on_press: root.capture()
&quot;&quot;&quot;)

class MainWidget(BoxLayout):
    def capture(self):
        print('captured!')
(Omitted)

</code></pre><p>↓Screen (It&rsquo;s just a reference because I processed it so rough that the face and background are not reflected&hellip;)
<img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/364550/b0520fe5-8618-b72e-dba4-0423bc7266d9.png" alt="Untitled.png"></p>
<h2 id="insert-face-detection-method">Insert Face Detection Method</h2>
<p>[Emotion.py]</p>
<pre><code>KEY ='your_key'
ENDPOINT ='your_endpoint'
face_client = FaceClient(ENDPOINT, CognitiveServicesCredentials(KEY))

 (Omitted)
    def capture(self):
        texture = self.ids['camera'].texture
        nparr = np.fromstring(texture.pixels, dtype=np.uint8)
        reshaped = np.reshape(nparr, (480,640,4))
        ret,buf = cv2.imencode('.jpg', reshaped)
        stream = io.BytesIO(buf)

        params = ['age','emotion','hair']
        result = face_client.face.detect_with_stream(
                     stream
                     , return_face_id=True
                     , return_face_landmarks=False
                     , return_face_attributes=params
                 )
(Omitted)
</code></pre><h2 id="result-display-popup">Result display popup</h2>
<p>[Emotion.py]</p>
<pre><code>(Omitted)
Layout = BoxLayout(orientation='vertical')
       faceatts = result[0].face_attributes
       Layout.add_widget(Label(text=(&quot;age: &quot;+ str(int(faceatts.age)))))
       Layout.add_widget(Label(text=&quot;emotion: &quot;))
       Layout.add_widget(Label(text=&quot;anger: &quot;+ str(faceatts.emotion.anger)))
       Layout.add_widget(Label(text=&quot;contempt: &quot;+ str(faceatts.emotion.contempt)))
       Layout.add_widget(Label(text=&quot;disgust: &quot;+ str(faceatts.emotion.disgust)))
       Layout.add_widget(Label(text=&quot;fear: &quot;+ str(faceatts.emotion.fear)))
       Layout.add_widget(Label(text=&quot;happiness: &quot;+ str(faceatts.emotion.happiness)))
       Layout.add_widget(Label(text=&quot;neutral: &quot;+ str(faceatts.emotion.neutral)))
       Layout.add_widget( Label(text=&quot;sadness: &quot;+ str(faceatts.emotion.sadness)))
       Layout.add_widget(Label(text=&quot;surprise: &quot;+ str(faceatts.emotion.surprise)))

       popupWindow = Popup(title=&quot;Results&quot;, content=Layout, size_hint=(None,None), size=(400,400))
       popupWindow.open()
     (Omitted)
</code></pre><p>##did it!
I will try using it.
<img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/364550/f1cf992b-cad8-afea-2228-f27881883b42.png" alt="Untitled 2.png">
Click the button!
<img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/364550/172f56b1-a397-5875-ddba-a586ecb9d57c.png" alt="Untitled 3.png">
Oh~
I was told that I was laughing properly! I&rsquo;m getting younger&hellip; (laughs) :relaxed:
After that, I wanted to stick to the look and learn about the mechanism: robot:
Thank you for reading!</p>
<pre><code># -*- coding: utf-8 -*-
from kivy.app import App
from kivy.uix.boxlayout import BoxLayout
from kivy.lang import Builder
import cv2
import numpy as np
import io
from azure.cognitiveservices.vision.face import FaceClient
from msrest.authentication import CognitiveServicesCredentials
from kivy.uix.label import Label
from kivy.uix.popup import Popup
KEY ='your_key'
ENDPOINT ='your_end_point'
face_client = FaceClient(ENDPOINT, CognitiveServicesCredentials(KEY))
Builder.load_string(&quot;&quot;&quot;
&lt;MainWidget&gt;:
    orientation:'vertical'
    Camera:
        id: camera
        resolution: (640, 480)
        play: True
    Button:
        text:'Capture'size_hint_y: None
        height: '48dp'
        on_press: root.capture()
&quot;&quot;&quot;)

class MainWidget(BoxLayout):
    def capture(self):
        texture = self.ids['camera'].texture
        nparr = np.fromstring(texture.pixels, dtype=np.uint8)
        reshaped = np.reshape(nparr, (480,640,4))
        ret,buf = cv2.imencode('.jpg', reshaped)
        stream = io.BytesIO(buf)
        params = ['age','emotion']
        result = face_client.face.detect_with_stream(  
                    stream
                    , return_face_id=True
                    , return_face_landmarks=False
                    , return_face_attributes=params 
            )
        Layout = BoxLayout(orientation='vertical')
        faceatts = result[0].face_attributes
        Layout.add_widget(Label(text=(&quot;age: &quot;+ str(int(faceatts.age)))))
        Layout.add_widget(Label(text=&quot;emotion: &quot;))      
        Layout.add_widget(Label(text=&quot;anger: &quot; + str(faceatts.emotion.anger)))
        Layout.add_widget(Label(text=&quot;contempt: &quot; + str(faceatts.emotion.contempt)))
        Layout.add_widget(Label(text=&quot;disgust: &quot; + str(faceatts.emotion.disgust)))
        Layout.add_widget(Label(text=&quot;fear: &quot; + str(faceatts.emotion.fear)))
        Layout.add_widget(Label(text=&quot;happiness: &quot; + str(faceatts.emotion.happiness)))
        Layout.add_widget(Label(text=&quot;neutral: &quot; + str(faceatts.emotion.neutral)))
        Layout.add_widget( Label(text=&quot;sadness: &quot; + str(faceatts.emotion.sadness)))
        Layout.add_widget(Label(text=&quot;surprise: &quot; + str(faceatts.emotion.surprise)))

        popupWindow = Popup(title=&quot;Results&quot;, content=Layout, size_hint=(None,None),size=(400,400)) 
        popupWindow.open()
        
class EmotionApp(App):
    title = &quot;E.A.&quot;
    def build(self):
        return MainWidget()

if __name__ == &quot;__main__&quot;:
    EmotionApp().run()


</code></pre>
    </article>
  </div>

  
  
  
  <script>
  window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
  ga('create', 'UA-169005401-1', 'auto');
  ga('send', 'pageview');
  </script>
  <script async src='https://www.google-analytics.com/analytics.js'></script>
  

  

  
<link rel="stylesheet" type="text/css" href="//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.0/cookieconsent.min.css" />
<script src="//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.0/cookieconsent.min.js"></script>
<script>
window.addEventListener("load", function(){
window.cookieconsent.initialise({
  "palette": {
    "popup": {
      "background": "#216942",
      "text": "#b2d192"
    },
    "button": {
      "background": "#afed71"
    }
  }
})});
</script>

</body>

</html>
