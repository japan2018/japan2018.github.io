<!doctype html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  
  <meta name="generator" content="Hugo 0.72.0" />

  
  <meta name="description" content="Some description">
  

  
  <link rel="apple-touch-icon" sizes="180x180" href="https://memotut.com/apple-touch-icon.png">

  
  <link rel="icon" type="image/png" sizes="32x32" href="https://memotut.com/favicon-32x32.png">

  
  <link rel="icon" type="image/png" sizes="16x16" href="https://memotut.com/favicon-16x16.png">

  
  <link rel="manifest" href="https://memotut.com/site.webmanifest">

  
  <link rel="mask-icon" href="https://memotut.com/safari-pinned-tab.svg" color="#5bbad5">

  <meta name="msapplication-TileColor" content="#da532c">

  <meta name="theme-color" content="#ffffff">

  
  <link rel="stylesheet" href="https://memotut.com/css/bootstrap.min.css" />

  
  <title>[Pokemon sword shield] I tried to visualize the basis of deep learning judgment by taking the Osanke classification as an example | Memo Tut</title>
  

  <style>
body {
  min-width: 300px;
}

.custom-navbar {
  margin-bottom: 1em;
  height: 60px;
}

.custom-navbar a {
  display: inline-block; 
  padding: 18px 0;
  margin-right: 1em; 
  font-weight: bold; 
}

.custom-navbar a:hover,
.custom-navbar a:focus {
  text-decoration: none; 
}

@media print {
  .custom-navbar {
    display: none;
  }
}

article {
  padding-bottom: 1em;
}

img {
  max-width: 100%;
}


body {
  background-color: #fff;
}



body {
  color: #212529;
}



a {
  color: #007bff;
}



a:hover,
a:focus {
  color: #0056b3;
}



.custom-navbar {
  background-color: #212529;
}



.custom-navbar a {
  color: rgba(255, 255, 255, 0.75);
}



.custom-navbar a:hover,
.custom-navbar a:focus {
  color: rgba(255, 255, 255, 1);
}



.container {
  max-width: 800px;
}



pre {
  display: block;
  padding: 9.5px;
  word-break: break-all;
  word-wrap: break-word;
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 4px;
}

pre code {
  padding: 0;
  font-size: inherit;
  color: inherit; 
  white-space: pre-wrap;
  background-color: transparent;
  border: none;
  border-radius: 0;
}

code {
  padding: 2px 4px;
  color: inherit; 
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 4px;
  font-size: .9em;
}



blockquote,
.blockquote {
  padding: 10px 20px;
  margin: 0 0 20px;
  font-size: 1em;
  border-left: 5px solid #6c757d;
}

</style>
</head>

<body>
  <nav class="custom-navbar">
  <div class="container">
    
    <a href="/">Posts</a>
    
    <a href="/tags/">Tags</a>
    
    <a href="/about/">About</a>
    
    <a href="/index.xml">RSS</a>
    
  </div>
</nav>
  
  <div class="container">
    <article>
      <h1>[Pokemon sword shield] I tried to visualize the basis of deep learning judgment by taking the Osanke classification as an example</h1>
<p>
  <small class="text-secondary">
  
  
  Dec 24, 2019
  </small>
  

<small><code><a href="https://memotut.com/tags/python">Python</a></code></small>


<small><code><a href="https://memotut.com/tags/deeplearning">DeepLearning</a></code></small>


<small><code><a href="https://memotut.com/tags/tensorflow">TensorFlow</a></code></small>


<small><code><a href="https://memotut.com/tags/pokemon">pokemon</a></code></small>

</p>
<pre><code>#Introduction
</code></pre>
<p>Do you guys play Pokemon? I bought it for the first time in 10 years ~~ I got it from Santa.
Aiming to get serious, we plan to stay home and select carefully during the year-end and New Year holidays.
I was wondering if AdventCalendar could do something with Pokemon news items, so I tried ** a method that shows the basis of judgment of deep learning model ** using Pokemon Okayama family classification as an example ** Saw.</p>
<p>*We will describe the method and prepare the dataset, so if you want to know only the result, please skip to <a href="#Result">Result</a>.</p>
<h1 id="method-to-show-the-judgment-basis-of-deep-learning-model-what-is-tcav">Method to show the judgment basis of deep learning model: What is TCAV</h1>
<p>Although deep learning is beginning to be implemented in various fields, social models tend to be a black box as to what the model makes decisions.
In recent years, research on &ldquo;explanatoryness&rdquo; and &ldquo;interpretability&rdquo; of models has been advanced.</p>
<p>Therefore, this time, I would like to try the method adopted by ICML2018, <a href="https://arxiv.org/abs/1711.11279">Quantitative Testing with Concept Activation Vectors (TCAV)</a>.</p>
<h2 id="summary">Summary</h2>
<ul>
<li>** Method to show the basis of judgment of neural network model **</li>
<li>Indicates the importance of the ** concept ** (color, gender, race, etc.) of the prediction class instead of the conventional method of calculating the importance for each pixel **</li>
<li>It generates explanations for each class** (≒global) instead of explanations for each image (≒local), so it is easy for humans to understand.</li>
<li><strong>Understand the explanation without ML model expertise</strong></li>
<li><strong>No need to retrain or change existing models you want to interpret</strong></li>
</ul>
<h2 id="concept-activation-vectors-cav-concept">Concept Activation Vectors (CAV) Concept</h2>
<p>Derive CAV by training a linear classifier between the concept image and a random counterexample to obtain the vector orthogonal to the decision boundary.
(It is faster to see the figure below).</p>
<p><img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/177134/2f5b3ae2-5fc9-9492-eefe-00a360b51748.png" alt="image.png"></p>
<p>*For more detailed paper memos, please see <a href="https://github.com/rio-kurihara/tcav/blob/master/paper_memo.md">here</a>.</p>
<h2 id="what-do-you-know">What do you know?</h2>
<ul>
<li>
<p>Quantify **&ldquo;concepts&rdquo; that the model is learning in a <strong>human-interpretable</strong> form
Example: Learning &ldquo;stripe pattern&rdquo; from &ldquo;dot pattern&rdquo; in &ldquo;zebra&rdquo; classification
You can also see learning in any layer, so you can also see how coarse/fine features are captured in shallow/deep layers.
<img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/177134/f937ecd7-de0e-3098-c656-5a5a2bf97059.png" alt="image.png"></p>
</li>
<li>
<p>Know the <strong>bias</strong> of your dataset
Example: In the &ldquo;apron&rdquo; class, the concept of &ldquo;female&rdquo; is related, in the &ldquo;rugby ball&rdquo; class, the concept of &ldquo;white&rdquo; is related
<img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/177134/1ac027dd-c5f5-9b2b-a40f-579d1fac6477.png" alt="image.png"><img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/177134/b4801e1e-3e41-ce2f-1231-2717e27cd922.png" alt="image.png"></p>
</li>
<li>
<p>Can be used as an image sorter (can be sorted based on the similarity to the conceptual image)</p>
</li>
</ul>
<h1 id="first-make-a-suitable-classifier">First make a suitable classifier</h1>
<p>This time, my goal was to run TCAV, so I made it a simple task.
Make a Pokemon Osanke classifier.</p>
<h2 id="dataset-preparation">Dataset preparation</h2>
<h3 id="-crawling">① Crawling</h3>
<p>I have collected the following images using <a href="https://icrawler.readthedocs.io/en/latest/builtin.html#search-engine-crawlers">icrawler</a>.
I will post the code.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> os
<span style="color:#f92672">from</span> icrawler.builtin <span style="color:#f92672">import</span> GoogleImageCrawler

save_dir <span style="color:#f92672">=</span><span style="color:#e6db74">&#39;../datasets/hibany&#39;</span>
os<span style="color:#f92672">.</span>makedirs(save_dir, exist_ok<span style="color:#f92672">=</span>True)

query <span style="color:#f92672">=</span><span style="color:#e6db74">&#39;Hibany&#39;</span>
max_num <span style="color:#f92672">=</span> <span style="color:#ae81ff">200</span>

google_crawler <span style="color:#f92672">=</span> GoogleImageCrawler(storage<span style="color:#f92672">=</span>{<span style="color:#e6db74">&#39;root_dir&#39;</span>: save_dir})
google_crawler<span style="color:#f92672">.</span>crawl(keyword<span style="color:#f92672">=</span>query, max_num<span style="color:#f92672">=</span>max_num)
</code></pre></div><h3 id="-pre-processing">② Pre-processing</h3>
<p>Only minimal processing.</p>
<ol>
<li>① Manually crop the image acquired by crawling into a square</li>
<li>Resize to 256 x 256
3.Split into train/val/test</li>
</ol>
<h3 id="misanke-image-sample">Misanke image sample</h3>
<p>Images gathered like this.
(By the way, I made a decision by HIBNY. I love flame type.)</p>
<table>
<thead>
<tr>
<th>Hibernie</th>
<th>Messon</th>
<th>Sarnori</th>
</tr>
</thead>
<tbody>
<tr>
<td><img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/177134/32436a9c-8ddf-00ea-5899-7ce24346c045.jpeg" alt="000003.jpg"></td>
<td><img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/177134/8a3f2a2e-03d5-8aa0-ac3c-18989bd72b0c.jpeg" alt="000003.jpg"></td>
<td><img width="200" alt="000006.png" src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/177134/90d8978c-3a59-bf9a-200f-47a17711deb2.png"></td>
</tr>
<tr>
<td>156</td>
<td>147</td>
<td>182</td>
</tr>
</tbody>
</table>
<p>The following Pokemon other than the three family members, images of characters, illustrations that were too deformed, etc. were also confused, so they are excluded by visual inspection.
~~ Kibana is cool ~~
<img width="200" alt="000075.png" src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/177134/e418605e-dc82-7839-7f5d-00cebd74b746.png"><img width="200" alt="000237.jpg" src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/177134/d262af05-e9b3-b57e-2bb5-7eb3fc82bab1.jpeg"> <img width="200" alt="000075.png" src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/177134/25908fb5-d3db-d00f-4df5-198b0e49cfce.png"></p>
<h2 id="classifier-creation">Classifier creation</h2>
<p>It is a simple CNN.</p>
<p><img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/177134/78b41da1-69c3-61e1-053c-5b5c64a55ba5.png" alt="image.png"></p>
<p>Since the test data has few images (about 15 images), the accuracy of the test data is flapping, but a classification model with an accuracy that would be sufficient for TCAV verification was created.
<img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/177134/0a2bb542-3513-97e8-7d20-9ab1ff04b746.png" alt="image.png"></p>
<p>Save the model as a .pb as you will need a <strong>.pb file</strong> for CAV calculation.
Next, prepare to see what the model is learning.</p>
<h1 id="preparation-for-executing-tcav">Preparation for executing TCAV</h1>
<p>We will prepare according to the following steps.
(The code I used this time is in <a href="https://github.com/rio-kurihara/tcav">here</a>.IwillwritetheREADMElater&hellip;)</p>
<h2 id="step1-preparation-of-conceptual-images-positive-example-and-negative-example">Step1: Preparation of conceptual images (positive example and negative example)</h2>
<p>The following images are prepared for the positive image.
We prepared several colors based on the hypothesis that the three families should be classified by looking at the colors.
(10 to 20 sheets will work, but it is better to have 50 to 200 sheets)</p>
<p>** Positive image sample **</p>
<p>| White | Red | Blue | Yellow | Green | Black |</p>
<ul>
<li>&mdash;|&mdash;-|&mdash;-|&mdash;-|&mdash;-|&mdash;-|
|<img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/177134/2cfaf774-15ba-b4ca-7020-3770636ec234.jpeg" alt="000001.jpg">|<img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/177134/8fcc890a-8983-f3c5-4dd6-ec98e21ac06b.jpeg" alt="000005.jpg">|<img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/177134/c8fd2313-89ba-5f88-817b-38779651d01d.jpeg" alt="000009.jpg">|<img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/177134/151f9d34-9ea9-52da-c651-8c878670ffae.jpeg" alt="000004.jpg">|<img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/177134/df20cc7e-8f89-65b5-2754-c9ea1e19e734.png" alt="000023.png">|<img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/177134/de50906c-eb00-04dd-02cf-ee08afa46d62.png" alt="000023.png"> |
22 | 20 | 15 | 18 | 21 | 17 |</li>
</ul>
<p>I&rsquo;m excluding those with too many mixed colors<img width="100" alt="000025.png" src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/177134/1b14f785-8da8-cd0d-d526-52283215ef20.png"></p>
<p>** Negative image sample **
It is desirable that it does not correspond to any of the above positive cases. (In this case, it is difficult to say that it does not correspond to any color.)
This time, I randomly picked images from <a href="http://www.vision.caltech.edu/Image_Datasets/Caltech101/">Caltech256</a>.</p>
<p>The directory structure of the images collected so far is as follows.
All conceptual image sets must be subdirectories.</p>
<pre><code>├── datasets
│  ├── for_tcav # TCAV data set
│  │ ├── black
│  │ ├── blue
│  │  ├── green
│  │  ├── hibany
│  │  ├── messon
│  │  ├── random500_0
│  │  ├── random500_1
│  │  ├── random500_2
│   │  ├── random500_3
│   │  ├── random500_4
│   │  ├── random500_5
│  │  ├── red
│   │  ├── sarunori
│  │ ├── white
│  │  └── yellow
│  └── splited # Dataset for creating image classification model
│  ├── test
│  │  ├── hibany
│  │  ├── messon
│  │ └── sarunori
│  ├── train
│  │  ├── hibany
│  │  ├── messon
│  │ └── sarunori
│  └── validation
│  ├── hibany
│  ├── messon
│  └── sarunori
</code></pre><h2 id="step2-implement-the-model-wrapper">Step2: Implement the model wrapper</h2>
<p>I will clone it first.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">git clone git<span style="color:#a6e22e">@github.com</span>:tensorflow<span style="color:#f92672">/</span>tcav<span style="color:#f92672">.</span>git
</code></pre></div><p>Here, create a wrapper to convey the model information to TCAV.
Add this class to tcav/model.py.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">SimepleCNNWrapper_public</span>(PublicImageModelWrapper):
    <span style="color:#66d9ef">def</span> __init__(self, sess, model_saved_path, labels_path):
        self<span style="color:#f92672">.</span>image_value_range <span style="color:#f92672">=</span> (<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1</span>)
        image_shape_v3 <span style="color:#f92672">=</span> [<span style="color:#ae81ff">256</span>, <span style="color:#ae81ff">256</span>, <span style="color:#ae81ff">3</span>]
        endpoints_v3 <span style="color:#f92672">=</span> dict(
            input<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;conv2d_1_input:0&#39;</span>,
            logit<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;activation_6/Softmax:0&#39;</span>,
            prediction<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;activation_6/Softmax:0&#39;</span>,
            pre_avgpool<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;max_pooling2d_3/MaxPool:0&#39;</span>,
            logit_weight<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;activation_6/Softmax:0&#39;</span>,
            logit_bias<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;dense_1/bias:0&#39;</span>,
        )

        self<span style="color:#f92672">.</span>sess <span style="color:#f92672">=</span> sess
        super(SimepleCNNWrapper_public, self)<span style="color:#f92672">.</span>__init__(sess,
                                                       model_saved_path,
                                                       labels_path,
                                                       image_shape_v3,
                                                       endpoints_v3,
                                                       scope<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;import&#39;</span>)
        self<span style="color:#f92672">.</span>model_name <span style="color:#f92672">=</span><span style="color:#e6db74">&#39;SimepleCNNWrapper_public&#39;</span>
</code></pre></div><p>Now you are ready. Let&rsquo;s see the result immediately.</p>
<p>#Result</p>
<p>Let&rsquo;s look at the concept (color this time) that is emphasized in each class.
The concepts that are not marked with * are important.</p>
<table>
<thead>
<tr>
<th>Hibani class</th>
<th>Messon class</th>
<th>Sarnoli class</th>
</tr>
</thead>
<tbody>
<tr>
<td><img width="600" alt="image.png" src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/177134/3e6b1e98-6007-152a-cf6f-87512cd266bc.png">Red/Yellow/White</td>
<td><img width="600" alt="image.png" src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/177134/b8978738-fded-01c6-95fb-95956c52b5d4.png">Red (!?)</td>
<td><img width="600" alt="image.png" src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/177134/bad9ff89-0a50-77d6-2b0a-4c28eb2cc6dd.png">Green</td>
</tr>
<tr>
<td><img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/177134/32436a9c-8ddf-00ea-5899-7ce24346c045.jpeg" alt="000003.jpg"></td>
<td><img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/177134/8a3f2a2e-03d5-8aa0-ac3c-18989bd72b0c.jpeg" alt="000003.jpg"></td>
<td><img width="200" alt="000006.png" src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/177134/90d8978c-3a59-bf9a-200f-47a17711deb2.png"></td>
</tr>
</tbody>
</table>
<p>I think that the result is similar to that of Hibani and Sarnori.
Since it is a mystery about Messon, it is necessary to consider it.
If you change the number of trials or the number of concept images/target images during the experiment, the results will change quite a bit, so I think that some more consideration is needed.
It will be different depending on how you choose the conceptual image, so it seems worth trying variously.</p>
<p>#Summary</p>
<p>I tried a method that shows the basis of judgment of the neural network model.
It was easy for humans to interpret, and <strong>&ldquo;intuitively-like&rdquo;</strong> results were obtained.
This time, I chose the color as the concept image because it is the Misanke classification, but it is difficult to prepare the concept image. ..
It requires a lot of preparation, but there is no need to retrain the model, and I think that it will be easy to use if you try the sequence once and get used to it.
Please try by all means try!</p>

    </article>
  </div>

  
  
  
  <script>
  window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
  ga('create', 'UA-169005401-1', 'auto');
  ga('send', 'pageview');
  </script>
  <script async src='https://www.google-analytics.com/analytics.js'></script>
  

  

  
<link rel="stylesheet" type="text/css" href="//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.0/cookieconsent.min.css" />
<script src="//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.0/cookieconsent.min.js"></script>
<script>
window.addEventListener("load", function(){
window.cookieconsent.initialise({
  "palette": {
    "popup": {
      "background": "#216942",
      "text": "#b2d192"
    },
    "button": {
      "background": "#afed71"
    }
  }
})});
</script>

</body>

</html>
