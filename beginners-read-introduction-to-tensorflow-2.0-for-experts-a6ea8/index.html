<!doctype html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  
  <meta name="generator" content="Hugo 0.72.0" />

  
  <meta name="description" content="Some description">
  

  
  <link rel="apple-touch-icon" sizes="180x180" href="https://memotut.com/apple-touch-icon.png">

  
  <link rel="icon" type="image/png" sizes="32x32" href="https://memotut.com/favicon-32x32.png">

  
  <link rel="icon" type="image/png" sizes="16x16" href="https://memotut.com/favicon-16x16.png">

  
  <link rel="manifest" href="https://memotut.com/site.webmanifest">

  
  <link rel="mask-icon" href="https://memotut.com/safari-pinned-tab.svg" color="#5bbad5">

  <meta name="msapplication-TileColor" content="#da532c">

  <meta name="theme-color" content="#ffffff">

  
  <link rel="stylesheet" href="https://memotut.com/css/bootstrap.min.css" />

  
  <title>Beginners read Introduction to TensorFlow 2.0 for experts | Memo Tut</title>
  

  <style>
body {
  min-width: 300px;
}

.custom-navbar {
  margin-bottom: 1em;
  height: 60px;
}

.custom-navbar a {
  display: inline-block; 
  padding: 18px 0;
  margin-right: 1em; 
  font-weight: bold; 
}

.custom-navbar a:hover,
.custom-navbar a:focus {
  text-decoration: none; 
}

@media print {
  .custom-navbar {
    display: none;
  }
}

article {
  padding-bottom: 1em;
}

img {
  max-width: 100%;
}


body {
  background-color: #fff;
}



body {
  color: #212529;
}



a {
  color: #007bff;
}



a:hover,
a:focus {
  color: #0056b3;
}



.custom-navbar {
  background-color: #212529;
}



.custom-navbar a {
  color: rgba(255, 255, 255, 0.75);
}



.custom-navbar a:hover,
.custom-navbar a:focus {
  color: rgba(255, 255, 255, 1);
}



.container {
  max-width: 800px;
}



pre {
  display: block;
  padding: 9.5px;
  word-break: break-all;
  word-wrap: break-word;
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 4px;
}

pre code {
  padding: 0;
  font-size: inherit;
  color: inherit; 
  white-space: pre-wrap;
  background-color: transparent;
  border: none;
  border-radius: 0;
}

code {
  padding: 2px 4px;
  color: inherit; 
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 4px;
  font-size: .9em;
}



blockquote,
.blockquote {
  padding: 10px 20px;
  margin: 0 0 20px;
  font-size: 1em;
  border-left: 5px solid #6c757d;
}

</style>
</head>

<body>
  <nav class="custom-navbar">
  <div class="container">
    
    <a href="/">Posts</a>
    
    <a href="/tags/">Tags</a>
    
    <a href="/about/">About</a>
    
    <a href="/index.xml">RSS</a>
    
  </div>
</nav>
  
  <div class="container">
    <article>
      <h1>Beginners read Introduction to TensorFlow 2.0 for experts</h1>
<p>
  <small class="text-secondary">
  
  
  Nov 14, 2019
  </small>
  

<small><code><a href="https://memotut.com/tags/python">Python</a></code></small>


<small><code><a href="https://memotut.com/tags/tensorflow">TensorFlow</a></code></small>


<small><code><a href="https://memotut.com/tags/tensorflow2.0">TensorFlow2.0</a></code></small>

</p>
<pre><code># Things to do: point_up:
</code></pre>
<p>Read the <a href="https://www.tensorflow.org/tutorials/quickstart/advanced">tensorflow 2.0 tutorial</a>.
I would like to investigate and supplement the things I did not understand well in the tutorial to make a memorandum.
So you may want to read it along with the tutorial.</p>
<p>#Background
I used to use chainer
When I was about to learn tensorflow, the mainstream moved to 2.0 before I knew it.
I don&rsquo;t even know the tensorflow1 system, but if I do it now, it seems reasonable to start with the 2 system and start reading the tensorflow2 tutorial.</p>
<p>#Environment
windows10
It was done in a virtual environment of anaconda
Environment construction is as follows</p>
<pre><code>It seems that only #python3.6 is supported, so the version is 3.6
conda create -n tensorflow2.0 python=3.6 anaconda
conda install tensorflow==2.0.0
conda install jupyter
</code></pre><h1 id="commentary">Commentary</h1>
<p>First, import the TensorFlow library into your program.</p>
<pre><code>from __future__ import absolute_import, division, print_function, unicode_literals

!pip install -q tensorflow-gpu==2.0.0-rc1
import tensorflow as tf

from tensorflow.keras.layers import Dense, Flatten, Conv2D
from tensorflow.keras import Model
</code></pre><p>Load and prepare the MNIST dataset.</p>
<pre><code>mnist = tf.keras.datasets.mnist

(x_train, y_train), (x_test, y_test) = mnist.load_data()

Push the # pixel value (0~255) into 0~1
x_train, x_test = x_train / 255.0, x_test / 255.0

It seems that you can extract the number of dimensions with # tf.newaxis
# Add dimension information to each data (each image)
# CNN seems to need dimension information. It may be a process that should not be done in reverse with full combination
x_train = x_train[..., tf.newaxis]
x_test = x_test[..., tf.newaxis]
</code></pre><p>Use tf.data to shuffle and batch the dataset.</p>
<pre><code>#10000 is the buffer size. Is 10,000 enough for CNN? ?
#32 is the batch size.
train_ds = tf.data.Dataset.from_tensor_slices(
    (x_train, y_train)).shuffle(10000).batch(32)
test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(32)
</code></pre><p>Create a tf.keras model using Keras&rsquo; model subclassing API.</p>
<pre><code>Inheritance of # Model class
class MyModel(Model):
Build a model with #init
  def __init__(self):
    super(MyModel, self).__init__()
# 32 filters
# Filter size is 3*3
# Activation function is Relu
    self.conv1 = Conv2D(32, 3, activation='relu')
# 2D data * Flatten the data of the number of filters to 1D
    self.flatten = Flatten()
    self.d1 = Dense(128, activation='relu')
# Activation function is softmax function because it is an output layer
    self.d2 = Dense(10, activation='softmax')

#x is the input image group? ?
# -&gt; tf.tensor type data seems to be image group image group
  def call(self, x):
    x = self.conv1(x)
    x = self.flatten(x)
    x = self.d1(x)
Exhaust the result calculated by #NN? ?
    return self.d2(x)

model = MyModel()
</code></pre><p>Choose an optimizer and loss function for training.</p>
<pre><code># Stable cross entropy error
loss_object = tf.keras.losses.SparseCategoricalCrossentropy()
# The most rumored ADAM now (a year ago?)
optimizer = tf.keras.optimizers.Adam()
</code></pre><p>Select metrics to measure model loss and accuracy. These metrics aggregate the values for each epoch and output the final result.</p>
<pre><code># Matrix average instance
train_loss = tf.keras.metrics.Mean(name='train_loss')
# Instance that gives the correct answer rate
train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')

test_loss = tf.keras.metrics.Mean(name='test_loss')
test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')
</code></pre><p>Train the model using tf.GradientTape.</p>
<pre><code>@tf.function
def train_step(image, label):
# with internal calculations recorded. I take it out with tape.~~
  with tf.GradientTape() as tape:
Prediction based on # image, loss calculation
    predictions = model(image)
    loss = loss_object(label, predictions)
Pass weights with # model.trainable_variables
  gradients = tape.gradient(loss, model.trainable_variables)
Update weights with # optimizer
  optimizer.apply_gradients(zip(gradients, model.trainable_variables))

#Calculate loss and acciracy with the previous instance
  train_loss(loss)
  train_accuracy(label, predictions)
</code></pre><p>Reference: <a href="https://qiita.com/mgmk2/items/59e070dfbdb95137e511">Introduction to TensorFlow 2.0 for low level API users</a></p>
<p>About <code>@tf.function</code>
There are mainly two ways to turn the NN:
Define-and-run: Define the calculation graph and then get the data
Define-by-run: The graph is executed and the result is obtained at the same time when the graph is defined as x + y like the normal code of python (<a href="https://qiita.com/halhorn/items/09a64e98a02022e6ccc2">Source</a>)
It seems that&rsquo;by&rsquo; is used in tensorflow 2.0.0 because it is easier to write.
But&rsquo;by&rsquo; is very slow, so it seems that by adding <code>@tf.function</code>, the same operation as&rsquo;and&rsquo; can be done.
For details, refer to the verification article etc. (<a href="https://qiita.com/nj_ryoo0/items/7f4495aaef07ccc6a303">https://qiita.com/nj_ryoo0/items/7f4495aaef07ccc6a303</a>)</p>
<p>Then test the model.</p>
<pre><code>@tf.function
def test_step(image, label):
  predictions = model(image)
  t_loss = loss_object(label, predictions)

  test_loss(t_loss)
  test_accuracy(label, predictions)
</code></pre><pre><code>EPOCHS = 5

for epoch in range (EPOCHS):
# Run train on all images in batch
  for image, label in train_ds:
    train_step(image, label)

# Run tests on all images in each batch
  for test_image, test_label in test_ds:
    test_step(test_image, test_label)

# Various display
  template ='Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}'
  print (template.format(epoch+1,
                         train_loss.result(),
                         train_accuracy.result()*100,
                         test_loss.result(),
                         test_accuracy.result()*100))
</code></pre><p>result</p>
<pre><code>WARNING:tensorflow:Layer my_model is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2. The layer has dtype float32 because it's dtype defaults to floatx.

If you intended to run this layer in float32, you can safely ignore this warning.If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.

Epoch 1, Loss: 0.14364087581634521, Accuracy: 95.62000274658203, Test Loss: 0.06367728859186172, Test Accuracy: 97.88999938964844
Epoch 2, Loss: 0.09373863786458969, Accuracy: 97.1483383178711, Test Loss: 0.056961096823215485, Test Accuracy: 98.07500457763672
Epoch 3, Loss: 0.07041392475366592, Accuracy: 97.84444427490234, Test Loss: 0.05455232039093971, Test Accuracy: 98.17666625976562
Epoch 4, Loss: 0.05662970244884491, Accuracy: 98.25749969482422, Test Loss: 0.05664524435997009, Test Accuracy: 98.19499969482422
Epoch 5, Loss: 0.047065384685993195, Accuracy: 98.54966735839844, Test Loss: 0.057572390884160995, Test Accuracy: 98.23799896240234
</code></pre><p>P.S.
＠を```で辞書登録したら便利だった</p>

    </article>
  </div>

  
  
  
  <script>
  window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
  ga('create', 'UA-169005401-1', 'auto');
  ga('send', 'pageview');
  </script>
  <script async src='https://www.google-analytics.com/analytics.js'></script>
  

  

  
<link rel="stylesheet" type="text/css" href="//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.0/cookieconsent.min.css" />
<script src="//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.0/cookieconsent.min.js"></script>
<script>
window.addEventListener("load", function(){
window.cookieconsent.initialise({
  "palette": {
    "popup": {
      "background": "#216942",
      "text": "#b2d192"
    },
    "button": {
      "background": "#afed71"
    }
  }
})});
</script>

</body>

</html>
