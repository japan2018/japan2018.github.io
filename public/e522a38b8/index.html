<!doctype html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  
  <meta name="generator" content="Hugo 0.72.0" />

  
  <meta name="description" content="Some description">
  

  
  <link rel="apple-touch-icon" sizes="180x180" href="https://memotut.com/apple-touch-icon.png">

  
  <link rel="icon" type="image/png" sizes="32x32" href="https://memotut.com/favicon-32x32.png">

  
  <link rel="icon" type="image/png" sizes="16x16" href="https://memotut.com/favicon-16x16.png">

  
  <link rel="manifest" href="https://memotut.com/site.webmanifest">

  
  <link rel="mask-icon" href="https://memotut.com/safari-pinned-tab.svg" color="#5bbad5">

  <meta name="msapplication-TileColor" content="#da532c">

  <meta name="theme-color" content="#ffffff">

  
  <link rel="stylesheet" href="https://memotut.com/css/bootstrap.min.css" />

  
  <title>[Python] [Implementation explanation] How to use Japanese version BERT in Google Colaboratory (PyTorch) | Memo Tut</title>
  

  <style>
body {
  min-width: 300px;
}

.custom-navbar {
  margin-bottom: 1em;
  height: 60px;
}

.custom-navbar a {
  display: inline-block; 
  padding: 18px 0;
  margin-right: 1em; 
  font-weight: bold; 
}

.custom-navbar a:hover,
.custom-navbar a:focus {
  text-decoration: none; 
}

@media print {
  .custom-navbar {
    display: none;
  }
}

article {
  padding-bottom: 1em;
}

img {
  max-width: 100%;
}


body {
  background-color: #fff;
}



body {
  color: #212529;
}



a {
  color: #007bff;
}



a:hover,
a:focus {
  color: #0056b3;
}



.custom-navbar {
  background-color: #212529;
}



.custom-navbar a {
  color: rgba(255, 255, 255, 0.75);
}



.custom-navbar a:hover,
.custom-navbar a:focus {
  color: rgba(255, 255, 255, 1);
}



.container {
  max-width: 800px;
}



pre {
  display: block;
  padding: 9.5px;
  word-break: break-all;
  word-wrap: break-word;
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 4px;
}

pre code {
  padding: 0;
  font-size: inherit;
  color: inherit; 
  white-space: pre-wrap;
  background-color: transparent;
  border: none;
  border-radius: 0;
}

code {
  padding: 2px 4px;
  color: inherit; 
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 4px;
  font-size: .9em;
}



blockquote,
.blockquote {
  padding: 10px 20px;
  margin: 0 0 20px;
  font-size: 1em;
  border-left: 5px solid #6c757d;
}

</style>
</head>

<body>
  <nav class="custom-navbar">
  <div class="container">
    
    <a href="/">Posts</a>
    
    <a href="/tags/">Tags</a>
    
    <a href="/about/">About</a>
    
    <a href="/index.xml">RSS</a>
    
  </div>
</nav>
  
  <div class="container">
    <article>
      <h1>[Python] [Implementation explanation] How to use Japanese version BERT in Google Colaboratory (PyTorch)</h1>
<p>
  <small class="text-secondary">
  
  
  May 14, 2020
  </small>
  

<small><code><a href="https://memotut.com/tags/python">Python</a></code></small>


<small><code><a href="https://memotut.com/tags/machine-learning"> machine learning</a></code></small>


<small><code><a href="https://memotut.com/tags/deeplearning"> DeepLearning</a></code></small>


<small><code><a href="https://memotut.com/tags/ai"> AI</a></code></small>


<small><code><a href="https://memotut.com/tags/pytorch"> PyTorch</a></code></small>

</p>
<pre><code>In this article, I will explain how to use the Japanese version of BERT with Google Colaboratory.
</code></pre>
<p>About the BERT itself, the book I wrote last year</p>
<p><a href="https://www.amazon.co.jp/dp/4839970254/">&ldquo;Learn while making! Development deep learning with PyTorch&rdquo;</a></p>
<p>Is explained in detail in.</p>
<p>If you want to know how BERT works, please see the above book.</p>
<p>Since the book only dealt with the English version, this post will explain how to use BERT in the Japanese version.
(I want to write about two after this article.)</p>
<p>The implementation code of this post is placed in the following GitHub repository.</p>
<p><a href="https://github.com/YutaroOgawa/BERT_Japanese_Google_Colaboratory">GitHub: How to use Japanese version of BERT on Google Colaboratory: Implementation code</a>
This is 1_Japanese_BERT_on_Google_Colaboratory.ipynb.</p>
<p>** Serialization list **
<a href="https://qiita.com/sugulu/items/e522a38b812b8edb8a54">[1] *This article [Implementation Description] How to use Japanese version BERT with Google Colaboratory (PyTorch)</a>
<a href="https://qiita.com/sugulu/items/697bd03499c1de9cf082">[2] [Implementation Description] Japanese version BERT with livedoor news classification: Google Colaboratory (PyTorch)</a>
<a href="https://qiita.com/sugulu/items/e3c01a0776a5df3ad552">[3] [Implementation explanation] Brain science and unsupervised learning. Classify MNIST by information maximization clustering</a>
<a href="https://qiita.com/sugulu/items/6a887e612edda9e35c0b">[4] [Implementation explanation] Japanese BERT x unsupervised learning (information maximization clustering) to classify livedoor news</a></p>
<hr>
<h2 id="preparation-1-install-mecab-on-google-colaboratory">Preparation 1: Install MeCab on Google Colaboratory</h2>
<p>Install MeCab, a tool for punctuation (morphological analysis).
Since it cannot be installed with pip, it is installed with apt.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#960050;background-color:#1e0010">!</span>apt install aptitude swig
<span style="color:#960050;background-color:#1e0010">!</span>aptitude install mecab libmecab<span style="color:#f92672">-</span>dev mecab<span style="color:#f92672">-</span>ipadic<span style="color:#f92672">-</span>utf8 git make curl xz<span style="color:#f92672">-</span>utils file <span style="color:#f92672">-</span>y
</code></pre></div><p>Install mecab-python3 with pip so that you can use MeCab from Python.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#960050;background-color:#1e0010">!</span>pip install mecab<span style="color:#f92672">-</span>python3
</code></pre></div><p>Install NEologd, a dictionary so that MeCab can use new words (new words recently).
(However, it is not used in the learned BERT)</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#960050;background-color:#1e0010">!</span>git clone <span style="color:#f92672">--</span>depth <span style="color:#ae81ff">1</span> https:<span style="color:#f92672">//</span>github<span style="color:#f92672">.</span>com<span style="color:#f92672">/</span>neologd<span style="color:#f92672">/</span>mecab<span style="color:#f92672">-</span>ipadic<span style="color:#f92672">-</span>neologd<span style="color:#f92672">.</span>git
<span style="color:#960050;background-color:#1e0010">!</span>echo yes <span style="color:#f92672">|</span> mecab<span style="color:#f92672">-</span>ipadic<span style="color:#f92672">-</span>neologd<span style="color:#f92672">/</span>bin<span style="color:#f92672">/</span>install<span style="color:#f92672">-</span>mecab<span style="color:#f92672">-</span>ipadic<span style="color:#f92672">-</span>neologd <span style="color:#f92672">-</span>n <span style="color:#f92672">-</span>a

</code></pre></div><p>Get the path to the new dictionary NEologd.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> subprocess

cmd<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;echo `mecab-config --dicdir`&#34;/mecab-ipadic-neologd&#34;&#39;</span>
path_neologd <span style="color:#f92672">=</span> (subprocess<span style="color:#f92672">.</span>Popen(cmd, stdout<span style="color:#f92672">=</span>subprocess<span style="color:#f92672">.</span>PIPE,
                           shell<span style="color:#f92672">=</span>True)<span style="color:#f92672">.</span>communicate()[<span style="color:#ae81ff">0</span>])<span style="color:#f92672">.</span>decode(<span style="color:#e6db74">&#39;utf-8&#39;</span>)
</code></pre></div><p>This completes the preparation of specifications for MeCab.</p>
<h4 id="remarks-ipadic-and-unidic">Remarks (IPAdic and UniDic)</h4>
<p><strong>Ipadic</strong> of the new dictionary mecab-ipadic-neologd is &ldquo;IPA dictionary&rdquo;.
The IPA dictionary is organized by &ldquo;IPA part-of-speech system&rdquo;.</p>
<p>You may have seen the expression <strong>UniDic</strong> besides IPAdic.
UniDic is a system organized by the &ldquo;NIKKAN Short Unit Automatic Analysis Dictionary&rdquo;.</p>
<p>For example, for Sudachi for morphological analysis, the default dictionary is UniDic.</p>
<p>UniDic and IPAdic have different parts of speech systems.
For example, there is no adjective verb in UniDic, and it becomes <strong>morphograph</strong> (<a href="https://www.ogiso.net/wiki/index.php?%BC%F8%B6%C8%BB%F1%CE%C1/UniDic%A4%CE%C9%CA%BB%EC%C2%CE%B7%CF">UniDic&rsquo;s part-of-speech system</a>).</p>
<p>It&rsquo;s been two and a half years since I joined a new graduate and started learning IT, and learning machine learning and deep learning.</p>
<p>Senior F, who has been my mentor since I was a new employee, is from Indonesia, but my Japanese is better than me.</p>
<p>Mr. F, an Indonesian mentor, also told me that the default of sudachi is not an adjective verb, but a shape verb, “I didn&rsquo;t learn such grammar at school, I heard it for the first time. I was surprised.</p>
<h2 id="preparation-2-mecab-operation-check">Preparation 2: MeCab operation check</h2>
<p>Then, actually write a sentence (morphological analysis) and check the operation of MeCab.</p>
<p>The sentence is
“I like machine learning.”
Let&rsquo;s.</p>
<p>First is the case where the new word dictionary NEologd is not used.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> MeCab

m<span style="color:#f92672">=</span>MeCab<span style="color:#f92672">.</span>Tagger(<span style="color:#e6db74">&#34;-Ochasen&#34;</span>)

text <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;I like machine learning.&#34;</span>

text_segmented <span style="color:#f92672">=</span> m<span style="color:#f92672">.</span>parse(text)
<span style="color:#66d9ef">print</span>(text_segmented)
</code></pre></div><p>(output)</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">I am my noun<span style="color:#f92672">-</span>pronoun<span style="color:#f92672">-</span>general
Hahaha Particle<span style="color:#f92672">-</span>Particle
Machine Kikai Machine Noun<span style="color:#f92672">-</span>General
Learning gakushu learning noun<span style="color:#f92672">-</span>sa strange connection
Ga ga ga particle<span style="color:#f92672">-</span>case particle<span style="color:#f92672">-</span>general
I like uki i like noun<span style="color:#f92672">-</span>adjective verb stem
It<span style="color:#e6db74">&#39;s death It&#39;</span>s auxiliary verb Special<span style="color:#f92672">/</span>death basic form
<span style="color:#f92672">..</span> <span style="color:#f92672">..</span> <span style="color:#f92672">..</span> Symbol<span style="color:#f92672">-</span>Punctuation
EOS
</code></pre></div><p><strong>-Ochasen</strong> of MeCab.Tagger(&quot;-Ochasen&rdquo;) is an output option.
this,
If set to <strong>-Owakati</strong>, only the separated words will be output.
If set to <strong>-Oyomi</strong>, only readings will be output.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">m<span style="color:#f92672">=</span>MeCab<span style="color:#f92672">.</span>Tagger(<span style="color:#e6db74">&#34;-Owakati&#34;</span>)
text_segmented <span style="color:#f92672">=</span> m<span style="color:#f92672">.</span>parse(text)
<span style="color:#66d9ef">print</span>(text_segmented)
</code></pre></div><p>The output is
<code>I like machine learning. </code>
is.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">m<span style="color:#f92672">=</span>MeCab<span style="color:#f92672">.</span>Tagger(<span style="color:#e6db74">&#34;-Oyomi&#34;</span>)
text_segmented <span style="color:#f92672">=</span> m<span style="color:#f92672">.</span>parse(text)
<span style="color:#66d9ef">print</span>(text_segmented)
</code></pre></div><p>, The output is
<code>I'm a kid. </code>
is.</p>
<p>Next, when using the new word dictionary NEologd.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">m<span style="color:#f92672">=</span>MeCab<span style="color:#f92672">.</span>Tagger(<span style="color:#e6db74">&#34;-Ochasen -d &#34;</span><span style="color:#f92672">+</span>str(path_neologd)) <span style="color:#75715e"># add path to NEologd</span>

text <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;I like machine learning.&#34;</span>

text_segmented <span style="color:#f92672">=</span> m<span style="color:#f92672">.</span>parse(text)
<span style="color:#66d9ef">print</span>(text_segmented)
</code></pre></div><p>(output)</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">I am my noun<span style="color:#f92672">-</span>pronoun<span style="color:#f92672">-</span>general
Hahaha Particle<span style="color:#f92672">-</span>Particle
Machine Learning Machine Learning Noun<span style="color:#f92672">-</span>Proper Noun<span style="color:#f92672">-</span>General
Ga ga ga particle<span style="color:#f92672">-</span>case particle<span style="color:#f92672">-</span>general
I like uki i like noun<span style="color:#f92672">-</span>adjective verb stem
It<span style="color:#e6db74">&#39;s death It&#39;</span>s auxiliary verb Special<span style="color:#f92672">/</span>death basic form
<span style="color:#f92672">..</span> <span style="color:#f92672">..</span> <span style="color:#f92672">..</span> Symbol<span style="color:#f92672">-</span>Punctuation
EOS
</code></pre></div><p>In order to use NEologd, add the option with -d to MeCab.Tagger and specify the path to NEologd.</p>
<p>When I wasn&rsquo;t using the new word dictionary, the word &ldquo;machine learning&rdquo; was split into &ldquo;machine&rdquo; and &ldquo;learning&rdquo;.
Using the new word dictionary, it is one word with &ldquo;machine learning&rdquo; (proper noun).</p>
<p>This is because the technical term &ldquo;machine learning&rdquo; is registered in the new language dictionary.</p>
<p>Even with the new word dictionary version, if you set <strong>-Owakati</strong>, only the separated words will be output.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">m<span style="color:#f92672">=</span>MeCab<span style="color:#f92672">.</span>Tagger(<span style="color:#e6db74">&#34;-Owakati -d &#34;</span><span style="color:#f92672">+</span>str(path_neologd)) <span style="color:#75715e"># add path to NEologd</span>
text_segmented <span style="color:#f92672">=</span> m<span style="color:#f92672">.</span>parse(text)
<span style="color:#66d9ef">print</span>(text_segmented)
</code></pre></div><p>The output is
<code>I like machine learning. </code>
Will be.</p>
<h2 id="preparation-3-prepared-japanese-bert-trained-model-and-morphological-analysis">Preparation 3: Prepared Japanese BERT trained model and morphological analysis</h2>
<p>Then, prepare the trained model and morphological analysis of the Japanese version BERT.</p>
<p>I can also use the BERT model from <a href="https://www.amazon.co.jp/dp/4839970254/">&ldquo;Learning while creating! Developing deep learning with PyTorch&rdquo;</a>, but it has recently been used as a standard. I&rsquo;m using Hugging Face&rsquo;s model.</p>
<p>Hugging means hugging in Japanese.</p>
<p>BERT model uses Mr. Hugging Face&rsquo;s, learned parameters in Japanese, and
Morphological analysis (Tokenizer) during learning is
<a href="https://www.nlp.ecei.tohoku.ac.jp/news-release/3284/">Published by Masatoshi Suzuki at Tohoku University (Professor Inui and Suzuki)</a>
Will be used.</p>
<p>This Japanese version of Tohoku University trained model is included in Hugging Face&rsquo;s OSS <strong>transformers</strong>, so it can be used directly from transformers.</p>
<p>First, install transformers version 2.9 with pip.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#960050;background-color:#1e0010">!</span>pip install transformers<span style="color:#f92672">==</span><span style="color:#ae81ff">2.9</span><span style="color:#f92672">.</span><span style="color:#ae81ff">0</span>
</code></pre></div><p><strong>Note</strong>
The transformers have been upgraded from version 2.8 to 2.9 on May 8, 2020.
With version 2.8, the file path to the Japanese data will be an error, so please be careful to install 2.9.</p>
<p>Now, import PyTorch, BERT model, and Japanese tokenizer for BERT (class to divide).</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> torch
<span style="color:#f92672">from</span> transformers.modeling_bert <span style="color:#f92672">import</span> BertModel
<span style="color:#f92672">from</span> transformers.tokenization_bert_japanese <span style="color:#f92672">import</span> BertJapaneseTokenizer
</code></pre></div><p>Prepare a tokenizer for Japanese. Specify&rsquo;bert-base-japanese-whole-word-masking&rsquo; in the argument.</p>
<pre><code class="language-python#" data-lang="python#">tokenizer = BertJapaneseTokenizer.from_pretrained('bert-base-japanese-whole-word-masking')
</code></pre><p>Prepare a model that has been learned in Japanese.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># BERT&#39;s model of Japanese-learned parameters</span>
model <span style="color:#f92672">=</span> BertModel<span style="color:#f92672">.</span>from_pretrained(<span style="color:#e6db74">&#39;bert-base-japanese-whole-word-masking&#39;</span>)
<span style="color:#66d9ef">print</span>(model)
</code></pre></div><p>The following is a brief check of the output model results.</p>
<pre><code>BertModel(
  (embeddings): BertEmbeddings(
    (word_embeddings): Embedding(32000, 768, padding_idx=0)
    (position_embeddings): Embedding(512, 768)
    (token_type_embeddings): Embedding(2, 768)
    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (encoder): BertEncoder(

...
</code></pre><p>Here, confirm the settings (config) of the Japanese version model.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">from</span> transformers <span style="color:#f92672">import</span> BertConfig

<span style="color:#75715e"># Tohoku University _ Check Japanese version settings</span>
config_japanese <span style="color:#f92672">=</span> BertConfig<span style="color:#f92672">.</span>from_pretrained(<span style="color:#e6db74">&#39;bert-base-japanese-whole-word-masking&#39;</span>)
<span style="color:#66d9ef">print</span>(config_japanese)
</code></pre></div><p>The output is below.</p>
<pre><code>BertConfig {
  &quot;architectures&quot;: [
    &quot;BertForMaskedLM&quot;
  ],
  &quot;attention_probs_dropout_prob&quot;: 0.1,
  &quot;hidden_act&quot;: &quot;gelu&quot;,
  &quot;hidden_dropout_prob&quot;: 0.1,
  &quot;hidden_size&quot;: 768,
  &quot;initializer_range&quot;: 0.02,
  &quot;intermediate_size&quot;: 3072,
  &quot;layer_norm_eps&quot;: 1e-12,
  &quot;max_position_embeddings&quot;: 512,
  &quot;model_type&quot;: &quot;bert&quot;,
  &quot;num_attention_heads&quot;: 12,
  &quot;num_hidden_layers&quot;: 12,
  &quot;pad_token_id&quot;: 0,
  &quot;type_vocab_size&quot;: 2,
  &quot;vocab_size&quot;: 32000
}
</code></pre><p>If you look at the settings, you can see that the word vector is 768 dimensions, the maximum number of words (subwords) is 512, the number of BERT layers is 12, and the vocabulary size is 32,000.</p>
<p>With the above, we have prepared a model for which Japanese has been learned and a tokenizer for Japanese to use before putting it in the model.</p>
<h2 id="handling-sentences-with-the-japanese-version-of-bert">Handling sentences with the Japanese version of BERT</h2>
<p>Finally, let&rsquo;s handle the sentence with the Japanese version of BERT.</p>
<p>&ldquo;I got fired from the company.&rdquo;
&ldquo;Only telework makes me sick.&rdquo;
&ldquo;I was fired from the company.&rdquo;</p>
<p>I will prepare three sentences.</p>
<p>Then, compare the three word vectors of <strong>&ldquo;fired&rdquo;</strong>, <strong>&ldquo;fired&rdquo;</strong>, <strong>&ldquo;dismissed&rdquo;</strong> in each sentence.</p>
<p>BERT is characterized in that the word vector changes depending on the context, so in the first and second sentences
Even with the same word &ldquo;**&rdquo;, the 768-dimensional vector representation will be different.</p>
<p>**And I&rsquo;m happy that the first sentence &ldquo;fire&rdquo; is closer to the third sentence &ldquo;fire&rdquo; than the second sentence &ldquo;fire&rdquo;. **</p>
<p>Measure the similarity of word vectors with the cosine similarity.</p>
<p>Let&rsquo;s implement it.
First, prepare the text.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">text1 <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;I got fired from the company.&#34;</span>
text2 <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;Only my telework hurt me.&#34;</span>
text3 <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;You have been fired from the company.&#34;</span>
</code></pre></div><p>Process text1 with Japanese version of BERT&rsquo;s word separator tokenizer.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># Separate words and convert to id</span>
input_ids1 <span style="color:#f92672">=</span> tokenizer<span style="color:#f92672">.</span>encode(text1, return_tensors<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;pt&#39;</span>) <span style="color:#75715e"># pt stands for PyTorch</span>

<span style="color:#66d9ef">print</span>(tokenizer<span style="color:#f92672">.</span>convert_ids_to_tokens(input_ids1[<span style="color:#ae81ff">0</span>]<span style="color:#f92672">.</span>tolist())) <span style="color:#75715e"># sentence</span>
<span style="color:#66d9ef">print</span>(input_ids1) <span style="color:#75715e"># id</span>
</code></pre></div><p>The output is</p>
<pre><code>['[CLS]','Company','Turn','Fire','Turn','That','That','. ','[SEP]']
tensor([[ 2, 811, 11, 13700, 7, 58, 10, 8, 3]])
</code></pre><p>Will be.</p>
<p>The word &ldquo;fire&rdquo; is the third, and the id was found to be 13700.</p>
<p>The second and third sentences are processed in the same way. The output of each is as follows.</p>
<pre><code>['[CLS]','Tele','## work','just','de','fire','ga','pain','##i','. ','[SEP]']
tensor([[ 2, 5521, 3118, 4027, 12, 13700, 14, 4897, 28457, 8,
             3]])

['[CLS]','Company','That','Dismissal','Sa','Le','Tha','. ','[SEP]']
tensor([[ 2, 811, 11, 7279, 26, 20, 10, 8, 3]])
</code></pre><p>I found that the second sentence &ldquo;fire&rdquo; was the fifth and the third sentence &ldquo;dismissal&rdquo; was the third.</p>
<p>Then, input this idized content into the Japanese BERT model and calculate the output vector.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># Type in Japanese BERT model</span>
result1 <span style="color:#f92672">=</span> model(input_ids1)

<span style="color:#66d9ef">print</span>(result1[<span style="color:#ae81ff">0</span>]<span style="color:#f92672">.</span>shape)
<span style="color:#66d9ef">print</span>(result1[<span style="color:#ae81ff">1</span>]<span style="color:#f92672">.</span>shape)

<span style="color:#75715e"># reult is sequence_output, pooled_output, (hidden_states), (attentions).</span>
However, hidden_states <span style="color:#f92672">and</span> attentions are optional <span style="color:#f92672">and</span> are <span style="color:#f92672">not</span> output by default<span style="color:#f92672">.</span>
</code></pre></div><p>The output is
<code>torch.Size([1, 9, 768]) torch.Size([1, 768])</code>
Will be.</p>
<p>9 represents the number of words (the number of subwords) in the first sentence.
768 is the embedding dimension of the word.
Therefore, since the first sentence &ldquo;fire&rdquo; was in the third, the word vector is
<code>result1[0][0][3][:]</code>
Will be.</p>
<p>The output of the BERT model calculation is
<code>outputs # sequence_output, pooled_output, (hidden_states), (attentions)</code>
(However, hidden_states and attentions are optional and are not output as standard).</p>
<p>Reference <a href="https://huggingface.co/transformers/model_doc/bert.html#bertmodel">Explanation of BerT Model forward</a></p>
<p>Similarly, find the word vector for the second sentence &ldquo;fire&rdquo; (fifth) and the third sentence &ldquo;dismissal&rdquo; (third).</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># Type in Japanese BERT model</span>
result2 <span style="color:#f92672">=</span> model(input_ids2)
result3 <span style="color:#f92672">=</span> model(input_ids3)

word_vec1 <span style="color:#f92672">=</span> result1[<span style="color:#ae81ff">0</span>][<span style="color:#ae81ff">0</span>][<span style="color:#ae81ff">3</span>][:] <span style="color:#75715e"># The first sentence “fire” (third)</span>
word_vec2 <span style="color:#f92672">=</span> result2[<span style="color:#ae81ff">0</span>][<span style="color:#ae81ff">0</span>][<span style="color:#ae81ff">5</span>][:] <span style="color:#75715e"># The second sentence “fire” (fifth)</span>
word_vec3 <span style="color:#f92672">=</span> result3[<span style="color:#ae81ff">0</span>][<span style="color:#ae81ff">0</span>][<span style="color:#ae81ff">3</span>][:] <span style="color:#75715e"># &#34;Dismissal&#34; in the third sentence (third)</span>
</code></pre></div><p>Finally, try to find the similarity.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># Find cosine similarity</span>
cos <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>CosineSimilarity(dim<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)
cos_sim_12 <span style="color:#f92672">=</span> cos(word_vec1, word_vec2)
cos_sim_13 <span style="color:#f92672">=</span> cos(word_vec1, word_vec3)

<span style="color:#66d9ef">print</span>(cos_sim_12)
<span style="color:#66d9ef">print</span>(cos_sim_13)
</code></pre></div><p>The output is
<code>tensor(0.6647, grad_fn=&lt;DivBackward0&gt;) tensor(0.7841, grad_fn=&lt;DivBackward0&gt;)</code></p>
<p>have become.</p>
<p>Therefore, the word expression processed by BERT is
The similarity between the first sentence &ldquo;fire&rdquo; and the second sentence &ldquo;fire&rdquo; is 0.66
The similarity between the first sentence &ldquo;fire&rdquo; and the third sentence &ldquo;dismissal&rdquo; is 0.78</p>
<p>And, you can see that the first sentence &ldquo;fire&rdquo; is close (highly similar) to the third sentence &ldquo;dismissal&rdquo;.</p>
<p>By using BERT, we were able to confirm that the same word &ldquo;fire&rdquo; has become a word vector whose meaning changes according to the context.</p>
<p>Above, [Implementation explanation] It was the method (PyTorch) of using the Japanese version BERT in Google Colaboratory.</p>
<hr>
<p>[Remarks] <a href="https://www.isidgroup.com/u/job.phtml?job_code=430">The AI Technology Development Team, which I lead, is looking for members. Click here if you are interested</a></p>
<p>[Disclaimer] The content itself of this article is the opinion/dissemination of the author, not the official opinion of the company to which the author belongs.</p>
<hr>
<p>References
● <a href="https://qiita.com/Fulltea/items/90f6ebe6dcceaf64eaef">Use mecab ipadic-NEologd with Google Colaboratory</a></p>

    </article>
  </div>

  
  
  
  <script>
  window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
  ga('create', 'UA-169005401-1', 'auto');
  ga('send', 'pageview');
  </script>
  <script async src='https://www.google-analytics.com/analytics.js'></script>
  

  

  
<link rel="stylesheet" type="text/css" href="//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.0/cookieconsent.min.css" />
<script src="//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.0/cookieconsent.min.js"></script>
<script>
window.addEventListener("load", function(){
window.cookieconsent.initialise({
  "palette": {
    "popup": {
      "background": "#216942",
      "text": "#b2d192"
    },
    "button": {
      "background": "#afed71"
    }
  }
})});
</script>

</body>

</html>
