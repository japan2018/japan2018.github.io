<!doctype html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  
  <meta name="generator" content="Hugo 0.72.0" />

  
  <meta name="description" content="Some description">
  

  
  <link rel="apple-touch-icon" sizes="180x180" href="https://memotut.com/apple-touch-icon.png">

  
  <link rel="icon" type="image/png" sizes="32x32" href="https://memotut.com/favicon-32x32.png">

  
  <link rel="icon" type="image/png" sizes="16x16" href="https://memotut.com/favicon-16x16.png">

  
  <link rel="manifest" href="https://memotut.com/site.webmanifest">

  
  <link rel="mask-icon" href="https://memotut.com/safari-pinned-tab.svg" color="#5bbad5">

  <meta name="msapplication-TileColor" content="#da532c">

  <meta name="theme-color" content="#ffffff">

  
  <link rel="stylesheet" href="https://memotut.com/css/bootstrap.min.css" />

  
  <title>[Python] A memorandum of studying and implementing deep learning | Memo Tut</title>
  

  <style>
body {
  min-width: 300px;
}

.custom-navbar {
  margin-bottom: 1em;
  height: 60px;
}

.custom-navbar a {
  display: inline-block; 
  padding: 18px 0;
  margin-right: 1em; 
  font-weight: bold; 
}

.custom-navbar a:hover,
.custom-navbar a:focus {
  text-decoration: none; 
}

@media print {
  .custom-navbar {
    display: none;
  }
}

article {
  padding-bottom: 1em;
}

img {
  max-width: 100%;
}


body {
  background-color: #fff;
}



body {
  color: #212529;
}



a {
  color: #007bff;
}



a:hover,
a:focus {
  color: #0056b3;
}



.custom-navbar {
  background-color: #212529;
}



.custom-navbar a {
  color: rgba(255, 255, 255, 0.75);
}



.custom-navbar a:hover,
.custom-navbar a:focus {
  color: rgba(255, 255, 255, 1);
}



.container {
  max-width: 800px;
}



pre {
  display: block;
  padding: 9.5px;
  word-break: break-all;
  word-wrap: break-word;
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 4px;
}

pre code {
  padding: 0;
  font-size: inherit;
  color: inherit; 
  white-space: pre-wrap;
  background-color: transparent;
  border: none;
  border-radius: 0;
}

code {
  padding: 2px 4px;
  color: inherit; 
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 4px;
  font-size: .9em;
}



blockquote,
.blockquote {
  padding: 10px 20px;
  margin: 0 0 20px;
  font-size: 1em;
  border-left: 5px solid #6c757d;
}

</style>
</head>

<body>
  <nav class="custom-navbar">
  <div class="container">
    
    <a href="/">Posts</a>
    
    <a href="/tags/">Tags</a>
    
    <a href="/about/">About</a>
    
    <a href="/index.xml">RSS</a>
    
  </div>
</nav>
  
  <div class="container">
    <article>
      <h1>[Python] A memorandum of studying and implementing deep learning</h1>
<p>
  <small class="text-secondary">
  
  
  May 23, 2020
  </small>
  

<small><code><a href="https://memotut.com/tags/python">Python</a></code></small>


<small><code><a href="https://memotut.com/tags/machine-learning"> machine learning</a></code></small>


<small><code><a href="https://memotut.com/tags/deep-learning"> deep learning</a></code></small>

</p>
<pre><code># Target audience
</code></pre>
<p>A person who has studied machine learning and deep learning in general, but who does not know how they are related when implementing.
Someone who wants to organize their mind.
I will not give a detailed mathematical explanation. See the chainer tutorial for a mathematical explanation.</p>
<h1 id="what-became-a-model-of-deep-learning">What became a model of deep learning</h1>
<p>Deep learning was created by imitating the mechanism of information transmission in human nerve cells. This has dramatically improved accuracy.</p>
<h1 id="how-the-neural-network-works">How the neural network works</h1>
<h2 id="neuron-modeling">Neuron modeling</h2>
<p>A neural network creates a mathematical model that reproduces the movements of human nerve cells on a computer. Although individual nerve cells have only simple arithmetic ability, they can be highly recognized and judged by being connected and interlocking with each other. We will reproduce the mechanism of information transmission by mathematical expressions while calculating matrices and special functions.</p>
<h3 id="neuron-model">Neuron model</h3>
<p><img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/616291/044ab567-88a5-ec97-2517-fabb286f5077.png" alt="image.png"></p>
<p><img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/616291/ba770518-09b0-107a-fe0e-1357065aa4bd.png" alt="image.png"></p>
<p>The neuron has multiple inputs and one output. One layer of a neural network consists of multiple neurons.
<strong>Weight</strong>: A coefficient to multiply each input. It corresponds to the transmission efficiency in synapses, and the larger the value, the more information is transmitted. During learning, it is automatically updated, so only the initial value is used.
<strong>Bias</strong>: A constant added to the sum of the product of the input and the weight. It adjusts the excitability of neurons. During learning, it is automatically updated, so only the initial value is used.
<strong>Activation function</strong>: Function that represents the excited state of the neuron. Without an activation function, the neuron&rsquo;s operation would be a mere sum of products, and the neural network would lose its ability to make complex expressions. There are various functions such as sigmoid function and ReLU, and the optimum function is determined to some extent according to the problem to be handled. You have to choose for each layer. See <a href="#Typeofactivationfunction">Type of activation function</a> for the description of the function.
By the way, <strong>w</strong> is a weight and <strong>b</strong> is a bias.</p>
<p>(Understanding) yolo, an image recognition library, can be recognized immediately after importing a wight file that has already been trained (weights, biases, and the number of layers have been automatically adjusted). However, if the learning environment is not taken into consideration, the recognition may be wrong, or even if additional learning is performed, it may not converge well. For example, as described in <a href="https://qiita.com/tetsuroito/items/e0944dfefd806e51364a">this article</a>,&ldquo;Sushi&quot;isrecognizedas&quot;hotdog&quot;inthelearningmodellearnedintheUnitedStates.Inotherwords,itcanbesaidthatthepersonality(calculationresult)changesdependingontheenvironment(dataset) where he grew up.</p>
<h2 id="networking-of-neurons-neural-network">Networking of neurons (neural network)</h2>
<p>A neural network is constructed by connecting a plurality of neurons to form a network. (x, v, y are neurons) Arrange them in layers as shown in the figure.
<img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/616291/53c33eef-a2f3-0f00-fef1-06f6a0bc8edc.jpeg" alt="news012_network1.jpg">
Figure Neural network model (quoted from machine learning starting with python)</p>
<p>The layers in the neural network are classified into an input layer, an intermediate layer, and an output layer. There is only one input layer and one output layer, but the number of intermediate layers can be increased. It is necessary to adjust because the calculation behavior changes by increasing the number of intermediate layers.
In a normal neural network, the output from one neuron is connected to the inputs of all neurons in the next layer.
<strong>Propagation</strong> that information is transmitted from input to output.
Information that goes back from the output to the input is called backpropagation. Backpropagation is performed when the weight and bias are updated by going back to the output result. Normally, it is done automatically, so if you know the adjustment parameters and the mechanism, it will be at a level where you can use it sufficiently.
The output value is compared with the correct answer label in the data set to calculate the error, and the weight and bias are automatically adjusted so as to approach the correct answer label.
#Two problems handled by neural network
Problems handled by neural networks are classified into classification problems and regression problems.</p>
<h2 id="neural-network-type">Neural network type</h2>
<p>** Fully connected type **: A model in which all neurons between layers are connected, as in &ldquo;Figure Neural network model&rdquo;.
<strong>Convolution type</strong>: Commonly used in the field of image processing. By performing the convolution on the image, it is possible to strengthen or weaken the feature in the image.
<strong>Recursion</strong>: A type of neural network that can handle context. It is often used for natural language processing and series data.</p>
<h2 id="return">Return</h2>
<p>Regression problem predicts continuous numerical values from data trends. The basic method is to collect discrete data, create an approximate function from the data, and calculate the predicted value of unknown data.
As a main example
・Predict weight from height
・Predict what to buy next based on the user&rsquo;s shopping tendency
・Predict tomorrow&rsquo;s stock price from past stock price trends
・Predict rent from property information</p>
<h3 id="simple-regression">Simple regression</h3>
<p>Predict one result from one input. It is used to express two relationships such as height and weight.</p>
<h3 id="multiple-regression">Multiple regression</h3>
<p>Predict one result from multiple inputs such as predicting rent from various information of the property. This is often used.</p>
<h2 id="classification">Classification</h2>
<p>A classification problem is a problem that classifies data into a plurality of defined frames. There are multiple outputs unlike the regression problem. The output represents the probability of being classified as it is.
As a main example
・Classify plants from leaf images
・Classify handwritten characters
・Determine what vehicle is in the image</p>
<p>#Flow when actually learning
##1 Dataset preparation</p>
<h3 id="what-is-a-dataset">What is a dataset?</h3>
<p>It is a data group in which the learning data x and the correct answer label y are one set. The dataset is further divided into <strong>training data</strong> and <strong>testing data</strong>. The training data is used to train the network, while the test data is used to evaluate the performance of the model created from the training data. Training data are usually more numerous than test data.
If the network learned from the learning data gives good results even with the test data, it means that the network can handle unknown data. If the test data does not give good results, there is something wrong with the network itself or the learning method. The ability of the network to handle unknown data is called <strong>generalization performance</strong>.
###1.1 Obtaining data
Learning data is collected manually or by scraping. It can be collected automatically, but it is the most difficult task because you can manually determine whether it is suitable data. For information on how to collect data, refer to <a href="https://qiita.com/nonbiri15/items/f5c5c4357458bfeb03bb">this article</a>.
###1.2 Creating a dataset
Datasets can be created manually or using libraries. You can get it by searching &ldquo;how to make a data set&rdquo;. In the case of an image, create it using the <strong>annotation tool</strong>. Much of the user&rsquo;s time is spent creating this dataset.
Images usually require thousands of images to get the right results, but it&rsquo;s difficult to collect them, so padding is usually done.</p>
<h4 id="dataset-structure">Dataset structure</h4>
<p>The training data and correct labels in the dataset are each a vector.
For example, the correct height label is represented by a vector in which the following numerical values are arranged.
[146.2 170.4 169.3 154.5 179.2]
In this case, there are 5 neurons in the output layer, and the bias and the gradient of weight are adjusted so that the output value of each neuron approaches the correct value.
In the case of a classification problem, the correct answer label is a vector in which the correct answer is 1 and all other than 0 are as follows.
[0 0 1 0 0]
A sequence of numbers with one 1 and the rest 0 is called <strong>one-hot expression</strong>.
###1.3 Data pre-processing
Format the data in a format that is easy to handle, such as normalization and standardization. For example, standardization has the effect of stabilizing and speeding up learning.
###1.4 (Conversion to One-Hot expression)
In the case of a classification problem, the correct answer is represented by onoe-hot expression. Therefore, at the time of implementation, put a conversion program for one-hot expression. When the number of neurons in the output layer is 3, it becomes as follows.
[1 0 0]
[0 1 0]
[0 0 1]
###1.5 Split training and test data
Finally, it is divided into training data and test data. The training data is used for learning, while the test data is used for evaluating the generalization performance of the learning result generated in the training data. There is no change in learning both, but the results after learning are compared in a graph. It seems that 20%~30% of the training data is good for the test data.
##2 Implementation of each layer
Work to create the types of the middle layer and output layer used for learning. Set the <strong>activation function</strong> and <strong>loss function</strong> of the middle layer and output layer respectively. It is a good idea to write it using class so that you can reuse it. Parameters should be declared as variables so that they can be changed from the outside.
If you do not want to understand but just want to learn quickly, copy and paste of another person is enough.
##3 Setting of initial value of parameter
<strong>Set the initial value of weight and bias</strong>.
Regarding the types of activation function <a href="https://newtechnologylifestyle.net/%E3%82%84%E3%81%A3%E3%81%B1%E3%82%8A%E3%82%88%E3%81%8F%E5%88%86%E3%81%8B%E3%82%89%E3%81%AA%E3%81%84%E6%B4%BB%E6%80%A7%E5%8C%96%E9%96%A2%E6%95%B0%E3%81%A8%E3%81%AF/">What is the activation function that I do not know well</a> is helpful.</p>
<p>##4 Hyper parameter adjustment
Set the first time, and modify the following four values according to the learning result. These are adjusted by the user many times according to the output evaluated by the test data. <a href="#hyperparameter">Click here</a>
·epoch
・Batch size
・Learning coefficient
・Optimization algorithm
##5 learning
Learning is the process of adjusting the connections between neurons. Learning goes through the following three processes.
###5.1 Derivation of error from correct answer
Use the <strong>loss function</strong> to derive the error.
###5.2 Gradient derivation
The <strong>gradient descent method</strong> is used to obtain the gradient. The loss function $L$ that emerges is partially differentiated by $w$ to obtain the gradient $\frac{∂L}{∂w}$.
<img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/616291/3d955d5b-6208-960e-b114-8ca2f7d782f3.png" alt="image.png">
(Citation: chaier tutorial)
###5.3 Backpropagation (error backpropagation)
The error between the output obtained by the forward propagation and the correct answer prepared in advance is propagated backward by one layer. Based on the propagated error, the update amount of weight and bias is obtained in each layer. The updated weight is obtained as follows.</p>
<pre><code class="language-mathw←w−η\frac{∂L}{∂w}" data-lang="mathw←w−η\frac{∂L}{∂w}"></code></pre><p>##6 Correctness rate evaluation
As a result of the learning, confirm what percentage of the training data can be correctly judged. What percentage of test data can be correctly judged is an important index for judging the success or failure of learning. The graph of epochs (number of learnings) and loss (number of incorrect answers) is output, and the user visually confirms it. Estimate what goes wrong from the movement of the graph, adjust the hyperparameters, and perform learning again.</p>
<p>#Problems due to multiple layers of deep learning</p>
<h2 id="trap-to-local-optimal-solution">Trap to local optimal solution</h2>
<p>The problem of being unable to reach the overall optimal solution by being trapped in the local optimal solution when finding the gradient. In some cases, the gradient is extremely reduced, and there is a problem that learning does not proceed. In order to find the optimal solution as a whole, it is necessary to leave the best state locally.
<img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/616291/afe2b1c6-00a4-2388-d831-da8939b25a4f.png" alt="image.png">
(Quoted: chainer tutorial)</p>
<h2 id="overlearning">Overlearning</h2>
<p>There is a neural network** It is optimized only for a certain range of data and learns, and it becomes impossible to deal with unknown data. **In machine learning, too much fitting to training data makes it impossible to estimate corresponding unknown input data. It can be said that only a specific pattern has fallen into a locally optimized solution. The generated network should be a little sloppy so that it can handle various data. The user adjusts various parameters described later in order to suppress this over-learning.
**If there are too many middle layers or neurons, over-learning occurs due to excessive expression. **Also, it may be caused by **insufficient number of training data samples**.</p>
<h2 id="gradient-disappearance">Gradient disappearance</h2>
<p>Problems mainly caused by using sigmoid function for activation function. The slope of the sigmoid function has a maximum value of 0.25 and approaches 0 as the distance from 0 increases. When backpropagating, the gradient of the activation function is applied to each gradient every time the layer is traced back. In the case of the sigmoid function, each gradient becomes smaller as it goes back up the layer, and the gradient disappears. Therefore, in deep learning, ReLU is often used as the activation function.</p>
<h2 id="it-takes-time-to-learn">It takes time to learn</h2>
<p>In multi-layer deep learning, the number of weights and biases becomes huge. As a result, learning can take days or weeks. To deal with this problem, you need to use a GPU, use a high-performance machine, do not make the network more complicated than necessary, and adjust parameters.
#User adjusted parameters
The problem with multi-layering is that the machine cannot adjust automatically, so the user needs to adjust for the result. As a result, many adjustments and learnings must be repeated to get better results. There are seven items to adjust. It takes some experience to adjust which one. The ones that are well tuned are hyperparameters, data expansion, and data preprocessing.
##1 Hyper Parameter
###epoch
Learning all the training data once counts as one epoch. If the number of epochs is too large, over-adaptation to the training data will occur, so it will be terminated at an appropriate number.</p>
<h3 id="batch-size">Batch size</h3>
<p>It refers to the number of data contained in each when the dataset is divided into several subsets. Parameters that affect learning time and performance. For details, refer to <a href="https://qiita.com/kenta1984/items/bad75a37d552510e4682">here</a>.</p>
<h3 id="learning-coefficient">Learning coefficient</h3>
<p>The factor by which the gradient descent gradient is multiplied. This allows the amount of gradient to be adjusted. If the learning coefficient is too large, the value of the loss function will oscillate or diverge as the parameters are repeatedly updated. On the contrary, if it is too small, it takes a long time to converge. The optimum value for this value has not been determined, so it is necessary to search empirically.</p>
<h3 id="optimization-algorithm">Optimization algorithm</h3>
<p>In the gradient descent method, the weight and bias are adjusted little by little based on the gradient, and the network is optimized so that the error is minimized. There are various algorithms for this optimization. [Click here] for explanation of each algorithm (List of #optimization algorithms)
##2 Initial value of weight and bias
An important hyper parameter related to the success or failure of learning. It is desirable to give random variation with a small value to some extent.
##3 Early termination
A method of stopping learning on the way. As the learning progresses, the error in the test data increases from the middle, which may lead to over-learning, so the learning is terminated before that. Even if the error becomes stagnant and the learning does not proceed, the process is ended to shorten the time.
##4 Data expansion
When the number of training data samples is small, overlearning is likely to occur. In that case, it is common to fill the sample with water. Learning various types of samples helps improve the generalization performance of the network. <a href="https://qiita.com/nuhsodnok/items/a3fb71bba4e0148e782f">This article</a> describes that the image is over-learning due to lack of samples when actually learning the image.</p>
<p>##5 Data preprocessing
Process input data in advance to make it easier to handle. Preprocessing can improve network performance and speed up learning. Much of the machine learning will be spent on this.
If you are using python, you can easily preprocess using a library called &ldquo;pandas&rdquo;.
There are various types of preprocessing such as normalization and standardization. For this method, refer to <a href="https://qiita.com/kazukiii/items/2600987798c62dd29dfc">this article</a>.
##6 dropout
Overlearning suppression technique. A technique to randomly erase neurons other than the output layer with a certain probability. Larger networks are more likely to overlearn, so using dropouts can reduce the network size.
##7 Regularization
Limit weights. By limiting the weight, the weight takes an extreme value and is prevented from being trapped in the **local optimal solution. **</p>
<p>#Typeofactivationfunction
##ReLU
Inputs less than or equal to 0 produce output 0. Functions above 0 produce the same output as the input. <strong>Often used for classification problems</strong>. In the classification problem, inputs less than 0 cannot be accepted, so they are often used for noise removal. When it is implemented by program, it can be implemented by rounding off 0 or less in the if statement.
<img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/616291/b47f7b7c-eab5-aff9-67f1-87a2f5ad02b5.png" alt="image.png"></p>
<h2 id="sigmoid-function">Sigmoid function</h2>
<p>As the input value increases, it converges to a constant value, so it is not used for large input values.
<img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/616291/93cdded5-509c-8357-61ea-8295e9696901.png" alt="image.png">
(wikipedia)</p>
<h2 id="softmax-function">Softmax function</h2>
<p>** A suitable function for dealing with classification problems. **Often used in the output layer of classification problems. Add all the outputs of this function from K=1 to n to get 1. Therefore, the softmax function has the property of normalizing the sum of all outputs to be 1 no matter what value the vector X consisting of multiple input values takes. **Therefore, it is compatible with the classification problem that treats the sum of the output values of the neurons in the output layer as 1.</p>
<pre><code class="language-math" data-lang="math">y=\frac{e^{x}}{\sum_{k=1}^{N}e^{x_k}}
</code></pre><h2 id="identity-function">Identity function</h2>
<p>A function that returns input as it is as output. **Often used as an activation function in the output layer of regression problems. **
#Loss function list</p>
<h2 id="mean-square-error">Mean-square error</h2>
<p>A loss function that is often used when you want to solve a <strong>regression problem</strong>.</p>
<pre><code class="language-math" data-lang="math">L= \frac{1}{N}\sum_{n=1}^{N}(t_n-y_n)^2
</code></pre><h2 id="cross-entropy-error">Cross entropy error</h2>
<p>A loss function that is often used when trying to solve a <strong>classification problem</strong>. The advantage is that the learning speed is fast when the separation between the output and the correct answer is large.</p>
<pre><code class="language-math" data-lang="math">L=\sum_{k=1}^{K}t_k(-\log(y_k))
</code></pre><p>#List of optimization algorithms
The following is a note of typical ones. For other optimization algorithms <a href="https://qiita.com/ZoneTsuyoshi/items/8ef6fa1e154d176e25b8#%E7%A2%BA%E7%8E%87%E7%9A%84%E5%8B%BE%E9%85%8D%E9%99%8D%E4%B8%8B%E6%B3%95sgd-stochastic-gradient-descent">here</a>.
##SGD (Stochastic Gradient Descent)
An algorithm that randomly calls a sample for each update. It has the characteristic that it is hard to be trapped in the local optimal solution. It can be implemented with simple code, but the amount of updates cannot be adjusted according to the progress of learning, so learning often takes time.
##AdaGard
The update amount is adjusted automatically. The learning rate gradually decreases as the learning progresses. Since the only constant to be set is the learning coefficient, there is no need to make adjustments.
##RMSProp
It overcomes the weakness of AdaGrad, where learning is stagnant due to a decrease in the amount of updates.
##Adam
An improved version of RMSprop. It seems to be used most often.</p>
<p>#reference
<a href="https://tutorials.chainer.org/en/tutorial.html">Chainer Tutorial</a>
<a href="https://www.atmarkit.co.jp/ait/articles/1811/20/news012.html">Introduction to machine learning with python</a>
<a href="https://www.amazon.co.jp/dp/B07GWHP9YM/ref=dp-kindle-redirect?_encoding=UTF8&amp;btkr=1">First Deep Learning: Learning Neural Networks and Back Propagation with Python</a>
<a href="https://www.kikagaku.ai/tutorial/basic_of_deep_learning/learn/tensorflow_neural_network_basic_forward">Basics of Kikagaku Deep Learning</a>
<a href="https://www.sbbit.jp/article/cont1/33345">https://www.sbbit.jp/article/cont1/33345</a>
<a href="https://qiita.com/nishiy-k/items/1e795f92a99422d4ba7b">https://qiita.com/nishiy-k/items/1e795f92a99422d4ba7b</a>
<a href="https://qiita.com/Lickey/items/b97c3450d7def207bfbf">https://qiita.com/Lickey/items/b97c3450d7def207bfbf</a></p>

    </article>
  </div>

  
  
  
  <script>
  window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
  ga('create', 'UA-169005401-1', 'auto');
  ga('send', 'pageview');
  </script>
  <script async src='https://www.google-analytics.com/analytics.js'></script>
  

  

  
<link rel="stylesheet" type="text/css" href="//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.0/cookieconsent.min.css" />
<script src="//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.0/cookieconsent.min.js"></script>
<script>
window.addEventListener("load", function(){
window.cookieconsent.initialise({
  "palette": {
    "popup": {
      "background": "#216942",
      "text": "#b2d192"
    },
    "button": {
      "background": "#afed71"
    }
  }
})});
</script>

</body>

</html>
