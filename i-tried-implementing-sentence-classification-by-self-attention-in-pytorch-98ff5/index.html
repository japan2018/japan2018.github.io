<!doctype html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  
  <meta name="generator" content="Hugo 0.72.0" />

  
  <meta name="description" content="Some description">
  

  
  <link rel="apple-touch-icon" sizes="180x180" href="https://memotut.com/apple-touch-icon.png">

  
  <link rel="icon" type="image/png" sizes="32x32" href="https://memotut.com/favicon-32x32.png">

  
  <link rel="icon" type="image/png" sizes="16x16" href="https://memotut.com/favicon-16x16.png">

  
  <link rel="manifest" href="https://memotut.com/site.webmanifest">

  
  <link rel="mask-icon" href="https://memotut.com/safari-pinned-tab.svg" color="#5bbad5">

  <meta name="msapplication-TileColor" content="#da532c">

  <meta name="theme-color" content="#ffffff">

  
  <link rel="stylesheet" href="https://memotut.com/css/bootstrap.min.css" />

  
  <title>I tried implementing sentence classification by Self Attention in PyTorch | Memo Tut</title>
  

  <style>
body {
  min-width: 300px;
}

.custom-navbar {
  margin-bottom: 1em;
  height: 60px;
}

.custom-navbar a {
  display: inline-block; 
  padding: 18px 0;
  margin-right: 1em; 
  font-weight: bold; 
}

.custom-navbar a:hover,
.custom-navbar a:focus {
  text-decoration: none; 
}

@media print {
  .custom-navbar {
    display: none;
  }
}

article {
  padding-bottom: 1em;
}

img {
  max-width: 100%;
}


body {
  background-color: #fff;
}



body {
  color: #212529;
}



a {
  color: #007bff;
}



a:hover,
a:focus {
  color: #0056b3;
}



.custom-navbar {
  background-color: #212529;
}



.custom-navbar a {
  color: rgba(255, 255, 255, 0.75);
}



.custom-navbar a:hover,
.custom-navbar a:focus {
  color: rgba(255, 255, 255, 1);
}



.container {
  max-width: 800px;
}



pre {
  display: block;
  padding: 9.5px;
  word-break: break-all;
  word-wrap: break-word;
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 4px;
}

pre code {
  padding: 0;
  font-size: inherit;
  color: inherit; 
  white-space: pre-wrap;
  background-color: transparent;
  border: none;
  border-radius: 0;
}

code {
  padding: 2px 4px;
  color: inherit; 
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 4px;
  font-size: .9em;
}



blockquote,
.blockquote {
  padding: 10px 20px;
  margin: 0 0 20px;
  font-size: 1em;
  border-left: 5px solid #6c757d;
}

</style>
</head>

<body>
  <nav class="custom-navbar">
  <div class="container">
    
    <a href="/">Posts</a>
    
    <a href="/tags/">Tags</a>
    
    <a href="/about/">About</a>
    
    <a href="/index.xml">RSS</a>
    
  </div>
</nav>
  
  <div class="container">
    <article>
      <h1>I tried implementing sentence classification by Self Attention in PyTorch</h1>
<p>
  <small class="text-secondary">
  
  
  Dec 25, 2019
  </small>
  

<small><code><a href="https://memotut.com/tags/python">Python</a></code></small>


<small><code><a href="https://memotut.com/tags/natural-language-processing"> natural language processing</a></code></small>


<small><code><a href="https://memotut.com/tags/pytorch"> PyTorch</a></code></small>


<small><code><a href="https://memotut.com/tags/attention"> Attention</a></code></small>


<small><code><a href="https://memotut.com/tags/sentence-classification"> sentence classification</a></code></small>

</p>
<pre><code>This article is from the 25th day of [Pytorch Advent Calendar 2019](https://qiita.com/advent-calendar/2019/pytorch)!
</code></pre>
<p>#Introduction</p>
<p><a href="https://qiita.com/m__k/items/646044788c5f94eadc8d">Previous</a> implemented Attention in Encoder-Decoder model, but this time, I will implement sentence classification in Self Attention.</p>
<p>The embedded expression of sentences in Self Attention is introduced in the following paper, and is also quoted in the famous paper &ldquo;Attention Is All You Need&rdquo; by Transformer.</p>
<ul>
<li><a href="https://arxiv.org/pdf/1703.03130.pdf">A STRUCTURED SELF-ATTENTIVE SENTENCE EMBEDDING</a></li>
</ul>
<p>This article implements the Self Attention introduced in this paper.</p>
<h2 id="reference">Reference</h2>
<p>Regarding the implementation, I referred to the following articles as <s>Mostly Marpakuri</s>.</p>
<p>① <a href="https://qiita.com/itok_msi/items/ad95425b6773985ef959">[self attention] Implement a document classification model that makes it easy to visualize the reason for prediction</a></p>
<p>In addition, we use torchtext that can be conveniently preprocessed for implementation, but for torchtext I also referred to the following article by the same person.</p>
<p>② <a href="https://qiita.com/itok_msi/items/1f3746f7e89a19dafac5">Easy Deep Natural Language Processing with torchtext</a></p>
<h1 id="how-it-works">How it works</h1>
<p>Although the mechanism of this paper is briefly explained in Reference (1), the algorithm is roughly divided into the following three steps.</p>
<ol>
<li>Convert text of length $n$ with Bidirectional LSTM (Hidden layer dimension is $u$) (each $h_i, (n\times 2u)$ in (a) below)</li>
<li>Attention is calculated by Neural Network using the values of each hidden layer of Bidirectional LSTM ($A=(A_{ij}), 1\leq i \leq r, 1\leq j \leq in the following figure (b). get n$)</li>
<li>The vector of each hidden layer of Bidirectional LSTM is weighted by each Attention$A_{ij}$ and the sentence embedding is acquired by Neural Network.</li>
</ol>
<p>Here, $d_a$ and $r$ when calculating Attention are hyperparameters. $d_a$ represents the size of the weight matrix when predicting Attention with Neural Network, and $r$ is a parameter corresponding to how many layers of Attention are stacked.</p>
<p><img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/240999/6cac0ea3-4d80-cc7d-5383-f197509396e1.png" alt="image.png"></p>
<p>The idea is very simple, the point is to let the neural network learn which words should be emphasized (weighted) when classifying sentences.</p>
<h1 id="implementation">Implementation</h1>
<p>Then, I will implement the above mechanism with PyTorch. The task to be solved is the negative/positive judgment of IMDb movie reviews. The data can be downloaded from the following.</p>
<ul>
<li><a href="http://ai.stanford.edu/~amaas/data/sentiment/">http://ai.stanford.edu/~amaas/data/sentiment/</a></li>
</ul>
<p>*The following implementation example is written on the assumption that it will run on Google Colab.</p>
<h2 id="import-library">Import library</h2>
<ul>
<li>Import various libraries used in implementation</li>
<li>Since the data set is in English, I think that the morphological analysis engine is ok, but for the time being I have prepared a function to perform some preprocessing with nltk (I do not mind it though there is a part to be preprocessed with torchtext). Please refer to <a href="https://qiita.com/m__k/items/ffd3b7774f2fde1083fa">here</a> for nltk.</li>
</ul>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py"><span style="color:#75715e"># torchtext</span>
<span style="color:#f92672">import</span> torchtext
<span style="color:#f92672">from</span> torchtext <span style="color:#f92672">import</span> data
<span style="color:#f92672">from</span> torchtext <span style="color:#f92672">import</span> datasets
<span style="color:#f92672">from</span> torchtext.vocab <span style="color:#f92672">import</span> GloVe
<span style="color:#f92672">from</span> torchtext.vocab <span style="color:#f92672">import</span> Vectors

<span style="color:#75715e"># pytorch</span>
<span style="color:#f92672">import</span> torch.nn <span style="color:#f92672">as</span> nn
<span style="color:#f92672">import</span> torch.optim <span style="color:#f92672">as</span> optim
<span style="color:#f92672">import</span> torch.nn.functional <span style="color:#f92672">as</span> F
<span style="color:#f92672">import</span> torch

<span style="color:#75715e"># Other things</span>
<span style="color:#f92672">import</span> os
<span style="color:#f92672">import</span> pickle
<span style="color:#f92672">import</span> numpy <span style="color:#f92672">as</span> np
<span style="color:#f92672">import</span> pandas <span style="color:#f92672">as</span> pd
<span style="color:#f92672">from</span> itertools <span style="color:#f92672">import</span> chain
<span style="color:#f92672">from</span> sklearn.model_selection <span style="color:#f92672">import</span> train_test_split
<span style="color:#f92672">from</span> sklearn.metrics <span style="color:#f92672">import</span> classification_report

<span style="color:#75715e"># Finally used to visualize the attention.</span>
<span style="color:#f92672">import</span> itertools
<span style="color:#f92672">import</span> random
<span style="color:#f92672">from</span> IPython.display <span style="color:#f92672">import</span> display, HTML

For preprocessing <span style="color:#66d9ef">with</span> <span style="color:#75715e">#nltk</span>
<span style="color:#f92672">import</span> re
<span style="color:#f92672">import</span> nltk
<span style="color:#f92672">from</span> nltk <span style="color:#f92672">import</span> stem
nltk<span style="color:#f92672">.</span>download(<span style="color:#e6db74">&#39;punkt&#39;</span>)

Prepare morpheme engine by <span style="color:#75715e">#nltk</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">nltk_analyzer</span>(text):
    stemmer <span style="color:#f92672">=</span> stem<span style="color:#f92672">.</span>LancasterStemmer()
    text <span style="color:#f92672">=</span> re<span style="color:#f92672">.</span>sub(re<span style="color:#f92672">.</span>compile(<span style="color:#e6db74">r</span><span style="color:#e6db74">&#39;[!-\/:-@[-`{-~]&#39;</span>),<span style="color:#e6db74">&#39;&#39;</span>, text)
    text <span style="color:#f92672">=</span> stemmer<span style="color:#f92672">.</span>stem(text)
    text <span style="color:#f92672">=</span> text<span style="color:#f92672">.</span>replace(<span style="color:#e6db74">&#39;</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&#39;</span>, <span style="color:#e6db74">``</span>) <span style="color:#75715e"># delete line breaks</span>
    text <span style="color:#f92672">=</span> text<span style="color:#f92672">.</span>replace(<span style="color:#e6db74">&#39;</span><span style="color:#ae81ff">\t</span><span style="color:#e6db74">&#39;</span>, <span style="color:#e6db74">``</span>) <span style="color:#75715e"># remove tab</span>
    morph <span style="color:#f92672">=</span> nltk<span style="color:#f92672">.</span>word_tokenize(text)
    <span style="color:#66d9ef">return</span> morph
</code></pre></div><h2 id="data-preparation">Data preparation</h2>
<ul>
<li>Download the dataset from the above URL and prepare a tsv file with the following format.</li>
<li>Prepare both train and test</li>
<li>Label the numbers as 0 for positive and 1 for negative.</li>
</ul>
<h4 id="reference-1">Reference</h4>
<p>For example, the method of preparing the data was as follows.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py">train_pos_dir <span style="color:#f92672">=</span><span style="color:#e6db74">&#39;aclImdb/train/pos/&#39;</span>
train_neg_dir <span style="color:#f92672">=</span><span style="color:#e6db74">&#39;aclImdb/train/neg/&#39;</span>

test_pos_dir <span style="color:#f92672">=</span><span style="color:#e6db74">&#39;aclImdb/test/pos/&#39;</span>
test_neg_dir <span style="color:#f92672">=</span><span style="color:#e6db74">&#39;aclImdb/test/neg/&#39;</span>

header <span style="color:#f92672">=</span> [<span style="color:#e6db74">&#39;text&#39;</span>,<span style="color:#e6db74">&#39;label&#39;</span>,<span style="color:#e6db74">&#39;label_id&#39;</span>]

train_pos_files <span style="color:#f92672">=</span> os<span style="color:#f92672">.</span>listdir(train_pos_dir)
train_neg_files <span style="color:#f92672">=</span> os<span style="color:#f92672">.</span>listdir(train_neg_dir)
test_pos_files <span style="color:#f92672">=</span> os<span style="color:#f92672">.</span>listdir(test_pos_dir)
test_neg_files <span style="color:#f92672">=</span> os<span style="color:#f92672">.</span>listdir(test_neg_dir)


<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">make_row</span>(root_dir, files, label, idx):
    row <span style="color:#f92672">=</span> []
    <span style="color:#66d9ef">for</span> file <span style="color:#f92672">in</span> files:
        tmp <span style="color:#f92672">=</span> []
        <span style="color:#66d9ef">with</span> open(root_dir <span style="color:#f92672">+</span> file,<span style="color:#e6db74">&#39;r&#39;</span>) <span style="color:#66d9ef">as</span> f:
            text <span style="color:#f92672">=</span> f<span style="color:#f92672">.</span>read()
            tmp<span style="color:#f92672">.</span>append(text)
            tmp<span style="color:#f92672">.</span>append(label)
            tmp<span style="color:#f92672">.</span>append(idx)
        row<span style="color:#f92672">.</span>append(tmp)
    <span style="color:#66d9ef">return</span> row

row <span style="color:#f92672">=</span> make_row(train_pos_dir, train_pos_files,<span style="color:#e6db74">&#39;pos&#39;</span>, <span style="color:#ae81ff">0</span>)
row <span style="color:#f92672">+=</span> make_row(train_neg_dir, train_neg_files,<span style="color:#e6db74">&#39;neg&#39;</span>, <span style="color:#ae81ff">1</span>)
train_df <span style="color:#f92672">=</span> pd<span style="color:#f92672">.</span>DataFrame(row, columns<span style="color:#f92672">=</span>header)


row <span style="color:#f92672">=</span> make_row(test_pos_dir, test_pos_files,<span style="color:#e6db74">&#39;pos&#39;</span>, <span style="color:#ae81ff">0</span>)
row <span style="color:#f92672">+=</span> make_row(test_neg_dir, test_neg_files,<span style="color:#e6db74">&#39;neg&#39;</span>, <span style="color:#ae81ff">1</span>)
test_df <span style="color:#f92672">=</span> pd<span style="color:#f92672">.</span>DataFrame(row, columns<span style="color:#f92672">=</span>header)
</code></pre></div><p>Prepare the data as above, and finally create the following dataframe (while deleting the label column because it is not necessary).</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py">train_df <span style="color:#f92672">=</span> pd<span style="color:#f92672">.</span>read_csv(imdb_dir <span style="color:#f92672">+</span><span style="color:#e6db74">&#39;train.tsv&#39;</span>, delimiter<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;</span><span style="color:#ae81ff">\t</span><span style="color:#e6db74">&#34;</span>, header<span style="color:#f92672">=</span>None)
train_df
</code></pre></div><p><img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/240999/009808d3-e8e5-c8b7-5575-c601800ac290.png" alt="image.png"></p>
<h2 id="preprocessing-with-torchtext">Preprocessing with torchtext</h2>
<ul>
<li>Quickly preprocess data with torchtext, get distributed expression of words, mini-batch, etc.</li>
<li>We used 200-dimensional GloVe for distributed expression of words. You can download it in torchtext, but I borrowed glove.6B.200d.txt from <a href="https://www.kaggle.com/incorpes/glove6b200d">here</a> because I don&rsquo;t want to download it every time. Please note that the size is large!</li>
</ul>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py">Put <span style="color:#75715e"># train.tsv, test.tsv here</span>
imdb_dir <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;drive/My Drive/Colab Notebooks/imdb_datasets/&#34;</span>

Put <span style="color:#75715e"># glove.6B.200d.txt here</span>
word_embedding_dir <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;drive/My Drive/Colab Notebooks/word_embedding_models/&#34;</span>

TEXT <span style="color:#f92672">=</span> data<span style="color:#f92672">.</span>Field(sequential<span style="color:#f92672">=</span>True, tokenize<span style="color:#f92672">=</span>nltk_analyzer, lower<span style="color:#f92672">=</span>True, include_lengths<span style="color:#f92672">=</span>True, batch_first<span style="color:#f92672">=</span>True)
LABEL <span style="color:#f92672">=</span> data<span style="color:#f92672">.</span>Field(sequential<span style="color:#f92672">=</span>False, use_vocab<span style="color:#f92672">=</span>False, is_target<span style="color:#f92672">=</span>True)

train, test <span style="color:#f92672">=</span> data<span style="color:#f92672">.</span>TabularDataset<span style="color:#f92672">.</span>splits(path<span style="color:#f92672">=</span>imdb_dir, train<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;train.tsv&#39;</span>, test<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;test.tsv&#39;</span>, format<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;tsv&#39;</span>,
      fields<span style="color:#f92672">=</span>[(<span style="color:#e6db74">&#39;Text&#39;</span>, TEXT), (<span style="color:#e6db74">&#39;Label&#39;</span>, LABEL)])

glove_vectors <span style="color:#f92672">=</span> Vectors(name<span style="color:#f92672">=</span>word_embedding_dir <span style="color:#f92672">+</span> <span style="color:#e6db74">&#34;glove.6B.200d.txt&#34;</span>)
TEXT<span style="color:#f92672">.</span>build_vocab(train, test, vectors<span style="color:#f92672">=</span>glove_vectors, min_freq<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
</code></pre></div><h2 id="hyper-parameter-settings-etc">Hyper parameter settings, etc.</h2>
<ul>
<li>For no particular reason, I used the following parameters.</li>
<li>Attention layer I made the following figure into 3 layers.</li>
</ul>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py"><span style="color:#75715e"># I want to use GPU</span>
device <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>device(<span style="color:#e6db74">&#34;cuda&#34;</span> <span style="color:#66d9ef">if</span> torch<span style="color:#f92672">.</span>cuda<span style="color:#f92672">.</span>is_available() <span style="color:#66d9ef">else</span> <span style="color:#e6db74">&#34;cpu&#34;</span>)

BATCH_SIZE <span style="color:#f92672">=</span> <span style="color:#ae81ff">100</span> <span style="color:#75715e"># batch size</span>
EMBEDDING_DIM <span style="color:#f92672">=</span> <span style="color:#ae81ff">200</span> <span style="color:#75715e"># word embedding dimension</span>
LSTM_DIM <span style="color:#f92672">=</span> <span style="color:#ae81ff">128</span> <span style="color:#75715e"># hidden layer dimension of LSTM</span>
VOCAB_SIZE <span style="color:#f92672">=</span>TEXT<span style="color:#f92672">.</span>vocab<span style="color:#f92672">.</span>vectors<span style="color:#f92672">.</span>size()[<span style="color:#ae81ff">0</span>] <span style="color:#75715e"># total number of words</span>
TAG_SIZE <span style="color:#f92672">=</span> <span style="color:#ae81ff">2</span> <span style="color:#75715e"># Since the negative/positive judgment is performed this time, the last size of the network is 2</span>
Size of weight matrix when DA <span style="color:#f92672">=</span> <span style="color:#ae81ff">64</span> <span style="color:#75715e"># Attention is calculated by Neural Network</span>
R <span style="color:#f92672">=</span> <span style="color:#ae81ff">3</span> <span style="color:#75715e"># Look at 3 layers of Attention</span>
</code></pre></div><h2 id="model-definition">Model definition</h2>
<h3 id="bidirectional-lstm">Bidirectional LSTM</h3>
<ul>
<li>Convert sentences with Bidrectional LSTM</li>
<li>Please refer to <a href="https://qiita.com/m__k/items/78a5125d719951ca98d3">here</a> for the specifications of PyTorch Bidirectional LSTM.</li>
</ul>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py"><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">BiLSTMEncoder</span>(nn<span style="color:#f92672">.</span>Module):
    <span style="color:#66d9ef">def</span> __init__(self, embedding_dim, lstm_dim, vocab_size):
        super(BiLSTMEncoder, self)<span style="color:#f92672">.</span>__init__()
        self<span style="color:#f92672">.</span>lstm_dim <span style="color:#f92672">=</span> lstm_dim
        self<span style="color:#f92672">.</span>word_embeddings <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Embedding(vocab_size, embedding_dim)

        <span style="color:#75715e"># Set learned word vector as embedding</span>
        self<span style="color:#f92672">.</span>word_embeddings<span style="color:#f92672">.</span>weight<span style="color:#f92672">.</span>data<span style="color:#f92672">.</span>copy_(TEXT<span style="color:#f92672">.</span>vocab<span style="color:#f92672">.</span>vectors)

        <span style="color:#75715e"># Set requires_grad to False to prevent the word vector from being updated by backpropagation</span>
        self<span style="color:#f92672">.</span>word_embeddings<span style="color:#f92672">.</span>requires_grad_ <span style="color:#f92672">=</span> False

        You can easily make a bidirectional LSTM <span style="color:#66d9ef">with</span> <span style="color:#75715e"># bidirectional=True</span>
        self<span style="color:#f92672">.</span>bilstm <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>LSTM(embedding_dim, lstm_dim, batch_first<span style="color:#f92672">=</span>True, bidirectional<span style="color:#f92672">=</span>True)
  
    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, text):
        embeds <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>word_embeddings(text)

        <span style="color:#75715e"># I want the vector of each hidden layer, so I receive the first return value</span>
        out, _ <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>bilstm(embeds)

        <span style="color:#75715e"># Returns the vector of each hidden layer in the forward direction and the backward direction in the state of being combined</span>
        <span style="color:#66d9ef">return</span> out
</code></pre></div><h3 id="self-attention-layer">Self Attention layer</h3>
<ul>
<li>Receive the vector of each hidden layer of Bidirectional LSTM and calculate Attention with Neural Network</li>
<li>According to the paper, I use <code>Tanh()</code> for the activation function, but since the article introduced by reference (1) uses <code>ReLU()</code>, it seems that either one is fine.</li>
</ul>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py"><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">SelfAttention</span>(nn<span style="color:#f92672">.</span>Module):
  <span style="color:#66d9ef">def</span> __init__(self, lstm_dim, da, r):
    super(SelfAttention, self)<span style="color:#f92672">.</span>__init__()
    self<span style="color:#f92672">.</span>lstm_dim <span style="color:#f92672">=</span> lstm_dim
    self<span style="color:#f92672">.</span>da <span style="color:#f92672">=</span> da
    self<span style="color:#f92672">.</span>r <span style="color:#f92672">=</span> r
    self<span style="color:#f92672">.</span>main <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Sequential(
        Since it <span style="color:#f92672">is</span> <span style="color:#75715e"># Bidirectional, the vector dimension of each hidden layer is twice the size.</span>
        nn<span style="color:#f92672">.</span>Linear(lstm_dim <span style="color:#f92672">*</span> <span style="color:#ae81ff">2</span>, da),
        nn<span style="color:#f92672">.</span>Tanh(),
        nn<span style="color:#f92672">.</span>Linear(da, r)
    )
  <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, out):
    <span style="color:#66d9ef">return</span> F<span style="color:#f92672">.</span>softmax(self<span style="color:#f92672">.</span>main(out), dim<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
</code></pre></div><h3 id="classifying-in-consideration-of-attention">Classifying in consideration of Attention</h3>
<ul>
<li>Weights each hidden layer vector with Attention weight and returns prediction for binary classification with Neural Network</li>
<li>To be honest, I do not really understand the processing after weighting each Attenion layer, so this time I tried processing with the following steps.</li>
</ul>
<ol>
<li>Weight the vector of each hidden layer of Bidirectional LSTM with the weight of three attention layers</li>
<li>Add each weighted vector to get m1, m2, m3</li>
<li>Combine the three vectors m1, m2, m3 as they are (the number of dimensions becomes <code>lstm_dim * 2 * 3</code>)</li>
</ol>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py">
<span style="color:#66d9ef">class</span> <span style="color:#a6e22e">SelfAttentionClassifier</span>(nn<span style="color:#f92672">.</span>Module):
  <span style="color:#66d9ef">def</span> __init__(self, lstm_dim, da, r, tagset_size):
    super(SelfAttentionClassifier, self)<span style="color:#f92672">.</span>__init__()
    self<span style="color:#f92672">.</span>lstm_dim <span style="color:#f92672">=</span> lstm_dim
    self<span style="color:#f92672">.</span>r <span style="color:#f92672">=</span> r
    self<span style="color:#f92672">.</span>attn <span style="color:#f92672">=</span> SelfAttention(lstm_dim, da, r)
    self<span style="color:#f92672">.</span>main <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(lstm_dim <span style="color:#f92672">*</span> <span style="color:#ae81ff">6</span>, tagset_size)

  <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, out):
    attention_weight <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>attn(out)
    m1 <span style="color:#f92672">=</span> (out <span style="color:#f92672">*</span> attention_weight[:,:,<span style="color:#ae81ff">0</span>]<span style="color:#f92672">.</span>unsqueeze(<span style="color:#ae81ff">2</span>))<span style="color:#f92672">.</span>sum(dim<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
    m2 <span style="color:#f92672">=</span> (out <span style="color:#f92672">*</span> attention_weight[:,:,<span style="color:#ae81ff">1</span>]<span style="color:#f92672">.</span>unsqueeze(<span style="color:#ae81ff">2</span>))<span style="color:#f92672">.</span>sum(dim<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
    m3 <span style="color:#f92672">=</span> (out <span style="color:#f92672">*</span> attention_weight[:,:,<span style="color:#ae81ff">2</span>]<span style="color:#f92672">.</span>unsqueeze(<span style="color:#ae81ff">2</span>))<span style="color:#f92672">.</span>sum(dim<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
    feats <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>cat([m1, m2, m3], dim<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
    <span style="color:#66d9ef">return</span> F<span style="color:#f92672">.</span>log_softmax(self<span style="color:#f92672">.</span>main(feats)), attention_weight
</code></pre></div><h3 id="model-declaration">model declaration</h3>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py">encoder <span style="color:#f92672">=</span> BiLSTMEncoder(EMBEDDING_DIM, LSTM_DIM, VOCAB_SIZE)<span style="color:#f92672">.</span>to(device)
classifier <span style="color:#f92672">=</span> SelfAttentionClassifier(LSTM_DIM, DA, R, TAG_SIZE)<span style="color:#f92672">.</span>to(device)
loss_function <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>NLLLoss()

<span style="color:#75715e"># You can combine optimizers into one by enclosing multiple models with from itertools import chain</span>
optimizer <span style="color:#f92672">=</span> optim<span style="color:#f92672">.</span>Adam(chain(encoder<span style="color:#f92672">.</span>parameters(), classifier<span style="color:#f92672">.</span>parameters()), lr<span style="color:#f92672">=</span><span style="color:#ae81ff">0.001</span>)

train_iter, test_iter <span style="color:#f92672">=</span> data<span style="color:#f92672">.</span>Iterator<span style="color:#f92672">.</span>splits((train, test), batch_sizes<span style="color:#f92672">=</span>(BATCH_SIZE, BATCH_SIZE), device<span style="color:#f92672">=</span>device, repeat<span style="color:#f92672">=</span>False, sort<span style="color:#f92672">=</span>False)
</code></pre></div><h2 id="let-them-learn">let them learn</h2>
<ul>
<li>I tried learning with Epoch 10 for the time being.</li>
<li>Loss is decreasing steadily, so it&rsquo;s OK for the time being</li>
</ul>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py">losses <span style="color:#f92672">=</span> []
<span style="color:#66d9ef">for</span> epoch <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">10</span>):
    all_loss <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>

    <span style="color:#66d9ef">for</span> idx, batch <span style="color:#f92672">in</span> enumerate(train_iter):
        batch_loss <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
        encoder<span style="color:#f92672">.</span>zero_grad()
        classifier<span style="color:#f92672">.</span>zero_grad()

        text_tensor <span style="color:#f92672">=</span> batch<span style="color:#f92672">.</span>Text[<span style="color:#ae81ff">0</span>]
        label_tensor <span style="color:#f92672">=</span> batch<span style="color:#f92672">.</span>Label
        out <span style="color:#f92672">=</span> encoder(text_tensor)
        score, attn <span style="color:#f92672">=</span> classifier(out)
        batch_loss <span style="color:#f92672">=</span> loss_function(score, label_tensor)
        batch_loss<span style="color:#f92672">.</span>backward()
        optimizer<span style="color:#f92672">.</span>step()
        all_loss <span style="color:#f92672">+=</span> batch_loss<span style="color:#f92672">.</span>item()
    <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;epoch&#34;</span>, epoch, <span style="color:#e6db74">&#34;</span><span style="color:#ae81ff">\t</span><span style="color:#e6db74">&#34;</span> ,<span style="color:#e6db74">&#34;loss&#34;</span>, all_loss)
<span style="color:#75715e">#epoch 0 loss 97.37978366017342</span>
<span style="color:#75715e">#epoch 1 loss 50.07680431008339</span>
<span style="color:#75715e">#epoch 2 loss 27.79373042844236</span>
<span style="color:#75715e">#epoch 3 loss 9.353876578621566</span>
<span style="color:#75715e">#epoch 4 loss 1.9509600398596376</span>
<span style="color:#75715e">#epoch 5 loss 0.22650832029466983</span>
<span style="color:#75715e">#epoch 6 loss 0.021685686125238135</span>
<span style="color:#75715e">#epoch 7 loss 0.011305359620109812</span>
<span style="color:#75715e">#epoch 8 loss 0.007448446772286843</span>
<span style="color:#75715e">#epoch 9 loss 0.005398457038154447</span>
</code></pre></div><h2 id="forecast--accuracy">Forecast &amp; Accuracy</h2>
<ul>
<li>I think the accuracy is worse than I expected&hellip;</li>
<li>Reference (1) said that the accuracy was about 90%, and it seems that there are various backsides that the implementation is different &hellip;</li>
</ul>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py">answer <span style="color:#f92672">=</span> []
prediction <span style="color:#f92672">=</span> []
<span style="color:#66d9ef">with</span> torch<span style="color:#f92672">.</span>no_grad():
    <span style="color:#66d9ef">for</span> batch <span style="color:#f92672">in</span> test_iter:

        text_tensor <span style="color:#f92672">=</span> batch<span style="color:#f92672">.</span>Text[<span style="color:#ae81ff">0</span>]
        label_tensor <span style="color:#f92672">=</span> batch<span style="color:#f92672">.</span>Label
    
        out <span style="color:#f92672">=</span> encoder(text_tensor)score, _ <span style="color:#f92672">=</span> classifier(out)
        _, pred <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>max(score, <span style="color:#ae81ff">1</span>)

        prediction <span style="color:#f92672">+=</span> list(pred<span style="color:#f92672">.</span>cpu()<span style="color:#f92672">.</span>numpy())
        answer <span style="color:#f92672">+=</span> list(label_tensor<span style="color:#f92672">.</span>cpu()<span style="color:#f92672">.</span>numpy())
<span style="color:#66d9ef">print</span>(classification_report(prediction, answer, target_names<span style="color:#f92672">=</span>[<span style="color:#e6db74">&#39;positive&#39;</span>,<span style="color:#e6db74">&#39;negative&#39;</span>]))
<span style="color:#75715e"># precision recall f1-score support</span>
<span style="color:#75715e">#</span>
<span style="color:#75715e"># positive 0.86 0.88 0.87 12103</span>
<span style="color:#75715e"># negative 0.89 0.86 0.87 12897</span>
<span style="color:#75715e">#</span>
<span style="color:#75715e"># accuracy 0.87 25000</span>
<span style="color:#75715e"># macro avg 0.87 0.87 0.87 25000</span>
<span style="color:#75715e">#weighted avg 0.87 0.87 0.87 25000</span>
</code></pre></div><h2 id="attention-visualization">Attention visualization</h2>
<ul>
<li>Highlight and visualize which word you are paying attention to.</li>
<li>For the highlighted function, I borrowed the source of reference ① as it is.</li>
<li>Please refer to <a href="https://qiita.com/m__k/items/3277555fbb0491808926">here</a> when displaying HTML with jupyter notebook.</li>
<li>I am doing strange processing such as for loop, but I just wanted to pick up one random item from test data and make a prediction. Sorry for the unhelpful implementation&hellip;</li>
</ul>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py"><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">highlight</span>(word, attn):
    html_color <span style="color:#f92672">=</span><span style="color:#e6db74">&#39;#</span><span style="color:#e6db74">%02X%02X%02X</span><span style="color:#e6db74">&#39;</span> <span style="color:#f92672">%</span>(<span style="color:#ae81ff">255</span>, int(<span style="color:#ae81ff">255</span><span style="color:#f92672">*</span>(<span style="color:#ae81ff">1</span><span style="color:#f92672">-</span>attn)), int(<span style="color:#ae81ff">255</span><span style="color:#f92672">*</span>(<span style="color:#ae81ff">1</span><span style="color:#f92672">-</span>attn)))
    <span style="color:#66d9ef">return</span><span style="color:#e6db74">&#39;&lt;span style=&#34;background-color: {}&#34;&gt;{}&lt;/span&gt;&#39;</span><span style="color:#f92672">.</span>format(html_color, word)

<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">mk_html</span>(sentence, attns):
    html <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;&#34;</span>
    <span style="color:#66d9ef">for</span> word, attn <span style="color:#f92672">in</span> zip(sentence, attns):
        html <span style="color:#f92672">+=</span><span style="color:#e6db74">&#39;&#39;</span> <span style="color:#f92672">+</span> highlight(
            TEXT<span style="color:#f92672">.</span>vocab<span style="color:#f92672">.</span>itos[word],
            attn
        )
    <span style="color:#66d9ef">return</span> html


id2ans <span style="color:#f92672">=</span> {<span style="color:#e6db74">&#39;0&#39;</span>:<span style="color:#e6db74">&#39;positive&#39;</span>, <span style="color:#e6db74">&#39;1&#39;</span>:<span style="color:#e6db74">&#39;negative&#39;</span>}

_, test_iter <span style="color:#f92672">=</span> data<span style="color:#f92672">.</span>Iterator<span style="color:#f92672">.</span>splits((train, test), batch_sizes<span style="color:#f92672">=</span>(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>), device<span style="color:#f92672">=</span>device, repeat<span style="color:#f92672">=</span>False, sort<span style="color:#f92672">=</span>False)

n <span style="color:#f92672">=</span> random<span style="color:#f92672">.</span>randrange(len(test_iter))

<span style="color:#66d9ef">for</span> batch <span style="color:#f92672">in</span> itertools<span style="color:#f92672">.</span>islice(test_iter, n<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>,n):
    x <span style="color:#f92672">=</span> batch<span style="color:#f92672">.</span>Text[<span style="color:#ae81ff">0</span>]
    y <span style="color:#f92672">=</span> batch<span style="color:#f92672">.</span>Label
    encoder_outputs <span style="color:#f92672">=</span> encoder(x)
    output, attn <span style="color:#f92672">=</span> classifier(encoder_outputs)
    pred <span style="color:#f92672">=</span> output<span style="color:#f92672">.</span>data<span style="color:#f92672">.</span>max(<span style="color:#ae81ff">1</span>, keepdim<span style="color:#f92672">=</span>True)[<span style="color:#ae81ff">1</span>]

    display(HTML(<span style="color:#e6db74">&#39;[correct answer]&#39;</span> <span style="color:#f92672">+</span> id2ans[str(y<span style="color:#f92672">.</span>item())] <span style="color:#f92672">+</span><span style="color:#e6db74">&#39;</span><span style="color:#ae81ff">\t</span><span style="color:#e6db74"> [prediction]&#39;</span> <span style="color:#f92672">+</span> id2ans[str(pred<span style="color:#f92672">.</span>item())] <span style="color:#f92672">+</span><span style="color:#e6db74">&#39;&lt;br&gt;&lt;br&gt;&#39;</span> ))
    <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(attn<span style="color:#f92672">.</span>size()[<span style="color:#ae81ff">2</span>]):
      display(HTML(mk_html(x<span style="color:#f92672">.</span>data[<span style="color:#ae81ff">0</span>], attn<span style="color:#f92672">.</span>data[<span style="color:#ae81ff">0</span>,:,i]) <span style="color:#f92672">+</span><span style="color:#e6db74">&#39;&lt;br&gt;&lt;br&gt;&#39;</span>))
</code></pre></div><p>I&rsquo;m sorry for it being small, but it will be displayed like this when visualized. The same sentence is displayed three times, but since there are three Attention layers, each layer shows which word is attention.
The attention level of the word differs slightly in each attention layer, but it seems that the attention is almost the same.</p>
<p><img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/240999/ca65a989-7c8e-8aa5-09ed-04c600a4661d.png" alt="image.png"></p>
<h1 id="supplement">Supplement</h1>
<h2 id="without-self-attention">Without Self Attention&hellip;</h2>
<ul>
<li>By the way, the accuracy was about 79.4% when the negative/positive judgment this time was solved only with Bidirectional LSTM without Self Attention.</li>
<li>When solving only with Bidirectional LSTM, use the following network and leave other parameters as they are.</li>
<li>Self Attention seems to have contributed greatly to improving the accuracy.</li>
</ul>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py"><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">BiLSTMEncoder</span>(nn<span style="color:#f92672">.</span>Module):
    <span style="color:#66d9ef">def</span> __init__(self, embedding_dim, lstm_dim, vocab_size, tagset_size):
        super(BiLSTMEncoder, self)<span style="color:#f92672">.</span>__init__()
        self<span style="color:#f92672">.</span>lstm_dim <span style="color:#f92672">=</span> lstm_dim
        self<span style="color:#f92672">.</span>word_embeddings <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Embedding(vocab_size, embedding_dim)
        self<span style="color:#f92672">.</span>word_embeddings<span style="color:#f92672">.</span>weight<span style="color:#f92672">.</span>data<span style="color:#f92672">.</span>copy_(TEXT<span style="color:#f92672">.</span>vocab<span style="color:#f92672">.</span>vectors)
        self<span style="color:#f92672">.</span>word_embeddings<span style="color:#f92672">.</span>requires_grad_ <span style="color:#f92672">=</span> False
        self<span style="color:#f92672">.</span>bilstm <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>LSTM(embedding_dim, lstm_dim, batch_first<span style="color:#f92672">=</span>True, bidirectional<span style="color:#f92672">=</span>True)
        self<span style="color:#f92672">.</span>hidden2tag <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(lstm_dim <span style="color:#f92672">*</span> <span style="color:#ae81ff">2</span>, tagset_size)
        self<span style="color:#f92672">.</span>softmax <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>LogSoftmax()
  
    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, text):
        embeds <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>word_embeddings(text)
        _, bilstm_hc <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>bilstm(embeds)
        bilstm_out <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>cat([bilstm_hc[<span style="color:#ae81ff">0</span>][<span style="color:#ae81ff">0</span>], bilstm_hc[<span style="color:#ae81ff">0</span>][<span style="color:#ae81ff">1</span>]], dim<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
        tag_space <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>hidden2tag(bilstm_out)
        tag_scores <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>softmax(tag_space<span style="color:#f92672">.</span>squeeze())
        <span style="color:#66d9ef">return</span> tag_scores
</code></pre></div><h1 id="in-conclusion">in conclusion</h1>
<ul>
<li>What I am a little worried about is the pattern that calculates Attention lexicographically as implemented in Transformer (the one that divides the embedding of a word into query, key, and value) and Attention is predicted by Neural Network in this paper. I don&rsquo;t really understand the difference in the patterns. Before I knew this paper, I thought that if I went to Attention, it would be annoying to get the inner product, so is there any way to calculate Attention?</li>
<li>Next, I would like to write something about Transformer!</li>
</ul>
<p>end</p>

    </article>
  </div>

  
  
  
  <script>
  window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
  ga('create', 'UA-169005401-1', 'auto');
  ga('send', 'pageview');
  </script>
  <script async src='https://www.google-analytics.com/analytics.js'></script>
  

  

  
<link rel="stylesheet" type="text/css" href="//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.0/cookieconsent.min.css" />
<script src="//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.0/cookieconsent.min.js"></script>
<script>
window.addEventListener("load", function(){
window.cookieconsent.initialise({
  "palette": {
    "popup": {
      "background": "#216942",
      "text": "#b2d192"
    },
    "button": {
      "background": "#afed71"
    }
  }
})});
</script>

</body>

</html>
