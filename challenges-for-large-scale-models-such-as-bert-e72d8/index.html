<!doctype html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  
  <meta name="generator" content="Hugo 0.72.0" />

  
  <meta name="description" content="Some description">
  

  
  <link rel="apple-touch-icon" sizes="180x180" href="https://memotut.com/apple-touch-icon.png">

  
  <link rel="icon" type="image/png" sizes="32x32" href="https://memotut.com/favicon-32x32.png">

  
  <link rel="icon" type="image/png" sizes="16x16" href="https://memotut.com/favicon-16x16.png">

  
  <link rel="manifest" href="https://memotut.com/site.webmanifest">

  
  <link rel="mask-icon" href="https://memotut.com/safari-pinned-tab.svg" color="#5bbad5">

  <meta name="msapplication-TileColor" content="#da532c">

  <meta name="theme-color" content="#ffffff">

  
  <link rel="stylesheet" href="https://memotut.com/css/bootstrap.min.css" />

  
  <title>Challenges for large-scale models such as BERT | Memo Tut</title>
  

  <style>
body {
  min-width: 300px;
}

.custom-navbar {
  margin-bottom: 1em;
  height: 60px;
}

.custom-navbar a {
  display: inline-block; 
  padding: 18px 0;
  margin-right: 1em; 
  font-weight: bold; 
}

.custom-navbar a:hover,
.custom-navbar a:focus {
  text-decoration: none; 
}

@media print {
  .custom-navbar {
    display: none;
  }
}

article {
  padding-bottom: 1em;
}

img {
  max-width: 100%;
}


body {
  background-color: #fff;
}



body {
  color: #212529;
}



a {
  color: #007bff;
}



a:hover,
a:focus {
  color: #0056b3;
}



.custom-navbar {
  background-color: #212529;
}



.custom-navbar a {
  color: rgba(255, 255, 255, 0.75);
}



.custom-navbar a:hover,
.custom-navbar a:focus {
  color: rgba(255, 255, 255, 1);
}



.container {
  max-width: 800px;
}



pre {
  display: block;
  padding: 9.5px;
  word-break: break-all;
  word-wrap: break-word;
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 4px;
}

pre code {
  padding: 0;
  font-size: inherit;
  color: inherit; 
  white-space: pre-wrap;
  background-color: transparent;
  border: none;
  border-radius: 0;
}

code {
  padding: 2px 4px;
  color: inherit; 
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 4px;
  font-size: .9em;
}



blockquote,
.blockquote {
  padding: 10px 20px;
  margin: 0 0 20px;
  font-size: 1em;
  border-left: 5px solid #6c757d;
}

</style>
</head>

<body>
  <nav class="custom-navbar">
  <div class="container">
    
    <a href="/">Posts</a>
    
    <a href="/tags/">Tags</a>
    
    <a href="/about/">About</a>
    
    <a href="/index.xml">RSS</a>
    
  </div>
</nav>
  
  <div class="container">
    <article>
      <h1>Challenges for large-scale models such as BERT</h1>
<p>
  <small class="text-secondary">
  
  
  Feb 25, 2020
  </small>
  

<small><code><a href="https://memotut.com/tags/python">Python</a></code></small>


<small><code><a href="https://memotut.com/tags/natural-language-processing"> natural language processing</a></code></small>


<small><code><a href="https://memotut.com/tags/machine-learning"> machine learning</a></code></small>


<small><code><a href="https://memotut.com/tags/nlp"> NLP</a></code></small>


<small><code><a href="https://memotut.com/tags/artificial-intelligence"> artificial intelligence</a></code></small>

</p>
<pre><code># Breakthrough model of natural language processing-BERT
</code></pre>
<p><a href="https://arxiv.org/pdf/1810.04805.pdf">BERT</a>[BidirectionalEncoderRepresentationsfromTransformers]wasannouncedtoGoogle&rsquo;steaminthefallof2018.Thisistheresultoflearningamodelinarevolutionarywayforaverylargenetworkwithalargeamountofdatausingatransformerarchitecture.Pleasereferto<a href="https://qiita.com/kayamin/items/9e73c93b5e791ff57925">Thisarticle</a> for learning method and accuracy.</p>
<p>In the field of machine learning where open source and free exchange of information are important, new ideas are published as papers at <a href="https://arxiv.org/">arxiv.org</a>, and models are shared on github. Doing is the basic way. As soon as new ideas are published, they can be referenced and reused by machine learning researchers and development teams around the world.</p>
<p>As a result of open source culture, months after BERT was released,
All big IT companies such as Open AI, Facebook, NVidia, and Baidu have created models based on this new architecture.</p>
<p>For the evolution from 2017, especially the breakthrough impact from BERT, see <a href="https://www.analyticsvidhya.com/blog/2019/08/complete-list-important-frameworks-nlp/See?utm_source=blog&amp;utm_medium=demystifying-bert-groundbreaking-nlp-framework">this nice diagram</a>.</p>
<p>A big model is more popular than #BERT</p>
<p>The cause of BERT accuracy improvement is not only the architecture and the amount of learning data, but also the number of parameters.
BERT is big. huge. Rather, it&rsquo;s big. The &ldquo;small&rdquo; version of BERT has the simplest version with 100 million parameters. Other models based on BERT are also getting bigger.</p>
<ul>
<li><a href="https://medium.com/syncedreview/openai-releases-1-5-billion-parameter-gpt-2-model-c34e97da56c0">Open AI GPT-2</a>: 1.5 billion</li>
<li><a href="https://nv-adlr.github.io/MegatronLM">Nvidia&rsquo;s Megatron-ML</a>: 8.3 billion
<a href="https://ai.googleblog.com/2020/01/towards-conversational-agent-that-can.html">Google&rsquo;s chatbot Meena announced in 2020</a>: 2.6 billion</li>
<li><a href="https://towardsdatascience.com/microsoft-open-sources-zero-and-deepspeed-the-technologies-behind-the-biggest-language-model-in-5fc70e331b2d">Microsoft T-NLG</a>: 17 billion</li>
</ul>
<p><img src="https://miro.medium.com/max/1400/1*eR1J9JiXRY0-VEGo5y1Xbw.png" alt="Model size drastic increase">
Source: <a href="https://towardsdatascience.com/microsoft-open-sources-zero-and-deepspeed-the-technologies-behind-the-biggest-language-model-in-5fc70e331b2d">Microsoft Open Sources ZeRO and DeepSpeed: The Technologies Behind the Biggest Language Model in History</a></p>
<p>Deep learning increases accuracy by increasing the amount of data and network size.
However, some problems occur in such a large network.</p>
<h1 id="to-prevent-global-warming-keep-the-weight-fixed-for-transfer-learning">To prevent global warming, keep the weight fixed for transfer learning</h1>
<p>It has not been disclosed how long it will take to learn hundreds of millions of parameters with a large amount of data, but it is thought that it would have cost tens of millions of yen for the electricity bill. In order to customize such a model to your own task, it is not realistic to train it with zero. Unless you use Google or Microsoft, you can only do transfer learning. If you train from scratch using your home computer, GPUs will replace heating. It may melt.</p>
<p>Learning is costly, but Inference [Predicting output for a particular input in a model] is also costly. AI Dungeon, which became a hot topic recently, felt this issue directly and <a href="https://medium.com/@aidungeon/how-we-scaled-ai-dungeon-2-to-support-over-1-000-000-users-d207d5623de9">Article</a> was also written. The memory requirements for models of this size are several gigabytes. Also, it takes time to process, so even if the accuracy is high, it is not an appropriate system when speed is important.</p>
<p>In summary, the natural language processing model recently developed by a large IT company has high accuracy, but learning and processing is difficult. It&rsquo;s hard in the cloud. It&rsquo;s even harder on the edge.
<a href="https://towardsdatascience.com/too-big-to-deploy-how-gpt-2-is-breaking-production-63ab29f0897c">Read more</a></p>
<p>#Improvement/Solution</p>
<p>Infinite budget for hardware purchase and cloud type-If you can get an infinite budget without approval, you can feel free to use tens of thousands of TPU. Without Google&rsquo;s budget, how can we use a model that delivers this kind of accuracy?</p>
<p>One solution is a reduced model like DistilBERT. Train a small model that is &ldquo;educated&rdquo; by the original model. The result of this learning method is that the accuracy is not as good as the original model, but the size can be reduced by tens of times. For &ldquo;Knowledge Distillation,&rdquo; see this article. ](<a href="https://medium.com/huggingface/distilbert-8cf3380435b5)">https://medium.com/huggingface/distilbert-8cf3380435b5)</a>.
A paper published in February 2020 <a href="https://arxiv.org/pdf/2002.06170.pdf">Paper published by Amazon</a> also introduced a method to reduce the transformer architecture.</p>
<p>With the ever-increasing need for accuracy, these challenges will not disappear. In order for general companies to use such a model, they need a transformer architecture with good hardware evolution, cloud cost and cost performance.</p>

    </article>
  </div>

  
  
  
  <script>
  window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
  ga('create', 'UA-169005401-1', 'auto');
  ga('send', 'pageview');
  </script>
  <script async src='https://www.google-analytics.com/analytics.js'></script>
  

  

  
<link rel="stylesheet" type="text/css" href="//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.0/cookieconsent.min.css" />
<script src="//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.0/cookieconsent.min.js"></script>
<script>
window.addEventListener("load", function(){
window.cookieconsent.initialise({
  "palette": {
    "popup": {
      "background": "#216942",
      "text": "#b2d192"
    },
    "button": {
      "background": "#afed71"
    }
  }
})});
</script>

</body>

</html>
