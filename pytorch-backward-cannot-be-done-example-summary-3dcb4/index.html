<!doctype html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  
  <meta name="generator" content="Hugo 0.72.0" />

  
  <meta name="description" content="Some description">
  

  
  <link rel="apple-touch-icon" sizes="180x180" href="https://memotut.com/apple-touch-icon.png">

  
  <link rel="icon" type="image/png" sizes="32x32" href="https://memotut.com/favicon-32x32.png">

  
  <link rel="icon" type="image/png" sizes="16x16" href="https://memotut.com/favicon-16x16.png">

  
  <link rel="manifest" href="https://memotut.com/site.webmanifest">

  
  <link rel="mask-icon" href="https://memotut.com/safari-pinned-tab.svg" color="#5bbad5">

  <meta name="msapplication-TileColor" content="#da532c">

  <meta name="theme-color" content="#ffffff">

  
  <link rel="stylesheet" href="https://memotut.com/css/bootstrap.min.css" />

  
  <title>pyTorch backward cannot be done example summary | Memo Tut</title>
  

  <style>
body {
  min-width: 300px;
}

.custom-navbar {
  margin-bottom: 1em;
  height: 60px;
}

.custom-navbar a {
  display: inline-block; 
  padding: 18px 0;
  margin-right: 1em; 
  font-weight: bold; 
}

.custom-navbar a:hover,
.custom-navbar a:focus {
  text-decoration: none; 
}

@media print {
  .custom-navbar {
    display: none;
  }
}

article {
  padding-bottom: 1em;
}

img {
  max-width: 100%;
}


body {
  background-color: #fff;
}



body {
  color: #212529;
}



a {
  color: #007bff;
}



a:hover,
a:focus {
  color: #0056b3;
}



.custom-navbar {
  background-color: #212529;
}



.custom-navbar a {
  color: rgba(255, 255, 255, 0.75);
}



.custom-navbar a:hover,
.custom-navbar a:focus {
  color: rgba(255, 255, 255, 1);
}



.container {
  max-width: 800px;
}



pre {
  display: block;
  padding: 9.5px;
  word-break: break-all;
  word-wrap: break-word;
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 4px;
}

pre code {
  padding: 0;
  font-size: inherit;
  color: inherit; 
  white-space: pre-wrap;
  background-color: transparent;
  border: none;
  border-radius: 0;
}

code {
  padding: 2px 4px;
  color: inherit; 
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 4px;
  font-size: .9em;
}



blockquote,
.blockquote {
  padding: 10px 20px;
  margin: 0 0 20px;
  font-size: 1em;
  border-left: 5px solid #6c757d;
}

</style>
</head>

<body>
  <nav class="custom-navbar">
  <div class="container">
    
    <a href="/">Posts</a>
    
    <a href="/tags/">Tags</a>
    
    <a href="/about/">About</a>
    
    <a href="/index.xml">RSS</a>
    
  </div>
</nav>
  
  <div class="container">
    <article>
      <h1>pyTorch backward cannot be done example summary</h1>
<p>
  <small class="text-secondary">
  
  
  Jan 27, 2020
  </small>
  

<small><code><a href="https://memotut.com/tags/python">Python</a></code></small>


<small><code><a href="https://memotut.com/tags/deep-learning"> deep learning</a></code></small>


<small><code><a href="https://memotut.com/tags/automatic-differentiation"> automatic differentiation</a></code></small>


<small><code><a href="https://memotut.com/tags/pytorch"> PyTorch</a></code></small>


<small><code><a href="https://memotut.com/tags/backward"> backward</a></code></small>

</p>
<pre><code>2020/1/27 Post
</code></pre>
<h1 id="0-target-audience-of-this-article">0. Target audience of this article</h1>
<ul>
<li>People who have touched python and have a good execution environment</li>
<li>People who have touched pyTorch to some extent</li>
<li>People who want to know about automatic differentiation by backward with machine learning with pyTorch</li>
<li>People who want to know that pyTorch can not be backward</li>
</ul>
<h1 id="1first-of-all">1.First of all</h1>
<p>Recently, the main focus of research on machine learning is the python language, because python has many libraries (called modules) for high-speed data analysis and calculation.
Among them, this time we will use a module called <strong>pyTorch</strong> and explain how automatic differentiation is performed and what can and cannot be done.</p>
<p>However, this article is like a memo to you, and it may be that you want it to be for reference only, and there may be incorrect expressions or phrases for the sake of brevity, but please understand that. I want you to.</p>
<p>Also, in this article, we will not actually learn using Network.
If you are interested in it, please refer to the link below.</p>
<p><a href="https://qiita.com/mslive/private/8e1f9a8467fff8dfd03c">Thorough explanation of CNNs with pyTorch</a></p>
<h1 id="2-install-pytorch">2. Install pyTorch</h1>
<p>If you are using pyTorch for the first time, you have to install it with cmd, because pyTorch is not yet installed in python.
Jump to the link below, select your own environment with ``QUICK START LOCALLY&rsquo;&rsquo; at the bottom of the page, and enter the command that appears with cmd etc. (you should copy and execute the command)</p>
<p><a href="https://pytorch.org/">Pytorch official site</a></p>
<h1 id="3-special-types-provided-by-pytorch">3. Special types provided by pyTorch</h1>
<p>Just like numpy has a type called ndarray, pyTorch has a type called ``<strong>Tensor type</strong>'&rsquo;.
Like the ndarray type, it can perform matrix calculations and is very similar to each other, but the Tensor type is excellent for machine learning in that it can use a GPU.
Because machine learning uses a GPU with a high calculation speed because it requires a considerable amount of calculation.
In addition, the Tensor type makes differentiation for updating the parameters of machine learning very easy.
The key to this article is how easy this is to do.</p>
<p>Please refer to the following link for the operation and explanation of Tensor type.</p>
<p><a href="https://qiita.com/mslive/private/241bfb42d852bb801b96">What is the Tensor type of pyTorch</a></p>
<h1 id="4-automatic-differential-backward">4. Automatic differential backward</h1>
<h3 id="4-1importing-pytorch">4-1.Importing pyTorch</h3>
<p>First, import so that you can use pyTorch.
From here, write to python file instead of cmd etc.
Use module by writing the following code.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ruby:filename.rb" data-lang="ruby:filename.rb">import torch
</code></pre></div><h3 id="4-2-example-of-automatic-differentiation">4-2. Example of automatic differentiation</h3>
<p>The following is a simple calculation program.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ruby:filename.rb" data-lang="ruby:filename.rb">x <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>tensor(<span style="color:#ae81ff">4</span><span style="color:#f92672">.</span><span style="color:#ae81ff">0</span>, requires_grad <span style="color:#f92672">=</span> <span style="color:#66d9ef">True</span>)
c <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>tensor(<span style="color:#ae81ff">8</span><span style="color:#f92672">.</span><span style="color:#ae81ff">0</span>)
b <span style="color:#f92672">=</span> <span style="color:#ae81ff">5</span><span style="color:#f92672">.</span><span style="color:#ae81ff">0</span>
y <span style="color:#f92672">=</span> c<span style="color:#f92672">*</span>x <span style="color:#f92672">+</span> b

print(y)

<span style="color:#f92672">-</span> <span style="color:#f92672">-----------</span> <span style="color:#66d9ef">Output</span> below <span style="color:#f92672">---------------</span>
tensor(<span style="color:#ae81ff">37</span><span style="color:#f92672">.</span>, grad_fn<span style="color:#f92672">=&lt;</span><span style="color:#66d9ef">AddBackward0</span><span style="color:#f92672">&gt;</span>)
</code></pre></div><p>This is a formula</p>
<pre><code class="language-math" data-lang="math">y = 8x+5
</code></pre><p>Is calculated when $x=4$ and $y$ is output as 37. In this output, &ldquo;<strong>grad_fn=&lt;AddBackward0&gt;</strong>&rdquo; is calculated by adding $y$. It is shown that it was calculated, and it is possible to differentiate by holding this in each variable.</p>
<p>This differentiation is as follows.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ruby:filename.rb" data-lang="ruby:filename.rb">y<span style="color:#f92672">.</span>backward()
</code></pre></div><p>This differentiates the values of all variables in $y$.</p>
<p>Nothing is output, so check it</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ruby:filename.rb" data-lang="ruby:filename.rb">print(x)
print(x<span style="color:#f92672">.</span>grad)

<span style="color:#f92672">-</span> <span style="color:#f92672">-----------</span> <span style="color:#66d9ef">Output</span> below <span style="color:#f92672">---------------</span>
tensor(<span style="color:#ae81ff">4</span><span style="color:#f92672">.</span>, requires_grad<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
tensor(<span style="color:#ae81ff">8</span><span style="color:#f92672">.</span>)
</code></pre></div><p>In this way, the differential information does not appear in the output of $x$, but you can see the differential value 8.0 of the variable name by setting ``<strong>x.grad</strong>'&rsquo;.</p>
<p>Here, it was said that the values of all variables were differentiated earlier, but when actually looking at the differential information of other variables</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ruby:filename.rb" data-lang="ruby:filename.rb">print(c<span style="color:#f92672">.</span>grad)
print(b<span style="color:#f92672">.</span>grad)

<span style="color:#f92672">-</span> <span style="color:#f92672">-----------</span> <span style="color:#66d9ef">Output</span> below <span style="color:#f92672">---------------</span>
<span style="color:#66d9ef">None</span>

<span style="color:#f92672">-</span> <span style="color:#f92672">-------------------------------------------------</span> <span style="color:#f92672">-------------------------</span>
<span style="color:#66d9ef">AttributeError</span> <span style="color:#66d9ef">Traceback</span> (most recent call last)
<span style="color:#f92672">&lt;</span>ipython<span style="color:#f92672">-</span>input<span style="color:#f92672">-</span><span style="color:#ae81ff">5</span><span style="color:#f92672">-</span><span style="color:#ae81ff">881</span>d89d572bd<span style="color:#f92672">&gt;</span> <span style="color:#66d9ef">in</span> <span style="color:#f92672">&lt;</span>module<span style="color:#f92672">&gt;</span>
      <span style="color:#ae81ff">1</span> print(c<span style="color:#f92672">.</span>grad)
<span style="color:#f92672">-</span> <span style="color:#f92672">---&gt;</span> <span style="color:#ae81ff">2</span> print(b<span style="color:#f92672">.</span>grad)

<span style="color:#e6db74">AttributeError</span>:<span style="color:#e6db74">&#39;float&#39;</span> object has no attribute<span style="color:#e6db74">&#39;grad&#39;</span>
</code></pre></div><p>The first output is <code>**None**''. Actually, when preparing the first variable, the variable **c** does not have </code><strong>requires_grad = True</strong>'&rsquo;.
This causes the variable <strong>c</strong> to be differentiated but interpreted as just a constant.</p>
<p>In addition, the second output has an error statement.
This is an error that I got when I tried to perform a differential calculation that can only be done by Tensor type which is a special type of pyTorch to other than Tensor type (this variable <strong>b</strong> is just a float type).</p>
<p>If you look at this, you can see that the Tensor type of pyTorch is very good, and if you set ``requires_grad = True&rsquo;&rsquo;, all the differential information will be calculated in one line.</p>
<h3 id="4-3-some-more-examples-of-automatic-differentiation">4-3. Some more examples of automatic differentiation</h3>
<p>Here is an example of a more complicated calculation.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ruby:filename.rb" data-lang="ruby:filename.rb">x <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>ones(<span style="color:#ae81ff">2</span>,<span style="color:#ae81ff">3</span>, requires_grad <span style="color:#f92672">=</span> <span style="color:#66d9ef">True</span>)
c <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>ones(<span style="color:#ae81ff">2</span>,<span style="color:#ae81ff">3</span>, requires_grad <span style="color:#f92672">=</span> <span style="color:#66d9ef">True</span>)
y <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>exp(x)<span style="color:#f92672">*</span>(c<span style="color:#f92672">*</span><span style="color:#ae81ff">3</span>) <span style="color:#f92672">+</span> torch<span style="color:#f92672">.</span>exp(x)

print(torch<span style="color:#f92672">.</span>exp(x))
print(c<span style="color:#f92672">*</span><span style="color:#ae81ff">3</span>)
print(y)

<span style="color:#f92672">-</span> <span style="color:#f92672">-----------</span> <span style="color:#66d9ef">Output</span> below <span style="color:#f92672">---------------</span>
tensor(<span style="color:#f92672">[[</span><span style="color:#ae81ff">2</span><span style="color:#f92672">.</span><span style="color:#ae81ff">7183</span>, <span style="color:#ae81ff">2</span><span style="color:#f92672">.</span><span style="color:#ae81ff">7183</span>, <span style="color:#ae81ff">2</span><span style="color:#f92672">.</span><span style="color:#ae81ff">7183</span><span style="color:#f92672">]</span>,
        <span style="color:#f92672">[</span><span style="color:#ae81ff">2</span><span style="color:#f92672">.</span><span style="color:#ae81ff">7183</span>, <span style="color:#ae81ff">2</span><span style="color:#f92672">.</span><span style="color:#ae81ff">7183</span>, <span style="color:#ae81ff">2</span><span style="color:#f92672">.</span><span style="color:#ae81ff">7183</span><span style="color:#f92672">]]</span>, grad_fn<span style="color:#f92672">=&lt;</span><span style="color:#66d9ef">ExpBackward</span><span style="color:#f92672">&gt;</span>)
tensor(<span style="color:#f92672">[[</span><span style="color:#ae81ff">3</span><span style="color:#f92672">.</span>, <span style="color:#ae81ff">3</span><span style="color:#f92672">.</span>, <span style="color:#ae81ff">3</span><span style="color:#f92672">.]</span>,
        <span style="color:#f92672">[</span><span style="color:#ae81ff">3</span><span style="color:#f92672">.</span>, <span style="color:#ae81ff">3</span><span style="color:#f92672">.</span>, <span style="color:#ae81ff">3</span><span style="color:#f92672">.]]</span>, grad_fn<span style="color:#f92672">=&lt;</span><span style="color:#66d9ef">MulBackward0</span><span style="color:#f92672">&gt;</span>)
tensor(<span style="color:#f92672">[[</span><span style="color:#ae81ff">10</span><span style="color:#f92672">.</span><span style="color:#ae81ff">8731</span>, <span style="color:#ae81ff">10</span><span style="color:#f92672">.</span><span style="color:#ae81ff">8731</span>, <span style="color:#ae81ff">10</span><span style="color:#f92672">.</span><span style="color:#ae81ff">8731</span><span style="color:#f92672">]</span>,
        <span style="color:#f92672">[</span><span style="color:#ae81ff">10</span><span style="color:#f92672">.</span><span style="color:#ae81ff">8731</span>, <span style="color:#ae81ff">10</span><span style="color:#f92672">.</span><span style="color:#ae81ff">8731</span>, <span style="color:#ae81ff">10</span><span style="color:#f92672">.</span><span style="color:#ae81ff">8731</span><span style="color:#f92672">]]</span>, grad_fn<span style="color:#f92672">=&lt;</span><span style="color:#66d9ef">AddBackward0</span><span style="color:#f92672">&gt;</span>)
</code></pre></div><p>First, <code>**torch.exp()**'' calculates $e^{element}$ for each element of the argument data. Each output is as you can see, this time </code>requires_grad = True&rsquo;&rsquo; was applied to both variables <strong>x</strong>, <strong>c</strong>.</p>
<p>Now, when backward is actually done, it becomes as follows.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ruby:filename.rb" data-lang="ruby:filename.rb">y<span style="color:#f92672">.</span>backward()

<span style="color:#f92672">-</span> <span style="color:#f92672">-----------</span> <span style="color:#66d9ef">Output</span> below <span style="color:#f92672">---------------</span>
<span style="color:#f92672">-</span> <span style="color:#f92672">-------------------------------------------------</span> <span style="color:#f92672">-------------------------</span>
<span style="color:#66d9ef">RuntimeError</span> <span style="color:#66d9ef">Traceback</span> (most recent call last)
<span style="color:#f92672">&lt;</span>ipython<span style="color:#f92672">-</span>input<span style="color:#f92672">-</span><span style="color:#ae81ff">11</span><span style="color:#f92672">-</span>ab75bb780f4c<span style="color:#f92672">&gt;</span> <span style="color:#66d9ef">in</span> <span style="color:#f92672">&lt;</span>module<span style="color:#f92672">&gt;</span>
<span style="color:#f92672">-</span> <span style="color:#f92672">---&gt;</span> <span style="color:#ae81ff">1</span> y<span style="color:#f92672">.</span>backward()

              <span style="color:#f92672">......</span>(<span style="color:#66d9ef">Omitted</span>) <span style="color:#f92672">......</span>

<span style="color:#e6db74">RuntimeError</span>: grad can be implicitly created only <span style="color:#66d9ef">for</span> scalar outputs
</code></pre></div><p>The error is output.
As written in this error, backwards can really only be done for scalar values (in short, data with only one value that is not a matrix or vector).</p>
<p>The actual solution is as follows.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ruby:filename.rb" data-lang="ruby:filename.rb">s <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>sum(y)

print(s)
<span style="color:#f92672">-</span> <span style="color:#f92672">-----------</span> <span style="color:#66d9ef">Output</span> below <span style="color:#f92672">---------------</span>
tensor(<span style="color:#ae81ff">65</span><span style="color:#f92672">.</span><span style="color:#ae81ff">2388</span>, grad_fn<span style="color:#f92672">=&lt;</span><span style="color:#66d9ef">SumBackward0</span><span style="color:#f92672">&gt;</span>)
</code></pre></div><p>This ``<strong>torch.sum()</strong>&rsquo;&rsquo; returns the result of adding all the elements of the argument.
Now we have a scalar value.
When actually doing backward</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ruby:filename.rb" data-lang="ruby:filename.rb">s<span style="color:#f92672">.</span>backward()
print(x<span style="color:#f92672">.</span>grad)
print(c<span style="color:#f92672">.</span>grad)

<span style="color:#f92672">-</span> <span style="color:#f92672">-----------</span> <span style="color:#66d9ef">Output</span> below <span style="color:#f92672">---------------</span>
tensor(<span style="color:#f92672">[[</span><span style="color:#ae81ff">10</span><span style="color:#f92672">.</span><span style="color:#ae81ff">8731</span>, <span style="color:#ae81ff">10</span><span style="color:#f92672">.</span><span style="color:#ae81ff">8731</span>, <span style="color:#ae81ff">10</span><span style="color:#f92672">.</span><span style="color:#ae81ff">8731</span><span style="color:#f92672">]</span>,
        <span style="color:#f92672">[</span><span style="color:#ae81ff">10</span><span style="color:#f92672">.</span><span style="color:#ae81ff">8731</span>, <span style="color:#ae81ff">10</span><span style="color:#f92672">.</span><span style="color:#ae81ff">8731</span>, <span style="color:#ae81ff">10</span><span style="color:#f92672">.</span><span style="color:#ae81ff">8731</span><span style="color:#f92672">]]</span>)
tensor(<span style="color:#f92672">[[</span><span style="color:#ae81ff">8</span><span style="color:#f92672">.</span><span style="color:#ae81ff">1548</span>, <span style="color:#ae81ff">8</span><span style="color:#f92672">.</span><span style="color:#ae81ff">1548</span>, <span style="color:#ae81ff">8</span><span style="color:#f92672">.</span><span style="color:#ae81ff">1548</span><span style="color:#f92672">]</span>,
        <span style="color:#f92672">[</span><span style="color:#ae81ff">8</span><span style="color:#f92672">.</span><span style="color:#ae81ff">1548</span>, <span style="color:#ae81ff">8</span><span style="color:#f92672">.</span><span style="color:#ae81ff">1548</span>, <span style="color:#ae81ff">8</span><span style="color:#f92672">.</span><span style="color:#ae81ff">1548</span><span style="color:#f92672">]]</span>)
</code></pre></div><p>In this way, it is multivariable, and the differentiation is performed firmly on the matrix.</p>
<h1 id="5-example-where-automatic-differentiation-backward-cannot-be-performed">5. Example where automatic differentiation backward cannot be performed</h1>
<p>From here, I will write an example that is not actually backwards.
From here on, I will add it as soon as I find new such cases and receive reports.</p>
<h3 id="5-1-example-where-the-variable-is-not-of-tensor-type">5-1. Example where the variable is not of Tensor type</h3>
<p>As explained in Example 5-2 of automatic differentiation above, it occurs when the variable to be differentiated is not <strong>Tensor type</strong> and ``<strong>requires_grad = True</strong>'&rsquo;.
The solution is simple, and the type should meet the requirements.</p>
<h3 id="5-2-example-where-the-final-output-is-not-a-scalar-value">5-2. Example where the final output is not a scalar value</h3>
<p>As explained in Example 5-3 of automatic differentiation above, it occurs when the variable to be differentiated is not a scalar value.
The solution is to make it a scalar value somehow.
For example, the summation of elements done in the above example could be done without breaking the shape of the matrix.</p>
<h3 id="5-3-example-of-overwriting-the-variable-to-be-differentiated">5-3. Example of overwriting the variable to be differentiated</h3>
<p>An example is shown below.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ruby:filename.rb" data-lang="ruby:filename.rb">x <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>tensor(<span style="color:#ae81ff">1</span><span style="color:#f92672">.</span><span style="color:#ae81ff">0</span>, requires_grad <span style="color:#f92672">=</span> <span style="color:#66d9ef">True</span>)
x <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>exp(x)
c <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>tensor(<span style="color:#ae81ff">1</span><span style="color:#f92672">.</span><span style="color:#ae81ff">0</span>, requires_grad <span style="color:#f92672">=</span> <span style="color:#66d9ef">True</span>)
c <span style="color:#f92672">=</span> c<span style="color:#f92672">*</span><span style="color:#ae81ff">3</span>
b <span style="color:#f92672">=</span> <span style="color:#ae81ff">5</span><span style="color:#f92672">.</span><span style="color:#ae81ff">0</span>
y <span style="color:#f92672">=</span> c<span style="color:#f92672">*</span>x <span style="color:#f92672">+</span> bprint(y)

<span style="color:#f92672">-</span> <span style="color:#f92672">-----------</span> <span style="color:#66d9ef">Output</span> below <span style="color:#f92672">---------------</span>
tensor(<span style="color:#ae81ff">13</span><span style="color:#f92672">.</span><span style="color:#ae81ff">1548</span>, grad_fn<span style="color:#f92672">=&lt;</span><span style="color:#66d9ef">AddBackward0</span><span style="color:#f92672">&gt;</span>)
</code></pre></div><p>If you write in a formula with a very simple example</p>
<pre><code class="language-math" data-lang="math">y = (c*3)*e^{x}+5
</code></pre><p>Where $c=1$ and $x=1$, and c and x can be differentiated from each other by ``requires_grad = True&rsquo;&rsquo;.
Actually the differential value is as follows.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ruby:filename.rb" data-lang="ruby:filename.rb">y<span style="color:#f92672">.</span>backward()
print(x<span style="color:#f92672">.</span>grad)
print(c<span style="color:#f92672">.</span>grad)

<span style="color:#f92672">-</span> <span style="color:#f92672">-----------</span> <span style="color:#66d9ef">Output</span> below <span style="color:#f92672">---------------</span>
<span style="color:#66d9ef">None</span>
<span style="color:#66d9ef">None</span>
</code></pre></div><p>How, neither x nor c has derivative value.
This is because by overwriting the variables, the calculation process of x and c (called the calculation graph) disappears (the definitions made at the beginning with x and c, and all past calculations that are not done here are <code>(It will be overwritten by</code>torch.exp()&rsquo;&rsquo; and ``*3&rsquo;').
However, in such an example, if you try to put it in optimizer (SGD etc.) prepared by torch, it will give an error.
An actual example is shown below.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ruby:filename.rb" data-lang="ruby:filename.rb">import torch<span style="color:#f92672">.</span>optim as optim
op <span style="color:#f92672">=</span> optim<span style="color:#f92672">.</span>SGD(<span style="color:#f92672">[</span>x,c<span style="color:#f92672">]</span>, lr<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span><span style="color:#f92672">.</span><span style="color:#ae81ff">0</span>)

<span style="color:#f92672">-</span> <span style="color:#f92672">-----------</span> <span style="color:#66d9ef">Output</span> below <span style="color:#f92672">---------------</span>
<span style="color:#f92672">-</span> <span style="color:#f92672">-------------------------------------------------</span> <span style="color:#f92672">-------------------------</span>
<span style="color:#66d9ef">ValueError</span> <span style="color:#66d9ef">Traceback</span> (most recent call last)
<span style="color:#f92672">&lt;</span>ipython<span style="color:#f92672">-</span>input<span style="color:#f92672">-</span><span style="color:#ae81ff">18</span><span style="color:#f92672">-</span><span style="color:#ae81ff">775027</span>da6d38<span style="color:#f92672">&gt;</span> <span style="color:#66d9ef">in</span> <span style="color:#f92672">&lt;</span>module<span style="color:#f92672">&gt;</span>
<span style="color:#f92672">-</span> <span style="color:#f92672">---&gt;</span> <span style="color:#ae81ff">1</span> op <span style="color:#f92672">=</span> optim<span style="color:#f92672">.</span>SGD(<span style="color:#f92672">[</span>x,c<span style="color:#f92672">]</span>, lr<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span><span style="color:#f92672">.</span><span style="color:#ae81ff">0</span>)

         <span style="color:#f92672">............</span>(<span style="color:#66d9ef">Omitted</span>) <span style="color:#f92672">............</span>

<span style="color:#e6db74">ValueError</span>: can<span style="color:#960050;background-color:#1e0010">&#39;</span>t optimize a non<span style="color:#f92672">-</span>leaf <span style="color:#66d9ef">Tensor</span>
</code></pre></div><p>An error is output when using SGD which is an optimizer.
If you are interested in the detailed explanation of this optimizer, please refer to the link below.</p>
<p><a href="https://qiita.com/mathlive/items/2c67efa2d451ea1da1b1">Thorough explanation of pyTorch optimizer SGD</a></p>
<p>I will briefly explain here, but this SGD class is preparing to update each parameter using the gradient information regarding the parameter &ldquo;<strong>[x,c]</strong>&rdquo; of the argument.
However, at this point, it gives an error that the calculation graph of these variables is broken.</p>
<p>The solution is to assign it to another variable without overwriting it, or write the expression directly.
As you can see that you assign to another variable, here is an example of writing an expression directly.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ruby:filename.rb" data-lang="ruby:filename.rb">x <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>tensor(<span style="color:#ae81ff">1</span><span style="color:#f92672">.</span><span style="color:#ae81ff">0</span>, requires_grad <span style="color:#f92672">=</span> <span style="color:#66d9ef">True</span>)
c <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>tensor(<span style="color:#ae81ff">1</span><span style="color:#f92672">.</span><span style="color:#ae81ff">0</span>, requires_grad <span style="color:#f92672">=</span> <span style="color:#66d9ef">True</span>)
b <span style="color:#f92672">=</span> <span style="color:#ae81ff">5</span><span style="color:#f92672">.</span><span style="color:#ae81ff">0</span>
y <span style="color:#f92672">=</span> c<span style="color:#f92672">*</span><span style="color:#ae81ff">3</span><span style="color:#f92672">*</span>torch<span style="color:#f92672">.</span>exp(x)
y <span style="color:#f92672">=</span> y <span style="color:#f92672">+</span> b
y<span style="color:#f92672">.</span>backward()

print(x<span style="color:#f92672">.</span>grad)
print(c<span style="color:#f92672">.</span>grad)

<span style="color:#f92672">-</span> <span style="color:#f92672">-----------</span> <span style="color:#66d9ef">Output</span> below <span style="color:#f92672">---------------</span>
tensor(<span style="color:#ae81ff">8</span><span style="color:#f92672">.</span><span style="color:#ae81ff">1548</span>)
tensor(<span style="color:#ae81ff">8</span><span style="color:#f92672">.</span><span style="color:#ae81ff">1548</span>)
</code></pre></div><p>Here, I intentionally divided the operation of <strong>y</strong> into the 4th and 5th lines.
Actually, there is no penalty for this <strong>y</strong> overwrite.
Because <strong>y</strong> is not a variable that we want to differentiate, it doesn&rsquo;t matter if the calculation is done properly.</p>
<h3 id="5-4-example-of-using-the-root-square-root">5-4. Example of using the root (square root)</h3>
<p>First, let us consider the following calculation.</p>
<pre><code class="language-math" data-lang="math">y = c\sqrt{x_1^2+x_2^2+x_3^2}
</code></pre><p>As you can see, this is the value obtained by multiplying the <strong>L2 norm</strong> (or simply distance) of the vector $[x_1, x_2, x_3]$ by c times.</p>
<p>This is shown programmatically below.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ruby:filename.rb" data-lang="ruby:filename.rb">x <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>tensor(<span style="color:#f92672">[</span><span style="color:#ae81ff">2</span><span style="color:#f92672">.</span><span style="color:#ae81ff">0</span>,<span style="color:#ae81ff">5</span><span style="color:#f92672">.</span><span style="color:#ae81ff">0</span>,<span style="color:#ae81ff">3</span><span style="color:#f92672">.</span><span style="color:#ae81ff">0</span><span style="color:#f92672">]</span>, requires_grad <span style="color:#f92672">=</span> <span style="color:#66d9ef">True</span>)
c <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>tensor(<span style="color:#ae81ff">2</span><span style="color:#f92672">.</span><span style="color:#ae81ff">0</span>)
y <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>sqrt(torch<span style="color:#f92672">.</span>sum(x<span style="color:#f92672">**</span><span style="color:#ae81ff">2</span>))
y <span style="color:#f92672">=</span> y<span style="color:#f92672">*</span>c
y<span style="color:#f92672">.</span>backward()
print(x<span style="color:#f92672">.</span>grad)

<span style="color:#f92672">-</span> <span style="color:#f92672">-----------</span> <span style="color:#66d9ef">Output</span> below <span style="color:#f92672">---------------</span>
tensor(<span style="color:#f92672">[</span><span style="color:#ae81ff">0</span><span style="color:#f92672">.</span><span style="color:#ae81ff">6489</span>, <span style="color:#ae81ff">1</span><span style="color:#f92672">.</span><span style="color:#ae81ff">6222</span>, <span style="color:#ae81ff">0</span><span style="color:#f92672">.</span><span style="color:#ae81ff">9733</span><span style="color:#f92672">]</span>)
</code></pre></div><p>It can be seen that the differential value of each element with respect to the vector <strong>x</strong> can be calculated properly.
Explaining the program for a while, the third line ``<strong>torch.sqrt(torch.sum(x**2))</strong>&rsquo;&rsquo; first squares each element of x, and sums each element. And, I&rsquo;m putting it in the route.</p>
<p>Now consider the following example with this equation.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ruby:filename.rb" data-lang="ruby:filename.rb">x <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>tensor(<span style="color:#f92672">[</span><span style="color:#ae81ff">0</span><span style="color:#f92672">.</span><span style="color:#ae81ff">0</span>,<span style="color:#ae81ff">0</span><span style="color:#f92672">.</span><span style="color:#ae81ff">0</span>,<span style="color:#ae81ff">0</span><span style="color:#f92672">.</span><span style="color:#ae81ff">0</span><span style="color:#f92672">]</span>, requires_grad <span style="color:#f92672">=</span> <span style="color:#66d9ef">True</span>)
c <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>tensor(<span style="color:#ae81ff">2</span><span style="color:#f92672">.</span><span style="color:#ae81ff">0</span>)
y <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>sqrt(torch<span style="color:#f92672">.</span>sum(x<span style="color:#f92672">**</span><span style="color:#ae81ff">2</span>))
y <span style="color:#f92672">=</span> y<span style="color:#f92672">*</span>c
y<span style="color:#f92672">.</span>backward()
print(x<span style="color:#f92672">.</span>grad)

<span style="color:#f92672">-</span> <span style="color:#f92672">-----------</span> <span style="color:#66d9ef">Output</span> below <span style="color:#f92672">---------------</span>
tensor(<span style="color:#f92672">[</span>nan, nan, nan<span style="color:#f92672">]</span>)
</code></pre></div><p>Now, all values of each element of the variable <strong>x</strong> are rewritten to 0.0 (distance of vector x is 0).
As a result, all derivative values became <strong>nan</strong>.
By doing this, the differential value of each element will take ∞ as a matter of course.
Because the derivative of the above equation is</p>
<pre><code class="language-math" data-lang="math">\frac{\partial y}{\partial x_1} = c\frac{x_1}{\sqrt{x_1^2+x_2^2+x_3^2}}
</code></pre><p>And since the distance of $x = 0$, it divides by 0.
When actually performing machine learning, the parameters are automatically updated, but if the value of the parameter becomes 0 even once in the process, if $\sqrt{x}$ exists in the calculation process, The phenomenon happens.
This causes loss to diverge or become nan where you can&rsquo;t see it.</p>
<p>The solution is as follows.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ruby:filename.rb" data-lang="ruby:filename.rb">x <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>tensor(<span style="color:#f92672">[</span><span style="color:#ae81ff">0</span><span style="color:#f92672">.</span><span style="color:#ae81ff">0</span>,<span style="color:#ae81ff">0</span><span style="color:#f92672">.</span><span style="color:#ae81ff">0</span>,<span style="color:#ae81ff">0</span><span style="color:#f92672">.</span><span style="color:#ae81ff">0</span><span style="color:#f92672">]</span>, requires_grad <span style="color:#f92672">=</span> <span style="color:#66d9ef">True</span>)
c <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>tensor(<span style="color:#ae81ff">2</span><span style="color:#f92672">.</span><span style="color:#ae81ff">0</span>)
y <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>norm(x)
y <span style="color:#f92672">=</span> y<span style="color:#f92672">*</span>c
y<span style="color:#f92672">.</span>backward()
print(x<span style="color:#f92672">.</span>grad)

<span style="color:#f92672">-</span> <span style="color:#f92672">-----------</span> <span style="color:#66d9ef">Output</span> below <span style="color:#f92672">---------------</span>
tensor(<span style="color:#f92672">[</span><span style="color:#ae81ff">0</span><span style="color:#f92672">.</span>, <span style="color:#ae81ff">0</span><span style="color:#f92672">.</span>, <span style="color:#ae81ff">0</span><span style="color:#f92672">.]</span>)
</code></pre></div><p>In this way, if you use ``<strong>torch.norm()</strong>&rsquo;&rsquo; on the third line, the differential value will be 0x instead of <strong>nan</strong>.
This torch.norm() does exactly the same thing as the calculation itself, but it seems that it has a mechanism to prevent division by 0 internally.</p>
<h3 id="5-5-example-using-in-place">5-5. Example using in-place</h3>
<p>In-place operation is provided in python, and you can do the following.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ruby:filename.rb" data-lang="ruby:filename.rb">i <span style="color:#f92672">+=</span> <span style="color:#ae81ff">1</span>
x <span style="color:#f92672">*=</span><span style="color:#ae81ff">3</span>
</code></pre></div><p>These are described by omitting the places where <code>**i = i+1**'' and </code><strong>x = x*3</strong>&rsquo;&rsquo; are usually written.
This notation seems to be faster, but it is not suitable for automatic differentiation.
An example is shown below.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ruby:filename.rb" data-lang="ruby:filename.rb">x <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>tensor(<span style="color:#ae81ff">3</span><span style="color:#f92672">.</span><span style="color:#ae81ff">0</span>, requires_grad <span style="color:#f92672">=</span> <span style="color:#66d9ef">True</span>)
c <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>tensor(<span style="color:#ae81ff">2</span><span style="color:#f92672">.</span><span style="color:#ae81ff">0</span>)
c <span style="color:#f92672">+=</span> <span style="color:#ae81ff">2</span><span style="color:#f92672">.</span><span style="color:#ae81ff">0</span>
x <span style="color:#f92672">+=</span> <span style="color:#ae81ff">2</span><span style="color:#f92672">.</span><span style="color:#ae81ff">0</span>
y <span style="color:#f92672">=</span> x <span style="color:#f92672">+</span> c

<span style="color:#f92672">-</span> <span style="color:#f92672">-----------</span> <span style="color:#66d9ef">Output</span> below <span style="color:#f92672">---------------</span>
<span style="color:#f92672">-</span> <span style="color:#f92672">-------------------------------------------------</span> <span style="color:#f92672">-------------------------</span>
<span style="color:#66d9ef">RuntimeError</span> <span style="color:#66d9ef">Traceback</span> (most recent call last)
<span style="color:#f92672">&lt;</span>ipython<span style="color:#f92672">-</span>input<span style="color:#f92672">-</span><span style="color:#ae81ff">94</span><span style="color:#f92672">-</span>beb1a427373d<span style="color:#f92672">&gt;</span> <span style="color:#66d9ef">in</span> <span style="color:#f92672">&lt;</span>module<span style="color:#f92672">&gt;</span>
      <span style="color:#ae81ff">2</span> c <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>tensor(<span style="color:#ae81ff">2</span><span style="color:#f92672">.</span><span style="color:#ae81ff">0</span>)
      <span style="color:#ae81ff">3</span> c <span style="color:#f92672">+=</span> <span style="color:#ae81ff">2</span><span style="color:#f92672">.</span><span style="color:#ae81ff">0</span>
<span style="color:#f92672">-</span> <span style="color:#f92672">---&gt;</span> <span style="color:#ae81ff">4</span> x <span style="color:#f92672">+=</span> <span style="color:#ae81ff">2</span><span style="color:#f92672">.</span><span style="color:#ae81ff">0</span>
      <span style="color:#ae81ff">5</span> y <span style="color:#f92672">=</span> x <span style="color:#f92672">+</span> c

<span style="color:#e6db74">RuntimeError</span>: a leaf <span style="color:#66d9ef">Variable</span> that requires grad has been used <span style="color:#66d9ef">in</span> an <span style="color:#66d9ef">in</span><span style="color:#f92672">-</span>place operation<span style="color:#f92672">.</span>
</code></pre></div><p>In this way, in-place operation is not possible for variables with ``requires_grad = True&rsquo;&rsquo; (of course, variable c has nothing to do with differentiation).</p>
<p>The solution is simple and you don&rsquo;t have to use in-place operation.
In other words, all you have to do is write it normally.</p>
<h3 id="5-6-example-of-using-cpu-and-gpu-together">5-6. Example of using cpu and gpu together</h3>
<p>An example is shown below.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ruby:filename.rb" data-lang="ruby:filename.rb">x <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>tensor(<span style="color:#ae81ff">3</span><span style="color:#f92672">.</span><span style="color:#ae81ff">0</span>, requires_grad <span style="color:#f92672">=</span> <span style="color:#66d9ef">True</span>)<span style="color:#f92672">.</span>cuda()
c <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>tensor(<span style="color:#ae81ff">2</span><span style="color:#f92672">.</span><span style="color:#ae81ff">0</span>, requires_grad <span style="color:#f92672">=</span> <span style="color:#66d9ef">True</span>)<span style="color:#f92672">.</span>cpu()
y <span style="color:#f92672">=</span> x<span style="color:#f92672">*</span>c
print(y)

<span style="color:#f92672">-</span> <span style="color:#f92672">-----------</span> <span style="color:#66d9ef">Output</span> below <span style="color:#f92672">---------------</span>
tensor(<span style="color:#ae81ff">6</span><span style="color:#f92672">.</span>, device<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;cuda:0&#39;</span>, grad_fn<span style="color:#f92672">=&lt;</span><span style="color:#66d9ef">MulBackward0</span><span style="color:#f92672">&gt;</span>)
</code></pre></div><p>Here, it is specified that the variable <strong>x</strong> uses gpu by &ldquo;<strong>.cuda()</strong>&rdquo; and the variable <strong>c</strong> uses cpu by &ldquo;<strong>.cpu()</strong>&rdquo;. is doing.
Also, both variables are in a differentiable state.
The output of the answer uses a gpu as ``<strong>device='cuda:0&rsquo;</strong>'&rsquo;.</p>
<p>Then try backward</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ruby:filename.rb" data-lang="ruby:filename.rb">y<span style="color:#f92672">.</span>backward()

<span style="color:#f92672">-</span> <span style="color:#f92672">-----------</span> <span style="color:#66d9ef">Output</span> below <span style="color:#f92672">---------------</span>
<span style="color:#f92672">-</span> <span style="color:#f92672">-------------------------------------------------</span> <span style="color:#f92672">-------------------------</span>
<span style="color:#66d9ef">RuntimeError</span> <span style="color:#66d9ef">Traceback</span> (most recent call last)
<span style="color:#f92672">&lt;</span>ipython<span style="color:#f92672">-</span>input<span style="color:#f92672">-</span><span style="color:#ae81ff">118</span><span style="color:#f92672">-</span><span style="color:#ae81ff">8117</span>d53c0658<span style="color:#f92672">&gt;</span> <span style="color:#66d9ef">in</span> <span style="color:#f92672">&lt;</span>module<span style="color:#f92672">&gt;</span>
<span style="color:#f92672">-</span> <span style="color:#f92672">---&gt;</span> <span style="color:#ae81ff">1</span> y<span style="color:#f92672">.</span>backward()
        <span style="color:#f92672">...........</span> (<span style="color:#66d9ef">Omitted</span>) <span style="color:#f92672">...............</span>

<span style="color:#e6db74">RuntimeError</span>: <span style="color:#66d9ef">Function</span> <span style="color:#66d9ef">MulBackward0</span> returned an invalid gradient at index <span style="color:#ae81ff">1</span><span style="color:#f92672">-</span>expected type torch<span style="color:#f92672">.</span>FloatTensor but got torch<span style="color:#f92672">.</span>cuda<span style="color:#f92672">.</span>FloatTensor
<span style="color:#e6db74">```This error occurs because the variable to be differentiated in this way uses different resources.
</span><span style="color:#e6db74">
</span><span style="color:#e6db74">The solution is to combine the resources used by each other.
</span><span style="color:#e6db74">Of course, it is not necessary to have variables that are not related to differentiation.
</span><span style="color:#e6db74">
</span><span style="color:#e6db74">### 5-7. Example that is not torch.Floattensor
</span><span style="color:#e6db74">Among the Tensor types provided by pyTorch, there are also int type, float type, double type, etc.
</span><span style="color:#e6db74">You can use this type as follows.
</span><span style="color:#e6db74">
</span><span style="color:#e6db74">```</span><span style="color:#e6db74">ruby</span>:filename<span style="color:#f92672">.</span>rb
a <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>tensor(<span style="color:#ae81ff">2</span>)
b <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>tensor(<span style="color:#ae81ff">2</span><span style="color:#f92672">.</span><span style="color:#ae81ff">134</span>)
c <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>tensor(<span style="color:#ae81ff">3</span><span style="color:#f92672">.</span><span style="color:#ae81ff">5</span>)
c <span style="color:#f92672">=</span> c<span style="color:#f92672">.</span>type(torch<span style="color:#f92672">.</span>int32)
d <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>tensor(<span style="color:#ae81ff">3</span><span style="color:#f92672">.</span><span style="color:#ae81ff">1514</span>, dtype <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>float64)
print(a)
print(b)
print(c)
print(d)

<span style="color:#f92672">-</span> <span style="color:#f92672">-----------</span> <span style="color:#66d9ef">Output</span> below <span style="color:#f92672">---------------</span>
tensor(<span style="color:#ae81ff">2</span>)
tensor(<span style="color:#ae81ff">2</span><span style="color:#f92672">.</span><span style="color:#ae81ff">1340</span>)
tensor(<span style="color:#ae81ff">3</span>, dtype<span style="color:#f92672">=</span>torch<span style="color:#f92672">.</span>int32)
tensor(<span style="color:#ae81ff">3</span><span style="color:#f92672">.</span><span style="color:#ae81ff">1514</span>, dtype<span style="color:#f92672">=</span>torch<span style="color:#f92672">.</span>float64)
</code></pre></div><p>In this way, you can add <code>**dtype = **'' at the time of declaration or give it as </code>**xxxx.type (type type) **'&rsquo;.
Furthermore, the type of each variable is seen as follows.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ruby:filename.rb" data-lang="ruby:filename.rb">print(a<span style="color:#f92672">.</span>dtype)
print(b<span style="color:#f92672">.</span>dtype)
print(c<span style="color:#f92672">.</span>dtype)
print(d<span style="color:#f92672">.</span>dtype)
<span style="color:#f92672">-</span> <span style="color:#f92672">-----------</span> <span style="color:#66d9ef">Output</span> below <span style="color:#f92672">---------------</span>
torch<span style="color:#f92672">.</span>int64
torch<span style="color:#f92672">.</span>float32
torch<span style="color:#f92672">.</span>int32
torch<span style="color:#f92672">.</span>float64
</code></pre></div><p>As you can see, if you pass an integer without specifying it at the time of declaration like variable a, it will automatically become <strong>int64</strong>, and if you pass a real number like variable b, it will automatically become <strong>float32</strong>.
What&rsquo;s even more interesting is that for the variable c, the cast to int32 causes the fractional part to disappear.</p>
<p>Based on the above, the actual calculation process is shown below as an example.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ruby:filename.rb" data-lang="ruby:filename.rb">x <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>tensor(<span style="color:#ae81ff">3</span><span style="color:#f92672">.</span><span style="color:#ae81ff">0</span>, dtype <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>int64, requires_grad <span style="color:#f92672">=</span> <span style="color:#66d9ef">True</span>)
c <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>tensor(<span style="color:#ae81ff">2</span><span style="color:#f92672">.</span><span style="color:#ae81ff">0</span>, requires_grad <span style="color:#f92672">=</span> <span style="color:#66d9ef">True</span>)
y <span style="color:#f92672">=</span> x<span style="color:#f92672">*</span>c

<span style="color:#f92672">-</span> <span style="color:#f92672">-----------</span> <span style="color:#66d9ef">Output</span> below <span style="color:#f92672">---------------</span>
<span style="color:#f92672">-</span> <span style="color:#f92672">-------------------------------------------------</span> <span style="color:#f92672">-------------------------</span>
<span style="color:#66d9ef">RuntimeError</span> <span style="color:#66d9ef">Traceback</span> (most recent call last)
<span style="color:#f92672">&lt;</span>ipython<span style="color:#f92672">-</span>input<span style="color:#f92672">-</span><span style="color:#ae81ff">22</span><span style="color:#f92672">-</span><span style="color:#ae81ff">7183168</span>e453f<span style="color:#f92672">&gt;</span> <span style="color:#66d9ef">in</span> <span style="color:#f92672">&lt;</span>module<span style="color:#f92672">&gt;</span>
<span style="color:#f92672">-</span> <span style="color:#f92672">---&gt;</span> <span style="color:#ae81ff">1</span> x <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>tensor(<span style="color:#ae81ff">3</span><span style="color:#f92672">.</span><span style="color:#ae81ff">0</span>, dtype <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>int64, requires_grad <span style="color:#f92672">=</span> <span style="color:#66d9ef">True</span>)
      <span style="color:#ae81ff">2</span> c <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>tensor(<span style="color:#ae81ff">2</span><span style="color:#f92672">.</span><span style="color:#ae81ff">0</span>, requires_grad <span style="color:#f92672">=</span> <span style="color:#66d9ef">True</span>)
      <span style="color:#ae81ff">3</span> y <span style="color:#f92672">=</span> x<span style="color:#f92672">*</span>c

<span style="color:#e6db74">RuntimeError</span>: <span style="color:#66d9ef">Only</span> <span style="color:#66d9ef">Tensors</span> of floating point dtype can require gradients
</code></pre></div><p>Here, we tried to make the variable <strong>x</strong> be an integer type.
If it is an integer type, it seems that you can not say ``**requires_grad = True **&rsquo;&rsquo; in the first place, and this error appears.</p>
<p>Let&rsquo;s rewrite it as float.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ruby:filename.rb" data-lang="ruby:filename.rb">x <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>tensor(<span style="color:#ae81ff">3</span><span style="color:#f92672">.</span><span style="color:#ae81ff">0</span>, dtype <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>float64, requires_grad <span style="color:#f92672">=</span> <span style="color:#66d9ef">True</span>)
c <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>tensor(<span style="color:#ae81ff">2</span><span style="color:#f92672">.</span><span style="color:#ae81ff">0</span>, requires_grad <span style="color:#f92672">=</span> <span style="color:#66d9ef">True</span>)
y <span style="color:#f92672">=</span> x<span style="color:#f92672">*</span>c
print(y)

<span style="color:#f92672">-</span> <span style="color:#f92672">-----------</span> <span style="color:#66d9ef">Output</span> below <span style="color:#f92672">---------------</span>
tensor(<span style="color:#ae81ff">6</span><span style="color:#f92672">.</span>, dtype<span style="color:#f92672">=</span>torch<span style="color:#f92672">.</span>float64, grad_fn<span style="color:#f92672">=&lt;</span><span style="color:#66d9ef">MulBackward0</span><span style="color:#f92672">&gt;</span>)
</code></pre></div><p>It worked fine.
Then we perform automatic differentiation.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ruby:filename.rb" data-lang="ruby:filename.rb">y<span style="color:#f92672">.</span>backward()

<span style="color:#f92672">-</span> <span style="color:#f92672">-----------</span> <span style="color:#66d9ef">Output</span> below <span style="color:#f92672">---------------</span>
<span style="color:#f92672">-</span> <span style="color:#f92672">-------------------------------------------------</span> <span style="color:#f92672">-------------------------</span>
<span style="color:#66d9ef">RuntimeError</span> <span style="color:#66d9ef">Traceback</span> (most recent call last)
<span style="color:#f92672">&lt;</span>ipython<span style="color:#f92672">-</span>input<span style="color:#f92672">-</span><span style="color:#ae81ff">8</span><span style="color:#f92672">-</span>ab75bb780f4c<span style="color:#f92672">&gt;</span> <span style="color:#66d9ef">in</span> <span style="color:#f92672">&lt;</span>module<span style="color:#f92672">&gt;</span>
<span style="color:#f92672">-</span> <span style="color:#f92672">---&gt;</span> <span style="color:#ae81ff">1</span> y<span style="color:#f92672">.</span>backward()
          <span style="color:#f92672">............</span>(<span style="color:#66d9ef">Omitted</span>) <span style="color:#f92672">............</span>

<span style="color:#e6db74">RuntimeError</span>: <span style="color:#66d9ef">Function</span> <span style="color:#66d9ef">MulBackward0</span> returned an invalid gradient at index <span style="color:#ae81ff">1</span><span style="color:#f92672">-</span>expected type torch<span style="color:#f92672">.</span>FloatTensor but got torch<span style="color:#f92672">.</span>DoubleTensor
</code></pre></div><p>I get an error.
The reason for this is simple, in fact <strong>backward() can only be used with torch.float32 type</strong>.
Strictly speaking, <strong>torch.float64</strong> prepared this time is handled as <strong>Double type</strong>, so backward() cannot be performed.</p>
<p>The solution is to just use <strong>torch.float32</strong> instead of <strong>torch.float64</strong>.</p>
<h3 id="5-8-example-using-tensor-type-array-vector-matrix">5-8. Example using tensor type array (vector, matrix)</h3>
<p>In actual machine learning, it is usual to prepare and use vectors and matrices as parameters.
An example is shown below.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ruby:filename.rb" data-lang="ruby:filename.rb">x <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>tensor(<span style="color:#f92672">[</span><span style="color:#ae81ff">10</span><span style="color:#f92672">.</span><span style="color:#ae81ff">0</span>,<span style="color:#ae81ff">20</span><span style="color:#f92672">.</span><span style="color:#ae81ff">0</span>,<span style="color:#ae81ff">30</span><span style="color:#f92672">.</span><span style="color:#ae81ff">0</span><span style="color:#f92672">]</span>, requires_grad <span style="color:#f92672">=</span> <span style="color:#66d9ef">True</span>)
c <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>tensor(<span style="color:#f92672">[</span><span style="color:#ae81ff">1</span><span style="color:#f92672">.</span><span style="color:#ae81ff">0</span>,<span style="color:#ae81ff">2</span><span style="color:#f92672">.</span><span style="color:#ae81ff">0</span>,<span style="color:#ae81ff">3</span><span style="color:#f92672">.</span><span style="color:#ae81ff">0</span><span style="color:#f92672">]</span>, requires_grad <span style="color:#f92672">=</span> <span style="color:#66d9ef">True</span>)
x<span style="color:#f92672">[</span><span style="color:#ae81ff">0</span><span style="color:#f92672">]</span> <span style="color:#f92672">=</span> c<span style="color:#f92672">[</span><span style="color:#ae81ff">0</span><span style="color:#f92672">]*</span>x<span style="color:#f92672">[</span><span style="color:#ae81ff">0</span><span style="color:#f92672">]</span>
x<span style="color:#f92672">[</span><span style="color:#ae81ff">1</span><span style="color:#f92672">]</span> <span style="color:#f92672">=</span> c<span style="color:#f92672">[</span><span style="color:#ae81ff">1</span><span style="color:#f92672">]*</span>x<span style="color:#f92672">[</span><span style="color:#ae81ff">1</span><span style="color:#f92672">]</span>
x<span style="color:#f92672">[</span><span style="color:#ae81ff">2</span><span style="color:#f92672">]</span> <span style="color:#f92672">=</span> c<span style="color:#f92672">[</span><span style="color:#ae81ff">2</span><span style="color:#f92672">]*</span>x<span style="color:#f92672">[</span><span style="color:#ae81ff">2</span><span style="color:#f92672">]</span>
y <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>sum(x)
print(y)

<span style="color:#f92672">-</span> <span style="color:#f92672">-----------</span> <span style="color:#66d9ef">Output</span> below <span style="color:#f92672">---------------</span>
tensor(<span style="color:#ae81ff">140</span><span style="color:#f92672">.</span>, grad_fn<span style="color:#f92672">=&lt;</span><span style="color:#66d9ef">SumBackward0</span><span style="color:#f92672">&gt;</span>)
</code></pre></div><p>This is a program that calculates the dot product of the vector <strong>x</strong> and the vector <strong>c</strong>.
Now, try backward().</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ruby:filename.rb" data-lang="ruby:filename.rb">y<span style="color:#f92672">.</span>backward()

<span style="color:#f92672">-</span> <span style="color:#f92672">-----------</span> <span style="color:#66d9ef">Output</span> below <span style="color:#f92672">---------------</span>
<span style="color:#f92672">-</span> <span style="color:#f92672">-------------------------------------------------</span> <span style="color:#f92672">-------------------------</span>
<span style="color:#66d9ef">RuntimeError</span> <span style="color:#66d9ef">Traceback</span> (most recent call last)
<span style="color:#f92672">&lt;</span>ipython<span style="color:#f92672">-</span>input<span style="color:#f92672">-</span><span style="color:#ae81ff">11</span><span style="color:#f92672">-</span>ab75bb780f4c<span style="color:#f92672">&gt;</span> <span style="color:#66d9ef">in</span> <span style="color:#f92672">&lt;</span>module<span style="color:#f92672">&gt;</span>
<span style="color:#f92672">-</span> <span style="color:#f92672">---&gt;</span> <span style="color:#ae81ff">1</span> y<span style="color:#f92672">.</span>backward()
        <span style="color:#f92672">.........</span>(<span style="color:#66d9ef">Omitted</span>)<span style="color:#f92672">..........</span>

<span style="color:#e6db74">RuntimeError</span>: one of the variables needed <span style="color:#66d9ef">for</span> gradient computation has been modified by an inplace <span style="color:#e6db74">operation</span>: <span style="color:#f92672">[</span>torch<span style="color:#f92672">.</span>FloatTensor <span style="color:#f92672">[]]</span>, which is output <span style="color:#ae81ff">0</span> of <span style="color:#66d9ef">SelectBackward</span>, is at version <span style="color:#ae81ff">3</span>; expected version <span style="color:#ae81ff">2</span> instead<span style="color:#f92672">.</span>Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch<span style="color:#f92672">.</span>autograd<span style="color:#f92672">.</span>set_detect_anomaly(<span style="color:#66d9ef">True</span>)<span style="color:#f92672">.</span>
</code></pre></div><p>I got an error like this:
The important thing here is that the error contains &ldquo;** gradient computation has been modified by an inplace operation**&rdquo;, so I gave an example for in-place, but it is not found anywhere in this program. .</p>
<p>Actually, this calculation of the array &ldquo;<strong>x[0] = c[0]*x[0]</strong>&rdquo; is equivalent to in-place.
If you look at it this way, it looks like an error like the above-mentioned variable overwrite, but be careful because it says that the error is due to in-place.
The solution is to use the following program.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ruby:filename.rb" data-lang="ruby:filename.rb">x <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>tensor(<span style="color:#f92672">[</span><span style="color:#ae81ff">10</span><span style="color:#f92672">.</span><span style="color:#ae81ff">0</span>,<span style="color:#ae81ff">20</span><span style="color:#f92672">.</span><span style="color:#ae81ff">0</span>,<span style="color:#ae81ff">30</span><span style="color:#f92672">.</span><span style="color:#ae81ff">0</span><span style="color:#f92672">]</span>, requires_grad <span style="color:#f92672">=</span> <span style="color:#66d9ef">True</span>)
c <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>tensor(<span style="color:#f92672">[</span><span style="color:#ae81ff">1</span><span style="color:#f92672">.</span><span style="color:#ae81ff">0</span>,<span style="color:#ae81ff">2</span><span style="color:#f92672">.</span><span style="color:#ae81ff">0</span>,<span style="color:#ae81ff">3</span><span style="color:#f92672">.</span><span style="color:#ae81ff">0</span><span style="color:#f92672">]</span>, requires_grad <span style="color:#f92672">=</span> <span style="color:#66d9ef">True</span>)
w <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>zeros(<span style="color:#ae81ff">3</span>)

w<span style="color:#f92672">[</span><span style="color:#ae81ff">0</span><span style="color:#f92672">]</span> <span style="color:#f92672">=</span> c<span style="color:#f92672">[</span><span style="color:#ae81ff">0</span><span style="color:#f92672">]*</span>x<span style="color:#f92672">[</span><span style="color:#ae81ff">0</span><span style="color:#f92672">]</span>
w<span style="color:#f92672">[</span><span style="color:#ae81ff">1</span><span style="color:#f92672">]</span> <span style="color:#f92672">=</span> c<span style="color:#f92672">[</span><span style="color:#ae81ff">1</span><span style="color:#f92672">]*</span>x<span style="color:#f92672">[</span><span style="color:#ae81ff">1</span><span style="color:#f92672">]</span>
w<span style="color:#f92672">[</span><span style="color:#ae81ff">2</span><span style="color:#f92672">]</span> <span style="color:#f92672">=</span> c<span style="color:#f92672">[</span><span style="color:#ae81ff">2</span><span style="color:#f92672">]*</span>x<span style="color:#f92672">[</span><span style="color:#ae81ff">2</span><span style="color:#f92672">]</span>
y <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>sum(w)
print(y)

<span style="color:#f92672">-</span> <span style="color:#f92672">-----------</span> <span style="color:#66d9ef">Output</span> below <span style="color:#f92672">---------------</span>
tensor(<span style="color:#ae81ff">140</span><span style="color:#f92672">.</span>, grad_fn<span style="color:#f92672">=&lt;</span><span style="color:#66d9ef">SumBackward0</span><span style="color:#f92672">&gt;</span>)
</code></pre></div><p>You just need to prepare a variable that has nothing to do with differentiation.
When actually doing backward()</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ruby:filename.rb" data-lang="ruby:filename.rb">y<span style="color:#f92672">.</span>backward()
print(x<span style="color:#f92672">.</span>grad)

<span style="color:#f92672">-</span> <span style="color:#f92672">-----------</span> <span style="color:#66d9ef">Output</span> below <span style="color:#f92672">---------------</span>
tensor(<span style="color:#f92672">[</span><span style="color:#ae81ff">1</span><span style="color:#f92672">.</span>, <span style="color:#ae81ff">2</span><span style="color:#f92672">.</span>, <span style="color:#ae81ff">3</span><span style="color:#f92672">.]</span>)
</code></pre></div><p>It&rsquo;s working fine.</p>
<h1 id="6-a-word">6. A word</h1>
<p>This time, I have summarized the automatic part that is invisible in pyTorch&rsquo;s backward and the example that can not be done.
We will continue to update this article as we find such examples.
I think there were many points that were difficult to read, but thank you for reading.</p>

    </article>
  </div>

  
  
  
  <script>
  window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
  ga('create', 'UA-169005401-1', 'auto');
  ga('send', 'pageview');
  </script>
  <script async src='https://www.google-analytics.com/analytics.js'></script>
  

  

  
<link rel="stylesheet" type="text/css" href="//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.0/cookieconsent.min.css" />
<script src="//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.0/cookieconsent.min.js"></script>
<script>
window.addEventListener("load", function(){
window.cookieconsent.initialise({
  "palette": {
    "popup": {
      "background": "#216942",
      "text": "#b2d192"
    },
    "button": {
      "background": "#afed71"
    }
  }
})});
</script>

</body>

</html>
