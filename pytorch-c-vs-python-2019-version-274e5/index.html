<!doctype html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  
  <meta name="generator" content="Hugo 0.72.0" />

  
  <meta name="description" content="Some description">
  

  
  <link rel="apple-touch-icon" sizes="180x180" href="https://memotut.com/apple-touch-icon.png">

  
  <link rel="icon" type="image/png" sizes="32x32" href="https://memotut.com/favicon-32x32.png">

  
  <link rel="icon" type="image/png" sizes="16x16" href="https://memotut.com/favicon-16x16.png">

  
  <link rel="manifest" href="https://memotut.com/site.webmanifest">

  
  <link rel="mask-icon" href="https://memotut.com/safari-pinned-tab.svg" color="#5bbad5">

  <meta name="msapplication-TileColor" content="#da532c">

  <meta name="theme-color" content="#ffffff">

  
  <link rel="stylesheet" href="https://memotut.com/css/bootstrap.min.css" />

  
  <title>PyTorch C&#43;&#43; VS Python (2019 version) | Memo Tut</title>
  

  <style>
body {
  min-width: 300px;
}

.custom-navbar {
  margin-bottom: 1em;
  height: 60px;
}

.custom-navbar a {
  display: inline-block; 
  padding: 18px 0;
  margin-right: 1em; 
  font-weight: bold; 
}

.custom-navbar a:hover,
.custom-navbar a:focus {
  text-decoration: none; 
}

@media print {
  .custom-navbar {
    display: none;
  }
}

article {
  padding-bottom: 1em;
}

img {
  max-width: 100%;
}


body {
  background-color: #fff;
}



body {
  color: #212529;
}



a {
  color: #007bff;
}



a:hover,
a:focus {
  color: #0056b3;
}



.custom-navbar {
  background-color: #212529;
}



.custom-navbar a {
  color: rgba(255, 255, 255, 0.75);
}



.custom-navbar a:hover,
.custom-navbar a:focus {
  color: rgba(255, 255, 255, 1);
}



.container {
  max-width: 800px;
}



pre {
  display: block;
  padding: 9.5px;
  word-break: break-all;
  word-wrap: break-word;
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 4px;
}

pre code {
  padding: 0;
  font-size: inherit;
  color: inherit; 
  white-space: pre-wrap;
  background-color: transparent;
  border: none;
  border-radius: 0;
}

code {
  padding: 2px 4px;
  color: inherit; 
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 4px;
  font-size: .9em;
}



blockquote,
.blockquote {
  padding: 10px 20px;
  margin: 0 0 20px;
  font-size: 1em;
  border-left: 5px solid #6c757d;
}

</style>
</head>

<body>
  <nav class="custom-navbar">
  <div class="container">
    
    <a href="/">Posts</a>
    
    <a href="/tags/">Tags</a>
    
    <a href="/about/">About</a>
    
    <a href="/index.xml">RSS</a>
    
  </div>
</nav>
  
  <div class="container">
    <article>
      <h1>PyTorch C++ VS Python (2019 version)</h1>
<p>
  <small class="text-secondary">
  
  
  Mar 24, 2020
  </small>
  

<small><code><a href="https://memotut.com/tags/python">Python</a></code></small>


<small><code><a href="https://memotut.com/tags/c&#43;&#43;">C&#43;&#43;</a></code></small>


<small><code><a href="https://memotut.com/tags/pytorch">PyTorch</a></code></small>

</p>
<pre><code>There are many types of deep learning frameworks such as PyTorch, Tensorflow, Keras.
</code></pre>
<p>This time, I would like to pay attention to <strong>PyTorch</strong>, which I often use!</p>
<p>Did you know that <strong>C++ version is released</strong> in addition to PyTorch and Python version?
This makes it easy to incorporate if you want to use Deep Learning as part of your C++ program processing!</p>
<p>Although it is a C++ version of PyTorch, I was wondering ** &ldquo;C++ may be faster than the Python version because it is a compiled language?&rdquo;</p>
<p>So, this time, I actually checked <strong><font color="Red">&quot;How much is the speed difference between C++ and Python?&ldquo;</font></strong>!
Also, I was concerned about the accuracy, so I checked it.</p>
<h1 id="things-used-for-comparative-experiments">Things used for comparative experiments</h1>
<h2 id="1-framework">1. Framework</h2>
<p>This time, I will use the C++ version of &ldquo;PyTorch&rdquo; as the title.
You can download it from the following sites, so please try it!</p>
<p>PyTorch Official: <a href="https://pytorch.org/">https://pytorch.org/</a>
<img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/374669/04a37334-9264-c9a6-3fd2-c7fdfb869519.png" alt="libtorch.png"></p>
<p>I downloaded with the above settings.
The &ldquo;Preview (Nightly) version&rdquo; always contains the latest files.
However, it is under development, so if you want to use the stable version, select &ldquo;Stable (1.4)&rdquo;.</p>
<p>Also, the &ldquo;Run this Command&rdquo; at the bottom is quite important, and if the build version of CXX is 11 or more, we recommend that you select the lower one.
Currently, most of them are CXX17, so I think it&rsquo;s ok below.
If you select the above, various library link errors etc. will occur and it will be a lot of trouble.</p>
<h2 id="2-model">2. Model</h2>
<p>This time, we will use <strong><font color="Red">Convolutional Autoencoder</font></strong> (Convolutional Autoencoder).
Can be obtained from my GitHub → <a href="https://github.com/koba-jon/pytorch_cpp">https://github.com/koba-jon/pytorch_cpp</a></p>
<p>This model maps the <strong>input image (high dimensional)</strong> to the <strong>latent space (low dimensional)</strong>, and this time based on this <strong>latent variable (low dimensional)</strong> The purpose is to generate (higher dimension)** and minimize the error between this and the input image.
After training, this model can generate a high-dimensional image again from a high-dimensional image through a low-dimensional space, so that a latent space that further characterizes the learning image can be obtained.
In other words, it has the role of dimension compression, and can be called a so-called nonlinear principal component analysis.
This is very useful because it has various uses such as **Dimension curse elimination**, **Transfer learning**, **Anomaly detection**.</p>
<p>Then, I will explain the structure of the model used.</p>
<ul>
<li>The image size is 1/2 times when convolution is once and twice when deconvolution</li>
<li>Stabilization of learning and acceleration of convergence</li>
<li>Possible range of latent variable is (-∞, +∞)</li>
<li>Pixel value range is [-1, +1]</li>
</ul>
<p>In anticipation of these effects, we constructed the following network.</p>
<table>
  <tr>
    <th colspan="2" rowspan="2">Operation</th>
    <th rowspan="2">Kernel Size</th>
    <th rowspan="2">Stride</th>
    <th rowspan="2">Padding</th>
    <th rowspan="2">Bias</th>
    <th colspan="2">Feature Map</th>
    <th rowspan="2">BN</th>
    <th rowspan="2">Activation</th>
  </tr>
  <tr>
    <th>Input</th>
    <th>Output</th>
  </tr>
  <tr>
    <td>1</td>
    <td rowspan="6">Convolution</td>
    <td rowspan="12">4</td>
    <td rowspan="12">2</td>
    <td rowspan="12">1</td>
    <td rowspan="12">False</td>
    <td>3</td>
    <td>64</td>
    <td></td>
    <td>ReLU</td>
  </tr>
    <td>2</td>
    <td>64</td>
    <td>128</td>
    <td>True</td>
    <td>ReLU</td>
  <tr>
    <td>3</td>
    <td>128</td>
    <td>256</td>
    <td>True</td>
    <td>ReLU</td>
  </tr>
  <tr>
    <td>4</td>
    <td>256</td>
    <td>512</td>
    <td>True</td>
    <td>ReLU</td>
  </tr>
  <tr>
    <td>5</td>
    <td>512</td>
    <td>512</td>
    <td>True</td>
    <td>ReLU</td>
  </tr>
  <tr>
    <td>6</td>
    <td>512</td>
    <td>512</td>
    <td></td>
    <td></td>
  </tr>
  <tr>
    <td>7</td>
    <td rowspan="6">Transposed Convolution</td>
    <td>512</td>
    <td>512</td>
    <td>True</td>
    <td>ReLU</td>
  </tr>
  <tr>
    <td>8</td>
    <td>512</td>
    <td>512</td>
    <td>True</td>
    <td>ReLU</td>
  </tr>
  <tr>
    <td>9</td>
    <td>512</td>
    <td>256</td>
    <td>True</td>
    <td>ReLU</td>
  </tr>
  <tr>
    <td>10</td>
    <td>256</td>
    <td>128</td>
    <td>True</td>
    <td>ReLU</td>
  </tr>
  <tr>
    <td>11</td>
    <td>128</td>
    <td>64</td>
    <td>True</td>
    <td>ReLU</td>
  </tr>
  <tr>
    <td>12</td>
    <td>64</td>
    <td>3</td>
    <td></td>
    <td>tanh</td>
  </tr>
</table>
<h2 id="3-data-set">3. Data set</h2>
<ul>
<li>CelebA (Large-scale CelebFaces Attributes) dataset <br>
<a href="http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html">http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html</a></li>
</ul>
<p>This time, we use CelebA data set, which is a data set of 202,599 face images (color) of celebrity people.
Since the image size is 178 x 218 [pixel], a little inconvenience may occur during deconvolution, so we resized to ** 64 x 64 [pixel] this time.
Of these, <strong>90% (182,340 images) were used for learning images</strong> and <strong>10% (20,259 images) were used for testing images</strong>.</p>
<p>When this is input to the model above, the latent space becomes (C,H,W) = (512,1,1). If you input an image of 128×128 [pixel] or more, the middle layer becomes a spatial latent space.</p>
<h1 id="comparison">Comparison</h1>
<p>This time, I&rsquo;m mainly investigating &ldquo;how much speed is different between C++ and Python&rdquo; **, but I would also like to compare speed and performance under the following 5 types of environments. ．</p>
<ul>
<li>CPU main operation
-Python
-C++</li>
<li>GPU main operation
-Python
-Non-deterministic
-Deterministic
-C++</li>
</ul>
<h2 id="1-differences-in-units-that-operate-on-the-main-cpu-or-gpu">1. Differences in units that operate on the main (CPU or GPU)</h2>
<ul>
<li>CPU<br>
Good at handling &ldquo;serial&rdquo; and &ldquo;complex&rdquo; instructions</li>
<li>GPU<br>
Good at handling &ldquo;parallel&rdquo; and &ldquo;simple&rdquo; instructions</li>
</ul>
<p>As can be seen from the above characteristics, GPU is overwhelmingly advantageous in calculation speed in deep learning that deals with images.</p>
<h3 id="1-python-implementation">(1) Python implementation</h3>
<ul>
<li>When using CPU</li>
</ul>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-Python:CPU.py" data-lang="Python:CPU.py">device <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>device(<span style="color:#e6db74">&#39;cpu&#39;</span>) <span style="color:#75715e"># use CPU</span>

model<span style="color:#f92672">.</span>to(device) <span style="color:#75715e"># move model to CPU</span>
image <span style="color:#f92672">=</span> image<span style="color:#f92672">.</span>to(device) <span style="color:#75715e"># move data to CPU</span>
</code></pre></div><ul>
<li>When using GPU</li>
</ul>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-Python:GPU.py" data-lang="Python:GPU.py">device <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>device(<span style="color:#e6db74">&#39;cuda&#39;</span>) <span style="color:#75715e"># Use default GPU</span>
device <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>device(<span style="color:#e6db74">&#39;cuda:0&#39;</span>) <span style="color:#75715e"># Use first GPU</span>
device <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>device(<span style="color:#e6db74">&#39;cuda:1&#39;</span>) <span style="color:#75715e"># Use second GPU</span>

model<span style="color:#f92672">.</span>to(device) <span style="color:#75715e"># move model to GPU</span>
image <span style="color:#f92672">=</span> image<span style="color:#f92672">.</span>to(device) <span style="color:#75715e"># move data to GPU</span>
</code></pre></div><h3 id="2-c-implementation">(2) C++ implementation</h3>
<ul>
<li>When using CPU</li>
</ul>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-C++:CPU.cpp" data-lang="C++:CPU.cpp">torch<span style="color:#f92672">::</span>Device device(torch<span style="color:#f92672">::</span>kCPU); <span style="color:#75715e">// use CPU
</span><span style="color:#75715e"></span>
model<span style="color:#f92672">-&gt;</span>to(device); <span style="color:#75715e">// move model to CPU
</span><span style="color:#75715e"></span>image <span style="color:#f92672">=</span> image.to(device); <span style="color:#75715e">// move data to CPU
</span></code></pre></div><ul>
<li>When using GPU</li>
</ul>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-C++:GPU.cpp" data-lang="C++:GPU.cpp">torch<span style="color:#f92672">::</span>Device device(torch<span style="color:#f92672">::</span>kCUDA); <span style="color:#75715e">// use default GPU
</span><span style="color:#75715e"></span>torch<span style="color:#f92672">::</span>Device device(torch<span style="color:#f92672">::</span>kCUDA, <span style="color:#ae81ff">0</span>); <span style="color:#75715e">// use the first GPU
</span><span style="color:#75715e"></span>torch<span style="color:#f92672">::</span>Device device(torch<span style="color:#f92672">::</span>kCUDA, <span style="color:#ae81ff">1</span>); <span style="color:#75715e">// use second GPU
</span><span style="color:#75715e"></span>
model<span style="color:#f92672">-&gt;</span>to(device); <span style="color:#75715e">// move model to GPU
</span><span style="color:#75715e"></span>image <span style="color:#f92672">=</span> image.to(device); <span style="color:#75715e">// move data to GPU
</span></code></pre></div><h2 id="2-difference-between-deterministic-and-non-deterministic-only-gpu-main-operation--python">2. Difference between deterministic and non-deterministic (only GPU main operation &amp; Python)</h2>
<p>In the Python version of PyTorch, in the case of learning using GPU, cuDNN is used to <strong>improve the learning speed</strong>.However, unlike C++, just because it speeds up learning doesn&rsquo;t mean that you can reproduce the exact same situation if you start learning again.</p>
<p>Therefore, the PyTorch formula states that it is necessary to make the behavior of cuDNN deterministic as follows in order to ensure reproducibility, and at the same time, the speed decreases.</p>
<p><a href="https://pytorch.org/docs/stable/notes/randomness.html">https://pytorch.org/docs/stable/notes/randomness.html</a></p>
<blockquote>
</blockquote>
<p>Deterministic mode can have a performance impact, depending on your model.This means that due to the deterministic nature of the model, the processing speed (i.e. processed batch items per second) can be lower than when the model is non-deterministic.</p>
<p>From an engineer&rsquo;s standpoint, we may be concerned about the presence or absence of reproducibility, and the speed may change depending on the presence or absence of reproducibility.</p>
<p>Unlike the &ldquo;rand&rdquo; function in C++, if you do not set the initial value of the random number, it will be random, so in order to ensure reproducibility in Python, <em>explicitly</em> set the initial value of the random number. You need to set it.
(The setting of the initial value of the random number does not affect the speed.)</p>
<p>The implementation is as follows.</p>
<ul>
<li>Deterministic case</li>
</ul>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-Python:deterministic.py" data-lang="Python:deterministic.py">seed <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
torch<span style="color:#f92672">.</span>manual_seed(seed)
torch<span style="color:#f92672">.</span>cuda<span style="color:#f92672">.</span>manual_seed(seed)
np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>seed(seed)
random<span style="color:#f92672">.</span>seed(seed)
torch<span style="color:#f92672">.</span>backends<span style="color:#f92672">.</span>cudnn<span style="color:#f92672">.</span>deterministic <span style="color:#f92672">=</span> True <span style="color:#75715e"># deterministic instead of slower</span>
torch<span style="color:#f92672">.</span>backends<span style="color:#f92672">.</span>cudnn<span style="color:#f92672">.</span>benchmark <span style="color:#f92672">=</span> False <span style="color:#75715e"># deterministic instead of slower</span>
</code></pre></div><ul>
<li>Non-deterministic case</li>
</ul>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-Python:non_deterministic.py" data-lang="Python:non_deterministic.py">torch<span style="color:#f92672">.</span>backends<span style="color:#f92672">.</span>cudnn<span style="color:#f92672">.</span>deterministic <span style="color:#f92672">=</span> False <span style="color:#75715e"># Speed up instead of being non-deterministic</span>
torch<span style="color:#f92672">.</span>backends<span style="color:#f92672">.</span>cudnn<span style="color:#f92672">.</span>benchmark <span style="color:#f92672">=</span> True <span style="color:#75715e"># Speed up when image size doesn&#39;t change</span>
</code></pre></div><h2 id="3-implementation-differences-between-programming-languages">3. Implementation differences between programming languages</h2>
<p>Even if the contents that you want to implement are the same, the <strong>notation and rules</strong> may change, or the <strong>required library</strong> may change, depending on the programming language.
Since Python and C++ are both object-oriented languages, the concepts are similar, but above all, Python is an interpreter type and C++ is a compiled type, so it must be implemented considering that dynamic typing does not work for C++.
Also, it should be taken into account that PyTorch&rsquo;s C++ API is currently under development, so some functions are not maintained.</p>
<p>Based on these points, I would like to introduce the differences in implementation between Python and C++, and the programs I implemented.</p>
<h3 id="1-library-usage">(1) Library usage</h3>
<p>In addition to the currently used Python library usage, the recommended library for implementation in C++ and the library usage of the program I actually wrote are also described.</p>
<table>
<thead>
<tr>
<th align="center"></th>
<th align="center">Python (Recommended)</th>
<th align="center">C++ (Recommended)</th>
<th align="center">C++ (Original)</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center">Process command line arguments</td>
<td align="center">argparse</td>
<td align="center">boost::program_options</td>
<td align="center">boost::program_options</td>
</tr>
<tr>
<td align="center">Model Design</td>
<td align="center">torch.nn</td>
<td align="center">torch::nn</td>
<td align="center">torch::nn</td>
</tr>
<tr>
<td align="center">Pre-processing (transform)</td>
<td align="center">torchvision.transforms</td>
<td align="center">torch::data::transforms (when performing various pre-processing before execution) <br> or <br> Self-made (when performing various pre-processing after execution)</td>
<td align="center">Self-made ( OpenCV used)</td>
</tr>
<tr>
<td align="center">Getting datasets (datasets)</td>
<td align="center">torchvision.datasets (using Pillow)</td>
<td align="center">Original (using OpenCV)</td>
<td align="center">Original (using OpenCV)</td>
</tr>
<tr>
<td align="center">Data loader (dataloader)</td>
<td align="center">torch.utils.data.DataLoader</td>
<td align="center">torch::data::make_data_loader (for class classification) <br> or <br> Self-made (for other than class classification)</td>
<td align="center">Self-made (using OpenMP)</td>
</tr>
<tr>
<td align="center">loss function (loss)</td>
<td align="center">torch.nn</td>
<td align="center">torch::nn</td>
<td align="center">torch::nn</td>
</tr>
<tr>
<td align="center">Optimization method (optimizer)</td>
<td align="center">torch.optim</td>
<td align="center">torch::optim</td>
<td align="center">torch::optim</td>
</tr>
<tr>
<td align="center">Error Back Propagation (backward)</td>
<td align="center">torch.Tensor.backward()</td>
<td align="center">torch::Tensor::backward()</td>
<td align="center">torch::Tensor::backward()</td>
</tr>
<tr>
<td align="center">Progress bar</td>
<td align="center">tqdm</td>
<td align="center">boost</td>
<td align="center">homebrew</td>
</tr>
</tbody>
</table>
<p><strong><font color="Red">Current time (2020/03/24)</font></strong>, it is as above.</p>
<p>When using the PyTorch library in C++, the class and function names are almost the same as Python.
This seems to be that the producer considers the user. Thank you very much!</p>
<p>Next, I will describe the points that you should pay particular attention when writing a PyTorch program in C++.</p>
<h3 id="2-model-design">(2) Model design</h3>
<p>Below is a partial excerpt of the program I wrote.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-C++:networks.hpp" data-lang="C++:networks.hpp"><span style="color:#66d9ef">using</span> <span style="color:#66d9ef">namespace</span> torch;
<span style="color:#66d9ef">namespace</span> po <span style="color:#f92672">=</span> boost<span style="color:#f92672">::</span>program_options;

<span style="color:#66d9ef">struct</span> <span style="color:#a6e22e">ConvolutionalAutoEncoderImpl</span> <span style="color:#f92672">:</span>nn<span style="color:#f92672">::</span>Module{
<span style="color:#66d9ef">private</span><span style="color:#f92672">:</span>
    nn<span style="color:#f92672">::</span>Sequential encoder, decoder;
<span style="color:#66d9ef">public</span><span style="color:#f92672">:</span>
    ConvolutionalAutoEncoderImpl(po<span style="color:#f92672">::</span>Variables_map <span style="color:#f92672">&amp;</span>vm);
    torch<span style="color:#f92672">::</span>Tensor forward(torch<span style="color:#f92672">::</span>Tensor x);
}

TORCH_MODULE(ConvolutionalAutoEncoder);
</code></pre></div><p>When designing the model, use the &ldquo;torch::nn&rdquo; class as in Python.
Also, the structure is used when creating the model. (There is also a class version, but it seems a bit complicated)
At this time, it should be noted that <strong><font color="Red">nn::Module is inherited</font></strong> like Python.
This is the same as how to write Python.</p>
<p>Next important thing is to change the name of the <strong><font color="Red">structure to &ldquo;[model name]Impl&quot;</font></strong>, and to add <strong><font color="Red" below the structure. > Add &ldquo;TORCH_MODULE([model name])&ldquo;</font></strong>.
If you do not do this, you will not be able to save or load the model.
Also, by setting &ldquo;TORCH_MODULE([model name])&rdquo;, you can declare the normal structure &ldquo;ConvolutionalAutoEncoderImpl&rdquo; as the structure &ldquo;ConvolutionalAutoEncoder&rdquo; for the model, but perhaps you will further inherit the class internally. Are you Therefore, when accessing a member variable, <strong><font color="Red">&rdquo;-&gt;&rdquo; (arrow operator)</font></strong> when accessing a member variable, for example &ldquo;model-&gt;to(device)&rdquo; Note that you need to use.</p>
<p>Next, related to the above matters, I will explain the points to note when using modules of the nn class.
You can use &ldquo;nn::Sequential&rdquo; like Python. To add modules to &ldquo;nn::Sequential&rdquo; in C++, use <strong><font color="Red">&quot;push_back&quot;</font></strong> like vector type.
Be careful here to use <strong><font color="Red">&rdquo;-&gt;&rdquo; (arrow operator)</font></strong> to call the &ldquo;push_back&rdquo; function.
The implementation example is as follows.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-C++:networks.cpp" data-lang="C++:networks.cpp">nn<span style="color:#f92672">::</span>Sequential sq;
sq<span style="color:#f92672">-&gt;</span>push_back(nn<span style="color:#f92672">::</span>Conv2d(nn<span style="color:#f92672">::</span>Conv2dOptions(<span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">64</span>, <span style="color:#75715e">/*kernel_size=*/</span><span style="color:#ae81ff">4</span>).stride(<span style="color:#ae81ff">2</span>).padding(<span style="color:#ae81ff">1</span>).bias(false)));
sq<span style="color:#f92672">-&gt;</span>push_back(nn<span style="color:#f92672">::</span>BatchNorm2d(<span style="color:#ae81ff">64</span>));
sq<span style="color:#f92672">-&gt;</span>push_back(nn<span style="color:#f92672">::</span>ReLU(nn<span style="color:#f92672">::</span>ReLUOptions().inplace(true)));
</code></pre></div><h3 id="3-transformdatasetsdataloader-original-work">(3) Transform/datasets/dataloader original work</h3>
<p>To create transform, datasets, dataloader by yourself, when you pass tensor type data to other variables, <strong>pass it using <font color="Red">&rdquo;.clone()&quot;</strong>. I got hooked here.
Is the tensor type related to the calculation graph? (Conjecture) If you don&rsquo;t set it like this, the value in the tensor may change.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-C++:transforms.cpp" data-lang="C++:transforms.cpp"><span style="color:#66d9ef">void</span> transforms<span style="color:#f92672">::</span>Normalize<span style="color:#f92672">::</span>forward(torch<span style="color:#f92672">::</span>Tensor <span style="color:#f92672">&amp;</span>data_in, torch<span style="color:#f92672">::</span>Tensor <span style="color:#f92672">&amp;</span>data_out){
    torch<span style="color:#f92672">::</span>Tensor data_out_src <span style="color:#f92672">=</span> (data_in<span style="color:#f92672">-</span><span style="color:#66d9ef">this</span><span style="color:#f92672">-&gt;</span>mean) <span style="color:#f92672">/</span> <span style="color:#66d9ef">this</span><span style="color:#f92672">-&gt;</span>std;
    data_out <span style="color:#f92672">=</span> data_out_src.clone();
    <span style="color:#66d9ef">return</span>;
}
</code></pre></div><h3 id="4-other-programs">(4) Other programs</h3>
<p>Other programs are almost the same as the Python version, and there is no particular place to be hooked.
Also, I made a class that I thought was a little difficult to use because it was different from the Python version.
Please refer to the following program on GitHub for specific programs.
<a href="https://github.com/koba-jon/pytorch_cpp/tree/master/ConvAE">https://github.com/koba-jon/pytorch_cpp/tree/master/ConvAE</a></p>
<p>Perhaps I may also write an article explaining the source code.
If you have an opinion that something is wrong here, please feel free to comment as we are very welcome.</p>
<h1 id="items-unified-across-programming-languages">Items unified across programming languages</h1>
<p>Basically, you can think that it is almost the same except where it can not be helped, such as that there is no Python library in C++.
Also, feel free to think that you have not changed from the GitHub program.</p>
<p>Specifically, the following contents have been unified when comparing the Python version and the C++ version.</p>
<ul>
<li>Image size (64 x 64 x 3)* Image type (learning method A image group = learning method B image group)</li>
<li>Batch size (16)</li>
<li>Size of latent space (1 x 1 x 512)</li>
<li>Optimization method (Adam, learning rate=0.0001, β1=0.5, β2=0.999)</li>
<li>Model structure</li>
<li>Model initialization method</li>
<li>Convolutional layer, deconvolutional layer: Mean 0.0, standard deviation 0.02</li>
<li>Batch normalization: average 1.0, standard deviation 0.02</li>
<li>How to load data</li>
<li>Only the path is acquired when the &ldquo;datasets&rdquo; class is initialized, and the image is read based on the path for the first time when actually operating.</li>
<li>Read only one set of data (one image and one pass) when the &ldquo;datasets&rdquo; class is running.</li>
<li>Execute &ldquo;transform&rdquo; when &ldquo;datasets&rdquo; class is running.</li>
<li>When the &ldquo;DataLoader&rdquo; class is running, the mini batch data is read in parallel from the &ldquo;datasets&rdquo; class.</li>
<li>How to shuffle the dataset</li>
<li>Shuffles during learning, but does not shuffle during inference.</li>
<li>For each epoch, shuffle at the very beginning of entering data.</li>
</ul>
<h1 id="experimental-result">Experimental result</h1>
<p>For each target to be compared, a 182,340 celebA 64×64 images were used to train a mini-batch convolution auto-encoder model by 1 [epoch] so as to minimize the L1 error.
<strong><font color="Red">&quot;Time per 1 [epoch]&ldquo;</font></strong> and **<font color="Red">&quot;GPU memory usage&quot;</font> I checked **.</p>
<p>Here, &ldquo;time per 1 [epoch]&rdquo; includes tqdm and the processing time of the function that I made.
I&rsquo;ve included this because it has little impact on the total processing time, and it&rsquo;s more convenient to use Visualization when actually using PyTorch.</p>
<p>Also, using the trained model, 20,259 test images were input to the model one by one and tested.
<strong><font color="Red">&quot;Average forward propagation speed&quot;</font></strong> and <strong><font color="Red">&quot;L1 error between input image and output image&quot;</font> I also checked &gt;</strong>.</p>
<p>Then, I learned and tested without launching anything other than the &ldquo;executable file&rdquo; and &ldquo;nvidia-smi&rdquo; (the one that was running from the beginning when Ubuntu was started).</p>
<table>
  <tr>
    <th colspan="2" rowspan="3"></th>
    <th colspan="2">CPU (Core i7-8700)</th>
    <th colspan="3">GPU (GeForce GTX 1070)</th>
  </tr>
  <tr>
    <th rowspan="2">Python</th>
    <th rowspan="2">C++</th>
    <th colspan="2">Python</th>
    <th rowspan="2">C++</th>
  </tr>
  <tr>
    <th>non-deterministic</th>
    <th>deterministic</th>
  </tr>
  <tr>
    <td rowspan="2">learning</td>
    <td>Time [time/epoch]</td>
    <td>1 hour 04 minutes 49 seconds</td>
    <td>1h03m00s</td>
    <td>5 minutes 53 seconds</td>
    <td>7 minutes 42 seconds</td>
    <td>17 minutes 36 seconds</td>
  </tr>
  <tr>
    <td>GPU memory [MiB]</td>
    <td>2</td>
    <td>9</td>
    <td>933</td>
    <td>913</td>
    <td>2941</td>
  </tr>
  <tr>
    <td rowspan="2">test</td>
    <td>Speed [seconds/data]</td>
    <td>0.01189</td>
    <td>0.01477</td>
    <td>0.00102</td>
    <td>0.00101</td>
    <td>0.00101</td>
  </tr>
  <tr>
    <td>L1 error (MAE)</td>
    <td>0.12621</td>
    <td>0.12958</td>
    <td>0.12325</td>
    <td>0.12104</td>
    <td>0.13158</td>
  </tr>
</table>
<p>C++ is a compiled language.
Therefore, I thought that I would beat the interpreted language Python&hellip; <strong>Both were good games</strong>.</p>
<p>Regarding learning time, we found that CPU is almost the same, and GPU is more than twice as slow as C++ in Python. (why?)
As for this result, the CPU is the same and it differs greatly only in the case of GPU, so</p>
<ul>
<li>Possibility that PyTorch C++ version of GPU does not have perfect maintenance and forward and back propagation by GPU are not optimized.</li>
<li>Possibility that it takes time to transfer the mini batch data acquired by the CPU to the GPU</li>
</ul>
<p>Is likely to be mentioned.
As the following ones have been experimented with, it is certain that <strong><font color="Red">Python is faster when main GPU is running</font></strong>.
<a href="https://www.noconote.work/entry/2019/01/11/151624">https://www.noconote.work/entry/2019/01/11/151624</a></p>
<p>Also, since the speed and performance of inference (test) is almost the same as Python, <strong><font color="Red">Currently Python may be better</font></strong>.</p>
<p>The memory usage of GPU is also large for some reason. (Although Relu&rsquo;s inplace is set to True&hellip;)</p>
<p>This is a deterministic/non-deterministic result of Python (GPU), but as the formula states, deterministic is slower.
After all, the time here changes. <br><br></p>
<p>#Conclusion</p>
<ul>
<li>
<p>Learning speed<br></p>
</li>
<li>
<p>1st place: Python version (non-deterministic, GPU main operation) <br></p>
</li>
<li>
<p>2nd place: Python version (deterministic, GPU main operation) <br></p>
</li>
<li>
<p>3rd place: C++ version (GPU main operation) <br></p>
</li>
<li>
<p>4th place: CPU main operation (Python version, C++ version equivalent) <br><br></p>
</li>
<li>
<p>Inference speed<br></p>
</li>
<li>
<p>1st place: GPU main operation (Python version, C++ version equivalent) <br></p>
</li>
<li>
<p>2nd place: CPU main operation (Python version, C++ version equivalent) <br><br></p>
</li>
<li>
<p>Performance<br></p>
</li>
<li>
<p>All are similar</p>
</li>
</ul>
<h1 id="in-conclusion">in conclusion</h1>
<p>This time, we compared the speed and performance of Python version and C++ version of PyTorch.</p>
<p>As a result, Python and C++ are almost the same in terms of performance, so I thought that <strong>using PyTorch of C++ is not a problem</strong>.
However, <strong><font color="Red">At this stage, it may not be highly recommended to do Cy PyTorch for speed</font></strong>.</p>
<p>Perhaps the C++ API is still under development, so it may be greatly improved in the future!
From now on, this is my expectation!</p>
<h1 id="reference-url">Reference URL</h1>
<ul>
<li><a href="https://pytorch.org/cppdocs/">https://pytorch.org/cppdocs/</a></li>
<li><a href="https://orizuru.io/blog/deep-learning/pytorch-cpp_01/">https://orizuru.io/blog/deep-learning/pytorch-cpp_01/</a></li>
<li><a href="https://www.noconote.work/entry/2019/01/08/200120">https://www.noconote.work/entry/2019/01/08/200120</a></li>
<li><a href="https://www.noconote.work/entry/2019/01/11/151624">https://www.noconote.work/entry/2019/01/11/151624</a></li>
<li><a href="https://github.com/pytorch/examples/tree/master/cpp">https://github.com/pytorch/examples/tree/master/cpp</a></li>
</ul>

    </article>
  </div>

  
  
  
  <script>
  window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
  ga('create', 'UA-169005401-1', 'auto');
  ga('send', 'pageview');
  </script>
  <script async src='https://www.google-analytics.com/analytics.js'></script>
  

  

  
<link rel="stylesheet" type="text/css" href="//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.0/cookieconsent.min.css" />
<script src="//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.0/cookieconsent.min.js"></script>
<script>
window.addEventListener("load", function(){
window.cookieconsent.initialise({
  "palette": {
    "popup": {
      "background": "#216942",
      "text": "#b2d192"
    },
    "button": {
      "background": "#afed71"
    }
  }
})});
</script>

</body>

</html>
