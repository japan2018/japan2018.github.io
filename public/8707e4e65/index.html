<!doctype html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  
  <meta name="generator" content="Hugo 0.72.0" />

  
  <meta name="description" content="Some description">
  

  
  <link rel="apple-touch-icon" sizes="180x180" href="https://memotut.com/apple-touch-icon.png">

  
  <link rel="icon" type="image/png" sizes="32x32" href="https://memotut.com/favicon-32x32.png">

  
  <link rel="icon" type="image/png" sizes="16x16" href="https://memotut.com/favicon-16x16.png">

  
  <link rel="manifest" href="https://memotut.com/site.webmanifest">

  
  <link rel="mask-icon" href="https://memotut.com/safari-pinned-tab.svg" color="#5bbad5">

  <meta name="msapplication-TileColor" content="#da532c">

  <meta name="theme-color" content="#ffffff">

  
  <link rel="stylesheet" href="https://memotut.com/css/bootstrap.min.css" />

  
  <title>[Python] An amateur stumbled on Deep Learning made from scratch Memo: Chapter 5 | Memo Tut</title>
  

  <style>
body {
  min-width: 300px;
}

.custom-navbar {
  margin-bottom: 1em;
  height: 60px;
}

.custom-navbar a {
  display: inline-block; 
  padding: 18px 0;
  margin-right: 1em; 
  font-weight: bold; 
}

.custom-navbar a:hover,
.custom-navbar a:focus {
  text-decoration: none; 
}

@media print {
  .custom-navbar {
    display: none;
  }
}

article {
  padding-bottom: 1em;
}

img {
  max-width: 100%;
}


body {
  background-color: #fff;
}



body {
  color: #212529;
}



a {
  color: #007bff;
}



a:hover,
a:focus {
  color: #0056b3;
}



.custom-navbar {
  background-color: #212529;
}



.custom-navbar a {
  color: rgba(255, 255, 255, 0.75);
}



.custom-navbar a:hover,
.custom-navbar a:focus {
  color: rgba(255, 255, 255, 1);
}



.container {
  max-width: 800px;
}



pre {
  display: block;
  padding: 9.5px;
  word-break: break-all;
  word-wrap: break-word;
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 4px;
}

pre code {
  padding: 0;
  font-size: inherit;
  color: inherit; 
  white-space: pre-wrap;
  background-color: transparent;
  border: none;
  border-radius: 0;
}

code {
  padding: 2px 4px;
  color: inherit; 
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 4px;
  font-size: .9em;
}



blockquote,
.blockquote {
  padding: 10px 20px;
  margin: 0 0 20px;
  font-size: 1em;
  border-left: 5px solid #6c757d;
}

</style>
</head>

<body>
  <nav class="custom-navbar">
  <div class="container">
    
    <a href="/">Posts</a>
    
    <a href="/tags/">Tags</a>
    
    <a href="/about/">About</a>
    
    <a href="/index.xml">RSS</a>
    
  </div>
</nav>
  
  <div class="container">
    <article>
      <h1>[Python] An amateur stumbled on Deep Learning made from scratch Memo: Chapter 5</h1>
<p>
  <small class="text-secondary">
  
  
  Jan 19, 2020
  </small>
  

<small><code><a href="https://memotut.com/tags/python">Python</a></code></small>


<small><code><a href="https://memotut.com/tags/machine-learning"> machine learning</a></code></small>


<small><code><a href="https://memotut.com/tags/deeplearning"> DeepLearning</a></code></small>

</p>
<pre><code>#Introduction
</code></pre>
<p>I suddenly started studying <a href="https://www.oreilly.co.jp/books/9784873117584/">[Deep Learning from scratch&ndash;the theory and implementation of deep learning learned with Python]</a> It is a memo of the trip.</p>
<p>The execution environment is macOS Mojave + Anaconda 2019.10, and the Python version is 3.7.4. For details, please refer to <a href="https://qiita.com/segavvy/items/1945aa1a0f91a1473555">Chapter 1 of this memo</a>.</p>
<p>(To other chapters of this memo: <a href="https://qiita.com/segavvy/items/1945aa1a0f91a1473555">Chapter 1</a>/<a href="https://qiita.com/segavvy/items/d8e9e70437e35083a459">Chapter2</a>/<a href="https://qiita.com/segavvy/items/6d79d0c3b4367869f4ea">Chapter3</a>/<a href="https://qiita.com/segavvy/items/bdad9fcda2f0da918e7c">Chapter4</a>/Chapter5/<a href="https://qiita.com/segavvy/items/ca4ac4c9ee1a126bff41">Chapter6</a>/<a href="https://qiita.com/segavvy/items/8541c6ae1868d9b2b805">Chapter7</a>/<a href="https://qiita.com/segavvy/items/3eb6ea0ea2ea68af68af68af2af6868">Chapter8</a>)/<a href="https://qiita.com/segavvy/items/4e8c36cac9c6f3543ffd">Summary</a>))</p>
<p>#5 Error Back Propagation</p>
<p>This chapter describes the backpropagation method for speeding up the calculation of weight parameters in learning neural networks.</p>
<p>By the way, the word &ldquo;propagation&rdquo; is a translation of Propagetion, but it is sometimes referred to as &ldquo;propagation&rdquo;, so Japanese is chaotic. I found a blog article that examined this area, so if you are interested, <a href="https://kiito.hatenablog.com/entry/2014/05/31/084701">Walk when you walk&gt; [Machine learning] The translation of &ldquo;Propagation&rdquo; is &ldquo;propagation&rdquo; or &ldquo;propagation&rdquo;? </a> please.</p>
<p>#5.1 Calculation Graph</p>
<p>The explanation by the calculation graph is very easy to understand when learning the error backpropagation method. Perhaps the 5 secrets of this book&rsquo;s popularity?</p>
<p>Learning is essential in neural nets, which requires finding the gradients of the weighting parameters and calculating the derivative of each parameter. Using the computational graph, we can see that the calculation of the derivative can be done very efficiently by backpropagating the local derivative from back to front.</p>
<p>#5.2 Chain rule</p>
<p>This is an explanation of the differential formula of the composite function and the explanation of the back propagation. I don&rsquo;t think there is any problem even if you read it easily, but if you want to understand it correctly, you need a high-school differential review.</p>
<p>I never remembered the formula of differentiation, so I decided to review it after a long time. However, I often study after my main job is over, and when it comes to books and websites, I&rsquo;m having a hard time thinking about sleepiness. It is quite difficult for a salaried worker to study in free time.</p>
<p>What I found there was <a href="https://www.try-it.jp/">Tri-It</a>, which is being run by a tutor. Although it is a video lesson, it is very easy to understand even though it is free, and it is divided into about 15 minutes, so it is also ideal for studying with a smartphone while moving or taking a break. Although it is a service for students, I think it is also very suitable for reviewing working people.</p>
<p>For reference, I&rsquo;ve posted a link to a video about the introductory part of differentiation. Please note that it is limited to private viewing.</p>
<ul>
<li><a href="https://www.youtube.com/playlist?list=PLiRy47VSZM60KH0iB8pzk0flIKGgO8LbV">Video Lesson Try IT&gt; Mathematics II Limits and Differential Functions</a></li>
<li><a href="https://www.youtube.com/playlist?list=PLiRy47VSZM63NPT-10IwMppTVT7pPEgYP">Video Lesson Try IT&gt; Mathematics II Differentiation</a></li>
</ul>
<p>#5.3 Back Propagation</p>
<p>A description of backpropagation for adder and multiply nodes. Back-propagation is easy to calculate because addition, multiplication, and differentiation are simple.</p>
<p>#5.4 Simple layer implementation</p>
<p>It is an implementation of the addition layer and multiplication layer in the previous section. Since the differentiation is simple, it is easy to implement.</p>
<h1 id="55-implementation-of-activation-function-layer">5.5 Implementation of activation function layer</h1>
<p>A layer implementation of the activation function.</p>
<h2 id="551-relu-layer">5.5.1 ReLU layer</h2>
<p>ReLU also has a simple derivative calculation, so it is easy to implement and will likely lead to faster learning. Below is the implemented code.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python:relu.py" data-lang="python:relu.py"><span style="color:#75715e"># coding: utf-8</span>


<span style="color:#66d9ef">class</span> <span style="color:#a6e22e">ReLU</span>:
    <span style="color:#66d9ef">def</span> __init__(self):
        <span style="color:#e6db74">&#34;&#34;&#34;ReLU layer
</span><span style="color:#e6db74">        &#34;&#34;&#34;</span>
        self<span style="color:#f92672">.</span>mask <span style="color:#f92672">=</span> None

    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x):
        <span style="color:#e6db74">&#34;&#34;&#34; forward propagation
</span><span style="color:#e6db74">        
</span><span style="color:#e6db74">        Args:
</span><span style="color:#e6db74">            x (numpy.ndarray): input
</span><span style="color:#e6db74">            
</span><span style="color:#e6db74">        Returns:
</span><span style="color:#e6db74">            numpy.ndarray: output
</span><span style="color:#e6db74">        &#34;&#34;&#34;</span>
        self<span style="color:#f92672">.</span>mask <span style="color:#f92672">=</span> (x <span style="color:#f92672">&lt;=</span> <span style="color:#ae81ff">0</span>)
        out <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>copy()
        out[self<span style="color:#f92672">.</span>mask] <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>

        <span style="color:#66d9ef">return</span> out

    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">backward</span>(self, dout):
        <span style="color:#e6db74">&#34;&#34;&#34; backpropagation
</span><span style="color:#e6db74">        
</span><span style="color:#e6db74">        Args:
</span><span style="color:#e6db74">            dout (numpy.ndarray): differential value transmitted from the right layer
</span><span style="color:#e6db74">        
</span><span style="color:#e6db74">        Returns:
</span><span style="color:#e6db74">            numpy.ndarray: derivative value
</span><span style="color:#e6db74">        &#34;&#34;&#34;</span>
        dout[self<span style="color:#f92672">.</span>mask] <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
        dx <span style="color:#f92672">=</span> dout

        <span style="color:#66d9ef">return</span> dx
</code></pre></div><h2 id="552-sigmoid-layer">5.5.2 Sigmoid layer</h2>
<p>The derivative of the sigmoid function is a little complicated, but you can calculate it little by little using the calculation graph. However, the derivative of $y=\frac{1}{x}$ required in the first &ldquo;/&rdquo; node and the derivative of $y=exp(x)$ in the &ldquo;$exp$&rdquo; node do not rewrite the formula. It is painful. The formula in this area is <a href="https://mathwords.net/bibun">Simplify all the 59 mathematical formulas&gt; calculus&gt; differential formulas learned with concrete examples sorted by importance</a> and recommend it.</p>
<p>After the calculation by the calculation graph, it transforms into the formula of (5.12) in the book, but at first I could not understand the flow of transforming the second line from the end to the last line. It will be easier to understand if you insert one line between them as follows.</p>
<pre><code class="language-math" data-lang="math">\begin{align}
\frac{\partial L}{\partial y}y^2exp(-x) &amp;= \frac{\partial L}{\partial y} \frac{1}{(1+exp(-x))^2 } exp(-x) \\
&amp;=\frac{\partial L}{\partial y} \frac{1}{1+exp(-x)} \frac{exp(-x)}{1+exp(-x)} \\
&amp;=\frac{\partial L}{\partial y} \frac{1}{1+exp(-x)} \biggl(\frac{1+exp(-x)}{1+exp(-x) }-\frac{1}{1+exp(-x)}\biggr) ← Added line \\
&amp;=\frac{\partial L}{\partial y} y(1-y)

\end{align}
</code></pre><p>What is surprising about this result is that the output of forward propagation, $y$, is very easy to compute. In the error backpropagation method, forward propagation is calculated first, so having the result will speed up learning. The characteristic that $ e^x $ is differentiated to be $ e^x $ is also important, and the person who thinks of the sigmoid function is really amazing.</p>
<p>The sigmoid layer is not used in this chapter, so the code is omitted.</p>
<p>#5.6 Affine/Softmax layer implementation</p>
<h2 id="561-affine-layer">5.6.1 Affine layer</h2>
<p>The calculation process of the back-propagation of Affine layer (5.13) is omitted, but @yuyasat [calculates components of back-propagation of Affine layer in a steady manner] The calculation process is summarized in d9cdd4401221df5375b6) for reference.</p>
<h2 id="562-batch-version-of-affine-layer">5.6.2 Batch version of Affine layer</h2>
<p>Below is the implemented code. In the code of the book, there is a consideration when the input data is a tensor (four-dimensional data), but it is not included because the usage is not known yet.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python:affine.py" data-lang="python:affine.py"><span style="color:#75715e"># coding: utf-8</span>
<span style="color:#f92672">import</span> numpy <span style="color:#f92672">as</span> np


<span style="color:#66d9ef">class</span> <span style="color:#a6e22e">Affine</span>:

    <span style="color:#66d9ef">def</span> __init__(self, W, b):
        <span style="color:#e6db74">&#34;&#34;&#34;Affine layer
</span><span style="color:#e6db74">        
</span><span style="color:#e6db74">        Args:
</span><span style="color:#e6db74">            W (numpy.ndarray): Weight
</span><span style="color:#e6db74">            b (numpy.ndarray): bias
</span><span style="color:#e6db74">        &#34;&#34;&#34;</span>
        self<span style="color:#f92672">.</span>W <span style="color:#f92672">=</span> W <span style="color:#75715e"># weight</span>
        self<span style="color:#f92672">.</span>b <span style="color:#f92672">=</span> b <span style="color:#75715e"># bias</span>
        self<span style="color:#f92672">.</span>x <span style="color:#f92672">=</span> None <span style="color:#75715e"># input</span>
        self<span style="color:#f92672">.</span>dW <span style="color:#f92672">=</span> None <span style="color:#75715e"># Weight derivative</span>
        self<span style="color:#f92672">.</span>db <span style="color:#f92672">=</span> None <span style="color:#75715e"># derivative of bias</span>

    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x):
        <span style="color:#e6db74">&#34;&#34;&#34; forward propagation
</span><span style="color:#e6db74">        
</span><span style="color:#e6db74">        Args:
</span><span style="color:#e6db74">            x (numpy.ndarray): input
</span><span style="color:#e6db74">            
</span><span style="color:#e6db74">        Returns:
</span><span style="color:#e6db74">            numpy.ndarray: output
</span><span style="color:#e6db74">        &#34;&#34;&#34;</span>
        self<span style="color:#f92672">.</span>x <span style="color:#f92672">=</span> x
        out <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>dot(x, self<span style="color:#f92672">.</span>W) <span style="color:#f92672">+</span> self<span style="color:#f92672">.</span>b

        <span style="color:#66d9ef">return</span> out

    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">backward</span>(self, dout):
        <span style="color:#e6db74">&#34;&#34;&#34; backpropagation
</span><span style="color:#e6db74">        
</span><span style="color:#e6db74">        Args:
</span><span style="color:#e6db74">            dout (numpy.ndarray): differential value transmitted from the right layer
</span><span style="color:#e6db74">
</span><span style="color:#e6db74">        Returns:
</span><span style="color:#e6db74">            numpy.ndarray: derivative value
</span><span style="color:#e6db74">        &#34;&#34;&#34;</span>
        dx <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>dot(dout, self<span style="color:#f92672">.</span>W<span style="color:#f92672">.</span>T)
        self<span style="color:#f92672">.</span>dW <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>dot(self<span style="color:#f92672">.</span>x<span style="color:#f92672">.</span>T, dout)self<span style="color:#f92672">.</span>db <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>sum(dout, axis<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)

        <span style="color:#66d9ef">return</span> dx
</code></pre></div><h2 id="563-softmax-with-loss-layer">5.6.3 Softmax-with-Loss layer</h2>
<p>Backpropagation of the layer with the softmax function and cross entropy error as a set.
The calculation process is described in detail in Appendix A of the book, but even if $ e^x $ is differentiated, it becomes $ e^x $, and if all one-hot vectors of teacher labels are added, it becomes 1. And others contribute to simplifying the formula.</p>
<p>The final result of $ (y_1-t_1,y_2-t_2,y_3-t_3) $ is surprising. It certainly shows the difference between the output of the neural network and the teacher label, and the calculation can be done at high speed. It seems that the cross entropy error is designed to be &ldquo;clean&rdquo; in this way when it is used as the loss function of the softmax function, and the person who considered the cross entropy error is really amazing.</p>
<p>Note that the backpropagation value must be divided by the number of batches to make it compatible with batches. In this book, there was only the explanation that &ldquo;the error per data is propagated to the previous layer by dividing the value to be propagated by the number of batches (batch_size)&rdquo;, and it was not easy to understand why it was necessary to divide by the number of batches. However, I was able to understand it from @Yoko303&rsquo;s explanation of the code <a href="https://qiita.com/Yoko303/items/09efd10161d0a764833d">Deep Learning from Soft Zero ~ Softmax-with-Loss Layer ~</a>.Inthebatchversionofforwardpropagation,thecrossentropyerrorisfinallysummedanddividedbythenumberofbatches(batch_size) to make one value. Even for back propagation, this part needs to be calculated, and its differential value is $ \frac{1}{batch_size} $. Below, I wrote a calculation graph for that part.</p>
<p><img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/139624/00f5667e-3ea9-8993-60d7-d5626bf71ea8.png" alt="Calculation Graph.png"></p>
<p>I think this understanding is correct, but please point out any mistakes.
Below is the implemented code.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python:softmax_with_loss.py" data-lang="python:softmax_with_loss.py"><span style="color:#75715e"># coding: utf-8</span>
<span style="color:#f92672">from</span> functions <span style="color:#f92672">import</span> softmax, cross_entropy_error


<span style="color:#66d9ef">class</span> <span style="color:#a6e22e">SoftmaxWithLoss</span>:
    <span style="color:#66d9ef">def</span> __init__(self):
        <span style="color:#e6db74">&#34;&#34;&#34;Softmax-with-Loss layer
</span><span style="color:#e6db74">        &#34;&#34;&#34;</span>
        self<span style="color:#f92672">.</span>loss <span style="color:#f92672">=</span> None <span style="color:#75715e"># loss</span>
        self<span style="color:#f92672">.</span>y <span style="color:#f92672">=</span> None <span style="color:#75715e"># softmax output</span>
        self<span style="color:#f92672">.</span>t <span style="color:#f92672">=</span> None <span style="color:#75715e"># Teacher data (one-hot vector)</span>

    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x, t):
        <span style="color:#e6db74">&#34;&#34;&#34; forward propagation
</span><span style="color:#e6db74">        
</span><span style="color:#e6db74">        Args:
</span><span style="color:#e6db74">            x (numpy.ndarray): input
</span><span style="color:#e6db74">            t (numpy.ndarray): Teacher data
</span><span style="color:#e6db74">
</span><span style="color:#e6db74">        Returns:
</span><span style="color:#e6db74">            float: cross entropy error
</span><span style="color:#e6db74">        &#34;&#34;&#34;</span>
        self<span style="color:#f92672">.</span>t <span style="color:#f92672">=</span> t
        self<span style="color:#f92672">.</span>y <span style="color:#f92672">=</span> softmax(x)
        self<span style="color:#f92672">.</span>loss <span style="color:#f92672">=</span> cross_entropy_error(self<span style="color:#f92672">.</span>y, self<span style="color:#f92672">.</span>t)

        <span style="color:#66d9ef">return</span> self<span style="color:#f92672">.</span>loss

    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">backward</span>(self, dout<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>):
        <span style="color:#e6db74">&#34;&#34;&#34; backpropagation
</span><span style="color:#e6db74">        
</span><span style="color:#e6db74">        Args:
</span><span style="color:#e6db74">            dout (float, optional): Differential value transmitted from the right layer. The default is 1.
</span><span style="color:#e6db74">
</span><span style="color:#e6db74">        Returns:
</span><span style="color:#e6db74">            numpy.ndarray: derivative value
</span><span style="color:#e6db74">        &#34;&#34;&#34;</span>
        batch_size <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>t<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>] <span style="color:#75715e"># number of batches</span>
        dx <span style="color:#f92672">=</span> (self<span style="color:#f92672">.</span>y<span style="color:#f92672">-</span>self<span style="color:#f92672">.</span>t) <span style="color:#f92672">*</span> (dout <span style="color:#f92672">/</span> batch_size)

        <span style="color:#66d9ef">return</span> dx
</code></pre></div><p>Note that the code in the book does not use <code>dout</code> in <code>backward</code>. Since <code>dout</code> is used only with 1 specification, there is no problem in operation, but I think it is probably a mistake.</p>
<p>#5.7 Implementation of error backpropagation</p>
<h2 id="571-overall-view-of-learning-neural-network">5.7.1 Overall view of learning neural network</h2>
<p>This is a review of the implementation flow. There is no particular stumbling block.</p>
<h2 id="572-implementation-of-neural-network-supporting-back-propagation-method">5.7.2 Implementation of neural network supporting back propagation method</h2>
<p>First is the implementation of a generic function. I brought only what I needed from the ones I wrote in the previous chapter.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python:functions.py" data-lang="python:functions.py"><span style="color:#75715e"># coding: utf-8</span>
<span style="color:#f92672">import</span> numpy <span style="color:#f92672">as</span> np


<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">softmax</span>(x):
    <span style="color:#e6db74">&#34;&#34;&#34; softmax function
</span><span style="color:#e6db74">    
</span><span style="color:#e6db74">    Args:
</span><span style="color:#e6db74">        x (numpy.ndarray): input
</span><span style="color:#e6db74">    
</span><span style="color:#e6db74">    Returns:
</span><span style="color:#e6db74">        numpy.ndarray: output
</span><span style="color:#e6db74">    &#34;&#34;&#34;</span>
    <span style="color:#75715e"># In case of batch processing, x is a 2D array of (number of batches, 10).</span>
    <span style="color:#75715e"># In this case, you have to use broadcast to successfully calculate for each image.</span>
    <span style="color:#75715e"># Here, np.max() and np.sum() are calculated with axis=-1 so that they can be used in both one and two dimensions.</span>
    <span style="color:#75715e"># Keep dimensions with keepdims=True so that you can broadcast as is.</span>
    c <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>max(x, axis<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>, keepdims<span style="color:#f92672">=</span>True)
    exp_a <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>exp(x<span style="color:#f92672">-</span>c) <span style="color:#75715e"># Overflow protection</span>
    sum_exp_a <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>sum(exp_a, axis<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>, keepdims<span style="color:#f92672">=</span>True)
    y <span style="color:#f92672">=</span> exp_a <span style="color:#f92672">/</span> sum_exp_a
    <span style="color:#66d9ef">return</span> y


<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">numerical_gradient</span>(f, x):
    <span style="color:#e6db74">&#34;&#34;&#34; Gradient calculation
</span><span style="color:#e6db74">    
</span><span style="color:#e6db74">    Args:
</span><span style="color:#e6db74">        f (function): Loss function
</span><span style="color:#e6db74">        x (numpy.ndarray): array of weight parameters for which you want to examine the gradient
</span><span style="color:#e6db74">    
</span><span style="color:#e6db74">    Returns:
</span><span style="color:#e6db74">        numpy.ndarray: gradient
</span><span style="color:#e6db74">    &#34;&#34;&#34;</span>
    h <span style="color:#f92672">=</span> <span style="color:#ae81ff">1e-4</span>
    grad <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>zeros_like(x)

    <span style="color:#75715e"># Enumerate elements of multidimensional array with np.nditer</span>
    it <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>nditer(x, flags<span style="color:#f92672">=</span>[<span style="color:#e6db74">&#39;multi_index&#39;</span>])
    <span style="color:#66d9ef">while</span> <span style="color:#f92672">not</span> it<span style="color:#f92672">.</span>finished:

        idx <span style="color:#f92672">=</span> it<span style="color:#f92672">.</span>multi_index <span style="color:#75715e"># it.multi_index is the element number in the enumeration</span>
        tmp_val <span style="color:#f92672">=</span> x[idx] <span style="color:#75715e"># save original value</span>

        <span style="color:#75715e">#f(x + h) calculation</span>
        x[idx] <span style="color:#f92672">=</span> tmp_val <span style="color:#f92672">+</span> h
        fxh1 <span style="color:#f92672">=</span> f()

        <span style="color:#75715e">#f(x-h) calculation</span>
        x[idx] <span style="color:#f92672">=</span> tmp_val<span style="color:#f92672">-</span>h
        fxh2 <span style="color:#f92672">=</span> f()

        <span style="color:#75715e">#Calculate slope</span>
        grad[idx] <span style="color:#f92672">=</span> (fxh1<span style="color:#f92672">-</span>fxh2) <span style="color:#f92672">/</span> (<span style="color:#ae81ff">2</span> <span style="color:#f92672">*</span> h)
    
        x[idx] <span style="color:#f92672">=</span> tmp_val <span style="color:#75715e"># returns value</span>
        it<span style="color:#f92672">.</span>iternext()

    <span style="color:#66d9ef">return</span> grad


<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">cross_entropy_error</span>(y, t):
    <span style="color:#e6db74">&#34;&#34;&#34;Calculation of cross entropy error
</span><span style="color:#e6db74">    
</span><span style="color:#e6db74">    Args:
</span><span style="color:#e6db74">        y (numpy.ndarray): Neural network output
</span><span style="color:#e6db74">        t (numpy.ndarray): correct answer label
</span><span style="color:#e6db74">    
</span><span style="color:#e6db74">    Returns:
</span><span style="color:#e6db74">        float: cross entropy error
</span><span style="color:#e6db74">    &#34;&#34;&#34;</span>

    <span style="color:#75715e">#Shape the shape when there is only one data (1 line per data)</span>
    <span style="color:#66d9ef">if</span> y<span style="color:#f92672">.</span>ndim <span style="color:#f92672">==</span> <span style="color:#ae81ff">1</span>:
        t <span style="color:#f92672">=</span> t<span style="color:#f92672">.</span>reshape(<span style="color:#ae81ff">1</span>, t<span style="color:#f92672">.</span>size)
        y <span style="color:#f92672">=</span> y<span style="color:#f92672">.</span>reshape(<span style="color:#ae81ff">1</span>, y<span style="color:#f92672">.</span>size)

    <span style="color:#75715e"># Calculate error and normalize by batch number</span>
    batch_size <span style="color:#f92672">=</span> y<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>]
    <span style="color:#66d9ef">return</span> <span style="color:#f92672">-</span>np<span style="color:#f92672">.</span>sum(t <span style="color:#f92672">*</span> np<span style="color:#f92672">.</span>log(y <span style="color:#f92672">+</span> <span style="color:#ae81ff">1e-7</span>)) <span style="color:#f92672">/</span> batch_size
</code></pre></div><p>And the class of neural networks. Since it is based on the code in the previous chapter, many parts are the same.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python:two_layer_net.py" data-lang="python:two_layer_net.py"><span style="color:#75715e"># coding: utf-8</span>
<span style="color:#f92672">import</span> numpy <span style="color:#f92672">as</span> np
<span style="color:#f92672">from</span> affine <span style="color:#f92672">import</span> Affine
<span style="color:#f92672">from</span> functions <span style="color:#f92672">import</span> numerical_gradient
<span style="color:#f92672">from</span> relu <span style="color:#f92672">import</span> ReLU
<span style="color:#f92672">from</span> softmax_with_loss <span style="color:#f92672">import</span> SoftmaxWithLoss


<span style="color:#66d9ef">class</span> <span style="color:#a6e22e">TwoLayerNet</span>:

    <span style="color:#66d9ef">def</span> __init__(self, input_size, hidden_size, output_size,
                 weight_init_std<span style="color:#f92672">=</span><span style="color:#ae81ff">0.01</span>):
        <span style="color:#e6db74">&#34;&#34;&#34; Two layer neural network
</span><span style="color:#e6db74">        
</span><span style="color:#e6db74">        Args:
</span><span style="color:#e6db74">            input_size (int): Number of neurons in the input layer
</span><span style="color:#e6db74">            hidden_size (int): number of hidden layer neurons
</span><span style="color:#e6db74">            output_size (int): Number of neurons in output layer
</span><span style="color:#e6db74">            weight_init_std (float, optional): Tuning parameter for initial value of weight. The default is 0.01.
</span><span style="color:#e6db74">        &#34;&#34;&#34;</span>

        <span style="color:#75715e"># Weight initialization</span>
        self<span style="color:#f92672">.</span>params <span style="color:#f92672">=</span> {}
        self<span style="color:#f92672">.</span>params[<span style="color:#e6db74">&#39;W1&#39;</span>] <span style="color:#f92672">=</span> weight_init_std <span style="color:#f92672">*</span> \
            np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>randn(input_size, hidden_size)
        self<span style="color:#f92672">.</span>params[<span style="color:#e6db74">&#39;b1&#39;</span>] <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>zeros(hidden_size)
        self<span style="color:#f92672">.</span>params[<span style="color:#e6db74">&#39;W2&#39;</span>] <span style="color:#f92672">=</span> weight_init_std <span style="color:#f92672">*</span> \
            np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>randn(hidden_size, output_size)
        self<span style="color:#f92672">.</span>params[<span style="color:#e6db74">&#39;b2&#39;</span>] <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>zeros(output_size)

        <span style="color:#75715e"># Layer generation</span>
        self<span style="color:#f92672">.</span>layers <span style="color:#f92672">=</span> {} <span style="color:#75715e"># Since Python 3.7, the storage order of the dictionary is retained, so OrderedDict is not required.self.layers[&#39;Affine1&#39;] = \</span>
            Affine(self<span style="color:#f92672">.</span>params[<span style="color:#e6db74">&#39;W1&#39;</span>], self<span style="color:#f92672">.</span>params[<span style="color:#e6db74">&#39;b1&#39;</span>])
        self<span style="color:#f92672">.</span>layers[<span style="color:#e6db74">&#39;Relu1&#39;</span>] <span style="color:#f92672">=</span> ReLU()
        self<span style="color:#f92672">.</span>layers[<span style="color:#e6db74">&#39;Affine2&#39;</span>] <span style="color:#f92672">=</span> \
            Affine(self<span style="color:#f92672">.</span>params[<span style="color:#e6db74">&#39;W2&#39;</span>], self<span style="color:#f92672">.</span>params[<span style="color:#e6db74">&#39;b2&#39;</span>])
    
        self<span style="color:#f92672">.</span>lastLayer <span style="color:#f92672">=</span> SoftmaxWithLoss()

    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">predict</span>(self, x):
        <span style="color:#e6db74">&#34;&#34;&#34; Neural network inference
</span><span style="color:#e6db74">        
</span><span style="color:#e6db74">        Args:
</span><span style="color:#e6db74">            x (numpy.ndarray): Input to neural network
</span><span style="color:#e6db74">        
</span><span style="color:#e6db74">        Returns:
</span><span style="color:#e6db74">            numpy.ndarray: neural network output
</span><span style="color:#e6db74">        &#34;&#34;&#34;</span>
        <span style="color:#75715e"># Propagate layer forward</span>
        <span style="color:#66d9ef">for</span> layer <span style="color:#f92672">in</span> self<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>values():
            x <span style="color:#f92672">=</span> layer<span style="color:#f92672">.</span>forward(x)

        <span style="color:#66d9ef">return</span> x

    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">loss</span>(self, x, t):
        <span style="color:#e6db74">&#34;&#34;&#34; Loss function value calculation
</span><span style="color:#e6db74">        
</span><span style="color:#e6db74">        Args:
</span><span style="color:#e6db74">            x (numpy.ndarray): Input to neural network
</span><span style="color:#e6db74">            t (numpy.ndarray): correct answer label
</span><span style="color:#e6db74">
</span><span style="color:#e6db74">        Returns:
</span><span style="color:#e6db74">            float: value of loss function
</span><span style="color:#e6db74">        &#34;&#34;&#34;</span>
        <span style="color:#75715e"># Inference</span>
        y <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>predict(x)

        <span style="color:#75715e"># Softmax-with-Loss Layer forward calculation</span>
        loss <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>lastLayer<span style="color:#f92672">.</span>forward(y, t)

        <span style="color:#66d9ef">return</span> loss

    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">accuracy</span>(self, x, t):
        <span style="color:#e6db74">&#34;&#34;&#34; Calculation of recognition accuracy
</span><span style="color:#e6db74">        
</span><span style="color:#e6db74">        Args:
</span><span style="color:#e6db74">            x (numpy.ndarray): Input to neural network
</span><span style="color:#e6db74">            t (numpy.ndarray): correct answer label
</span><span style="color:#e6db74">        
</span><span style="color:#e6db74">        Returns:
</span><span style="color:#e6db74">            float: recognition accuracy
</span><span style="color:#e6db74">        &#34;&#34;&#34;</span>
        y <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>predict(x)
        y <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>argmax(y, axis<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
        t <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>argmax(t, axis<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
        
        accuracy <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>sum(y <span style="color:#f92672">==</span> t) <span style="color:#f92672">/</span> x<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>]
        <span style="color:#66d9ef">return</span> accuracy

    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">numerical_gradient</span>(self, x, t):
        <span style="color:#e6db74">&#34;&#34;&#34; Calculate the gradient for the weight parameter by numerical differentiation
</span><span style="color:#e6db74">        
</span><span style="color:#e6db74">        Args:
</span><span style="color:#e6db74">            x (numpy.ndarray): Input to neural network
</span><span style="color:#e6db74">            t (numpy.ndarray): correct answer label
</span><span style="color:#e6db74">        
</span><span style="color:#e6db74">        Returns:
</span><span style="color:#e6db74">            dictionary: dictionary that stores the gradient
</span><span style="color:#e6db74">        &#34;&#34;&#34;</span>
        grads <span style="color:#f92672">=</span> {}
        grads[<span style="color:#e6db74">&#39;W1&#39;</span>] <span style="color:#f92672">=</span> \
            numerical_gradient(<span style="color:#66d9ef">lambda</span>: self<span style="color:#f92672">.</span>loss(x, t), self<span style="color:#f92672">.</span>params[<span style="color:#e6db74">&#39;W1&#39;</span>])
        grads[<span style="color:#e6db74">&#39;b1&#39;</span>] <span style="color:#f92672">=</span> \
            numerical_gradient(<span style="color:#66d9ef">lambda</span>: self<span style="color:#f92672">.</span>loss(x, t), self<span style="color:#f92672">.</span>params[<span style="color:#e6db74">&#39;b1&#39;</span>])
        grads[<span style="color:#e6db74">&#39;W2&#39;</span>] <span style="color:#f92672">=</span> \
            numerical_gradient(<span style="color:#66d9ef">lambda</span>: self<span style="color:#f92672">.</span>loss(x, t), self<span style="color:#f92672">.</span>params[<span style="color:#e6db74">&#39;W2&#39;</span>])
        grads[<span style="color:#e6db74">&#39;b2&#39;</span>] <span style="color:#f92672">=</span> \
            numerical_gradient(<span style="color:#66d9ef">lambda</span>: self<span style="color:#f92672">.</span>loss(x, t), self<span style="color:#f92672">.</span>params[<span style="color:#e6db74">&#39;b2&#39;</span>])

        <span style="color:#66d9ef">return</span> grads

    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">gradient</span>(self, x, t):
        <span style="color:#e6db74">&#34;&#34;&#34; Gradient for weight parameter is calculated by error backpropagation method
</span><span style="color:#e6db74">        
</span><span style="color:#e6db74">         Args:
</span><span style="color:#e6db74">            x (numpy.ndarray): Input to neural network
</span><span style="color:#e6db74">            t (numpy.ndarray): correct answer label
</span><span style="color:#e6db74">        
</span><span style="color:#e6db74">        Returns:
</span><span style="color:#e6db74">            dictionary: dictionary that stores the gradient
</span><span style="color:#e6db74">        &#34;&#34;&#34;</span>
        <span style="color:#75715e">#Propagation</span>
        self<span style="color:#f92672">.</span>loss(x, t) <span style="color:#75715e"># Propagate forward to calculate the loss value</span>

        <span style="color:#75715e">#Backpropagation</span>
        dout <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>lastLayer<span style="color:#f92672">.</span>backward()
        <span style="color:#66d9ef">for</span> layer <span style="color:#f92672">in</span> reversed(list(self<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>values())):
            dout <span style="color:#f92672">=</span> layer<span style="color:#f92672">.</span>backward(dout)

        <span style="color:#75715e"># Extract differential value of each layer</span>
        grads <span style="color:#f92672">=</span> {}
        grads[<span style="color:#e6db74">&#39;W1&#39;</span>] <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>layers[<span style="color:#e6db74">&#39;Affine1&#39;</span>]<span style="color:#f92672">.</span>dW
        grads[<span style="color:#e6db74">&#39;b1&#39;</span>] <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>layers[<span style="color:#e6db74">&#39;Affine1&#39;</span>]<span style="color:#f92672">.</span>db
        grads[<span style="color:#e6db74">&#39;W2&#39;</span>] <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>layers[<span style="color:#e6db74">&#39;Affine2&#39;</span>]<span style="color:#f92672">.</span>dW
        grads[<span style="color:#e6db74">&#39;b2&#39;</span>] <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>layers[<span style="color:#e6db74">&#39;Affine2&#39;</span>]<span style="color:#f92672">.</span>db

        <span style="color:#66d9ef">return</span> grads
</code></pre></div><p>Note that the code in the book uses <code>OrderedDict</code>, but here we use a normal <code>dict</code>. Since Python 3.7, the insertion order of <code>dict</code> objects is saved <sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>.</p>
<h2 id="573-checking-the-gradient-of-the-backpropagation-method">5.7.3 Checking the gradient of the backpropagation method</h2>
<p>This code compares the gradient obtained by the back propagation method with the gradient obtained by numerical differentiation.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python:gradient_check.py" data-lang="python:gradient_check.py"><span style="color:#75715e"># coding: utf-8</span>
<span style="color:#f92672">import</span> os
<span style="color:#f92672">import</span> sys
<span style="color:#f92672">import</span> numpy <span style="color:#f92672">as</span> np
<span style="color:#f92672">from</span> two_layer_net <span style="color:#f92672">import</span> TwoLayerNet
sys<span style="color:#f92672">.</span>path<span style="color:#f92672">.</span>append(os<span style="color:#f92672">.</span>pardir) <span style="color:#75715e"># add parent directory to path</span>
<span style="color:#f92672">from</span> dataset.mnist <span style="color:#f92672">import</span> load_mnist


<span style="color:#75715e"># Load MNIST training and test data</span>
(x_train, t_train), (x_test, t_test) <span style="color:#f92672">=</span> \
    load_mnist(normalize<span style="color:#f92672">=</span>True, one_hot_label<span style="color:#f92672">=</span>True)

<span style="color:#75715e"># 2 layer neural work generation</span>
network <span style="color:#f92672">=</span> TwoLayerNet(input_size<span style="color:#f92672">=</span><span style="color:#ae81ff">784</span>, hidden_size<span style="color:#f92672">=</span><span style="color:#ae81ff">50</span>, output_size<span style="color:#f92672">=</span><span style="color:#ae81ff">10</span>)

<span style="color:#75715e"># Data preparation for verification</span>
x_batch <span style="color:#f92672">=</span> x_train[:<span style="color:#ae81ff">3</span>]
t_batch <span style="color:#f92672">=</span> t_train[:<span style="color:#ae81ff">3</span>]

<span style="color:#75715e"># Gradient calculation by numerical differentiation and back propagation method</span>
grad_numerical <span style="color:#f92672">=</span> network<span style="color:#f92672">.</span>numerical_gradient(x_batch, t_batch)
grad_backprop <span style="color:#f92672">=</span> network<span style="color:#f92672">.</span>gradient(x_batch, t_batch)

<span style="color:#75715e"># Check the difference of each weight</span>
<span style="color:#66d9ef">for</span> key <span style="color:#f92672">in</span> grad_numerical<span style="color:#f92672">.</span>keys():
    <span style="color:#75715e">#Calculating the absolute value of the difference</span>
    diff <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>abs(grad_backprop[key]<span style="color:#f92672">-</span>grad_numerical[key])
    <span style="color:#75715e"># Show average and maximum</span>
    <span style="color:#66d9ef">print</span>(f<span style="color:#e6db74">&#34;{key}: [average of differences]{np.average(diff):.10f} [maximum differences]{np.max(diff):.10f}&#34;</span>)
</code></pre></div><p>In the book, I only checked the average absolute difference, but I also checked the maximum absolute difference.</p>
<pre><code>W1: [average of difference] 0.0000000003 [maximum difference] 0.0000000080
b1: [average of differences] 0.0000000021 [maximum differences] 0.0000000081
W2: [Average difference] 0.0000000063 [Maximum difference] 0.0000000836
b2: [Average difference] 0.0000001394 [Maximum difference] 0.0000002334
</code></pre><p>Since <code>b2</code> is a value with about 7 digits after the decimal point, it seems that the error is a little larger than in the book. There may be a bad point in the implementation. If you have any questions, please point them out: sweat:</p>
<h2 id="574-learning-using-error-backpropagation">5.7.4 Learning using error backpropagation</h2>
<p>Below is the learning code.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python:mnist.py" data-lang="python:mnist.py"><span style="color:#75715e"># coding: utf-8</span>
<span style="color:#f92672">import</span> os
<span style="color:#f92672">import</span> sys
<span style="color:#f92672">import</span> matplotlib.pylab <span style="color:#f92672">as</span> plt
<span style="color:#f92672">import</span> numpy <span style="color:#f92672">as</span> np
<span style="color:#f92672">from</span> two_layer_net <span style="color:#f92672">import</span> TwoLayerNet
sys<span style="color:#f92672">.</span>path<span style="color:#f92672">.</span>append(os<span style="color:#f92672">.</span>pardir) <span style="color:#75715e"># add parent directory to path</span>
<span style="color:#f92672">from</span> dataset.mnist <span style="color:#f92672">import</span> load_mnist


<span style="color:#75715e"># Load MNIST training and test data</span>
(x_train, t_train), (x_test, t_test) <span style="color:#f92672">=</span> \
    load_mnist(normalize<span style="color:#f92672">=</span>True, one_hot_label<span style="color:#f92672">=</span>True)

<span style="color:#75715e"># Hyper parameter setting</span>
iters_num <span style="color:#f92672">=</span> <span style="color:#ae81ff">10000</span> <span style="color:#75715e"># Number of updates</span>
batch_size <span style="color:#f92672">=</span> <span style="color:#ae81ff">100</span> <span style="color:#75715e"># batch size</span>
learning_rate <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.1</span> <span style="color:#75715e"># learning rate</span>

<span style="color:#75715e"># Result record list</span>
train_loss_list <span style="color:#f92672">=</span> [] <span style="color:#75715e"># Transition of loss function value</span>
train_acc_list <span style="color:#f92672">=</span> [] <span style="color:#75715e"># Recognition accuracy for training data</span>
test_acc_list <span style="color:#f92672">=</span> [] <span style="color:#75715e"># Recognition accuracy for test data</span>

train_size <span style="color:#f92672">=</span> x_train<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>] <span style="color:#75715e"># size of training data</span>
iter_per_epoch <span style="color:#f92672">=</span> max(int(train_size <span style="color:#f92672">/</span> batch_size), <span style="color:#ae81ff">1</span>) <span style="color:#75715e"># 1 number of iterations per epoch</span>

<span style="color:#75715e"># 2 layer neural work generation</span>
network <span style="color:#f92672">=</span> TwoLayerNet(input_size<span style="color:#f92672">=</span><span style="color:#ae81ff">784</span>, hidden_size<span style="color:#f92672">=</span><span style="color:#ae81ff">50</span>, output_size<span style="color:#f92672">=</span><span style="color:#ae81ff">10</span>)

<span style="color:#75715e"># Start learning</span>
<span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(iters_num):

    <span style="color:#75715e"># Mini batch generationbatch_mask = np.random.choice(train_size, batch_size, replace=False)</span>
    x_batch <span style="color:#f92672">=</span> x_train[batch_mask]
    t_batch <span style="color:#f92672">=</span> t_train[batch_mask]

    <span style="color:#75715e">#Gradient calculation</span>
    grad <span style="color:#f92672">=</span> network<span style="color:#f92672">.</span>gradient(x_batch, t_batch)

    <span style="color:#75715e"># Update weight parameter</span>
    <span style="color:#66d9ef">for</span> key <span style="color:#f92672">in</span> (<span style="color:#e6db74">&#39;W1&#39;</span>,<span style="color:#e6db74">&#39;b1&#39;</span>,<span style="color:#e6db74">&#39;W2&#39;</span>,<span style="color:#e6db74">&#39;b2&#39;</span>):
        network<span style="color:#f92672">.</span>params[key] <span style="color:#f92672">-=</span> learning_rate <span style="color:#f92672">*</span> grad[key]
    
    <span style="color:#75715e"># Loss function value calculation</span>
    loss <span style="color:#f92672">=</span> network<span style="color:#f92672">.</span>loss(x_batch, t_batch)
    train_loss_list<span style="color:#f92672">.</span>append(loss)

    <span style="color:#75715e">#1 Calculation of recognition accuracy for each epoch</span>
    <span style="color:#66d9ef">if</span> i <span style="color:#f92672">%</span>iter_per_epoch <span style="color:#f92672">==</span> <span style="color:#ae81ff">0</span>:
        train_acc <span style="color:#f92672">=</span> network<span style="color:#f92672">.</span>accuracy(x_train, t_train)
        test_acc <span style="color:#f92672">=</span> network<span style="color:#f92672">.</span>accuracy(x_test, t_test)
        train_acc_list<span style="color:#f92672">.</span>append(train_acc)
        test_acc_list<span style="color:#f92672">.</span>append(test_acc)

        <span style="color:#75715e"># Progress display</span>
        <span style="color:#66d9ef">print</span>(f<span style="color:#e6db74">&#39;[number of updates]{i:&gt;4} [value of loss function]{loss:.4f} &#39;</span>
              f<span style="color:#e6db74">&#39;[Recognition accuracy of training data]{train_acc:.4f} [Recognition accuracy of test data]{test_acc:.4f}&#39;</span>)

<span style="color:#75715e"># Draw the transition of the value of the loss function</span>
x <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>arange(len(train_loss_list))
plt<span style="color:#f92672">.</span>plot(x, train_loss_list, label<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;loss&#39;</span>)
plt<span style="color:#f92672">.</span>xlabel(<span style="color:#e6db74">&#39;iteration&#39;</span>)
plt<span style="color:#f92672">.</span>ylabel(<span style="color:#e6db74">&#39;loss&#39;</span>)
plt<span style="color:#f92672">.</span>xlim(left<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)
plt<span style="color:#f92672">.</span>ylim(bottom<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)
plt<span style="color:#f92672">.</span>show()

<span style="color:#75715e">#Draw the transition of recognition accuracy of training data and test data</span>
x2 <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>arange(len(train_acc_list))
plt<span style="color:#f92672">.</span>plot(x2, train_acc_list, label<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;train acc&#39;</span>)
plt<span style="color:#f92672">.</span>plot(x2, test_acc_list, label<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;test acc&#39;</span>, linestyle<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;--&#39;</span>)
plt<span style="color:#f92672">.</span>xlabel(<span style="color:#e6db74">&#39;epochs&#39;</span>)
plt<span style="color:#f92672">.</span>ylabel(<span style="color:#e6db74">&#39;accuracy&#39;</span>)
plt<span style="color:#f92672">.</span>xlim(left<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)
plt<span style="color:#f92672">.</span>ylim(<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1.0</span>)
plt<span style="color:#f92672">.</span>legend(loc<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;lower right&#39;</span>)
plt<span style="color:#f92672">.</span>show()
</code></pre></div><p>And it is the same result as the previous chapter.</p>
<pre><code>[Number of updates] 0 [Loss function value] 2.3008 [Training data recognition accuracy] 0.0926 [Test data recognition accuracy] 0.0822
[Number of updates] 600 [Value of loss function] 0.2575 [Recognition accuracy of training data] 0.9011 [Recognition accuracy of test data] 0.9068
[Number of updates] 1200 [Loss function value] 0.2926 [Training data recognition accuracy] 0.9219 [Test data recognition accuracy] 0.9242
[Number of updates] 1800 [Loss function value] 0.2627 [Recognition accuracy of training data] 0.9324 [Recognition accuracy of test data] 0.9341
[Number of updates] 2400 [Value of loss function] 0.0899 [Recognition accuracy of training data] 0.9393 [Recognition accuracy of test data] 0.9402
[Number of updates] 3000 [Value of loss function] 0.1096 [Recognition accuracy of training data] 0.9500 [Recognition accuracy of test data] 0.9483
[Number of updates] 3600 [Loss function value] 0.1359 [Training data recognition accuracy] 0.9559 [Test data recognition accuracy] 0.9552
[Number of updates] 4200 [Loss function value] 0.1037 [Training data recognition accuracy] 0.9592 [Test data recognition accuracy] 0.9579
[Number of updates] 4800 [Loss function value] 0.1065 [Training data recognition accuracy] 0.9639 [Test data recognition accuracy] 0.9600
[Number of updates] 5400 [Value of loss function] 0.0419 [Recognition accuracy of training data] 0.9665 [Recognition accuracy of test data] 0.9633
[Number of updates] 6000 [Loss function value] 0.0393 [Training data recognition accuracy] 0.9698 [Test data recognition accuracy] 0.9649
[Number of updates] 6600 [Loss function value] 0.0575 [Recognition accuracy of training data] 0.9718 [Recognition accuracy of test data] 0.9663
[Number of updates] 7200 [Loss function value] 0.0850 [Recognition accuracy of training data] 0.9728 [Recognition accuracy of test data] 0.9677
[Number of updates] 7800 [Value of loss function] 0.0403 [Recognition accuracy of training data] 0.9749 [Recognition accuracy of test data] 0.9686
[Number of updates] 8400 [Loss function value] 0.0430 [Training data recognition accuracy] 0.9761 [Test data recognition accuracy] 0.9685
[Number of updates] 9000 [Loss function value] 0.0513 [Recognition accuracy of training data] 0.9782 [Recognition accuracy of test data] 0.9715
[Number of updates] 9600 [Value of loss function] 0.0584 [Recognition accuracy of training data] 0.9777 [Recognition accuracy of test data] 0.9707
</code></pre><p>![Screenshot 2020-01-19 1.20.06.png](<a href="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/139624/0643261b-637f-ec84-f7a2-(9912c794fe51.png)">https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/139624/0643261b-637f-ec84-f7a2-(9912c794fe51.png)</a>
![Screenshot 2020-01-19 1.20.59.png](<a href="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/139624/34ef7d7c-5208-2759-b014-(1657ddae97c8.png)">https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/139624/34ef7d7c-5208-2759-b014-(1657ddae97c8.png)</a></p>
<p>Compared to the results in the previous chapter, the recognition accuracy increases faster. Eventually it was around 97%. Since the only difference between numerical differentiation and error backpropagation is the method of calculating the gradient, the change from the sigmoid function to the ReLU function seems to lead to improvement.</p>
<p>#5.8 Summary</p>
<p>The calculation graph is easy to understand. Moreover, it was well understood that the output layer and the loss function were designed so that the differential value could be easily obtained.</p>
<p>That&rsquo;s it for this chapter. If you have any mistakes, I would appreciate it if you could point me out.
(To other chapters of this memo: <a href="https://qiita.com/segavvy/items/1945aa1a0f91a1473555">Chapter 1</a>/<a href="https://qiita.com/segavvy/items/d8e9e70437e35083a459">Chapter2</a>/<a href="https://qiita.com/segavvy/items/6d79d0c3b4367869f4ea">Chapter3</a>/<a href="https://qiita.com/segavvy/items/bdad9fcda2f0da918e7c">Chapter4</a>/Chapter5/<a href="https://qiita.com/segavvy/items/ca4ac4c9ee1a126bff41">Chapter6</a>/<a href="https://qiita.com/segavvy/items/8541c6ae1868d9b2b805">Chapter7</a>/<a href="https://qiita.com/segavvy/items/3eb6ea0ea2ea68af68af68af2af6868">Chapter8</a>)/<a href="https://qiita.com/segavvy/items/4e8c36cac9c6f3543ffd">Summary</a>))</p>
<section class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1" role="doc-endnote">
<p>See &ldquo;Improving Python&rsquo;s data model&rdquo; in <a href="https://docs.python.org/en/3/whatsnew/3.7.html">What&rsquo;s New In Python 3.7</a>. <a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</section>

    </article>
  </div>

  
  
  
  <script>
  window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
  ga('create', 'UA-169005401-1', 'auto');
  ga('send', 'pageview');
  </script>
  <script async src='https://www.google-analytics.com/analytics.js'></script>
  

  

  
<link rel="stylesheet" type="text/css" href="//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.0/cookieconsent.min.css" />
<script src="//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.0/cookieconsent.min.js"></script>
<script>
window.addEventListener("load", function(){
window.cookieconsent.initialise({
  "palette": {
    "popup": {
      "background": "#216942",
      "text": "#b2d192"
    },
    "button": {
      "background": "#afed71"
    }
  }
})});
</script>

</body>

</html>
