<!doctype html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  
  <meta name="generator" content="Hugo 0.72.0" />

  
  <meta name="description" content="Some description">
  

  
  <link rel="apple-touch-icon" sizes="180x180" href="https://memotut.com/apple-touch-icon.png">

  
  <link rel="icon" type="image/png" sizes="32x32" href="https://memotut.com/favicon-32x32.png">

  
  <link rel="icon" type="image/png" sizes="16x16" href="https://memotut.com/favicon-16x16.png">

  
  <link rel="manifest" href="https://memotut.com/site.webmanifest">

  
  <link rel="mask-icon" href="https://memotut.com/safari-pinned-tab.svg" color="#5bbad5">

  <meta name="msapplication-TileColor" content="#da532c">

  <meta name="theme-color" content="#ffffff">

  
  <link rel="stylesheet" href="https://memotut.com/css/bootstrap.min.css" />

  
  <title>[Python] Reinforcement learning Learning from today | Memo Tut</title>
  

  <style>
body {
  min-width: 300px;
}

.custom-navbar {
  margin-bottom: 1em;
  height: 60px;
}

.custom-navbar a {
  display: inline-block; 
  padding: 18px 0;
  margin-right: 1em; 
  font-weight: bold; 
}

.custom-navbar a:hover,
.custom-navbar a:focus {
  text-decoration: none; 
}

@media print {
  .custom-navbar {
    display: none;
  }
}

article {
  padding-bottom: 1em;
}

img {
  max-width: 100%;
}


body {
  background-color: #fff;
}



body {
  color: #212529;
}



a {
  color: #007bff;
}



a:hover,
a:focus {
  color: #0056b3;
}



.custom-navbar {
  background-color: #212529;
}



.custom-navbar a {
  color: rgba(255, 255, 255, 0.75);
}



.custom-navbar a:hover,
.custom-navbar a:focus {
  color: rgba(255, 255, 255, 1);
}



.container {
  max-width: 800px;
}



pre {
  display: block;
  padding: 9.5px;
  word-break: break-all;
  word-wrap: break-word;
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 4px;
}

pre code {
  padding: 0;
  font-size: inherit;
  color: inherit; 
  white-space: pre-wrap;
  background-color: transparent;
  border: none;
  border-radius: 0;
}

code {
  padding: 2px 4px;
  color: inherit; 
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 4px;
  font-size: .9em;
}



blockquote,
.blockquote {
  padding: 10px 20px;
  margin: 0 0 20px;
  font-size: 1em;
  border-left: 5px solid #6c757d;
}

</style>
</head>

<body>
  <nav class="custom-navbar">
  <div class="container">
    
    <a href="/">Posts</a>
    
    <a href="/tags/">Tags</a>
    
    <a href="/about/">About</a>
    
    <a href="/index.xml">RSS</a>
    
  </div>
</nav>
  
  <div class="container">
    <article>
      <h1>[Python] Reinforcement learning Learning from today</h1>
<p>
  <small class="text-secondary">
  
  
  Jan 20, 2020
  </small>
  

<small><code><a href="https://memotut.com/tags/python">Python</a></code></small>

</p>
<pre><code>You will now travel from Sapporo in Hokkaido to Naha in Okinawa. With reinforcement learning.
</code></pre>
<p>There are two goals for this Japan-longitudinal trip:</p>
<ul>
<li><b> Goal 1: Reach the goal. </b> First of all, it&rsquo;s all about not reaching the goal.</li>
<li><b> Goal 2: Discover the shortest route. </b> Once you have found the route to the goal, find the shortest of them.</li>
</ul>
<p>And I would like to apply the following rules.</p>
<ul>
<li><b>A city once visited will never be visited again. </b>If you make a mistake and visit the same city twice, the game is over.</li>
</ul>
<h1 id="why-not-use-a-graph-search-method">Why not use a graph search method?</h1>
<p>Yes, that&rsquo;s right. You can easily find it by using the graph search method. For more information</p>
<ul>
<li><a href="https://qiita.com/maskot1977/items/e1819b7a1053eb9f7d61">Basics of graph theory</a></li>
<li><a href="https://qiita.com/maskot1977/items/b55c6d1c85bcf0ec0d08">Graph theory basics with matplotlib animation</a></li>
</ul>
<p>Look at But here, our goal is to learn about reinforcement learning today.</p>
<p>#Prefectural office location data in Japan</p>
<p><a href="https://qiita.com/maskot1977/items/e1819b7a1053eb9f7d61">Graph theory basis</a>,<a href="https://qiita.com/maskot1977/items/b55c6d1c85bcf0ec0d08">Graphtheorybasiswithmatplotlibanimation</a> Use the following data.</p>
<h2 id="latitudelongitude-of-prefectural-office">Latitude/longitude of prefectural office</h2>
<p>Download the latitude/longitude data of the prefectural office location.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> urllib.request
url <span style="color:#f92672">=</span><span style="color:#e6db74">&#39;https://raw.githubusercontent.com/maskot1977/ipython_notebook/master/toydata/location.txt&#39;</span>
urllib<span style="color:#f92672">.</span>request<span style="color:#f92672">.</span>urlretrieve(url,<span style="color:#e6db74">&#39;location.txt&#39;</span>) <span style="color:#75715e"># Download data</span>
</code></pre></div><pre><code>('location.txt', &lt;http.client.HTTPMessage at 0x110e78ba8&gt;)
</code></pre>
<p>Read the downloaded data.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> pandas <span style="color:#f92672">as</span> pd
location <span style="color:#f92672">=</span> pd<span style="color:#f92672">.</span>read_csv(<span style="color:#e6db74">&#39;location.txt&#39;</span>)<span style="color:#f92672">.</span>values
pd<span style="color:#f92672">.</span>DataFrame(location)
</code></pre></div><table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>0</th>
      <th>1</th>
      <th>2</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Sapporo</td>
      <td>43.0642</td>
      <td>141.347</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Aomori</td>
      <td>40.8244</td>
      <td>140.74</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Morioka</td>
      <td>39.7036</td>
      <td>141.153</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Sendai</td>
      <td>38.2689</td>
      <td>140.872</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>44</th>
      <td>Miyazaki</td>
      <td>31.9111</td>
      <td>131.424</td>
    </tr>
    <tr>
      <th>45</th>
      <td>Kagoshima</td>
      <td>31.5603</td>
      <td>130.558</td>
    </tr>
    <tr>
      <th>46</th>
      <td>Naha</td>
      <td>26.2125</td>
      <td>127.681</td>
    </tr>
  </tbody>
</table>
<p>47 rows × 3 columns</p>
<p>Visualize the loaded data.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">%</span>matplotlib inline
<span style="color:#f92672">import</span> matplotlib.pyplot <span style="color:#f92672">as</span> plt
plt<span style="color:#f92672">.</span>figure(figsize<span style="color:#f92672">=</span>(<span style="color:#ae81ff">11</span>, <span style="color:#ae81ff">9</span>))
plt<span style="color:#f92672">.</span>scatter(location[:, <span style="color:#ae81ff">2</span>], location[:, <span style="color:#ae81ff">1</span>])
<span style="color:#66d9ef">for</span> city, y, x <span style="color:#f92672">in</span> location:
    plt<span style="color:#f92672">.</span>text(x, y, city, alpha<span style="color:#f92672">=</span><span style="color:#ae81ff">0.5</span>, size<span style="color:#f92672">=</span><span style="color:#ae81ff">12</span>)
plt<span style="color:#f92672">.</span>grid()
</code></pre></div><p><img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/211162/9d229df2-5acc-ea25-a45b-87697345ebde.png" alt="output_4_1.png"></p>
<h2 id="walking-movement-between-prefectural-offices">Walking movement between prefectural offices</h2>
<p>There is data that I checked on Google map how long it takes to move between prefectural offices on foot (if you can not move on foot, ferry will be used). Download this.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">url <span style="color:#f92672">=</span><span style="color:#e6db74">&#39;https://raw.githubusercontent.com/maskot1977/ipython_notebook/master/toydata/walk.txt&#39;</span>
urllib<span style="color:#f92672">.</span>request<span style="color:#f92672">.</span>urlretrieve(url,<span style="color:#e6db74">&#39;walk.txt&#39;</span>)
</code></pre></div><pre><code>('walk.txt', &lt;http.client.HTTPMessage at 0x1220b8a58&gt;)
</code></pre>
<p>Read the downloaded data.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">walk <span style="color:#f92672">=</span> pd<span style="color:#f92672">.</span>read_csv(<span style="color:#e6db74">&#39;walk.txt&#39;</span>, sep<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;\s+&#39;</span>)
walk
</code></pre></div><table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Town1</th>
      <th>Town2</th>
      <th>Hour</th>
      <th>Ferry</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Sapporo</td>
      <td>Aomori</td>
      <td>55</td>
      <td>True</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Akita</td>
      <td>Aomori</td>
      <td>36</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Akita</td>
      <td>Sendai</td>
      <td>45</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Akita</td>
      <td>Niigata</td>
      <td>52</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>96</th>
      <td>Nagasaki</td>
      <td>Kumamoto</td>
      <td>18</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>97</th>
      <td>Nagasaki</td>
      <td>Saga</td>
      <td>20</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>98</th>
      <td>Kagoshima</td>
      <td>Naha</td>
      <td>26</td>
      <td>NaN</td>
    </tr>
  </tbody>
</table>
<p>99 rows × 4 columns</p>
<p>that? I just noticed that the ferry between Kagoshima and Naha hasn&rsquo;t become a ferry&hellip;well, this does not affect the data analysis. Let&rsquo;s visualize the relationship between cities obtained here.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">plt<span style="color:#f92672">.</span>figure(figsize<span style="color:#f92672">=</span>(<span style="color:#ae81ff">11</span>, <span style="color:#ae81ff">9</span>))
<span style="color:#66d9ef">for</span> city, y, x <span style="color:#f92672">in</span> location:
    plt<span style="color:#f92672">.</span>text(x, y, city, alpha<span style="color:#f92672">=</span><span style="color:#ae81ff">0.4</span>, size<span style="color:#f92672">=</span><span style="color:#ae81ff">8</span>)
    
<span style="color:#66d9ef">for</span> w <span style="color:#f92672">in</span> walk<span style="color:#f92672">.</span>values:
    t1 <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>where(location[:, <span style="color:#ae81ff">0</span>] <span style="color:#f92672">==</span> w[<span style="color:#ae81ff">0</span>])[<span style="color:#ae81ff">0</span>][<span style="color:#ae81ff">0</span>]
    t2 <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>where(location[:, <span style="color:#ae81ff">0</span>] <span style="color:#f92672">==</span> w[<span style="color:#ae81ff">1</span>])[<span style="color:#ae81ff">0</span>][<span style="color:#ae81ff">0</span>]
    n1, y1, x1 <span style="color:#f92672">=</span> location[t1]
    n2, y2, x2 <span style="color:#f92672">=</span> location[t2]
    plt<span style="color:#f92672">.</span>plot([x1, x2], [y1, y2])
    
plt<span style="color:#f92672">.</span>scatter(location[:, <span style="color:#ae81ff">2</span>], location[:, <span style="color:#ae81ff">1</span>])
plt<span style="color:#f92672">.</span>grid()
</code></pre></div><p><img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/211162/459549a4-36c5-506e-167f-990adee4ae70.png" alt="output_12_0.png"></p>
<p>The city where this prefectural capital is located is called <b>“vertex”</b>, the line between cities is called <b>“edge”</b>, and the vertex and the vertex are directly connected by the edge. Will be called <b>adjacent</b>. I have data on walking time between cities, but for ease of interpretation, I will use the information on &ldquo;edges&rdquo; but not the data on walking time in the subsequent analysis.</p>
<h1 id="distance-matrix-between-prefectural-offices">Distance matrix between prefectural offices</h1>
<p>Find the distance matrix between prefectural offices from the latitude and longitude of the prefectural offices.```python
import numpy as np
from scipy.spatial import distance</p>
<p>dist_mat = distance.cdist(location[:, 1:], location[:, 1:], metric='euclidean&rsquo;) # Euclidean distance</p>
<pre><code>
Create a function that visualizes a matrix with a colormap.

```python
import pandas as pd
def visualize_matrix(matrix):
    plt.figure(figsize=(12, 10))
    plt.imshow(matrix, interpolation='nearest', cmap=plt.cm.coolwarm)
    plt.colorbar()
    tick_marks = np.arange(len(matrix))
    plt.xticks(tick_marks, matrix.columns, rotation=90)
    plt.yticks(tick_marks, matrix.columns)
    plt.tight_layout()
</code></pre><p>Visualizing the obtained distance matrix</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">visualize_matrix(pd<span style="color:#f92672">.</span>DataFrame(dist_mat, columns<span style="color:#f92672">=</span>location[:, <span style="color:#ae81ff">0</span>]))
</code></pre></div><p><img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/211162/220acebd-7de2-6957-d975-e80e8861659c.png" alt="output_8_0.png"></p>
<p>This distance matrix is used for two purposes in this analysis.</p>
<ul>
<li>One is the calculation of the distance from your current location to your destination, Naha.</li>
<li>The other is the calculation of the total distance traveled from Sapporo, the departure point, to the current location.</li>
</ul>
<h1 id="agent">Agent</h1>
<p>In reinforcement learning, the &ldquo;brain&rdquo; that you learn is called <b>&quot;agent&quot;</b>. The agent grasps the action options that can be taken, and through learning, adjusts the judgment weight of the action.</p>
<h2 id="action-options-you-can-take">Action options you can take</h2>
<p>The variable theta (θ, theta) represents the action options that can be taken at that time. In terms of graph theory, it represents an &ldquo;edge&rdquo; (the connection between vertices). Let&rsquo;s calculate.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> numpy <span style="color:#f92672">as</span> np
theta_zero <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>full((len(location), len(location)), np<span style="color:#f92672">.</span>nan)

<span style="color:#66d9ef">for</span> w <span style="color:#f92672">in</span> walk<span style="color:#f92672">.</span>values:
    i <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>where(location[:, <span style="color:#ae81ff">0</span>] <span style="color:#f92672">==</span> w[<span style="color:#ae81ff">0</span>])[<span style="color:#ae81ff">0</span>][<span style="color:#ae81ff">0</span>]
    j <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>where(location[:, <span style="color:#ae81ff">0</span>] <span style="color:#f92672">==</span> w[<span style="color:#ae81ff">1</span>])[<span style="color:#ae81ff">0</span>][<span style="color:#ae81ff">0</span>]
    theta_zero[i, j] <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span>
    theta_zero[j, i] <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span>
</code></pre></div><p>With the above, the number of the city adjacent to the city $i$ can be obtained by <code>theta_zero[i]</code>.</p>
<h2 id="probability-of-action-to-take">Probability of action to take</h2>
<p>The variable pi (π, pi) shall represent the probability of the action to take at that time. In terms of graph theory, it represents &ldquo;edge weight&rdquo;. In reinforcement learning, this <code>pi</code> is called <b>“policy” (policy)</b>, and this is optimized.</p>
<p>First, as an initial value, when there are multiple choices, consider the case where one is randomly selected with equal probability.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">normalize_pi</span>(theta):
    [m, n] <span style="color:#f92672">=</span> theta<span style="color:#f92672">.</span>shape
    pi <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>zeros((m, n))
    <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">0</span>, m):
        pi[i, :] <span style="color:#f92672">=</span> theta[i, :] <span style="color:#f92672">/</span> np<span style="color:#f92672">.</span>nansum(theta[i, :])
        
    <span style="color:#66d9ef">return</span> np<span style="color:#f92672">.</span>nan_to_num(pi)
</code></pre></div><p>In this way, the initial value of <code>pi</code>, <code>pi_zero</code>, is obtained.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">pi_zero <span style="color:#f92672">=</span> normalize_pi(theta_zero)
</code></pre></div><p>Let&rsquo;s visualize <code>pi_zero</code>.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">visualize_matrix(pd<span style="color:#f92672">.</span>DataFrame(pi_zero, columns<span style="color:#f92672">=</span>location[:, <span style="color:#ae81ff">0</span>]))
</code></pre></div><p><img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/211162/47630b3f-4f7d-de7d-587c-66364ea8aa84.png" alt="output_19_0.png"></p>
<p>Note that <code>pi_zero</code> is not a symmetric matrix. For example, there is a side from Sapporo to Aomori and another side from Aomori to Sapporo, but the weight is not the same. This is because there is only one option from Sapporo to the next city, while there are multiple options from Aomori to the next city.</p>
<h2 id="about-action-and-state">About &ldquo;action&rdquo; and &ldquo;state&rdquo;</h2>
<p>Reinforcement learning handles &ldquo;behavior&rdquo; and &ldquo;state&rdquo; separately. As a result of selecting a certain action in a certain state, it transits to a different state.</p>
<p>The example dealt with in this article is a special case where it is not necessary to distinguish between &ldquo;action&rdquo; and &ldquo;state&rdquo;. In other words, &ldquo;state&rdquo; refers to the &ldquo;current city&rdquo;, &ldquo;action&rdquo; refers to the &ldquo;next city&rdquo;, and that is the next &ldquo;state&rdquo;. The code is easier because you don&rsquo;t have to distinguish between actions and states.</p>
<p>#Random walk</p>
<p>First of all, let&rsquo;s try to travel through Japan with this policy <code>pi</code> as it is <code>pi_zero</code>. When you arrive at a city, it is a random walk that randomly decides which city to go next.</p>
<h2 id="probabilistically-choose-the-next-action">Probabilistically choose the next action</h2>
<p>Create a function that selects the next city according to the probability indicated by the policy <code>pi</code>.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">get_next</span>(town, pi):
    <span style="color:#66d9ef">return</span> np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>choice(range(len(pi)), p<span style="color:#f92672">=</span>pi[town, :])
</code></pre></div><p>For example, according to <code>pi_zero</code>, the next city of Tokyo (No. 12) is calculated as follows.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">get_next(<span style="color:#ae81ff">12</span>, pi_zero)
</code></pre></div><pre><code>13
</code></pre>
<p>Of course, since it&rsquo;s decided randomly, the result will change each time it is run.</p>
<h2 id="search-by-random-walk">Search by random walk</h2>
<p>Let&rsquo;s do a random walk search.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">explore</span>(pi, start<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>, goal<span style="color:#f92672">=</span><span style="color:#ae81ff">46</span>): <span style="color:#75715e"># Start is 0 (Sapporo), goal is 46 (Naha)</span>
    route <span style="color:#f92672">=</span> [start] <span style="color:#75715e"># list to put travel history</span>
    town <span style="color:#f92672">=</span> start
    <span style="color:#66d9ef">while</span> True:
        next_town <span style="color:#f92672">=</span> get_next(town, pi) <span style="color:#75715e"># determine the next city</span>
        <span style="color:#66d9ef">if</span> next_town <span style="color:#f92672">in</span> route: <span style="color:#75715e"># After visiting the same city twice</span>
            <span style="color:#66d9ef">break</span> <span style="color:#75715e"># game over</span>

        route<span style="color:#f92672">.</span>append(next_town) <span style="color:#75715e"># put the next city in history</span>

        <span style="color:#66d9ef">if</span> next_town <span style="color:#f92672">==</span> goal: <span style="color:#75715e"># arrive at the goal</span>
            <span style="color:#66d9ef">break</span> <span style="color:#75715e"># congratulations</span>
        <span style="color:#66d9ef">else</span>: <span style="color:#75715e"># Not a goal yet</span>
            town <span style="color:#f92672">=</span> next_town <span style="color:#75715e"># Set &#34;next city&#34; as &#34;current position&#34;</span>

    <span style="color:#66d9ef">return</span> route
</code></pre></div><p>Start random walk according to <code>pi_zero</code></p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">route <span style="color:#f92672">=</span> explore(pi_zero)
route
</code></pre></div><pre><code>[0, 1, 2, 3, 4, 5]
</code></pre>
<p>It&rsquo;s random, so of course the results change each time you run it. If it&rsquo;s hard to imagine a city number alone, converting it to a city name like this would help.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">[location[i, <span style="color:#ae81ff">0</span>] <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> route]
</code></pre></div><pre><code>['Sapporo','Aomori','Morioka','Sendai','Akita','Yamagata']
</code></pre>
<p>In this example, I walked from Sapporo to Yamagata and made a mistake at the next destination, so I chose the city I had visited, so the game is over.</p>
<p>By the way, is it possible to get from Sapporo to Naha with such a random walk?</p>
<h2 id="search-result-evaluation-index">Search result evaluation index</h2>
<p>Let&rsquo;s make a method to evaluate the quality of the search results. Calculate the straight line distance from your current location to your destination (Naha) and the total distance traveled from your departure location (Sapporo) to your current location.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">evaluate</span>(route, goal<span style="color:#f92672">=</span><span style="color:#ae81ff">46</span>):
    dist_goal <span style="color:#f92672">=</span> dist_mat[route[<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>], goal]
    len_route <span style="color:#f92672">=</span> sum([dist_mat[route[i], route[i <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span>]] <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(len(route)<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>)])
    <span style="color:#66d9ef">return</span> [dist_goal, len_route]
</code></pre></div><p><code>dist_goal</code> is the &ldquo;straight line distance from the current location to the destination (Naha)&rdquo;, and <code>len_route</code> is the &ldquo;total travel distance from the departure location (Sapporo) to the current location&rdquo;.</p>
<p>Then, it is an execution example.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">route <span style="color:#f92672">=</span> explore(pi_zero)
[location[i, <span style="color:#ae81ff">0</span>] <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> route], evaluate(route)
</code></pre></div><pre><code>(['Sapporo','Aomori'], [19.597025248636594, 2.3205099949149073])
</code></pre>
<p>In this example, the game is over by choosing the way from Sapporo to Aomori and back from Aomori to Sapporo. The distance from the current location (Aomori) to the destination (Naha) is <code>19.597025248636594</code>, and the distance from the departure location (Sapporo) to the current location (Aomori) is <code>2.3205099949149073</code>.</p>
<p>As a goal of Japan&rsquo;s long-distance travel, we will look for a route where this <code>dist_goal</code> becomes zero, and find a route with the smallest <code>len_route</code> among them. It is not good if the <code>len_route</code> is short.</p>
<p>From now on, I will try the game that moves from Sapporo to Naha many times, but when a certain result is obtained, let&rsquo;s make a function to judge whether it is the best one so far.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">best_dist_goal <span style="color:#f92672">=</span> <span style="color:#ae81ff">1000000</span> <span style="color:#75715e"># Initialize an unrealistically large value</span>
best_len_route <span style="color:#f92672">=</span> <span style="color:#ae81ff">1000000</span> <span style="color:#75715e"># Initialize a value that is too large</span>

<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">is_best_ever</span>():
    <span style="color:#66d9ef">if</span> best_dist_goal <span style="color:#f92672">&gt;=</span> dist_goal: <span style="color:#75715e"># best if dist_goal is less than or equal to</span>
        <span style="color:#66d9ef">if</span> best_dist_goal <span style="color:#f92672">&gt;</span>dist_goal: <span style="color:#75715e"># if less than best dist_goal value in the past</span>
            <span style="color:#66d9ef">return</span> True <span style="color:#75715e"># Best everelif best_len_route &gt;len_route: # if less than best len_route value in the past</span>
            <span style="color:#66d9ef">return</span> True <span style="color:#75715e"># Best ever</span>
        <span style="color:#66d9ef">else</span>:
            <span style="color:#66d9ef">return</span> False <span style="color:#75715e">#Not the best</span>
    <span style="color:#66d9ef">else</span>:
        <span style="color:#66d9ef">return</span> False <span style="color:#75715e">#Not the best</span>
</code></pre></div><h2 id="search-execution">Search execution</h2>
<p>Now that we are ready, we will repeat the random walk search 50,000 times.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">%%</span>time <span style="color:#75715e"># measure execution time</span>
pi <span style="color:#f92672">=</span> pi_zero<span style="color:#f92672">.</span>copy() <span style="color:#75715e"># Initialize pi</span>
theta <span style="color:#f92672">=</span> theta_zero<span style="color:#f92672">.</span>copy() <span style="color:#75715e"># initialize theta</span>

best_dist_goal <span style="color:#f92672">=</span> <span style="color:#ae81ff">1000000</span> <span style="color:#75715e"># Initialize an unrealistically large value</span>
best_len_route <span style="color:#f92672">=</span> <span style="color:#ae81ff">1000000</span> <span style="color:#75715e"># Initialize a value that is too large</span>

dist_goal_history0 <span style="color:#f92672">=</span> [] <span style="color:#75715e"># dist_goal history</span>
len_route_history0 <span style="color:#f92672">=</span> [] <span style="color:#75715e"># len_route history</span>
best_route0 <span style="color:#f92672">=</span> [] <span style="color:#75715e"># best route</span>

<span style="color:#66d9ef">for</span> itera <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">50000</span>): <span style="color:#75715e"># repeat 50,000 times</span>
    route <span style="color:#f92672">=</span> explore(pi) <span style="color:#75715e"># explore</span>
    dist_goal, len_route <span style="color:#f92672">=</span> evaluate(route) <span style="color:#75715e"># evaluate</span>
    dist_goal_history0<span style="color:#f92672">.</span>append(dist_goal) <span style="color:#75715e"># record</span>
    len_route_history0<span style="color:#f92672">.</span>append(len_route) <span style="color:#75715e"># record</span>
    
    <span style="color:#66d9ef">if</span> is_best_ever(): <span style="color:#75715e">#If the best in the past</span>
        best_dist_goal <span style="color:#f92672">=</span> dist_goal <span style="color:#75715e"># Best dist_goal</span>
        best_len_route <span style="color:#f92672">=</span> len_route <span style="color:#75715e"># best len_route</span>
        best_route0 <span style="color:#f92672">=</span> route <span style="color:#75715e"># best route</span>
</code></pre></div><pre><code>CPU times: user 10 s, sys: 118 ms, total: 10.2 s
Wall time: 11.8 s
</code></pre>
<h2 id="search-result">Search result</h2>
<p>Create a function to visualize the result. First, the function that visualizes the distribution of the obtained <code>dist_goal</code> <code>len_route</code></p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">draw_histgrams</span>(dist_goal_history, len_route_history):
    plt<span style="color:#f92672">.</span>figure(figsize<span style="color:#f92672">=</span>(max(len_route_history) <span style="color:#f92672">/</span> <span style="color:#ae81ff">4</span>, max(dist_goal_history) <span style="color:#f92672">/</span> <span style="color:#ae81ff">4</span>))
    x_max <span style="color:#f92672">=</span> max(dist_goal_history <span style="color:#f92672">+</span> len_route_history)
    ax <span style="color:#f92672">=</span> plt<span style="color:#f92672">.</span>subplot(<span style="color:#ae81ff">2</span>,<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>)
    ax<span style="color:#f92672">.</span>hist(dist_goal_history, label<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;dist_goal&#39;</span>, bins<span style="color:#f92672">=</span><span style="color:#ae81ff">20</span>)
    ax<span style="color:#f92672">.</span>set_xlim([<span style="color:#ae81ff">0</span>, x_max])
    ax<span style="color:#f92672">.</span>grid()
    ax<span style="color:#f92672">.</span>legend()
    ax <span style="color:#f92672">=</span> plt<span style="color:#f92672">.</span>subplot(<span style="color:#ae81ff">2</span>,<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">2</span>)
    ax<span style="color:#f92672">.</span>hist(len_route_history, label<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;len_route&#39;</span>, bins<span style="color:#f92672">=</span><span style="color:#ae81ff">20</span>)
    ax<span style="color:#f92672">.</span>set_xlim([<span style="color:#ae81ff">0</span>, x_max])
    ax<span style="color:#f92672">.</span>grid()
    ax<span style="color:#f92672">.</span>legend()
</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">draw_histgrams(dist_goal_history0, len_route_history0)
</code></pre></div><p><img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/211162/abd1ab34-ca1b-bdd3-ddda-e25684e33c78.png" alt="output_35_0.png"></p>
<p>It&rsquo;s hard to get to Naha after starting from Sapporo. If you are lucky, you may get there. You can see that there are many cases where the game is over at the beginning of the search.</p>
<p>Next, let&rsquo;s see the relationship between <code>dist_goal</code> and <code>len_route</code>.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">draw_scatter</span>(dist_goal_history, len_route_history):
    plt<span style="color:#f92672">.</span>figure(figsize<span style="color:#f92672">=</span>(max(len_route_history) <span style="color:#f92672">/</span> <span style="color:#ae81ff">4</span>, max(dist_goal_history) <span style="color:#f92672">/</span> <span style="color:#ae81ff">4</span>))
    plt<span style="color:#f92672">.</span>scatter(len_route_history, dist_goal_history, alpha<span style="color:#f92672">=</span><span style="color:#ae81ff">0.5</span>)
    plt<span style="color:#f92672">.</span>ylabel(<span style="color:#e6db74">&#39;dist_goal&#39;</span>)
    plt<span style="color:#f92672">.</span>xlabel(<span style="color:#e6db74">&#39;len_route&#39;</span>)
    plt<span style="color:#f92672">.</span>ylim([<span style="color:#ae81ff">0</span>, max(dist_goal_history) <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span>])
    plt<span style="color:#f92672">.</span>xlim([<span style="color:#ae81ff">0</span>, max(len_route_history) <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span>])
    plt<span style="color:#f92672">.</span>grid()
    plt<span style="color:#f92672">.</span>show()
</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">draw_scatter(dist_goal_history0, len_route_history0)
</code></pre></div><p><img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/211162/5d7aa545-5928-5d25-7e53-b5a567c65550.png" alt="output_37_0.png"></p>
<p>You can see that you have not reached Naha, and that even if the game is over in the same place, there are a wide range of distances to reach it (choose various routes).</p>
<p>Next, let&rsquo;s see how <code>dist_goal</code> <code>len_route</code> went through.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">visualize_history</span>(history, interval<span style="color:#f92672">=</span><span style="color:#ae81ff">50</span>, window<span style="color:#f92672">=</span><span style="color:#ae81ff">50</span>):
    plt<span style="color:#f92672">.</span>plot(range(<span style="color:#ae81ff">0</span>, len(history), interval), <span style="color:#75715e"># maximum value of the whole</span>
             [np<span style="color:#f92672">.</span>array(history)[:i]<span style="color:#f92672">.</span>max() <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(len(history)) <span style="color:#66d9ef">if</span> (i<span style="color:#f92672">%</span>interval) <span style="color:#f92672">==</span> <span style="color:#ae81ff">1</span>], label<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;max&#39;</span>)
    plt<span style="color:#f92672">.</span>plot(range(window, len(history)<span style="color:#f92672">+</span>window, interval),
             [np<span style="color:#f92672">.</span>array(history)[i:i <span style="color:#f92672">+</span> window]<span style="color:#f92672">.</span>mean() <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(len(history)) <span style="color:#66d9ef">if</span> (i<span style="color:#f92672">%</span>interval) <span style="color:#f92672">==</span> <span style="color:#ae81ff">1</span>],
             label<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;mean(recent)&#39;</span>) <span style="color:#75715e"># average value of the most recent interval times</span>
    plt<span style="color:#f92672">.</span>plot(range(<span style="color:#ae81ff">0</span>, len(history), interval),
             [np<span style="color:#f92672">.</span>array(history)[:i]<span style="color:#f92672">.</span>mean() <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(len(history)) <span style="color:#66d9ef">if</span> (i<span style="color:#f92672">%</span>interval) <span style="color:#f92672">==</span> <span style="color:#ae81ff">1</span>], label<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;mean&#39;</span>) <span style="color:#75715e"># overall mean</span>
    plt<span style="color:#f92672">.</span>plot(range(<span style="color:#ae81ff">0</span>, len(history), interval),
             [np<span style="color:#f92672">.</span>array(history)[:i]<span style="color:#f92672">.</span>min() <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(len(history)) <span style="color:#66d9ef">if</span> (i<span style="color:#f92672">%</span>interval) <span style="color:#f92672">==</span> <span style="color:#ae81ff">1</span>], label<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;min&#39;</span>) <span style="color:#75715e"># overall minimum</span>
    plt<span style="color:#f92672">.</span>legend()
    plt<span style="color:#f92672">.</span>grid()
    plt<span style="color:#f92672">.</span>show()
</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">visualize_history(dist_goal_history0)
</code></pre></div><p><img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/211162/c492a5aa-21ed-1d4d-e1e4-7ef04c3dfe64.png" alt="output_39_0.png"></p>
<p><code>dist_goal</code> occasionally updates the minimum value, but it does not reach Naha. And no matter how many times the search is repeated, the average value does not improve. This is natural because I haven&rsquo;t learned anything.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">visualize_history(len_route_history0)
</code></pre></div><p><img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/211162/2acfecd1-1fbe-2422-de43-7efacbd6b944.png" alt="output_40_0.png"></p>
<p>Similarly, <code>len_route</code> occasionally updates the maximum value, but it does not reach Naha, and the average value does not improve even if the search is repeated. This is natural because I haven&rsquo;t learned anything.</p>
<p>Finally, let&rsquo;s display the best route found in this random walk.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">draw_route</span>(route):
    plt<span style="color:#f92672">.</span>figure(figsize<span style="color:#f92672">=</span>(<span style="color:#ae81ff">11</span>, <span style="color:#ae81ff">9</span>))
    <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> route:
        plt<span style="color:#f92672">.</span>text(location[i, <span style="color:#ae81ff">2</span>], location[i, <span style="color:#ae81ff">1</span>], location[i, <span style="color:#ae81ff">0</span>], alpha<span style="color:#f92672">=</span><span style="color:#ae81ff">0.4</span>, size<span style="color:#f92672">=</span><span style="color:#ae81ff">12</span>)
    plt<span style="color:#f92672">.</span>grid()
    plt<span style="color:#f92672">.</span>plot([location[i][<span style="color:#ae81ff">2</span>] <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> route], [location[i][<span style="color:#ae81ff">1</span>] <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> route])
    plt<span style="color:#f92672">.</span>ylabel(<span style="color:#e6db74">&#39;latitude&#39;</span>)
    plt<span style="color:#f92672">.</span>xlabel(<span style="color:#e6db74">&#39;longitude&#39;</span>)
    plt<span style="color:#f92672">.</span>show()
</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">draw_route(best_route0)
</code></pre></div><p><img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/211162/082a9ded-66c0-a0f0-f208-a3090adeba04.png" alt="output_42_0.png"></p>
<p>Although I did my best, I could only reach Saga. If you are lucky, you may reach Naha. But I never learn it.</p>
<h1 id="policy-gradient-method">Policy gradient method</h1>
<p>Well then, it&rsquo;s time to start <b>reinforcement learning</b>. Reinforcement learning methods can be broadly divided into <b>Policy gradient method</b> and <b>Value iteration method</b>. First, let&rsquo;s do the policy gradient method.</p>
<h2 id="update-policy-pi">Update policy pi</h2>
<p>The obtained route <code>route</code> can be evaluated as good or bad in terms of &ldquo;how close the end point of the route is to the goal&rdquo;. Update the policy <code>pi</code> so that the smaller the distance to the goal, the higher the probability of choosing an edge on that route in future searches.</p>
<pre><code class="language-pythondef" data-lang="pythondef">    new_pi = pi.copy() # copy numpy array
    for i in range(len(route)-1): # for each side on the route
        town1 = route[i] # City at the starting point of side i
        town2 = route[i + 1] # City at the end of side i
        new_pi[town1, town2] += eta / (dist_mat[route[-1], goal] + 1)
        # Try to get higher score as the end of the route is closer to the goal
    
    return normalize_pi(new_pi) # update pi
</code></pre><p>The last reason to use <code>normalize_pi</code> is to adjust the sum of row values to 1.0.</p>
<h2 id="search-execution-1">Search execution</h2>
<p>Let&rsquo;s start the exploration.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">%%</span>time
pi <span style="color:#f92672">=</span> pi_zero<span style="color:#f92672">.</span>copy()

best_dist_goal <span style="color:#f92672">=</span> <span style="color:#ae81ff">1000000</span>
best_len_route <span style="color:#f92672">=</span> <span style="color:#ae81ff">1000000</span>

dist_goal_history1 <span style="color:#f92672">=</span> []
len_route_history1 <span style="color:#f92672">=</span> []
best_route1 <span style="color:#f92672">=</span> []
<span style="color:#66d9ef">for</span> itera <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">50000</span>):
    route <span style="color:#f92672">=</span> explore(pi)
    dist_goal, len_route <span style="color:#f92672">=</span> evaluate(route)
    dist_goal_history1<span style="color:#f92672">.</span>append(dist_goal)
    len_route_history1<span style="color:#f92672">.</span>append(len_route)
        
    pi <span style="color:#f92672">=</span> update_pi(pi, route)
    
    <span style="color:#66d9ef">if</span> is_best_ever():
        best_dist_goal <span style="color:#f92672">=</span> dist_goal
        best_len_route <span style="color:#f92672">=</span> len_route
        best_route1 <span style="color:#f92672">=</span> route
</code></pre></div><pre><code>CPU times: user 59.9 s, sys: 340 ms, total: 1min
Wall time: 1min 1s
</code></pre>
<h2 id="search-result-1">Search result</h2>
<p>Below is an example of the results. The results will differ each time you run, and in some cases you may not reach your destination Naha even after repeating 50,000 learnings.</p>
<p>The distribution of <code>dist_goal</code> <code>len_route</code> obtained is very different from the distribution by random walk. The number of arrivals at my destination, Naha, has increased overwhelmingly.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">draw_histgrams(dist_goal_history1, len_route_history1)
</code></pre></div><p><img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/211162/9dbfc8dc-c14c-e476-c68c-4e25fe307cb2.png" alt="output_46_0.png"></p>
<p>The relationship of <code>dist_goal</code> <code>len_route</code> is as follows.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">draw_scatter(dist_goal_history1, len_route_history1)
</code></pre></div><p><img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/211162/d3e6705e-fcc7-b031-3eb0-135cc1a95bae.png" alt="output_47_0.png"></p>
<p>The history of <code>dist_goal</code>. It reached the destination Naha much earlier than the random walk, and after that, it became easier to reach the destination.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">visualize_history(dist_goal_history1)
</code></pre></div><p><img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/211162/e8d54467-4f41-b51c-7501-6a3490e31cb8.png" alt="output_48_0.png"></p>
<p>It is the first 5000 times in the history of <code>dist_goal</code>.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">visualize_history(dist_goal_history1[:<span style="color:#ae81ff">5000</span>])
</code></pre></div><p><img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/211162/e15d753b-756d-1b61-4d2c-4d720746775d.png" alt="output_49_0.png"></p>
<p>Similarly, the history of <code>len_route</code>.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">visualize_history(len_route_history1)
</code></pre></div><p><img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/211162/c158245c-26b2-a0e9-ae2b-396cac19e5c5.png" alt="output_50_0.png"></p>
<p>It is the first 5000 times in the history of <code>len_route</code>.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">visualize_history(len_route_history1[:<span style="color:#ae81ff">5000</span>])
</code></pre></div><p><img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/211162/8dc2df3d-3526-2927-d94e-0f347abc09e5.png" alt="output_51_0.png"></p>
<p>This is what was output as the best route. You can see that the closest route is selected. Only one is regrettable. I am going to Kumamoto from Fukuoka via Saga. I should have been heading directly from Fukuoka to Kumamoto, not via Saga.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">draw_route(best_route1)
</code></pre></div><p><img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/211162/40e90136-18cf-35b6-7eec-ae0e3929b6a2.png" alt="output_52_0.png"></p>
<p>Save the obtained policy <code>pi</code> as the alias <code>pi_pg</code>.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">pi_pg <span style="color:#f92672">=</span> pi<span style="color:#f92672">.</span>copy()
</code></pre></div><p>Visualize that policy <code>pi_pg</code></p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">visualize_matrix(pd<span style="color:#f92672">.</span>DataFrame(pi_pg, columns<span style="color:#f92672">=</span>location[:, <span style="color:#ae81ff">0</span>]))
</code></pre></div><p><img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/211162/423e8850-174c-7950-59a9-748008fbd48e.png" alt="output_54_0.png"></p>
<p>Absolutely important routes are red. For example, when you arrive in Kagoshima, there can only be Naha next. Selecting anything else (Kumamoto, Miyazaki) is absolutely useless. It shows it well.</p>
<p>This way you can see how it changed compared to the initial value.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">visualize_matrix(pd<span style="color:#f92672">.</span>DataFrame(pi_pg<span style="color:#f92672">-</span>pi_zero, columns<span style="color:#f92672">=</span>location[:, <span style="color:#ae81ff">0</span>]))
</code></pre></div><p><img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/211162/b03a15b8-c922-ecc2-4e15-97ac9929a3db.png" alt="output_55_0.png"></p>
<p>Insignificant places have hardly changed. For example, the Shikoku and Kanto regions seem to be less important.</p>
<h1 id="epsilon-greedy-method">Epsilon-Greedy method</h1>
<p>In contrast to the above “policy gradient method”, the Epsilon-Greedy method introduced here and the Q-learning introduced next are called <b>Value Iteration Method</b>. ..</p>
<p>In the value iteration method, the policy somehow seems to be called &ldquo;Q&rdquo; instead of &ldquo;π&rdquo;. <code>pi</code> starts with a perfect equal probability, whereas <code>Q</code> starts with a non-uniform random probability.</p>
<h2 id="policy-q">Policy Q</h2>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">generate_Q</span>(theta): <span style="color:#75715e"># Generate non-uniform random probabilities from possible choices theta</span>
    [m, n] <span style="color:#f92672">=</span> theta<span style="color:#f92672">.</span>shape
    Q <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>zeros((m, n))
    rand <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>rand(m, n)
    <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(m):
        <span style="color:#66d9ef">for</span> j <span style="color:#f92672">in</span> range(n):
            <span style="color:#66d9ef">if</span> theta[i, j] <span style="color:#f92672">==</span> <span style="color:#ae81ff">1</span>:
                Q[i, j] <span style="color:#f92672">=</span> rand[i, j]
        
    <span style="color:#66d9ef">return</span> normalize_pi(Q)
</code></pre></div><p>Creates an initial <code>Q_zero</code> for the non-uniform policy <code>Q</code></p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">Q_zero <span style="color:#f92672">=</span> generate_Q(theta_zero)
</code></pre></div><p>Visualize the generated <code>Q_zero</code>.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">visualize_matrix(pd<span style="color:#f92672">.</span>DataFrame(Q_zero, columns<span style="color:#f92672">=</span>location[:, <span style="color:#ae81ff">0</span>]))
</code></pre></div><p><img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/211162/f7d9d45f-2388-5c6f-b406-585362aec1f8.png" alt="output_59_0.png"></p>
<p>Here is the difference between <code>Q_zero</code> and <code>pi</code>.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">visualize_matrix(pd<span style="color:#f92672">.</span>DataFrame(Q_zero<span style="color:#f92672">-</span>pi_zero, columns<span style="color:#f92672">=</span>location[:, <span style="color:#ae81ff">0</span>]))
</code></pre></div><p><img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/211162/2766fd0c-c7c0-c4db-d99c-35a1f78badfe.png" alt="output_60_0.png"></p>
<h2 id="how-to-choose-the-next-action">How to choose the next action</h2>
<p>In the policy gradient method, the next action <code>get_next</code> was randomly selected according to the probability indicated by the policy <code>pi</code>. In the value iteration method, rewrite the next action <code>get_next</code> as follows.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">get_next</span>(town, Q, epsilon<span style="color:#f92672">=</span><span style="color:#ae81ff">0.1</span>):
    <span style="color:#66d9ef">if</span> np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>rand() <span style="color:#f92672">&lt;</span>epsilon:
        <span style="color:#66d9ef">return</span> np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>choice(range(len(Q)), p<span style="color:#f92672">=</span>Q[town, :])
    <span style="color:#66d9ef">else</span>:
        <span style="color:#66d9ef">return</span> np<span style="color:#f92672">.</span>nanargmax(Q[town, :])
<span style="color:#e6db74">```In other words, by introducing a new parameter `</span>epsilon<span style="color:#e6db74">`, there are cases where the behavior is randomly selected according to the probability indicated by the policy `</span>Q<span style="color:#e6db74">` and where the policy `</span>Q<span style="color:#e6db74">` takes the maximum value. Initially, `</span>epsilon<span style="color:#e6db74">` sets a large value. In other words, it gives you more chances to make random choices. After that, gradually reduce the `</span>epsilon<span style="color:#960050;background-color:#1e0010">`</span> <span style="color:#f92672">and</span> reduce the chance of making random selections<span style="color:#f92672">.</span>

<span style="color:#75715e">## reward</span>

<span style="color:#f92672">&lt;</span>b<span style="color:#f92672">&gt;</span>Introducing the concept of <span style="color:#960050;background-color:#1e0010">“</span>reward<span style="color:#960050;background-color:#1e0010">”</span><span style="color:#f92672">&lt;/</span>b<span style="color:#f92672">&gt;</span>, <span style="color:#ae81ff">1</span> point <span style="color:#66d9ef">if</span> the goal <span style="color:#f92672">is</span> achieved (when the goal <span style="color:#f92672">is</span> reached), <span style="color:#ae81ff">1</span> <span style="color:#66d9ef">if</span> it fails (when the same city <span style="color:#f92672">is</span> visited twice <span style="color:#f92672">and</span> the game <span style="color:#f92672">is</span> over)<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span> The points <span style="color:#f92672">and</span> the progress are <span style="color:#ae81ff">0</span> points<span style="color:#f92672">.</span>

With the method <span style="color:#e6db74">`sarsa`</span> shown below, when updating the <span style="color:#e6db74">`Q`</span> value <span style="color:#66d9ef">for</span> a certain action (edge on the route), <span style="color:#66d9ef">while</span> also referring to the <span style="color:#e6db74">`Q`</span> value <span style="color:#66d9ef">for</span> the next action (next edge on the route) I will update<span style="color:#f92672">.</span> At this time, the time discount rate <span style="color:#e6db74">`gamma`</span> <span style="color:#f92672">is</span> applied<span style="color:#f92672">.</span>

<span style="color:#e6db74">``</span><span style="color:#960050;background-color:#1e0010">`</span>python
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">sarsa</span>(town, Q, prev_t, next_t, reward, eta<span style="color:#f92672">=</span><span style="color:#ae81ff">0.1</span>, gamma<span style="color:#f92672">=</span><span style="color:#ae81ff">0.9</span>, goal<span style="color:#f92672">=</span><span style="color:#ae81ff">46</span>):
    <span style="color:#66d9ef">if</span> reward <span style="color:#f92672">==</span> <span style="color:#ae81ff">1</span>: <span style="color:#75715e">#dist_mat[town, goal] == 0:</span>
        Q[prev_t, town] <span style="color:#f92672">=</span> Q[prev_t, town] <span style="color:#f92672">+</span> eta <span style="color:#f92672">*</span> (reward<span style="color:#f92672">-</span>Q[prev_t, town])
    <span style="color:#66d9ef">elif</span> reward <span style="color:#f92672">&gt;=</span> <span style="color:#ae81ff">0</span>:
        Q[prev_t, town] <span style="color:#f92672">=</span> Q[prev_t, town] <span style="color:#f92672">+</span> eta <span style="color:#f92672">*</span> (reward<span style="color:#f92672">-</span>Q[prev_t, town] <span style="color:#f92672">+</span> gamma <span style="color:#f92672">*</span> Q[town, next_t])
    <span style="color:#66d9ef">else</span>:
        Q[prev_t, town] <span style="color:#f92672">=</span> Q[prev_t, town]<span style="color:#f92672">-</span>eta <span style="color:#f92672">*</span> Q[prev_t, town]
        
    <span style="color:#66d9ef">return</span> normalize_pi(Q)
</code></pre></div><p>In this way, the rewards earned at the destination influence the choice of behaviors closer to the destination. &ldquo;Negative rewards&rdquo; when the game is over will also negatively influence the choice of past actions leading up to that point.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">explore_epsilon_greedy</span>(Q, epsilon<span style="color:#f92672">=</span><span style="color:#ae81ff">0.1</span>, eta<span style="color:#f92672">=</span><span style="color:#ae81ff">0.1</span>, gamma<span style="color:#f92672">=</span><span style="color:#ae81ff">0.9</span>, start<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>, goal<span style="color:#f92672">=</span><span style="color:#ae81ff">46</span>, min_epsilon<span style="color:#f92672">=</span><span style="color:#ae81ff">0.01</span>):
    epsilon <span style="color:#f92672">=</span> max(epsilon, min_epsilon)
    route <span style="color:#f92672">=</span> [start]
    town <span style="color:#f92672">=</span> get_next(start, Q, epsilon)
    prev_t <span style="color:#f92672">=</span> start
    <span style="color:#66d9ef">while</span> True:
        <span style="color:#66d9ef">if</span> town <span style="color:#f92672">in</span> route:
            reward <span style="color:#f92672">=</span> <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>
            Q <span style="color:#f92672">=</span> sarsa(town, Q, prev_t, next_t, reward, eta, gamma)
            <span style="color:#66d9ef">break</span>
        <span style="color:#66d9ef">elif</span> town <span style="color:#f92672">==</span> goal:
            reward <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span>
            route<span style="color:#f92672">.</span>append(town)
            Q <span style="color:#f92672">=</span> sarsa(town, Q, prev_t, next_t, reward, eta, gamma)
            <span style="color:#66d9ef">break</span>
        <span style="color:#66d9ef">else</span>:
            reward <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
            route<span style="color:#f92672">.</span>append(town)
            next_t <span style="color:#f92672">=</span> get_next(town, Q, epsilon)
            Q <span style="color:#f92672">=</span> sarsa(town, Q, prev_t, next_t, reward, eta, gamma)
            prev_t <span style="color:#f92672">=</span> town
            town <span style="color:#f92672">=</span> next_t
    
    <span style="color:#66d9ef">return</span> [route, Q]
</code></pre></div><h2 id="search-execution-2">Search execution</h2>
<p>Let&rsquo;s start the exploration.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">%%</span>time
eta <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.1</span> <span style="color:#75715e"># learning rate</span>
gamma <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.9</span> <span style="color:#75715e"># time discount rate</span>
epsilon <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.5</span>
Q <span style="color:#f92672">=</span> Q_zero<span style="color:#f92672">.</span>copy()

best_dist_goal <span style="color:#f92672">=</span> <span style="color:#ae81ff">1000000</span>
best_len_route <span style="color:#f92672">=</span> <span style="color:#ae81ff">1000000</span>

best_route2 <span style="color:#f92672">=</span> []
dist_goal_history2 <span style="color:#f92672">=</span> []
len_route_history2 <span style="color:#f92672">=</span> []
<span style="color:#66d9ef">for</span> itera <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">50000</span>):
    epsilon <span style="color:#f92672">=</span> epsilon <span style="color:#f92672">*</span> <span style="color:#ae81ff">0.99</span>
    route, Q <span style="color:#f92672">=</span> explore_epsilon_greedy(Q, epsilon, eta, gamma)
    dist_goal, len_route <span style="color:#f92672">=</span> evaluate(route)
    dist_goal_history2<span style="color:#f92672">.</span>append(dist_goal)
    len_route_history2<span style="color:#f92672">.</span>append(len_route)

    <span style="color:#66d9ef">if</span> is_best_ever():
        best_dist_goal <span style="color:#f92672">=</span> dist_goal
        best_len_route <span style="color:#f92672">=</span> len_route
        best_route2 <span style="color:#f92672">=</span> route
</code></pre></div><pre><code>CPU times: user 7min 50s, sys: 948 ms, total: 7min 50s
Wall time: 7min 53s
</code></pre>
<h2 id="search-result-2">Search result</h2>
<p>This is an example of the search result by the Epsilon-Greedy method. Actually, the result is not very stable, and the result changes quite a lot each time I run it (try it).</p>
<p>First, the distribution of <code>dist_goal</code> and <code>len_route</code></p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">draw_histgrams(dist_goal_history2, len_route_history2)
</code></pre></div><p><img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/211162/9e94ae0b-b190-4f5b-c217-addba1649842.png" alt="output_65_0.png"></p>
<p>Relationship between <code>dist_goal</code> and <code>len_route</code></p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">draw_scatter(dist_goal_history2, len_route_history2)
</code></pre></div><p><img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/211162/100c52b8-5097-6791-1f2e-5d18cf4c7a58.png" alt="output_66_0.png"></p>
<p>History of <code>dist_goal</code></p>
<p>Larger learning rate <code>eta</code> will result in faster convergence, but more likely to fall into a local solution. Smaller values will result in slower convergence, but less chance of falling into a local solution.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">visualize_history(dist_goal_history2)
</code></pre></div><p><img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/211162/d824ced3-b3ab-04e8-7b47-c6518a3747a5.png" alt="output_67_0.png"></p>
<p>First 5000 times in the history of <code>dist_goal</code></p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">visualize_history(dist_goal_history2[:<span style="color:#ae81ff">5000</span>])
</code></pre></div><p><img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/211162/22b512be-25b3-2b2b-7e83-2d526bc9ff50.png" alt="output_68_0.png"></p>
<p>History of <code>len_route</code></p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">visualize_history(len_route_history2)
</code></pre></div><p><img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/211162/91ff166e-14b1-2e90-2fa1-8fe2176f8180.png" alt="output_69_0.png"></p>
<p>First 5000 times in the history of <code>len_route</code></p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">visualize_history(len_route_history2[:<span style="color:#ae81ff">5000</span>])
</code></pre></div><p><img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/211162/ad6ca411-89bd-0edb-5953-6c1fe50bdbec.png" alt="output_70_0.png"></p>
<p>Shortest path obtained by Epsilon-Greedy method</p>
<p>Here again, the result was a little different from the true shortest route, and it was a regrettable result. Compare with the results obtained with the policy gradient method.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">draw_route(best_route2)
</code></pre></div><p><img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/211162/395b2a38-349a-8afa-0cd7-476e574d492a.png" alt="output_71_0.png"></p>
<p>In the policy gradient method, the distance to the destination Naha was used to update the <code>pi</code> value, so learning a route that simultaneously satisfies &ldquo;Goal 1: Go to the goal&rdquo; and &ldquo;Goal 2: Find the shortest route&rdquo; was doing. Therefore, no matter how many times you execute it, the result will be close to the shortest path.</p>
<p>In this Epsilon-Greedy method, the &ldquo;reward&rdquo; given only reflects &ldquo;whether or not the goal is reached&rdquo; and &ldquo;whether the game is over (has visited the same city twice)&rdquo;. Therefore, we will learn how to &ldquo;Goal 1: Get to the goal&rdquo;, but not &ldquo;Goal 2: Find the shortest route&rdquo;. For this reason, it is possible to get a result close to the shortest path by chance, but in fact it often converges to a result far from the shortest path (try it).</p>
<p>Let&rsquo;s think about how to make learning for “Goal 2: Finding the shortest path” possible with the Epsilon-Greedy method (not covered here).</p>
<p>Save the <code>Q</code> value after learning with Epsilon-Greedy method as <code>Q_eg</code>.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">Q_eg <span style="color:#f92672">=</span> Q<span style="color:#f92672">.</span>copy()
</code></pre></div><p>Its value will be:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">visualize_matrix(pd<span style="color:#f92672">.</span>DataFrame(Q_eg, columns<span style="color:#f92672">=</span>location[:, <span style="color:#ae81ff">0</span>]))
</code></pre></div><p><img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/211162/ffd1622c-6c93-4ed2-ea6a-260004b1f291.png" alt="output_73_0.png"></p>
<p>Let&rsquo;s compare with <code>pi_pg</code> obtained by the policy gradient method.</p>
<p>#Q learningAs another value iterative method, <b>Q-learning</b> is famous. It is basically similar to the Epsilon-Greedy method, but the major difference is that it does not include the randomness that occurs when choosing the &ldquo;next action.&rdquo; It is said that convergence will be faster accordingly.</p>
<p>However, as long as I execute the following code several times, it may converge faster but it does not always happen, and it often falls into a local solution (it cannot reach the destination Naha). There is an impression that it will be.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">Q_learning</span>(town, Q, prev_t, reward, eta<span style="color:#f92672">=</span><span style="color:#ae81ff">0.1</span>, gamma<span style="color:#f92672">=</span><span style="color:#ae81ff">0.9</span>, goal<span style="color:#f92672">=</span><span style="color:#ae81ff">46</span>):
    <span style="color:#66d9ef">if</span> reward <span style="color:#f92672">==</span> <span style="color:#ae81ff">1</span>: <span style="color:#75715e">#dist_mat[town, goal] == 0:</span>
        Q[prev_t, town] <span style="color:#f92672">=</span> Q[prev_t, town] <span style="color:#f92672">+</span> eta <span style="color:#f92672">*</span> (reward<span style="color:#f92672">-</span>Q[prev_t, town])
    <span style="color:#66d9ef">elif</span> reward <span style="color:#f92672">&gt;=</span> <span style="color:#ae81ff">0</span>:
        Q[prev_t, town] <span style="color:#f92672">=</span> Q[prev_t, town] <span style="color:#f92672">+</span> eta <span style="color:#f92672">*</span> (reward<span style="color:#f92672">-</span>Q[prev_t, town] <span style="color:#f92672">+</span> gamma <span style="color:#f92672">*</span> np<span style="color:#f92672">.</span>nanmax(Q[town, :]))
    <span style="color:#66d9ef">else</span>:
        Q[prev_t, town] <span style="color:#f92672">=</span> Q[prev_t, town]<span style="color:#f92672">-</span>eta <span style="color:#f92672">*</span> Q[prev_t, town]
        
    <span style="color:#66d9ef">return</span> normalize_pi(Q)
</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">explore_Q_learning</span>(Q, epsilon<span style="color:#f92672">=</span><span style="color:#ae81ff">0.1</span>, eta<span style="color:#f92672">=</span><span style="color:#ae81ff">0.1</span>, gamma<span style="color:#f92672">=</span><span style="color:#ae81ff">0.9</span>, start<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>, goal<span style="color:#f92672">=</span><span style="color:#ae81ff">46</span>):
    prev_t <span style="color:#f92672">=</span> start
    route <span style="color:#f92672">=</span> [start]
    town <span style="color:#f92672">=</span> get_next(start, Q, epsilon)
    <span style="color:#66d9ef">while</span> True:
        <span style="color:#66d9ef">if</span> town <span style="color:#f92672">in</span> route:
            reward <span style="color:#f92672">=</span> <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>
            Q <span style="color:#f92672">=</span> Q_learning(town, Q, prev_t, reward, eta, gamma)
            <span style="color:#66d9ef">break</span>
        <span style="color:#66d9ef">elif</span> town <span style="color:#f92672">==</span> goal:
            reward <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span>
            route<span style="color:#f92672">.</span>append(town)
            Q <span style="color:#f92672">=</span> Q_learning(town, Q, prev_t, reward, eta, gamma)
            <span style="color:#66d9ef">break</span>
        <span style="color:#66d9ef">else</span>:
            reward <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
            dist_goal, len_route <span style="color:#f92672">=</span> evaluate(route)
            <span style="color:#66d9ef">if</span> best_dist_goal <span style="color:#f92672">&gt;</span>dist_goal:
                reward <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span>
            route<span style="color:#f92672">.</span>append(town)
            next_t <span style="color:#f92672">=</span> get_next(town, Q, epsilon)
            Q <span style="color:#f92672">=</span> Q_learning(town, Q, prev_t, reward, eta, gamma)
            prev_t <span style="color:#f92672">=</span> town
            town <span style="color:#f92672">=</span> next_t
    
    <span style="color:#66d9ef">return</span> [route, Q]
</code></pre></div><h2 id="search-execution-3">Search execution</h2>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">%%</span>time
eta <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.1</span> <span style="color:#75715e"># learning rate</span>
gamma <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.9</span> <span style="color:#75715e"># time discount rate</span>
epsilon <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.5</span>
Q <span style="color:#f92672">=</span> Q_zero<span style="color:#f92672">.</span>copy()

best_dist_goal <span style="color:#f92672">=</span> <span style="color:#ae81ff">1000000</span>
best_len_route <span style="color:#f92672">=</span> <span style="color:#ae81ff">1000000</span>

best_route3 <span style="color:#f92672">=</span> []
dist_goal_history3 <span style="color:#f92672">=</span> []
len_route_history3 <span style="color:#f92672">=</span> []
<span style="color:#66d9ef">for</span> itera <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">50000</span>):
    epsilon <span style="color:#f92672">=</span> epsilon <span style="color:#f92672">*</span> <span style="color:#ae81ff">0.99</span>
    route, Q <span style="color:#f92672">=</span> explore_Q_learning(Q, epsilon, eta, gamma)
    dist_goal, len_route <span style="color:#f92672">=</span> evaluate(route)
    dist_goal_history3<span style="color:#f92672">.</span>append(dist_goal)
    len_route_history3<span style="color:#f92672">.</span>append(len_route)

    <span style="color:#66d9ef">if</span> is_best_ever():
        best_dist_goal <span style="color:#f92672">=</span> dist_goal
        best_len_route <span style="color:#f92672">=</span> len_route
        best_route3 <span style="color:#f92672">=</span> route
</code></pre></div><pre><code>CPU times: user 9min 50s, sys: 1.41 s, total: 9min 52s
Wall time: 9min 54s
</code></pre>
<h2 id="search-result-3">Search result</h2>
<p>This is an example of the search result by Q learning. As with the Epsilon-Greedy method, the results are not very stable, and the results can change quite a lot each time you run it (try it out).</p>
<p>Distribution of <code>dist_goal</code> and <code>len_route</code></p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">draw_histgrams(dist_goal_history3, len_route_history3)
</code></pre></div><p><img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/211162/37435ebe-e084-722d-3110-8f8eeb1a882f.png" alt="output_81_0.png"></p>
<p>Relationship between <code>dist_goal</code> and <code>len_route</code></p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">draw_scatter(dist_goal_history3, len_route_history3)
</code></pre></div><p><img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/211162/0b098528-5f4c-e58a-b2b2-ae85ba327c30.png" alt="output_82_0.png"></p>
<p>History of <code>dist_goal</code></p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">visualize_history(dist_goal_history3)
</code></pre></div><p><img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/211162/5bbdec23-960e-1475-8d60-4cbce5fdf03f.png" alt="output_83_0.png"></p>
<p>First 5000 times in the history of <code>dist_goal</code></p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">visualize_history(dist_goal_history3[:<span style="color:#ae81ff">5000</span>])
</code></pre></div><p><img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/211162/f097d343-4b04-d251-4189-dc6420fa27f1.png" alt="output_84_0.png"></p>
<p>History of <code>len_route</code></p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">visualize_history(len_route_history3)
</code></pre></div><p><img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/211162/d572ac90-390e-a8ca-5a29-03b04680bfac.png" alt="output_85_0.png"></p>
<p>First 5000 times in the history of <code>len_route</code></p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">visualize_history(len_route_history3[:<span style="color:#ae81ff">5000</span>])
</code></pre></div><p><img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/211162/c333130d-442e-4d94-9d74-280d2d65e51e.png" alt="output_86_0.png"></p>
<p>Shortest path obtained by Q learning</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">draw_route(best_route3)
</code></pre></div><p><img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/211162/4b2af6e2-78a0-8d1d-d799-c9b2a11e2e8e.png" alt="output_87_0.png"></p>
<p>This is different from the true shortest path, as can be seen clearly in this example calculation. The reason is, similar to the one mentioned in the case of Epsilon-Greedy method, because of the problem of the design of &ldquo;reward&rdquo;, learning is done to &ldquo;goal 1: reach the goal&rdquo;, but &ldquo;goal 2: find the shortest path&rdquo; This is because we have not learned to do so.</p>
<p>Also, as mentioned above, Q-learning may converge faster within my observation range, but even if it is calculated 50,000 times, it often does not reach the destination Naha. I think that the &ldquo;shortest path obtained by Q-learning&rdquo; tends to be longer than the &ldquo;shortest path obtained by the Epsilon-Greedy method.&rdquo; The reason is that the randomness when selecting the &ldquo;next action&rdquo; is suppressed, so if you happen to reach the goal on the selected route, even if the route is long, the opportunity to change it is Epsilon- I think it&rsquo;s because it is less than the Greedy method.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">Q_qlearn <span style="color:#f92672">=</span> Q<span style="color:#f92672">.</span>copy()
</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">visualize_matrix(pd<span style="color:#f92672">.</span>DataFrame(Q_qlearn, columns<span style="color:#f92672">=</span>location[:, <span style="color:#ae81ff">0</span>]))
</code></pre></div><p><img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/211162/898c8090-3944-2363-8862-98b04954d148.png" alt="output_89_0.png"></p>
<p>so. That&rsquo;s all about reinforcement learning. No, Okinawa is far.</p>
<p>There is something called <b>Deep Reinforcement Learning</b> that mixes deep learning with this, but on another occasion. Chao.</p>

    </article>
  </div>

  
  
  
  <script>
  window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
  ga('create', 'UA-169005401-1', 'auto');
  ga('send', 'pageview');
  </script>
  <script async src='https://www.google-analytics.com/analytics.js'></script>
  

  

  
<link rel="stylesheet" type="text/css" href="//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.0/cookieconsent.min.css" />
<script src="//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.0/cookieconsent.min.js"></script>
<script>
window.addEventListener("load", function(){
window.cookieconsent.initialise({
  "palette": {
    "popup": {
      "background": "#216942",
      "text": "#b2d192"
    },
    "button": {
      "background": "#afed71"
    }
  }
})});
</script>

</body>

</html>
