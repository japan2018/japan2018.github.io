<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>doc2vec on Memo Tut</title>
    <link>https://memotut.com/tags/doc2vec/</link>
    <description>Recent content in doc2vec on Memo Tut</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 13 Feb 2020 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://memotut.com/tags/doc2vec/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Summary about Doc2Vec</title>
      <link>https://memotut.com/summary-about-doc2vec-5ea94/</link>
      <pubDate>Thu, 13 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>https://memotut.com/summary-about-doc2vec-5ea94/</guid>
      <description>#Introduction  This time, I studied Doc2Vec as an extension of Word2Vec. &amp;ldquo;Document classification&amp;rdquo; and &amp;ldquo;document grouping (clustering)&amp;rdquo; are tasks that are often requested in natural language processing, but in order to implement them, a distributed representation of the document itself is required. If you use Doc2Vec, you can directly get the distributed representation.
##reference I have referred to the following to understand Doc2Vec.
 (doc2vec(Paragraph Vector) algorithm ](https://kitayamalab.wordpress.com/2016/12/10/doc2vecparagraph-vector-%e3%81%ae%e3%82%a2%e3%83%ab%e3%82%b4%e3%83%aa%e3%82%ba%e3%83%a0/) Distributed Representations of Sentences and Documents (original paper) Doc2Vec mechanism and document similarity calculation tutorial using gensim How to use natural language processing technology-I tried to predict the quality of papers using Doc2Vec and DAN!</description>
    </item>
    
    <item>
      <title>I made a learning kit of word2vec/doc2vec/GloVe/fastText</title>
      <link>https://memotut.com/i-made-a-learning-kit-of-word2vec-doc2vec-glove-fasttext-8240c/</link>
      <pubDate>Sat, 21 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://memotut.com/i-made-a-learning-kit-of-word2vec-doc2vec-glove-fasttext-8240c/</guid>
      <description>This article is the 21st day of [Natural Language Processing #2 Advent Calendar 2019](https://qiita.com/advent-calendar/2019/nlp2).  By the way, it&amp;rsquo;s my birthday today. Please celebrate ~~ M&amp;rsquo;s book that sets a deadline for birthday ~ ~
#Introduction
In the word embedding world, BERT has been terrifying over the past year, and even ELMo is becoming less visible. Sometimes I still want to use legacy distributed representations like word2vec or GloVe. In addition, you may want to learn with your data (at least for me) So, I made a learning kit for word2vec/doc2vec/GloVe/fastText for myself, so I will publish it.</description>
    </item>
    
  </channel>
</rss>