<!doctype html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  
  <meta name="generator" content="Hugo 0.72.0" />

  
  <meta name="description" content="Some description">
  

  
  <link rel="apple-touch-icon" sizes="180x180" href="https://memotut.com/apple-touch-icon.png">

  
  <link rel="icon" type="image/png" sizes="32x32" href="https://memotut.com/favicon-32x32.png">

  
  <link rel="icon" type="image/png" sizes="16x16" href="https://memotut.com/favicon-16x16.png">

  
  <link rel="manifest" href="https://memotut.com/site.webmanifest">

  
  <link rel="mask-icon" href="https://memotut.com/safari-pinned-tab.svg" color="#5bbad5">

  <meta name="msapplication-TileColor" content="#da532c">

  <meta name="theme-color" content="#ffffff">

  
  <link rel="stylesheet" href="https://memotut.com/css/bootstrap.min.css" />

  
  <title>Get started with object detection lightly | Memo Tut</title>
  

  <style>
body {
  min-width: 300px;
}

.custom-navbar {
  margin-bottom: 1em;
  height: 60px;
}

.custom-navbar a {
  display: inline-block; 
  padding: 18px 0;
  margin-right: 1em; 
  font-weight: bold; 
}

.custom-navbar a:hover,
.custom-navbar a:focus {
  text-decoration: none; 
}

@media print {
  .custom-navbar {
    display: none;
  }
}

article {
  padding-bottom: 1em;
}

img {
  max-width: 100%;
}


body {
  background-color: #fff;
}



body {
  color: #212529;
}



a {
  color: #007bff;
}



a:hover,
a:focus {
  color: #0056b3;
}



.custom-navbar {
  background-color: #212529;
}



.custom-navbar a {
  color: rgba(255, 255, 255, 0.75);
}



.custom-navbar a:hover,
.custom-navbar a:focus {
  color: rgba(255, 255, 255, 1);
}



.container {
  max-width: 800px;
}



pre {
  display: block;
  padding: 9.5px;
  word-break: break-all;
  word-wrap: break-word;
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 4px;
}

pre code {
  padding: 0;
  font-size: inherit;
  color: inherit; 
  white-space: pre-wrap;
  background-color: transparent;
  border: none;
  border-radius: 0;
}

code {
  padding: 2px 4px;
  color: inherit; 
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 4px;
  font-size: .9em;
}



blockquote,
.blockquote {
  padding: 10px 20px;
  margin: 0 0 20px;
  font-size: 1em;
  border-left: 5px solid #6c757d;
}

</style>
</head>

<body>
  <nav class="custom-navbar">
  <div class="container">
    
    <a href="/">Posts</a>
    
    <a href="/tags/">Tags</a>
    
    <a href="/about/">About</a>
    
    <a href="/index.xml">RSS</a>
    
  </div>
</nav>
  
  <div class="container">
    <article>
      <h1>Get started with object detection lightly</h1>
<p>
  <small class="text-secondary">
  
  
  Dec 15, 2019
  </small>
  

<small><code><a href="https://memotut.com/tags/python">Python</a></code></small>


<small><code><a href="https://memotut.com/tags/opencv"> OpenCV</a></code></small>


<small><code><a href="https://memotut.com/tags/object-detection"> object detection</a></code></small>


<small><code><a href="https://memotut.com/tags/objectdetection"> ObjectDetection</a></code></small>

</p>
<pre><code>This article is the 15th day article of [Kinki University Advent Calendar 2019](https://qiita.com/advent-calendar/2019/kindai).
</code></pre>
<p>In this article, I will write about object detection roughly.
It is written with reference to the contents of the survey paper &ldquo;<a href="https://arxiv.org/abs/1905.05055">Object Detection in 20 Years: A Survey</a>&rdquo;.</p>
<h1 id="1-what-is-object-detection">1. What is Object Detection?</h1>
<p>Object detection is the task of detecting instances of classes and semantic objects in images and videos. In the old days, it was a difficult task to get accuracy because there are two kinds of guidelines, classify and semantic. In recent years, as with other tasks in the CV field, the power of deep learning continues to improve accuracy at a dizzying rate.</p>
<h2 id="11-what-is-the-latest-object-detection-method">1.1 What is the latest object detection method?</h2>
<p>You can check SOTA (State-of-the-Art) method with <a href="https://paperswithcode.com/sota/object-detection-on-coco">Object Detection on COCO test-dev</a>.</p>
<p>![Screenshot 2019-11-13 16.48.32.png](<a href="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/249063/f7d08d73-f5ad-7b5e-6ca9-(dea2152c85f5.png)">https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/249063/f7d08d73-f5ad-7b5e-6ca9-(dea2152c85f5.png)</a></p>
<p>SOTA at the time of writing (November 13, 2019) uses Cascade Mask R-CNN (Triple-CNN) using <a href="https://arxiv.org/abs/1909.03625">CBNet: A Novel Composite Backbone Network Architecture for Object Detection</a>.ResNeXt152,multi-scale)&ldquo;andmAP53.3(COCO).</p>
<p>Due to high competition in the CV field, it is being updated soon in 3 months to 6 months. <a href="https://arxiv.org/abs/1811.04533v3">M2Det</a>,whichwasSOTAaroundJanuary2019,ismAP44.2(COCO),soithasincreasedbyabout10pointsinoneyear.Tolearnthelatestobjectdetectionusingdeeplearning,refertohoya012&rsquo;spapersummary(<a href="https://github.com/hoya012/deep_learning_object_detection)">https://github.com/hoya012/deep_learning_object_detection)</a>.</p>
<h1 id="2-history-of-object-detection">2. History of object detection</h1>
<p>The history of object detection is divided into two terms. &ldquo;Before the Deep Learning Attack&rdquo; from 2001 to 2013 and &ldquo;After the Deep Learning Attack&rdquo; from 2014. Neural networks have been booming in recent years due to improvements in machine specifications and the use of GPUs in recent years, but in conjunction with them, object detection has also evolved.</p>
<p><img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/249063/0e98175a-2d9b-78a8-34e4-3276e16da07f.png" alt="image.png">
The figure is taken from <a href="https://arxiv.org/abs/1905.05055">Object Detection in 20 Years: A Survey</a>.</p>
<p>Before the deep learning attack, the object was detected by considering the process of extracting the feature by looking at the numerical value of the image, but after the attack, the configuration and mechanism of the neural network are considered and adjusted. (Of course, it is important to learn the process of extracting features)</p>
<blockquote>
<p>In SNS, the human resources who have been called feature extraction entertainers have been required. In the deep world, human resources called hyperparameter adjusting entertainers are needed [Source required]</p>
</blockquote>
<p>Below is a brief description of the key technologies of each term.</p>
<h2 id="21-before-the-deep-attack">2.1 Before the Deep attack</h2>
<p>The object detection before the deep attack has been performed by the sliding window detection that extracts the features and moves the window showing the region to judge. The following is a typical method.</p>
<h3 id="vj-det-2000">VJ Det (2000)</h3>
<p>A real-time detector of human face called Viola Jones Detector. This is a type of detector that extracts Haar-like features that focus on the difference in brightness and performs cascade classification using a sliding window.
Haar-like feature is simply the sum of pixels in a certain area.
&ldquo;<a href="http://labs.eecs.tottori-u.ac.jp/sd/Member/oyamada/OpenCV/html/py_tutorials/py_objdetect/py_face_detection/py_face_detection.html#face-detection">Face Detection Using Harr Cascades</a>‚Äù describes the face detection method, and sample code for performing cascade detection with OpenCV.</p>
<p>It is an operation that calculates the added value of pixels and checks whether the patterns match.</p>
<p><img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/249063/103d8aff-523f-9e86-9248-91e448b1fc39.png" alt="image.png"></p>
<p>You can download the learning result (xml) of the detector here.
<a href="https://github.com/opencv/opencv/tree/master/data/haarcascades">https://github.com/opencv/opencv/tree/master/data/haarcascades</a></p>
<p>If you do not build OpenCV installation, it looks like this.</p>
<pre><code>pip install opencv-python opencv-contrib-python
</code></pre><p>VJDetector itself can be used like this.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> cv2
img <span style="color:#f92672">=</span> cv2<span style="color:#f92672">.</span>imread(<span style="color:#e6db74">&#34;input.jpg&#34;</span>)
detector <span style="color:#f92672">=</span> cv2<span style="color:#f92672">.</span>CascadeClassifier(<span style="color:#e6db74">&#34;haarcascade_frontalface_default.xml&#34;</span>)
gray_img <span style="color:#f92672">=</span> cv2<span style="color:#f92672">.</span>cvtColor(img, cv2<span style="color:#f92672">.</span>COLOR_BGR2GRAY)
face <span style="color:#f92672">=</span> detector<span style="color:#f92672">.</span>detectMultiScale(gray_img,scaleFactor<span style="color:#f92672">=</span><span style="color:#ae81ff">1.1</span>, minNeighbors<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>, minSize<span style="color:#f92672">=</span>(<span style="color:#ae81ff">30</span>, <span style="color:#ae81ff">30</span>))
<span style="color:#66d9ef">for</span> (x, y, w, h) <span style="color:#f92672">in</span> face:
  cv2<span style="color:#f92672">.</span>rectangle(img, (x, y), (x<span style="color:#f92672">+</span>w, y<span style="color:#f92672">+</span>h), (<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">300</span>), <span style="color:#ae81ff">4</span>)
cv2<span style="color:#f92672">.</span>imwrite(<span style="color:#e6db74">&#34;output.jpg&#34;</span>, img)
</code></pre></div><p>You can use it simply by throwing the xml file to cv2.CascadeClassifier.</p>
<p>Here is an example I actually used.
<img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/249063/fb6bd634-8a9e-2efd-f151-4aa17fba0615.png" alt="image.png">
It recognizes the face of the president and puts on a mosaic! (I&rsquo;m afraid this will become an international problem)</p>
<h3 id="hog-det-2006">HOG Det (2006)</h3>
<p>This is a detector that extracts the HOG features focusing on the distribution of the brightness gradient direction and classifies them by SVM while performing a sliding window.
For human detection, it is said that HOG features that can capture contour information are better than Haar features of contrast.
In OpenCV, it is implemented as cv2.HOGDescriptor. The code looks something like this.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> cv2
img <span style="color:#f92672">=</span> cv2<span style="color:#f92672">.</span>imread(<span style="color:#e6db74">&#34;input.jpg&#34;</span>)
hog <span style="color:#f92672">=</span> cv2<span style="color:#f92672">.</span>HOGDescriptor()
hog<span style="color:#f92672">.</span>setSVMDetector(cv2<span style="color:#f92672">.</span>HOGDescriptor_getDefaultPeopleDetector())
hogParams <span style="color:#f92672">=</span> {<span style="color:#e6db74">&#39;winStride&#39;</span>: (<span style="color:#ae81ff">8</span>, <span style="color:#ae81ff">8</span>),<span style="color:#e6db74">&#39;padding&#39;</span>: (<span style="color:#ae81ff">32</span>, <span style="color:#ae81ff">32</span>),<span style="color:#e6db74">&#39;scale&#39;</span>: <span style="color:#ae81ff">1.05</span>}
human, r <span style="color:#f92672">=</span> hog<span style="color:#f92672">.</span>detectMultiScale(img, <span style="color:#f92672">**</span>hogParams)
<span style="color:#66d9ef">for</span> (x, y, w, h) <span style="color:#f92672">in</span> human:
  cv2<span style="color:#f92672">.</span>rectangle(img, (x, y), (x<span style="color:#f92672">+</span>w, y<span style="color:#f92672">+</span>h), (<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">300</span>), <span style="color:#ae81ff">4</span>)
cv2<span style="color:#f92672">.</span>imshow(<span style="color:#e6db74">&#34;results human detect by def detector&#34;</span>, img)
cv2<span style="color:#f92672">.</span>waitKey(<span style="color:#ae81ff">0</span>)
</code></pre></div><h2 id="22-after-the-deep-attack">2.2 After the Deep attack</h2>
<p>Since the arrival of the Deep, the field of object detection has dramatically improved in accuracy. There are various technical terms such as CNN, VGG, and ResNet. These techniques are basically tasks derived from the image classification task, and the task of object detection tends to improve based on them.
The methods are divided according to the handling of two processes, classification and position estimation. A one-shot detector that performs both simultaneously and a two-shot detector that performs position estimation and then classification.
<img width="893" alt="Screenshots 2019-12-15 0.05.22.png" src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/249063/d01c1e20-9b05-9606-ed16-5ceb1b7a34c5.png">
In general, One-shot Detector has excellent detection speed, and Two-shot Detector has excellent detection accuracy.
(With the latest method, I honestly feel that there is not much difference.)</p>
<h3 id="one-shot-detector">One-shot Detector</h3>
<p>One-shot Detector is a detection method that performs image classification and position detection simultaneously. In many cases, it can be classified into YOLO system and SSD system. Of the one-shot detectors, which have excellent detection speed, the SSD system is often faster, and it is often said that the YOLO system is superior in detection accuracy.</p>
<ul>
<li>
<p><a href="https://arxiv.org/abs/1506.02640">YOLO</a></p>
</li>
<li>
<p><a href="https://arxiv.org/abs/1512.02325">SSD</a></p>
</li>
</ul>
<h3 id="two-shot-detector">Two-shot Detector</h3>
<p>Techniques starting with RCNN are mainly used in two-level classification. In recent years, we sometimes perform semantic segmentation that performs classification on a pixel-by-pixel basis, and it is an impression that it often exceeds One-shot Detector in terms of accuracy. (Of course, the labeled dataset that supports it is the basis)</p>
<ul>
<li><a href="https://arxiv.org/abs/1311.2524">RCNN</a></li>
</ul>
<h1 id="3-deep-library">3. Deep library</h1>
<p>Basically, there is a tendency to gain processing power by using the GPU, so it is necessary to link all libraries with CUDA. (Of course you can use only the CPU)
To be honest, it is really hard to match the version around that.</p>
<h3 id="tensorflowthis-is-a-library-developed-by-google-the-version-upgrade-is-progressing-with-tremendous-momentum-and-it-becomes-difficult-to-match-the-version-with-cuda-please-stop-making-api-changes-without-mercy-small-voice">tensorflowThis is a library developed by Google. The version upgrade is progressing with tremendous momentum, and it becomes difficult to match the version with CUDA. Please stop making API changes without mercy (small voice)</h3>
<h3 id="pytorch">pytorch</h3>
<p>A library of images that are in battle with tensorflow. I have the impression that pytorch is often used for the recent reproduction and implementation of Tsuyotsuyo papers. Especially popular with young people (selfish image)</p>
<h3 id="chainer">chainer</h3>
<p>This is a library that PFN has stopped updating recently. I like it for a while, but I think it will be a tough option for new use.</p>
<h3 id="keras">keras</h3>
<p>This library works with tensorflow etc. as the back end. The degree of abstraction is so high that even beginners of deep learning can easily create an image classification network using CNN. However, if you try to deal with something more advanced than classification, you have to write tensorflow code, and the definition of the loss function becomes cumbersome.</p>
<h1 id="4-tools-that-support-the-deep-environment">4. Tools that support the deep environment</h1>
<p>Here are the keywords for tools that are likely to be useful in supporting a deep development environment. I personally recommend nvidia-docker + native python + pip + etc..</p>
<h3 id="docker-kubernetes">Docker (kubernetes)</h3>
<p>It is a great tool that can save the environment by using virtual containers (vocabulary)
From v19, natively GPU compatible. Before that, you can create a container that uses GPU by installing nvidia-docker. Especially, it is easy to match the version with CUDA and it is easy to reproduce the environment of the library. The disadvantage is the learning cost of Docker&hellip;</p>
<h3 id="anaconda">Anaconda</h3>
<p>It is a python version and library management tool used for scientific and technical calculations. It often pollutes terminals for religious reasons.</p>
<h3 id="pyenv">pyenv</h3>
<p>There are various Python library management tools and they are divided into denominations (I don&rsquo;t quite understand the difference between virtualenv and pyenv).</p>
<h1 id="five-at-the-end">Five. At the end</h1>
<p>Since all tasks in the CV field are in intense competition, they are evolving at a great speed. We do not recommend this because it is a hot industry to enter as basic research, but I think that it is a task that can still be challenged as an application of these technologies.
Especially when implementing the reproduction of papers, it is necessary to write what is not written in the paper, what to do with the processing not written in the paper, and to write the processing that is not in the library in the first place. Yes. Recommended when you have time.
Problems such as resources and processing time that have existed so far are being solved, and it is becoming very easy to handle, so why not give it a try!</p>
<p>I ran out on the way, so I may add it in the future.</p>
<h1 id="references">References</h1>
<ul>
<li><a href="https://arxiv.org/abs/1905.05055">Object Detection in 20 Years: A Survey</a>: The history and methods of object detection are written in detail, so if you are interested, please read it. Ôºé</li>
<li><a href="https://arxiv.org/ads/1809.02165">Deep Learning for Generic Object Detection: A Survey</a>: This document mainly describes methods using deep learning.</li>
<li><a href="http://labs.eecs.tottori-u.ac.jp/sd/Member/oyamada/OpenCV/html/py_tutorials/py_tutorials.html">OpenCV-Python Tutorial</a></li>
<li><a href="https://arxiv.org/abs/1512.02325">SSD:Single Shot multi-box Detector</a></li>
<li><a href="https://qiita.com/mshinoda88/items/9770ee671ea27f2c81a9">History summary on object detection</a></li>
</ul>

    </article>
  </div>

  
  
  
  <script>
  window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
  ga('create', 'UA-169005401-1', 'auto');
  ga('send', 'pageview');
  </script>
  <script async src='https://www.google-analytics.com/analytics.js'></script>
  

  

  
<link rel="stylesheet" type="text/css" href="//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.0/cookieconsent.min.css" />
<script src="//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.0/cookieconsent.min.js"></script>
<script>
window.addEventListener("load", function(){
window.cookieconsent.initialise({
  "palette": {
    "popup": {
      "background": "#216942",
      "text": "#b2d192"
    },
    "button": {
      "background": "#afed71"
    }
  }
})});
</script>

</body>

</html>
