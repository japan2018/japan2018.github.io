<!doctype html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  
  <meta name="generator" content="Hugo 0.72.0" />

  
  <meta name="description" content="Some description">
  

  
  <link rel="apple-touch-icon" sizes="180x180" href="https://japan2018.github.io/apple-touch-icon.png">

  
  <link rel="icon" type="image/png" sizes="32x32" href="https://japan2018.github.io/favicon-32x32.png">

  
  <link rel="icon" type="image/png" sizes="16x16" href="https://japan2018.github.io/favicon-16x16.png">

  
  <link rel="manifest" href="https://japan2018.github.io/site.webmanifest">

  
  <link rel="mask-icon" href="https://japan2018.github.io/safari-pinned-tab.svg" color="#5bbad5">

  <meta name="msapplication-TileColor" content="#da532c">

  <meta name="theme-color" content="#ffffff">

  
  <link rel="stylesheet" href="https://japan2018.github.io/css/bootstrap.min.css" />

  
  <title>[Linear regression] Number of explanatory variables and coefficient of determination (adjusted for degrees of freedom) | Some Title</title>
  

  <style>
body {
  min-width: 300px;
}

.custom-navbar {
  margin-bottom: 1em;
  height: 60px;
}

.custom-navbar a {
  display: inline-block; 
  padding: 18px 0;
  margin-right: 1em; 
  font-weight: bold; 
}

.custom-navbar a:hover,
.custom-navbar a:focus {
  text-decoration: none; 
}

@media print {
  .custom-navbar {
    display: none;
  }
}

article {
  padding-bottom: 1em;
}

img {
  max-width: 100%;
}


body {
  background-color: #fff;
}



body {
  color: #212529;
}



a {
  color: #007bff;
}



a:hover,
a:focus {
  color: #0056b3;
}



.custom-navbar {
  background-color: #212529;
}



.custom-navbar a {
  color: rgba(255, 255, 255, 0.75);
}



.custom-navbar a:hover,
.custom-navbar a:focus {
  color: rgba(255, 255, 255, 1);
}



.container {
  max-width: 800px;
}



pre {
  display: block;
  padding: 9.5px;
  word-break: break-all;
  word-wrap: break-word;
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 4px;
}

pre code {
  padding: 0;
  font-size: inherit;
  color: inherit; 
  white-space: pre-wrap;
  background-color: transparent;
  border: none;
  border-radius: 0;
}

code {
  padding: 2px 4px;
  color: inherit; 
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 4px;
  font-size: .9em;
}



blockquote,
.blockquote {
  padding: 10px 20px;
  margin: 0 0 20px;
  font-size: 1em;
  border-left: 5px solid #6c757d;
}

</style>
</head>

<body>
  <nav class="custom-navbar">
  <div class="container">
    
    <a href="/">Posts</a>
    
    <a href="/tags/">Tags</a>
    
    <a href="/about/">About</a>
    
    <a href="/index.xml">RSS</a>
    
  </div>
</nav>
  
  <div class="container">
    <article>
      <h1>[Linear regression] Number of explanatory variables and coefficient of determination (adjusted for degrees of freedom)</h1>
<p>
  <small class="text-secondary">
  
  
  Apr 18, 2020
  </small>
  

<small><code><a href="https://japan2018.github.io/tags/python">Python</a></code></small>


<small><code><a href="https://japan2018.github.io/tags/matplotlib"> matplotlib</a></code></small>


<small><code><a href="https://japan2018.github.io/tags/ai"> AI</a></code></small>


<small><code><a href="https://japan2018.github.io/tags/statistics"> statistics</a></code></small>


<small><code><a href="https://japan2018.github.io/tags/sklearn"> sklearn</a></code></small>

</p>
<pre><code># at first
</code></pre>
<p>For the explanatory variables of linear regression,
&ldquo;It is better not to use variables unrelated to the objective variable for learning&rdquo; (*1)
I have the image.
However, while studying linear regression analysis,
I have learned that the coefficient of determination has the following properties.</p>
<p>&ldquo;When an explanatory variable is added, the coefficient of determination cannot fall below that before addition. (So, for accuracy comparison of models with different numbers of explanatory variables, use the &ldquo;coefficient of determination with adjusted degrees of freedom&rdquo;. )&rsquo;&rsquo; (*2)</p>
<p>The author, who knew the property, wanted to verify the following [Summary].</p>
<h1 id="overview">Overview</h1>
<p>Regarding the author&rsquo;s assumption (*1) and the nature of the coefficient of determination (*2), I tried the following two points (①, ②) (by actually making a model).
This article describes the verification method and results.</p>
<ul>
<li>① is theoretically valid, and ② is my assumption.</li>
</ul>
<p>① Comparison of coefficient of determination
→ When an explanatory variable unrelated to the objective variable is added, the coefficient of determination does not fall below the coefficient of determination before addition.</p>
<p>②Comparison of coefficient of determination with adjusted degrees of freedom
→ In the case of the coefficient of determination with adjusted degrees of freedom, isn&rsquo;t the value &ldquo;before adding&rdquo; a variable unrelated to the objective variable higher?</p>
<h1 id="premise">Premise</h1>
<p>[Linear regression]
A method that expresses the objective variable as &ldquo;primary combination (linear combination) of explanatory variables&rdquo; with respect to existing explanatory variables and objective variables. Since it is expressed by a linear combination, the expressed result (estimated value) is a straight line graph.
<img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/618640/e30a34b1-f0a3-a8cc-8431-9d1473d3f920.png" alt="Normdist_regression.png"></p>
<p>[Coefficient of determination]
The created linear regression model is an index that indicates &ldquo;how good the data fits&rdquo; and is expressed by the following calculation formula.
$ R^{2}=1-\dfrac {\sum e^{2}}{\sum \left( y-\overline {y}\right) ^{2}} $</p>
<p>${\sum e^{2}}: Residual fluctuation (sum of squares of difference between objective variable and estimated value) $
$ {\Sum \left( y-\overline {y}\right) ^{2}}: Total fluctuation (sum of squares of difference between objective variable and its mean) $</p>
<p>[Degree of freedom adjusted R-squared]
An index that adds the number of explanatory variables (k) to the coefficient of determination. Similar to the coefficient of determination, it indicates &ldquo;how good the fit is for the data&rdquo;.
${\widehat {R}^{2}}=1-\dfrac {n-1}{n-k}\left( 1-R^{2}\right) $</p>
<p>${n}: Data count $
${k}: Number of explanatory variables $</p>
<h1 id="verification-details">Verification details</h1>
<p>The two points (① and ②) described in the above [Summary] will be verified by the following methods.</p>
<p>[Data to prepare]
(1) Explanatory variables (related to objective variables)
→ Created using random numbers. The objective variable is created by linear combination of this variable.
(2) Explanatory variable (irrelevant to the objective variable)
→ Created using random numbers.
(3) Objective variable
→ Create with primary combination of explanatory variables in (1) above.</p>
<p>① Comparison of coefficient of determination
→ Check the difference in the coefficient of determination between &lt;Model 1&gt; and &lt;Model 2&gt; below (Model 2-Model 1).
&lt;Model 1&gt; (n pieces)
A model created with the variables (1 to n) of the above variable (1)
&lt;Model 2&gt; (n × m pieces)
For one of the above [Model 1],
A model created with &ldquo;Explanatory variables used for the model&rdquo; and &ldquo;Variables above (2) (1 to m)&rdquo;</p>
<p>②Comparison of coefficient of determination with adjusted degrees of freedom
→ For the above verification ①, confirm that the &ldquo;coefficient of determination&rdquo; is &ldquo;coefficient of determination adjusted for degrees of freedom&rdquo;.</p>
<h1 id="verification-procedure">Verification procedure</h1>
<p>*The following is the flow of the [verification code] as a procedure.
[1] Data creation
→ Create a data frame all_data that has the above variables (1) to (3).</p>
<p>(1) Explanatory variables (related to objective variables)
→ col_rel_<a href="n=0to10">n</a>
(2) Explanatory variable (irrelevant to the objective variable)
→ col_no_rel_<a href="m=0to10">m</a>
(3) Objective variable
→ target</p>
<p>[2] Model creation
→Create &lt;Model 1&gt; and &lt;Model 2&gt; above.
*Stored as dict type dict_models (as value), and key is as follows.</p>
<p>&lt;Model 1&gt; (n pieces)
→ nothing_ [n] _ [m]
*N and m are the number of explanatory variables used for learning.
N is the number of variables (1), m is the number of variables (2)
&lt;Model 2&gt; (n × m pieces)
→ contain_ [n] _ [m]</p>
<ul>
<li>*N and m have the same meaning as in &lt;Model 1&gt; above.</li>
</ul>
<p>[3] Calculate R-squared and R-squared adjusted R-squared
→ Hold both values in dict type (as value).
Each dict name is as follows.</p>
<p>Determination coefficient: dict_R2
R-adjusted coefficient of determination: dict_adjusted_R2</p>
<p>[4] Combine the aggregation results into one data frame
[5] Create two line graphs
→ The vertical axis is the coefficient of determination (adjusted degrees of freedom),</p>
<ul>
<li>The horizontal axis is &ldquo;the number of the above variables (2) used for model learning&rdquo;.</li>
</ul>
<h1 id="verification-code">Verification code</h1>
<pre><code class="language-python:experiment.jpynb" data-lang="python:experiment.jpynb">import random

import numpy as np
import pandas as pd

import matplotlib.pyplot as plt

from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split

# [Parameter]
# Seed value
random.seed(10)

#Number of data
data_num = 10 ** 4

# Number of items (rel: related to the target variable, no_rel: not related to the target variable)
group_num = 10

dict_group_num = dict()
dict_group_num[&quot;rel&quot;] = group_num # related to the objective variable
dict_group_num[&quot;no_rel&quot;] = group_num #Not related to the objective variable

# [Data creation]
#Explanatory variable
col_format ='col_{group}_{index}'

all_data = pd.DataFrame({
    col_format.format(
        group = key
        , index = i
    ):[random.randint(0 ,1000) for _ in range(10 ** 4)]
    for key ,val in dict_group_num.items()
    for i in range(val)
})

# Objective variable
w_list = [random.randint(1 ,10) for i in range(dict_group_num[&quot;rel&quot;])]
target_series = pd.Series(np.zeros(data_num))
for i in range(dict_group_num[&quot;rel&quot;]):
    w = w_list[i]
    col = col_format.format(
        group = &quot;rel&quot;
        , index = i
    )
    
    #Confirmation
    print('-' * 50)
    print(target_series.head())
    print(&quot;w=&quot; ,w)
    print(all_data[col].head())
    print(w * all_data[col].head())
    
    target_series = target_series + w * all_data[col]

    #Confirmation
    display(target_series.head())
    
all_data['target'] = target_series

# [Model creation]
# All combinations of explanatory variables
dict_features = {}
key_format = &quot;{type_}_{n}_{m}&quot;

#1. &quot;col_rel_x&quot; (n)
type_ = &quot;nothing&quot;
for n in range(dict_group_num[&quot;rel&quot;]):
    cols = (col_format.format(
                group = &quot;rel&quot;
                , index = i
            ) for i in range(n+1)]
    
    key = key_format.format(
        type_ = type_
        , n = n + 1
        , m = 0
    )
    dict_features[key] = cols

#2. &quot;col_rel_x&quot; (n) and &quot;col_no_rel_x&quot; (m)
type_ = &quot;contain&quot;
for n in range(dict_group_num[&quot;rel&quot;]):
    cols_rel = (col_format.format(
                group = &quot;rel&quot;
                , index = i
            ) for i in range(n+1)]
    for m in range(dict_group_num[&quot;no_rel&quot;]):
        cols_no_rel = (col_format.format(
                    group = &quot;no_rel&quot;
                    , index = i
                ) for i in range(m+1)]
        cols = cols_rel + cols_no_rel
        key = key_format.format(
            type_ = type_
            , n = n + 1
            , m = m + 1
        )
        dict_features[key] = cols

#Confirmation
type_ = &quot;nothing&quot;
print(&quot;-&quot; * 50 ,type_)
_dict = {key:val for key ,val in dict_features.items() if key[:len(type_)] == type_}
print(list(_dict.items())[:5])

type_ = &quot;contain&quot;
print(&quot;-&quot; * 50 ,type_)
_dict = {key:val for key ,val in dict_features.items() if key[:len(type_)] == type_}
print(list(_dict.items())[:5])

#Model creation
dict_models = {}
for key ,feature_cols in dict_features.items():
    # Split into explanatory and objective variables
    train_X = all_data[feature_cols]train_y = all_data['target']
    #Model creation
    model = LinearRegression()
    model.fit(train_X, train_y)
    
    dict_models[key] = model
    
    #Confirmation
    print(&quot;-&quot; * 50)
    print(&quot;key={}&quot;.format(key))
    print(model.intercept_)
    print(model.coef_)

#Confirmation
list(dict_models.keys())[:5]

#Coefficient of determination, adjusted coefficient of freedom
dict_R2 = {}
dict_adjusted_R2 = {}

for key ,feature_cols in dict_models.items():
    # Get model
    model = dict_models[key]
    
    # Extract all data (numpy)
    feature_cols = dict_features[key]
    X = np.array(all_data[feature_cols])
    y = np.array(all_data[&quot;target&quot;])
    
    #Coefficient of determination
    R2 = model.score(X ,y)
    dict_R2[key] = R2
    
    #Coefficient of determination with adjusted degrees of freedom
    n = data_num
    k = int(key.split(&quot;_&quot;)[1]) + int(key.split(&quot;_&quot;)[2])
    adjusted_R2 = 1-((n-1)/(n-k)) * (1-R2)
    dict_adjusted_R2[key] = adjusted_R2

#【inspection result】
R2_df = pd.DataFrame({
    &quot;key&quot;:list(dict_R2.keys())
    , &quot;R2&quot;:list(dict_R2.values())
})
adjusted_R2_df = pd.DataFrame({
    &quot;key&quot;:list(dict_adjusted_R2.keys())
    , &quot;adjusted_R2&quot;:list(dict_adjusted_R2.values())
})
result_df = pd.merge(R2_df, adjusted_R2_df ,on=&quot;key&quot; ,how='outer')
result_df['rel_num'] = result_df[&quot;key&quot;].str.split(&quot;_&quot; ,expand=True)[1].astype(int)
result_df['no_rel_num'] = result_df[&quot;key&quot;].str.split(&quot;_&quot; ,expand=True)[2].astype(int)

#Confirmation
print(len(R2_df))
print(len(adjusted_R2_df))
print(len(result_df))
display(R2_df.head())
display(adjusted_R2_df.head())
display(result_df.head())

# [Graph creation]
# Coefficient of determination
value = &quot;R2&quot;
fig = plt.figure(figsize=(10,10))

for i in range(dict_group_num[&quot;rel&quot;]):
    rel_num = i + 1
    df = result_df.query(&quot;rel_num == {}&quot;.format(rel_num))
    base = df.query(&quot;no_rel_num == 0&quot;)[value]
    x = df[&quot;no_rel_num&quot;]
    y = df[value].apply(lambda x:x-base)
    
    #Confirmation
# print(&quot;-&quot; * 50)
# print(&quot;base={}&quot;.format(base))
# print(&quot;x={}&quot;.format(x))
# print(&quot;y={}&quot;.format(y))
    
    plt.plot(x, y, marker ='o'
             , label=&quot;rel_num={}&quot;.format(rel_num))

plt.title(&quot;Diff of {}&quot;.format(value))
plt.legend(loc=&quot;upper left&quot; ,bbox_to_anchor=(1, 1))
plt.xlabel(&quot;no_rel_num&quot;)
plt.ylabel(value)
plt.grid()
plt.show()

#Save graph
fig.savefig(&quot;plot_R2.png&quot;)

#Coefficient of determination adjusted for degrees of freedom
value = &quot;adjusted_R2&quot;
fig = plt.figure(figsize=(10,10))

for i in range(dict_group_num[&quot;rel&quot;]):
    rel_num = i + 1
    df = result_df.query(&quot;rel_num == {}&quot;.format(rel_num))
    base = df.query(&quot;no_rel_num == 0&quot;)[value]
    x = df[&quot;no_rel_num&quot;]
    y = df[value].apply(lambda x:x-base)
    
    #Confirmation
# print(&quot;-&quot; * 50)
# print(&quot;base={}&quot;.format(base))
# print(&quot;x={}&quot;.format(x))
# print(&quot;y={}&quot;.format(y))
    
    plt.plot(x, y, marker ='o'
             , label=&quot;rel_num={}&quot;.format(rel_num))

plt.title(&quot;Diff of {}&quot;.format(value))
plt.legend(loc=&quot;upper left&quot; ,bbox_to_anchor=(1, 1))
plt.xlabel(&quot;no_rel_num&quot;)
plt.ylabel(value)
plt.grid()
plt.show()

#Save graph
fig.savefig(&quot;plot_adjusted_R2.png&quot;)
</code></pre><h1 id="inspection-result">inspection result</h1>
<p>[Graph 1] Coefficient of determination
The coefficient of determination increases monotonically as the number of added explanatory variables &ldquo;irrelevant to the objective variable&rdquo; &ldquo;irrelevant to the objective variable&rdquo; increases regardless of the number (rel_num) of explanatory variables.
In other words, it was confirmed that it was according to the theory.
<img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/618640/01a4461a-2dd4-1655-12f2-764d619dc4f6.png" alt="plot_R2.png">
[Graph 2] R-squared adjusted coefficient of freedom
About the number of explanatory variables &ldquo;related to the objective variable&rdquo;,
When it is 1 to 4, it is better to add the explanatory variable &ldquo;Not related to the objective variable&rdquo;.
(1 to 4 is not enough explanation, it seems that it is better to add a little information.)
At 9 to 10 pieces, there is almost no change.
(It seems that 9 to 10 are enough to explain the objective variable.)
On the other hand, in the case of 5 to 8, the coefficient of determination is higher when the explanatory variable &ldquo;no relation to the objective variable&rdquo; is added.
It was confirmed that it was as I expected.
<img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/618640/618f4107-3347-9dbe-f8c4-9b7f84ec3545.png" alt="plot_adjusted_R2.png"></p>
<p>#Summary
When comparing models using the coefficient of determination, we must be careful about the number of explanatory variables used for learning.
As can be confirmed from the verification results, in the coefficient of determination, the coefficient of determination is high when the explanatory variables are &ldquo;there are many inclusive of the other part&rdquo; (even when variables not suitable for use are added). , Or equal.
Therefore, in that case, it seems better to use the &ldquo;coefficient of determination adjusted by the degree of freedom&rdquo; for a more appropriate comparison.</p>

    </article>
  </div>

  
  
  
  <script>
  window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
  ga('create', 'UA-123456789-1', 'auto');
  ga('send', 'pageview');
  </script>
  <script async src='https://www.google-analytics.com/analytics.js'></script>
  

  

  
<link rel="stylesheet" type="text/css" href="//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.0/cookieconsent.min.css" />
<script src="//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.0/cookieconsent.min.js"></script>
<script>
window.addEventListener("load", function(){
window.cookieconsent.initialise({
  "palette": {
    "popup": {
      "background": "#216942",
      "text": "#b2d192"
    },
    "button": {
      "background": "#afed71"
    }
  }
})});
</script>

</body>

</html>
