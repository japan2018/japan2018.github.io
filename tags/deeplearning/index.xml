<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title> DeepLearning on Some Title</title>
    <link>https://japan2018.github.io/tags/deeplearning/</link>
    <description>Recent content in  DeepLearning on Some Title</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 04 May 2020 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://japan2018.github.io/tags/deeplearning/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Difference between keras and tf.keras and compatibility verification #1</title>
      <link>https://japan2018.github.io/difference-between-keras-and-tf.keras-and-compatibility-verification-#1-fcfbf/</link>
      <pubDate>Mon, 04 May 2020 00:00:00 +0000</pubDate>
      
      <guid>https://japan2018.github.io/difference-between-keras-and-tf.keras-and-compatibility-verification-#1-fcfbf/</guid>
      <description>#Keras development end  It&amp;rsquo;s been a while since I posted. As you can see on github, it seems that development of the multi-backend Keras ended in April 2020. It will be replaced by Keras in Tensorflow in the future. keras-team (For convenience, the multi-backend Keras is expressed as mb-keras, and the Tensorflow Keras is expressed as tf-keras.) I would like to compare and verify mb-keras and tf-keras in this article.</description>
    </item>
    
    <item>
      <title>Display image after Data Augmentation with Pytorch</title>
      <link>https://japan2018.github.io/display-image-after-data-augmentation-with-pytorch-feb9f/</link>
      <pubDate>Mon, 10 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>https://japan2018.github.io/display-image-after-data-augmentation-with-pytorch-feb9f/</guid>
      <description>##background  **I want to display the image after data augmentation! **
I decided to implement it.
Data Augmentation is a technology to inflate one image, and the following operations are added.
 Random Crop Random Horizontal Flip (the image is horizontally flipped with a certain probability) Random Erasing (randomly add noise to a part of the image) Random Affine (scales images randomly)  There are many other things.
##Implementation</description>
    </item>
    
    <item>
      <title>XAI : AdvImg - Adversarial Images</title>
      <link>https://japan2018.github.io/xai-advimg-adversarial-images-fdc7a/</link>
      <pubDate>Mon, 06 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>https://japan2018.github.io/xai-advimg-adversarial-images-fdc7a/</guid>
      <description>#Target  I tried to generate a hostile image using Microsoft Cognitive Toolkit (CNTK).
It is assumed that CNTK and NVIDIA GPU CUDA are installed.
#Introduction
What is a hostile image? A very interesting model in the field of deep learning is the Adversarial Generating Network (GAN) [1].AdversarialExamples[2][3] influenced the birth of GAN. We will focus on the images here, so we will be dealing with hostile images.
Adversarial images are known to be a very effective way to deceive trained image recognition models.</description>
    </item>
    
    <item>
      <title>Tips [Replace/Replace/Convert/Change/Update/Replace] Replacing the Placeholder of INPUT in FreezeGraphed .pb file of Tensorflow</title>
      <link>https://japan2018.github.io/tips-replace-replace-convert-change-update-replace-replacing-the-placeholder-of-input-in-freezegraphed-.pb-file-of-tensorflow-fed7e/</link>
      <pubDate>Mon, 30 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://japan2018.github.io/tips-replace-replace-convert-change-update-replace-replacing-the-placeholder-of-input-in-freezegraphed-.pb-file-of-tensorflow-fed7e/</guid>
      <description>**`[1, ?, ?, 3]`** The **`Placeholder`** of the .pb file defined with the input size of **`[1, 513, 513, 3]`** Sample program that regenerates .pb by forcibly replacing it with **`Placeholder`**.  For the name=&#39;image&#39; part, freely specify the name of the placeholder after replacement. The placeholder name of the model before conversion is specified in the image:0 part of input_map={&#39;image:0&#39;: inputs}.
import tensorflow as tf from tensorflow.tools.graph_transforms import TransformGraph with tf.</description>
    </item>
    
    <item>
      <title>Computer Vision : Object Detection Part1 - Bounding Box preprocessing</title>
      <link>https://japan2018.github.io/computer-vision-object-detection-part1-bounding-box-preprocessing-f94eb/</link>
      <pubDate>Fri, 20 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://japan2018.github.io/computer-vision-object-detection-part1-bounding-box-preprocessing-f94eb/</guid>
      <description>#Target  Summary of object detection using Microsoft Cognitive Toolkit (CNTK).
Part 1 prepares for object detection using Microsoft Cognitive Toolkit. As a training data set for object detection, Microsoft Common Objects in Context (COCO) provided by Microsoft is used.
We will introduce them in the following order.
 Object detection by neural network Pretreatment of bounding box Dimension Clustering for creating Anchor Box Create a file to be read by the built-in reader provided by CNTK  #Introduction</description>
    </item>
    
  </channel>
</rss>