<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>NLP on Memo Tut</title>
    <link>https://memotut.com/tags/nlp/</link>
    <description>Recent content in NLP on Memo Tut</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 10 May 2020 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://memotut.com/tags/nlp/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>[Python] [Natural language processing] I tried to visualize the comments of each member in the Slack community</title>
      <link>https://memotut.com/dca470fbc/</link>
      <pubDate>Sun, 10 May 2020 00:00:00 +0000</pubDate>
      
      <guid>https://memotut.com/dca470fbc/</guid>
      <description>## About this article  In this article, I will introduce a method to visualize what each member says in the Slack community with Wordcloud.
The source code can be found here [https://github.com/sota0121/slack-msg-analysis) :octocat:
I would also like to read: [Natural language processing] I tried to visualize the topics raised this week in the Slack community
table of contents  Usage and output example Get message from Slack Pre-processing: table creation/cleaning/morphological analysis/normalization/stopword removal Pre-processing: Extracting important words (tf-idf) Visualization processing with Wordcloud Bonus  *I would like to summarize the preprocessing in another article in the future</description>
    </item>
    
    <item>
      <title>[Python] [Natural language processing] I tried to visualize the topics raised this week in the Slack community</title>
      <link>https://memotut.com/41630aa02/</link>
      <pubDate>Sat, 09 May 2020 00:00:00 +0000</pubDate>
      
      <guid>https://memotut.com/41630aa02/</guid>
      <description>## About this article  In this article, I will introduce a method of using Wordcloud to visualize what topics were raised within a certain period (here one week) of the Slack community.
The source code can be found here [https://github.com/sota0121/slack-msg-analysis) :octocat:
I would also like to read: [Natural language processing] I tried to visualize the comments of each member in the Slack community
table of contents  Usage and output example Get message from Slack Pre-processing: message mart table creation Pre-treatment: Cleaning Pre-processing: Morphological analysis (Janome) Pre-processing: Normalization Pre-processing: Stop word removal Pre-processing: Extract important words (tf-idf) Visualization processing with Wordcloud Bonus  *I would like to summarize the preprocessing in another article in the future</description>
    </item>
    
    <item>
      <title>[Python] Implemented AllenNLP linkage function in Japanese analysis tool Konoha</title>
      <link>https://memotut.com/f1d29cb43/</link>
      <pubDate>Sun, 03 May 2020 00:00:00 +0000</pubDate>
      
      <guid>https://memotut.com/f1d29cb43/</guid>
      <description>### Introduction  &amp;lt;img src=&amp;quot;https://github-link-card.s3.ap-northeast-1.amazonaws.com/himkt/konoha.png&amp;quot;width=&amp;quot;460px&amp;quot;&amp;gt;
We are developing a morphological analysis library called konoha. We have implemented the AllenNLP integration for this library, so this time we will introduce it. By using this function, Japanese text can be passed to the allennlp train command without any preprocessing such as segmentation.
AllenNLP AllenNLP is a natural language processing library developed by Allen Institute for Artificial Intelligence. AllenNLp is a very powerful tool, but if you want to handle Japanese data, it may be necessary to perform pre-processing that performs morphological analysis beforehand.</description>
    </item>
    
    <item>
      <title>[Python] Easily learn 100 language processing knocks 2020 with Google Colaboratory</title>
      <link>https://memotut.com/e4694d184/</link>
      <pubDate>Sat, 18 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>https://memotut.com/e4694d184/</guid>
      <description># 100 language processing What is knock 2020  100 language processing knock 2020
It is a collection of problems on the above site. I quote from the site.
 100 Language Processing Knock is a collection of problems aimed at learning programming, data analysis, and research skills in a fun way while tackling practical and exciting challenges.
 I am very grateful to be able to publish such wonderful things for free.</description>
    </item>
    
    <item>
      <title>[Python] 100 language processing knocks 2020 version released! What has changed</title>
      <link>https://memotut.com/3c17943ce/</link>
      <pubDate>Wed, 08 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>https://memotut.com/3c17943ce/</guid>
      <description>#Introduction  100 knocks of language processing that have been popular for a long time as a collection of problems that you can enjoy learning the basics of natural language processing, its 2020 version was released on 4/6 ! This is the first revision in five years. 2015 version but those who are interested, those who feel sorry that the 15th edition Qiita article is no longer useful, 15th edition For the person who was doing halfway, but the 20-year edition is coming out and it seems to be heartbreaking, I will summarize what has changed.</description>
    </item>
    
    <item>
      <title>[Python] Challenges for large-scale models such as BERT</title>
      <link>https://memotut.com/e72d82635/</link>
      <pubDate>Tue, 25 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>https://memotut.com/e72d82635/</guid>
      <description># Breakthrough model of natural language processing-BERT  BERT[BidirectionalEncoderRepresentationsfromTransformers]wasannouncedtoGoogle&amp;rsquo;steaminthefallof2018.Thisistheresultoflearningamodelinarevolutionarywayforaverylargenetworkwithalargeamountofdatausingatransformerarchitecture.PleaserefertoThisarticle for learning method and accuracy.
In the field of machine learning where open source and free exchange of information are important, new ideas are published as papers at arxiv.org, and models are shared on github. Doing is the basic way. As soon as new ideas are published, they can be referenced and reused by machine learning researchers and development teams around the world.</description>
    </item>
    
    <item>
      <title>[Python] Introduction of ALBERT (using MeCab&#43;Sentencepiece) model that learned large-scale Japanese business news corpus</title>
      <link>https://memotut.com/b41dcf018/</link>
      <pubDate>Mon, 17 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>https://memotut.com/b41dcf018/</guid>
      <description>#Introduction  Previously, Japanese BERT pre-learned modelandXLNetpre-learnedmodel It is the stockmark Morinaga who posted the introduction articles such as. Thank you for reading the article on model release to many people.
This time, we will release the pre-learned Japanese model of ALBERT. Now, while various pre-learned models have been proposed, why do you publish ALBERT Japanese model?ALBERT is not just SOTA as described as A Lite BERT , Because it is a model that makes the BERT lighter while maintaining and improving accuracy.</description>
    </item>
    
    <item>
      <title>[Python] A story of trying to reproduce Katsuno Isono who does not react to inconvenient things by natural language processing.</title>
      <link>https://memotut.com/57eb54ac3/</link>
      <pubDate>Sun, 16 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>https://memotut.com/57eb54ac3/</guid>
      <description>#Introduction  COTOHA API is a Japanese natural language processing API. It is an API that has various functions and is quite fun to use even a free account can be used 1000/day.
COTOHA API
By the way, do you know Isono-kun? Yes, this is Katsuno Isono from a certain national anime.
The name of this Isono-kun is called something in the play, but when it&amp;rsquo;s inconvenient, he doesn&amp;rsquo;t hear or reacts so-called &amp;ldquo;human-like&amp;rdquo;.</description>
    </item>
    
    <item>
      <title>[Python] Miya is not enough for our communication.</title>
      <link>https://memotut.com/e63f6e582/</link>
      <pubDate>Thu, 13 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>https://memotut.com/e63f6e582/</guid>
      <description>#TL;DR  By using the COTOHA API, we have proposed a poem that is close to what you want to convey from the Hyakunin Isshu. As a gateway to more &amp;ldquo;elegant&amp;rdquo; communication, you can bridge between engineers and non-engineers. (May be)
| What I want to convey | Proposed Waka | Meaning | |&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;- &amp;mdash;&amp;mdash;&amp;mdash;|&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;- &amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;|&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;- &amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;ndash; &amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;ndash; &amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;| | I feel very lonely to hear that I will be transferred.</description>
    </item>
    
    <item>
      <title>[Python] I made a library that divides Japanese sentences nicely</title>
      <link>https://memotut.com/353598500/</link>
      <pubDate>Mon, 16 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://memotut.com/353598500/</guid>
      <description>#Introduction  Recently, the development of natural language processing technology has been remarkable and is being applied in various fields. Although I often do business using natural language processing technology and AI, the most troublesome (but important) work is related to various preprocessing.
The main pre-processing that you will perform for most tasks are:
 Cleaning  Removes noise in text such as HTML tags and symbols   Normalization  Full-width/half-width and unification of capital letters and small letters   ** sentence segmentation **  Detects sentence breaks and splits   Wordization  Split sentence into a sequence of words   Remove stopword  Remove unnecessary words for the task you want to solve    I mainly use Python, but I didn&amp;rsquo;t have a suitable library for ** Japanese sentence breaks **, and I had to write similar code every time.</description>
    </item>
    
    <item>
      <title>[Python] Hands-on to visualize your tweet while understanding BERT</title>
      <link>https://memotut.com/09fda4c5c/</link>
      <pubDate>Thu, 12 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://memotut.com/09fda4c5c/</guid>
      <description>This article is the 13th day article of [NTT Communications Advent Calendar 2019](https://qiita.com/advent-calendar/2019/nttcom).  Yesterday was @nitky&amp;rsquo;s article, &amp;ldquo;We deal with threat intelligence in a mood&amp;rdquo; (https://qiita.com/nitky/items/ccefd0353b74aa21e357).
#Introduction Hi, my name is yuki uchida, and I belong to the SkyWay team of NTT Communications. This article is an article that visualizes your tweet using the language model BERT used for natural language processing, which was recently applied to Google search. I will write in a hands-on format so that as many people as possible can try it out, so if you are interested, please give it a try.</description>
    </item>
    
    <item>
      <title>[Python] I tried MindMeld for the first time</title>
      <link>https://memotut.com/9e9d80d24/</link>
      <pubDate>Wed, 04 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://memotut.com/9e9d80d24/</guid>
      <description>This article was posted as the 10th day of the Cisco Systems Japan Advent Calendar 2019 by Cisco colleagues.  Click here for past calendars 2017 edition: https://qiita.com/advent-calendar/2017/cisco
 -&amp;gt; Distribute policy tag with LISP, tried TrustSec with CML/VIRL 2018 version: https://qiita.com/advent-calendar/2018/cisco -&amp;gt; I tried using Cisco&amp;rsquo;s SaaS monitoring and behavior detection tool for multi-cloud environment monitoring  This year, the 3rd year since joining the company (Advent Calendar has also participated for 3 consecutive years!</description>
    </item>
    
    <item>
      <title>[Python] I made a library konoha to switch the tokenizer to a nice one.</title>
      <link>https://memotut.com/bb9ffa4d9/</link>
      <pubDate>Thu, 14 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>https://memotut.com/bb9ffa4d9/</guid>
      <description># TL; DR  I will introduce konoha, a library for tokenizing sentences. (Formerly tiny_tokenizer) You can use it like â†“. What?
from konoha import WordTokenizer sentence = &amp;#34;I am studying natural language processing&amp;#34; tokenizer = WordTokenizer(&amp;#39;MeCab&amp;#39;) print(tokenizer.tokenize(sentence)) # -&amp;gt; [natural, language, processing, study, study, do, do, masu] tokenizer = WordTokenizer(&amp;#39;Kytea&amp;#39;) print(tokenizer.tokenize(sentence)) # -&amp;gt; [natural, language, processing, study, study, te, i, ma, su] tokenizer = WordTokenizer(&amp;#39;Sentencepiece&amp;#39;, model_path=&amp;#34;data/model.spm&amp;#34;) print(tokenizer.tokenize(sentence)) # -&amp;gt; [, natural, language, processing, studying, doing, doing] Introduction: What is a tokenizer?</description>
    </item>
    
    <item>
      <title>[Python] 100 amateur language processing knocks: Summary</title>
      <link>https://memotut.com/fb50ba809/</link>
      <pubDate>Thu, 04 May 2017 00:00:00 +0000</pubDate>
      
      <guid>https://memotut.com/fb50ba809/</guid>
      <description>It is a summary of the challenge record of [100 language processing knock 2015](http://www.cl.ecei.tohoku.ac.jp/nlp100/).  :warning:** This is not a challenge record of 100 processing of language knock 2020.Theold2015version is the target. Please note: bangbang:**
Challenge environment Ubuntu 16.04 LTS + Python 3.5.2 :: Anaconda 4.1.1 (64-bit). (Only Problem 00andProblem01arePython2.7.)
#Chapter 1: Preparatory exercise
 Reviewing somewhat advanced programming language topics while working on materials that deal with text and strings.</description>
    </item>
    
  </channel>
</rss>