<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>BigQuery on Some Title</title>
    <link>https://japan2018.github.io/tags/bigquery/</link>
    <description>Recent content in BigQuery on Some Title</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 22 Feb 2020 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://japan2018.github.io/tags/bigquery/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>I tried using the BigQuery Storage API</title>
      <link>https://japan2018.github.io/i-tried-using-the-bigquery-storage-api-fa93d/</link>
      <pubDate>Sat, 22 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>https://japan2018.github.io/i-tried-using-the-bigquery-storage-api-fa93d/</guid>
      <description>#Introduction  Basic machine learning procedure: â‘¡Let&amp;rsquo;s prepare data In the Pytohn environment, the process of importing the table created by BigQuery in Pandas Dataframe format has been started. It was
However, if the size of the table becomes large, it will take a long time. Probably, there were many people who had such troubles. Then, a new service called BigQuery Storage API came out.
I heard that it is 7 to 8 times faster, but what is it?</description>
    </item>
    
  </channel>
</rss>