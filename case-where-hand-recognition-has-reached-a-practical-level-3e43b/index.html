<!doctype html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  
  <meta name="generator" content="Hugo 0.72.0" />

  
  <meta name="description" content="Some description">
  

  
  <link rel="apple-touch-icon" sizes="180x180" href="https://memotut.com/apple-touch-icon.png">

  
  <link rel="icon" type="image/png" sizes="32x32" href="https://memotut.com/favicon-32x32.png">

  
  <link rel="icon" type="image/png" sizes="16x16" href="https://memotut.com/favicon-16x16.png">

  
  <link rel="manifest" href="https://memotut.com/site.webmanifest">

  
  <link rel="mask-icon" href="https://memotut.com/safari-pinned-tab.svg" color="#5bbad5">

  <meta name="msapplication-TileColor" content="#da532c">

  <meta name="theme-color" content="#ffffff">

  
  <link rel="stylesheet" href="https://memotut.com/css/bootstrap.min.css" />

  
  <title>Case where hand recognition has reached a practical level | Memo Tut</title>
  

  <style>
body {
  min-width: 300px;
}

.custom-navbar {
  margin-bottom: 1em;
  height: 60px;
}

.custom-navbar a {
  display: inline-block; 
  padding: 18px 0;
  margin-right: 1em; 
  font-weight: bold; 
}

.custom-navbar a:hover,
.custom-navbar a:focus {
  text-decoration: none; 
}

@media print {
  .custom-navbar {
    display: none;
  }
}

article {
  padding-bottom: 1em;
}

img {
  max-width: 100%;
}


body {
  background-color: #fff;
}



body {
  color: #212529;
}



a {
  color: #007bff;
}



a:hover,
a:focus {
  color: #0056b3;
}



.custom-navbar {
  background-color: #212529;
}



.custom-navbar a {
  color: rgba(255, 255, 255, 0.75);
}



.custom-navbar a:hover,
.custom-navbar a:focus {
  color: rgba(255, 255, 255, 1);
}



.container {
  max-width: 800px;
}



pre {
  display: block;
  padding: 9.5px;
  word-break: break-all;
  word-wrap: break-word;
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 4px;
}

pre code {
  padding: 0;
  font-size: inherit;
  color: inherit; 
  white-space: pre-wrap;
  background-color: transparent;
  border: none;
  border-radius: 0;
}

code {
  padding: 2px 4px;
  color: inherit; 
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 4px;
  font-size: .9em;
}



blockquote,
.blockquote {
  padding: 10px 20px;
  margin: 0 0 20px;
  font-size: 1em;
  border-left: 5px solid #6c757d;
}

</style>
</head>

<body>
  <nav class="custom-navbar">
  <div class="container">
    
    <a href="/">Posts</a>
    
    <a href="/tags/">Tags</a>
    
    <a href="/about/">About</a>
    
    <a href="/index.xml">RSS</a>
    
  </div>
</nav>
  
  <div class="container">
    <article>
      <h1>Case where hand recognition has reached a practical level</h1>
<p>
  <small class="text-secondary">
  
  
  Dec 19, 2019
  </small>
  

<small><code><a href="https://memotut.com/tags/python">Python</a></code></small>


<small><code><a href="https://memotut.com/tags/machine-learning"> machine learning</a></code></small>


<small><code><a href="https://memotut.com/tags/tensorflow"> TensorFlow</a></code></small>


<small><code><a href="https://memotut.com/tags/mediapipe"> MediaPipe</a></code></small>


<small><code><a href="https://memotut.com/tags/handtracking"> handtracking</a></code></small>

</p>
<pre><code>In HandTracking of MediaPipe (Google), I will write about the gesture implementation that is being talked about in Issue.
</code></pre>
<h1 id="it-worked-like-this">It worked like this</h1>
<p>Seeing is believing that the picture is worth a thousand words.
<img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/35973/c3a119b0-364d-c5ea-dca4-52d3242c34f0.gif" alt="output.gif"></p>
<h1 id="mediapipe">MediaPipe</h1>
<p>With the announcement of MediaPipe at CVPR 2019 in June this year,
Hand Tracking has been released as one of the applications.
MediaPipe doesn&rsquo;t require you to write code called ML pipeline
A framework for building ML applications using only visualization tools.</p>
<p><img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/35973/96f2923d-9881-e53c-1faa-9d0ecb177cd0.png" alt="image.png"></p>
<p>People who have done <a href="http://wiki.ros.org/ja">ROS</a> will find it easy to imagine the tool.
In addition to HandTracking, object tracking applications for face recognition, hair segmentation, and object recognition are available.</p>
<p>#HandTracking multi-platform support status
It works in the following environment.</p>
<ul>
<li>Desktop (C++)</li>
<li>iOS (ObjC)</li>
<li>Android (Java)</li>
<li><a href="https://github.com/google/mediapipe/blob/master/mediapipe/docs/web.md">WebAssembly</a>(willbereleasedattheendof'19)</li>
</ul>
<p>Actually confirmed the operation on the desktop (Linux) and iOS.
If you want to try HandTracking immediately, you can try <a href="https://github.com/google/mediapipe/blob/master/mediapipe/docs/web.md">Demo</a> of WebAssembly version.</p>
<p>#What&rsquo;s amazing about this Hand Tracking?</p>
<p>It is necessary to realize both recognition accuracy and processing speed in order for HandTracking to reach a usable level.
In general, it is difficult to achieve a balance between recognition accuracy and processing speed due to the trade-off relationship.</p>
<p>As an approach, by focusing on the palm recognition, the number of anchors (areas) is reduced to 3 to 1/5 compared to the approach that includes fingers, and high accuracy is achieved.
As a result, the amount of calculation can be reduced, and high speed and high accuracy can be achieved.</p>
<p>See the blog for details.
<a href="https://ai.googleblog.com/2019/08/on-device-real-time-hand-tracking-with.html">On-Device, Real-Time Hand Tracking with MediaPipe</a></p>
<p>#Where can I use hand recognition?</p>
<p><img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/35973/49f721cc-634a-f217-8703-ca42f7fef7b7.gif" alt="r-vte.gif"></p>
<p>It can be used in PC screen operation and VR space.</p>
<ul>
<li>Interaction with virtual objects</li>
<li>Gesture-based control interface</li>
<li>Understanding sign language</li>
</ul>
<p>Gestures are required to support these tasks.
However, it seems that there are no plans to implement gestures in MediaPipe&rsquo;s HandTracking.</p>
<ul>
<li><a href="https://github.com/google/mediapipe/issues/40">Identification of Gesture in hand tracking model #40</a></li>
<li><a href="https://github.com/google/mediapipe/issues/82">Details on Gesture Recognition Approach #82</a></li>
</ul>
<h2 id="trying-to-implement-a-gesture">Trying to implement a gesture</h2>
<p>This time, I tried implementing the distance index of the blog introduced in Issue<a href="https://github.com/google/mediapipe/issues/40">#40</a>.</p>
<ul>
<li><a href="https://medium.com/tensorflow/move-mirror-an-ai-experiment-with-pose-estimation-in-the-browser-using-tensorflow-js-2f7b769f9b23">Move Mirror: An AI Experiment with Pose Estimation in the Browser using TensorFlow.js</a></li>
</ul>
<p>In the blog, I tried using whole body posture estimation (PoseNet), but it seems that it can be used with hand gestures.
The implementation is to scale the vector, perform L2 normalization, and obtain the cosine similarity.
Lastly, I searched with VPTree. .. .. It doesn&rsquo;t work.</p>
<p>Once again, I recall the implementation of HandTracking.</p>
<p>Two models are used for this hand recognition.</p>
<ul>
<li>Palm detector model (BlazePalm) palm_detection.tflite</li>
<li>Hand landmark model hand_landmark.tflite</li>
</ul>
<p>As a processing procedure,</p>
<ol>
<li>Identify palm direction and area</li>
<li>Infer the hand mark from the area after rotating it according to the direction</li>
</ol>
<p>I implemented it by saying &ldquo;Ah, rotation, should I rotate it?&rdquo;</p>
<h1 id="introduction">Introduction</h1>
<p>The environment was built on Mac.
The Python version of HandTracking is not accurate, so the accuracy is not good enough.
Please acknowledge and try it.</p>
<p>Regarding the introduction, MediaPipe has an extension of Tensorflow.
It seems that it will be implemented in Tensorflow in the future, but it is troublesome to install it because a build is required.
So I skipped over and introduced a custom model.
<a href="https://github.com/metalwhale/hand_tracking/blob/master/palm_detection_without_custom_op.tflite">palm_detection_without_custom_op.tflite</a></p>
<h3 id="build-from-command-line">build from command line</h3>
<pre><code>$ git clone https://github.com/metalwhale/hand_tracking
$ cd hand_tracking
$ wget https://raw.githubusercontent.com/wolterlw/hand_tracking/optical_flow/hand_tracker2.py

// Changes to get the value before rotation
$ diff -u hand_tracker2.py.1 hand_tracker2.py
- -- hand_tracker2.py.1 2019-12-19 18:14:51.767858700 +0900
+++ hand_tracker2.py 2019-12-19 18:14:32.905475000 +0900
@@ -269,6 +269,7 @@
         Minv = np.linalg.inv(Mtr)
         kp_orig = (self._pad1(reg) @ Minv.T)[:,:2]
         hand['joints'] = kp_orig
+ hand['base_joints'] = reg
         return hand

     def __call__(self, img, hands=None):

$ wget https://gist.githubusercontent.com/otmb/d8837508d2694b11fbbda8229b9bb4ec/raw/5f8088239110549d0cab527699f8770b50990c87/hand_gesture.py
$ pip3 install opencv-python tensorflow
$ pip3 install scikit-learn
$ pip3 install vptree
$ mkdir gestures
// Add an appropriate gesture image to the gestures folder and execute
$python3 hand_gesture.py
</code></pre><p>I wanted to introduce a gesture on iOS, but it seems that rotation cannot be obtained from the API so far.
<a href="https://github.com/google/mediapipe/issues/237">Access hand landmarks position in iOS #237</a></p>
<h2 id="20191225-addendum">2019/12/25 Addendum</h2>
<p>In the above procedure, adding a gesture image is troublesome, so we prepared a code that extracted feature points.
The following gestures are supported.
<img width="400" alt="gestures.png" src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/35973/01e58b7a-0d13-6a14-d0e5-61f8fb84e7d5.png"></p>
<pre><code>$ wget https://gist.githubusercontent.com/otmb/d756d3359d74be070f92812db9b13f3c/raw/e7c0e1c64f4c61870c27e5af4c682a9e18b7249e/hand_gesture.py
</code></pre><p>I think some people have tried it and thought that the recognition accuracy was not good enough.
To check the actual recognition accuracy of MediaPipe, try <a href="https://storage.googleapis.com/mediapipe-viz.appspot.com/wasm-demos/hand-tracking/hand_tracking_demo.html">Demo</a> of WebAssembly version. I think that is good.</p>
<h1 id="in-conclusion">in conclusion</h1>
<p>This time, I tried the gesture implementation.
Finally, HandTracking has finally reached a practical level.
It seems that various uses will come out next year (2020)!</p>

    </article>
  </div>

  
  
  
  <script>
  window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
  ga('create', 'UA-169005401-1', 'auto');
  ga('send', 'pageview');
  </script>
  <script async src='https://www.google-analytics.com/analytics.js'></script>
  

  

  
<link rel="stylesheet" type="text/css" href="//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.0/cookieconsent.min.css" />
<script src="//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.0/cookieconsent.min.js"></script>
<script>
window.addEventListener("load", function(){
window.cookieconsent.initialise({
  "palette": {
    "popup": {
      "background": "#216942",
      "text": "#b2d192"
    },
    "button": {
      "background": "#afed71"
    }
  }
})});
</script>

</body>

</html>
