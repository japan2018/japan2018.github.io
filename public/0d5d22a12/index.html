<!doctype html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  
  <meta name="generator" content="Hugo 0.72.0" />

  
  <meta name="description" content="Some description">
  

  
  <link rel="apple-touch-icon" sizes="180x180" href="https://memotut.com/apple-touch-icon.png">

  
  <link rel="icon" type="image/png" sizes="32x32" href="https://memotut.com/favicon-32x32.png">

  
  <link rel="icon" type="image/png" sizes="16x16" href="https://memotut.com/favicon-16x16.png">

  
  <link rel="manifest" href="https://memotut.com/site.webmanifest">

  
  <link rel="mask-icon" href="https://memotut.com/safari-pinned-tab.svg" color="#5bbad5">

  <meta name="msapplication-TileColor" content="#da532c">

  <meta name="theme-color" content="#ffffff">

  
  <link rel="stylesheet" href="https://memotut.com/css/bootstrap.min.css" />

  
  <title>[Python] Understanding the k-means method | Memo Tut</title>
  

  <style>
body {
  min-width: 300px;
}

.custom-navbar {
  margin-bottom: 1em;
  height: 60px;
}

.custom-navbar a {
  display: inline-block; 
  padding: 18px 0;
  margin-right: 1em; 
  font-weight: bold; 
}

.custom-navbar a:hover,
.custom-navbar a:focus {
  text-decoration: none; 
}

@media print {
  .custom-navbar {
    display: none;
  }
}

article {
  padding-bottom: 1em;
}

img {
  max-width: 100%;
}


body {
  background-color: #fff;
}



body {
  color: #212529;
}



a {
  color: #007bff;
}



a:hover,
a:focus {
  color: #0056b3;
}



.custom-navbar {
  background-color: #212529;
}



.custom-navbar a {
  color: rgba(255, 255, 255, 0.75);
}



.custom-navbar a:hover,
.custom-navbar a:focus {
  color: rgba(255, 255, 255, 1);
}



.container {
  max-width: 800px;
}



pre {
  display: block;
  padding: 9.5px;
  word-break: break-all;
  word-wrap: break-word;
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 4px;
}

pre code {
  padding: 0;
  font-size: inherit;
  color: inherit; 
  white-space: pre-wrap;
  background-color: transparent;
  border: none;
  border-radius: 0;
}

code {
  padding: 2px 4px;
  color: inherit; 
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 4px;
  font-size: .9em;
}



blockquote,
.blockquote {
  padding: 10px 20px;
  margin: 0 0 20px;
  font-size: 1em;
  border-left: 5px solid #6c757d;
}

</style>
</head>

<body>
  <nav class="custom-navbar">
  <div class="container">
    
    <a href="/">Posts</a>
    
    <a href="/tags/">Tags</a>
    
    <a href="/about/">About</a>
    
    <a href="/index.xml">RSS</a>
    
  </div>
</nav>
  
  <div class="container">
    <article>
      <h1>[Python] Understanding the k-means method</h1>
<p>
  <small class="text-secondary">
  
  
  Nov 2, 2019
  </small>
  

<small><code><a href="https://memotut.com/tags/python">Python</a></code></small>


<small><code><a href="https://memotut.com/tags/data-analysis"> data analysis</a></code></small>


<small><code><a href="https://memotut.com/tags/k-means"> K-means</a></code></small>


<small><code><a href="https://memotut.com/tags/clustering"> clustering</a></code></small>

</p>
<pre><code>#Introduction
</code></pre>
<p>Here is a summary of what I learned about the k-means method.
The most basic clustering algorithm.</p>
<p>##reference
In understanding the k-means method, I have referred to the following.</p>
<ul>
<li><a href="https://www.amazon.co.jp/dp/4339027510/ref=cm_sw_r_tw_dp_U_x_MkUJDb8R66941">Introduction to machine learning for language processing (natural language processing series)</a>DaiyaTakamura(Author),ManabuOkumura(supervision) Publisher; Corona</li>
<li><a href="https://www.amazon.co.jp/dp/4797393963/ref=cm_sw_r_tw_dp_U_x_UHqVDbP9A69TP">Essence of machine learning</a>KoichiKato(Author) Publisher; SB Creative Co., Ltd.</li>
</ul>
<p>#k-means method overview</p>
<h2 id="what-is-k-means-method">What is k-means method</h2>
<p>The k-means method is an algorithm that first divides the data into appropriate clusters, and then adjusts the data so that the data can be divided in a good manner using the average of the clusters. Since it is an algorithm that creates k clusters of arbitrary designation, it is called the k-means method (called the k-point average method).</p>
<h2 id="k-means-algorithm">k-means algorithm</h2>
<p>Specifically, the k-mean method goes through the following steps.</p>
<ol>
<li>Randomly allocate clusters for each point $x_{i}$</li>
<li>Compute the center of gravity for the points assigned to each cluster</li>
<li>For each point, calculate the distance from the center of gravity calculated above and reassign to the cluster with the closest distance.</li>
<li>Repeat steps 2 and 3 until the assigned cluster does not change</li>
</ol>
<p>Expressed in the figure, it is an image that clusters converge in the order of (a) → (b) → (c) → (d) as shown below.
At the stage of (b), first, a cluster is appropriately allocated to each point, and its center of gravity is calculated (the center of gravity is shown by a red star). In (c), the cluster is reassigned based on the distance from the center of gravity. (The new center of gravity is shown in red stars and the old center of gravity is shown in thin red stars). This process is repeated, and converges in such a way that the cluster does not change, as in (d), and it is complete.</p>
<p><img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/395230/f690ea20-8e83-639d-9bd5-739a5e3fe57c.png" alt="Figure 1.png"></p>
<p>Implement the #k-means method</p>
<h2 id="implementation-of-k-means-method-without-using-library">Implementation of k-means method without using library</h2>
<p>Below is the implementation of the k-means method without using the library.
<a href="https://www.amazon.co.jp/dp/4797393963/ref=cm_sw_r_tw_dp_U_x_UHqVDbP9A69TP">Essence of machine learning</a> The notes are written in the code.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> numpy <span style="color:#f92672">as</span> np
<span style="color:#f92672">import</span> itertools

<span style="color:#66d9ef">class</span> <span style="color:#a6e22e">KMeans</span>:
    <span style="color:#66d9ef">def</span> __init__(self, n_clusters, max_iter <span style="color:#f92672">=</span> <span style="color:#ae81ff">1000</span>, random_seed <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>):
        self<span style="color:#f92672">.</span>n_clusters <span style="color:#f92672">=</span> n_clusters
        self<span style="color:#f92672">.</span>max_iter <span style="color:#f92672">=</span> max_iter
        self<span style="color:#f92672">.</span>random_state <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>RandomState(random_seed)
        
    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">fit</span>(self, X):
        <span style="color:#75715e">#Generate a generator that repeatedly creates labels for the specified number of clusters (feeling like 0,1,2,0,1,2,0,1,2...)</span>
        cycle <span style="color:#f92672">=</span> itertools<span style="color:#f92672">.</span>cycle(range(self<span style="color:#f92672">.</span>n_clusters))
        <span style="color:#75715e"># Randomly assign cluster labels to each data point</span>
        self<span style="color:#f92672">.</span>labels_ <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>fromiter(itertools<span style="color:#f92672">.</span>islice(cycle, X<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>]), dtype <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>int)
        self<span style="color:#f92672">.</span>random_state<span style="color:#f92672">.</span>shuffle(self<span style="color:#f92672">.</span>labels_)
        labels_prev <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>zeros(X<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>])
        count <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
        self<span style="color:#f92672">.</span>cluster_centers_ <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>zeros((self<span style="color:#f92672">.</span>n_clusters, X<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">1</span>]))
        
        <span style="color:#75715e">#End if the cluster to which each data point belongs has stopped changing, or has passed a certain number of iterations</span>
        <span style="color:#66d9ef">while</span> (<span style="color:#f92672">not</span> (self<span style="color:#f92672">.</span>labels_ <span style="color:#f92672">==</span> labels_prev)<span style="color:#f92672">.</span>all() <span style="color:#f92672">and</span> count <span style="color:#f92672">&lt;</span>self<span style="color:#f92672">.</span>max_iter):
            <span style="color:#75715e">#Calculate the centroid of each cluster at that time</span>
            <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(self<span style="color:#f92672">.</span>n_clusters):
                XX <span style="color:#f92672">=</span> X[self<span style="color:#f92672">.</span>labels_ <span style="color:#f92672">==</span> i, :]
                self<span style="color:#f92672">.</span>cluster_centers_[i, :] <span style="color:#f92672">=</span> XX<span style="color:#f92672">.</span>mean(axis <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>)
            <span style="color:#75715e"># Compute the distance between each data point and the centroid of each cluster by brute force</span>
            dist <span style="color:#f92672">=</span> ((X[:, :, np<span style="color:#f92672">.</span>newaxis]<span style="color:#f92672">-</span>self<span style="color:#f92672">.</span>cluster_centers_<span style="color:#f92672">.</span>T[np<span style="color:#f92672">.</span>newaxis, :, :]) <span style="color:#f92672">**</span> <span style="color:#ae81ff">2</span>)<span style="color:#f92672">.</span>sum(axis <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span>)
            <span style="color:#75715e">#Remember the previous cluster label. If the previous label and label do not change, the program ends.</span>
            labels_prev <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>labels_
            <span style="color:#75715e">#As a result of recalculation, assign the label of the closest cluster</span>
            self<span style="color:#f92672">.</span>labels_ <span style="color:#f92672">=</span> dist<span style="color:#f92672">.</span>argmin(axis <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span>)
            count <span style="color:#f92672">+=</span> <span style="color:#ae81ff">1</span>
            
    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">predict</span>(self, X):
        dist <span style="color:#f92672">=</span> ((X[:, :, np<span style="color:#f92672">.</span>newaxis]<span style="color:#f92672">-</span>self<span style="color:#f92672">.</span>cluster_centers_<span style="color:#f92672">.</span>T[np<span style="color:#f92672">.</span>newaxis, :, :]) <span style="color:#f92672">**</span> <span style="color:#ae81ff">2</span>)<span style="color:#f92672">.</span>sum(axis <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span>)
        labels <span style="color:#f92672">=</span> dist<span style="color:#f92672">.</span>argmin(axis <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span>)
        <span style="color:#66d9ef">return</span> labels
</code></pre></div><h2 id="verification">Verification</h2>
<p>Below is the one that verified whether clustering is really possible with this algorithm.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> matplotlib.pyplot <span style="color:#f92672">as</span> plt

<span style="color:#75715e">#Create a suitable dataset</span>
np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>seed(<span style="color:#ae81ff">0</span>)
points1 <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>randn(<span style="color:#ae81ff">80</span>, <span style="color:#ae81ff">2</span>)
points2 <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>randn(<span style="color:#ae81ff">80</span>, <span style="color:#ae81ff">2</span>) <span style="color:#f92672">+</span> np<span style="color:#f92672">.</span>array([<span style="color:#ae81ff">4</span>,<span style="color:#ae81ff">0</span>])
points3 <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>randn(<span style="color:#ae81ff">80</span>, <span style="color:#ae81ff">2</span>) <span style="color:#f92672">+</span> np<span style="color:#f92672">.</span>array([<span style="color:#ae81ff">5</span>,<span style="color:#ae81ff">8</span>])

points <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>r_[points1, points2, points3]
np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>shuffle(points)

<span style="color:#75715e">#Create a model that divides into 3 clusters</span>
model <span style="color:#f92672">=</span> KMeans(<span style="color:#ae81ff">3</span>)
model<span style="color:#f92672">.</span>fit(points)

<span style="color:#66d9ef">print</span>(model<span style="color:#f92672">.</span>labels_)
</code></pre></div><p>Then the output looks like this:
You can see that there are three wonderful labels.</p>
<pre><code>[1 0 2 1 0 0 2 0 1 2 0 0 2 0 2 2 0 0 2 1 2 0 1 2 0 1 2 1 0 1 0 0 0 2 0 2 0
 1 1 0 0 0 0 1 2 0 0 0 2 1 0 2 1 0 2 0 2 1 1 1 1 1 0 2 0 2 2 0 0 0 0 0 2 0
 2 2 2 1 0 2 1 2 0 0 2 0 1 2 1 1 1 2 2 2 1 2 2 2 1 2 1 0 0 0 0 0 2 0 1 0 1
 2 0 1 1 0 1 2 1 1 1 2 2 1 2 1 0 1 1 2 0 1 0 1 1 1 0 2 1 0 0 1 2 2 2 1 0 0
 0 2 2 2 0 0 1 2 0 2 2 2 1 2 2 2 2 2 1 1 0 1 2 1 1 2 0 1 1 1 1 0 2 1 0 1 1
 2 1 2 2 2 1 2 0 1 2 2 2 0 0 0 0 1 1 2 1 1 1 2 2 0 0 1 1 2 0 0 1 0 1 1 2 1
 0 1 2 1 0 1 2 2 1 1 2 1 2 1 0 1 1 2]
</code></pre><p>Let&rsquo;s illustrate this with matplotlib.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">markers <span style="color:#f92672">=</span> [<span style="color:#e6db74">&#34;+&#34;</span>, <span style="color:#e6db74">&#34;*&#34;</span>, <span style="color:#e6db74">&#34;o&#34;</span>]
color <span style="color:#f92672">=</span> [<span style="color:#e6db74">&#39;r&#39;</span>,<span style="color:#e6db74">&#39;b&#39;</span>,<span style="color:#e6db74">&#39;g&#39;</span>]
<span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">3</span>):
    p <span style="color:#f92672">=</span> points[model<span style="color:#f92672">.</span>labels_ <span style="color:#f92672">==</span> i, :]
    plt<span style="color:#f92672">.</span>scatter(p[:, <span style="color:#ae81ff">0</span>], p[:, <span style="color:#ae81ff">1</span>], marker <span style="color:#f92672">=</span> markers[i], color <span style="color:#f92672">=</span> color[i])
    
plt<span style="color:#f92672">.</span>show()
</code></pre></div><p>The output is here. You can see that clustering is done without problems.</p>
<p><img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/395230/7bf0e478-eae1-4b71-ea51-e825bf86441e.png" alt="DOWNLOAD.png">
#Next
The problem with this k-means method is that the accuracy changes depending on the initial random cluster allocation. I would like to challenge the implementation of k-means++, which is trying to overcome that problem.</p>

    </article>
  </div>

  
  
  
  <script>
  window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
  ga('create', 'UA-169005401-1', 'auto');
  ga('send', 'pageview');
  </script>
  <script async src='https://www.google-analytics.com/analytics.js'></script>
  

  

  
<link rel="stylesheet" type="text/css" href="//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.0/cookieconsent.min.css" />
<script src="//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.0/cookieconsent.min.js"></script>
<script>
window.addEventListener("load", function(){
window.cookieconsent.initialise({
  "palette": {
    "popup": {
      "background": "#216942",
      "text": "#b2d192"
    },
    "button": {
      "background": "#afed71"
    }
  }
})});
</script>

</body>

</html>
