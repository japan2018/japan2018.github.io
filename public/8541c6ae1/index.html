<!doctype html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  
  <meta name="generator" content="Hugo 0.72.0" />

  
  <meta name="description" content="Some description">
  

  
  <link rel="apple-touch-icon" sizes="180x180" href="https://memotut.com/apple-touch-icon.png">

  
  <link rel="icon" type="image/png" sizes="32x32" href="https://memotut.com/favicon-32x32.png">

  
  <link rel="icon" type="image/png" sizes="16x16" href="https://memotut.com/favicon-16x16.png">

  
  <link rel="manifest" href="https://memotut.com/site.webmanifest">

  
  <link rel="mask-icon" href="https://memotut.com/safari-pinned-tab.svg" color="#5bbad5">

  <meta name="msapplication-TileColor" content="#da532c">

  <meta name="theme-color" content="#ffffff">

  
  <link rel="stylesheet" href="https://memotut.com/css/bootstrap.min.css" />

  
  <title>[Python] An amateur stumbled upon Deep Learning made from scratch Memo: Chapter 7 | Memo Tut</title>
  

  <style>
body {
  min-width: 300px;
}

.custom-navbar {
  margin-bottom: 1em;
  height: 60px;
}

.custom-navbar a {
  display: inline-block; 
  padding: 18px 0;
  margin-right: 1em; 
  font-weight: bold; 
}

.custom-navbar a:hover,
.custom-navbar a:focus {
  text-decoration: none; 
}

@media print {
  .custom-navbar {
    display: none;
  }
}

article {
  padding-bottom: 1em;
}

img {
  max-width: 100%;
}


body {
  background-color: #fff;
}



body {
  color: #212529;
}



a {
  color: #007bff;
}



a:hover,
a:focus {
  color: #0056b3;
}



.custom-navbar {
  background-color: #212529;
}



.custom-navbar a {
  color: rgba(255, 255, 255, 0.75);
}



.custom-navbar a:hover,
.custom-navbar a:focus {
  color: rgba(255, 255, 255, 1);
}



.container {
  max-width: 800px;
}



pre {
  display: block;
  padding: 9.5px;
  word-break: break-all;
  word-wrap: break-word;
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 4px;
}

pre code {
  padding: 0;
  font-size: inherit;
  color: inherit; 
  white-space: pre-wrap;
  background-color: transparent;
  border: none;
  border-radius: 0;
}

code {
  padding: 2px 4px;
  color: inherit; 
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 4px;
  font-size: .9em;
}



blockquote,
.blockquote {
  padding: 10px 20px;
  margin: 0 0 20px;
  font-size: 1em;
  border-left: 5px solid #6c757d;
}

</style>
</head>

<body>
  <nav class="custom-navbar">
  <div class="container">
    
    <a href="/">Posts</a>
    
    <a href="/tags/">Tags</a>
    
    <a href="/about/">About</a>
    
    <a href="/index.xml">RSS</a>
    
  </div>
</nav>
  
  <div class="container">
    <article>
      <h1>[Python] An amateur stumbled upon Deep Learning made from scratch Memo: Chapter 7</h1>
<p>
  <small class="text-secondary">
  
  
  Feb 11, 2020
  </small>
  

<small><code><a href="https://memotut.com/tags/python">Python</a></code></small>


<small><code><a href="https://memotut.com/tags/machine-learning"> machine learning</a></code></small>


<small><code><a href="https://memotut.com/tags/deeplearning"> DeepLearning</a></code></small>

</p>
<pre><code>#Introduction
</code></pre>
<p>I suddenly started to study <a href="https://www.oreilly.co.jp/books/9784873117584/">&ldquo;Deep Learning from scratch-the theory and implementation of deep learning learned with Python&rdquo;</a> It is a memo of the trip.</p>
<p>The execution environment is macOS Mojave + Anaconda 2019.10, and the Python version is 3.7.4. For details, please refer to <a href="https://qiita.com/segavvy/items/1945aa1a0f91a1473555">Chapter 1 of this memo</a>.</p>
<p>(To other chapters of this memo: <a href="https://qiita.com/segavvy/items/1945aa1a0f91a1473555">Chapter 1</a>/<a href="https://qiita.com/segavvy/items/d8e9e70437e35083a459">Chapter2</a>/<a href="https://qiita.com/segavvy/items/6d79d0c3b4367869f4ea">Chapter3</a>/<a href="https://qiita.com/segavvy/items/bdad9fcda2f0da918e7c">Chapter4</a>/<a href="https://qiita.com/segavvy/items/8707e4e65aa7fa357d8a">Chapter5</a>/<a href="https://qiita.com/segavvy/items/ca4ac4c9ee1a126bff41">Chapter6</a>/Chapter7/<a href="https://qiita.com/segavvy/items/3eb6ea0ea2af689696">Chapter8</a>)/<a href="https://qiita.com/segavvy/items/4e8c36cac9c6f3543ffd">Summary</a>))</p>
<p>#7 Convolutional Neural Network</p>
<p>This chapter describes Convolutional Neural Networks (CNN).</p>
<h2 id="71-overall-structure">7.1 Overall structure</h2>
<p>In addition to the Affine layer, Softmax layer, and ReLU layer that have been used so far, Convolution (convolution) layer and Pooling layer will appear.</p>
<h2 id="72-convolutional-layer">7.2 Convolutional layer</h2>
<p>The description of the convolutional layer is easier to read if you have a little bit of image processing.</p>
<p>&ldquo;The image is usually a 3D shape in the vertical/horizontal/channel directions,&rdquo; but the image is 2D data in the vertical/horizontal directions. I don&rsquo;t know some people think that.</p>
<p>&ldquo;Channel&rdquo; here refers to information for each color such as RGB. With grayscale (only black and white shading) data such as MNIST, the density of one point can be represented by one value, so only one channel is needed, but in a color image, one point is red, green, or blue. Since it is expressed by the depth of three values of (RGB), 3 channels are required. The color information channels are not limited to RGB, but include CMYK, HSV, and alpha with transparency. For more details, googling with &ldquo;RGB CMYK&rdquo; etc. will give you a lot of explanations (although there are a lot of print-oriented stories).</p>
<p>The word &ldquo;filter&rdquo; is also special, and in image processing it refers to the processing used to extract only the necessary parts (such as contours) from an image or to remove unnecessary information. For those who are not familiar, I think that it will be easier to understand by understanding the outline of convolution filters in image processing. @t-tkd3a&rsquo;s <a href="https://qiita.com/t-tkd3a/items/d5f52212e3b941bc36cf">3x3 convolution filter result image</a> is recommended because it is easy to imagine.</p>
<p>As an aside, this book seems to be a rule that does not add the long note notation for katakana with 3 or more sounds like &ldquo;Layer&rdquo;. However, the “filter” has a long sound notation, so it may not be the same. Speaking of which, when Microsoft switched the katakana notation system in 2008 <sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>, I was in charge of developing a packaged application for Windows, and I had to revise programs and manuals. It was tough. Before that, I was also involved in removing the half-width kana from the GUI in Windows 98&hellip; ‥ This industry, really Japanese is inconvenient :sweat:</p>
<p>Let&rsquo;s go back and move on.</p>
<h2 id="73-pooling-layer">7.3 Pooling layer</h2>
<p>Regarding the pooling layer, there were no particular stumbling blocks.</p>
<h2 id="74-implementation-of-convolutionpooling-layer">7.4 Implementation of Convolution/Pooling layer</h2>
<p>The implementation of the Convolution layer and Pooling layer is short, but it is complicated because the shape of the target data changes with <code>im2col</code>, <code>numpy.ndarray.reshape</code> and <code>numpy.ndarray.transpose</code>. It was confusing at first, but I was able to understand it by referring to @daizutabi&rsquo;s <a href="https://qiita.com/daizutabi/items/856042fb1ea9da150741">Implementing &ldquo;Deep Learning from scratch&rdquo; Convolution/Pooling layer</a>. ..</p>
<p>First, the implementation of the Convolution layer. I have a lot of comments, because I can&rsquo;t follow my head without writing the shape.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python:convolution.py" data-lang="python:convolution.py"><span style="color:#75715e"># coding: utf-8</span>
<span style="color:#f92672">import</span> os
<span style="color:#f92672">import</span> sys
<span style="color:#f92672">import</span> numpy <span style="color:#f92672">as</span> np
sys<span style="color:#f92672">.</span>path<span style="color:#f92672">.</span>append(os<span style="color:#f92672">.</span>pardir) <span style="color:#75715e"># add parent directory to path</span>
<span style="color:#f92672">from</span> common.util <span style="color:#f92672">import</span> im2col, col2im


<span style="color:#66d9ef">class</span> <span style="color:#a6e22e">Convolution</span>:
    <span style="color:#66d9ef">def</span> __init__(self, W, b, stride<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, pad<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>):
        <span style="color:#e6db74">&#34;&#34;&#34;Convolution layer
</span><span style="color:#e6db74">        
</span><span style="color:#e6db74">        Args:
</span><span style="color:#e6db74">            W (numpy.ndarray): Filter (weight), shape is (FN, C, FH, FW).
</span><span style="color:#e6db74">            b (numpy.ndarray): Bias, shape is (FN).
</span><span style="color:#e6db74">            stride (int, optional): stride, default is 1.
</span><span style="color:#e6db74">            pad (int, optional): padding, default 0.
</span><span style="color:#e6db74">        &#34;&#34;&#34;</span>
        self<span style="color:#f92672">.</span>W <span style="color:#f92672">=</span> W
        self<span style="color:#f92672">.</span>b <span style="color:#f92672">=</span> b
        self<span style="color:#f92672">.</span>stride <span style="color:#f92672">=</span> stride
        self<span style="color:#f92672">.</span>pad <span style="color:#f92672">=</span> pad

        self<span style="color:#f92672">.</span>dW <span style="color:#f92672">=</span> None <span style="color:#75715e"># Weight derivative</span>
        self<span style="color:#f92672">.</span>db <span style="color:#f92672">=</span> None <span style="color:#75715e"># derivative of bias</span>

        self<span style="color:#f92672">.</span>x <span style="color:#f92672">=</span> None <span style="color:#75715e"># Forward propagation input required for back propagation</span>
        self<span style="color:#f92672">.</span>col_x <span style="color:#f92672">=</span> None <span style="color:#75715e"># col expansion result of input at the time of forward propagation, which is necessary for back propagation</span>
        self<span style="color:#f92672">.</span>col_W <span style="color:#f92672">=</span> None <span style="color:#75715e"># Result of col expansion of forward propagation filter, which is required for back propagation</span>

    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x):
        <span style="color:#e6db74">&#34;&#34;&#34; forward propagation
</span><span style="color:#e6db74">        
</span><span style="color:#e6db74">        Args:
</span><span style="color:#e6db74">            x (numpy.ndarray): Input. The shape is (N, C, H, W).
</span><span style="color:#e6db74">            
</span><span style="color:#e6db74">        Returns:
</span><span style="color:#e6db74">            numpy.ndarray: Output. The shape is (N, FN, OH, OW).
</span><span style="color:#e6db74">        &#34;&#34;&#34;</span>
        FN, C, FH, FW <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>W<span style="color:#f92672">.</span>shape <span style="color:#75715e"># FN: number of filters, C: number of channels, FH: filter height, FW: width</span>
        N, x_C, H, W <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>shape <span style="color:#75715e"># N: batch size, x_C: number of channels, H: height of input data, W: width</span>
        <span style="color:#66d9ef">assert</span> C <span style="color:#f92672">==</span> x_C, f<span style="color:#e6db74">&#39;Channel count mismatch![C]{C}, [x_C]{x_C}&#39;</span>

        <span style="color:#75715e"># Output size calculation</span>
        <span style="color:#66d9ef">assert</span> (H <span style="color:#f92672">+</span> <span style="color:#ae81ff">2</span> <span style="color:#f92672">*</span> self<span style="color:#f92672">.</span>pad<span style="color:#f92672">-</span>FH) <span style="color:#f92672">%</span>self<span style="color:#f92672">.</span>stride <span style="color:#f92672">==</span> <span style="color:#ae81ff">0</span>,<span style="color:#e6db74">&#39;OH is indivisible! &#39;</span>
        <span style="color:#66d9ef">assert</span> (W <span style="color:#f92672">+</span> <span style="color:#ae81ff">2</span> <span style="color:#f92672">*</span> self<span style="color:#f92672">.</span>pad<span style="color:#f92672">-</span>FW) <span style="color:#f92672">%</span>self<span style="color:#f92672">.</span>stride <span style="color:#f92672">==</span> <span style="color:#ae81ff">0</span>,<span style="color:#e6db74">&#39;OW is indivisible! &#39;</span>
        OH <span style="color:#f92672">=</span> int((H <span style="color:#f92672">+</span> <span style="color:#ae81ff">2</span> <span style="color:#f92672">*</span> self<span style="color:#f92672">.</span>pad<span style="color:#f92672">-</span>FH) <span style="color:#f92672">/</span> self<span style="color:#f92672">.</span>stride <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span>)
        OW <span style="color:#f92672">=</span> int((W <span style="color:#f92672">+</span> <span style="color:#ae81ff">2</span> <span style="color:#f92672">*</span> self<span style="color:#f92672">.</span>pad<span style="color:#f92672">-</span>FW) <span style="color:#f92672">/</span> self<span style="color:#f92672">.</span>stride <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span>)

        <span style="color:#75715e"># Expand input data</span>
        <span style="color:#75715e"># (N, C, H, W) → (N * OH * OW, C * FH * FW)</span>
        col_x <span style="color:#f92672">=</span> im2col(x, FH, FW, self<span style="color:#f92672">.</span>stride, self<span style="color:#f92672">.</span>pad)

        <span style="color:#75715e"># Expand filter</span>
        <span style="color:#75715e"># (FN, C, FH, FW) → (C * FH * FW, FN)</span>
        col_W <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>W<span style="color:#f92672">.</span>reshape(FN, <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>)<span style="color:#f92672">.</span>T

        <span style="color:#75715e">#Calculate the output (the calculation for col_x, col_W, b is exactly the same as the Affine layer)</span>
        <span style="color:#75715e"># (N * OH * OW, C * FH * FW)・(C * FH * FW, FN) → (N * OH * OW, FN)</span>
        out <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>dot(col_x, col_W) <span style="color:#f92672">+</span> self<span style="color:#f92672">.</span>b

        <span style="color:#75715e">#Result formatting</span>
        <span style="color:#75715e"># (N * OH * OW, FN) → (N, OH, OW, FN) → (N, FN, OH, OW)</span>
        out <span style="color:#f92672">=</span> out<span style="color:#f92672">.</span>reshape(N, OH, OW, FN)<span style="color:#f92672">.</span>transpose(<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>)

        <span style="color:#75715e">#Save for back propagation</span>
        self<span style="color:#f92672">.</span>x <span style="color:#f92672">=</span> x
        self<span style="color:#f92672">.</span>col_x <span style="color:#f92672">=</span> col_x
        self<span style="color:#f92672">.</span>col_W <span style="color:#f92672">=</span> col_W

        <span style="color:#66d9ef">return</span> out

    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">backward</span>(self, dout):
        <span style="color:#e6db74">&#34;&#34;&#34; backpropagation
</span><span style="color:#e6db74">        
</span><span style="color:#e6db74">        Args:
</span><span style="color:#e6db74">            dout (numpy.ndarray): Differential value and shape transmitted from the layer on the right are (N, FN, OH, OW).
</span><span style="color:#e6db74">        
</span><span style="color:#e6db74">        Returns:
</span><span style="color:#e6db74">            numpy.ndarray: derivative (slope), shape is (N, C, H, W).
</span><span style="color:#e6db74">        &#34;&#34;&#34;</span>
        FN, C, FH, FW <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>W<span style="color:#f92672">.</span>shape <span style="color:#75715e"># Differentiated shape is the same as W (FN, C, FH, FW)</span>

        <span style="color:#75715e"># Expand differential value from right layer</span>
        <span style="color:#75715e"># (N, FN, OH, OW) → (N, OH, OW, FN) → (N * OH * OW, FN)</span>
        dout <span style="color:#f92672">=</span> dout<span style="color:#f92672">.</span>transpose(<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">1</span>)<span style="color:#f92672">.</span>reshape(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, FN)<span style="color:#75715e">#Differential value calculation (Calculation for col_x, col_W, b is exactly the same as Affine layer)</span>
        dcol_x <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>dot(dout, self<span style="color:#f92672">.</span>col_W<span style="color:#f92672">.</span>T) <span style="color:#75715e"># → (N * OH * OW, C * FH * FW)</span>
        self<span style="color:#f92672">.</span>dW <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>dot(self<span style="color:#f92672">.</span>col_x<span style="color:#f92672">.</span>T, dout) <span style="color:#75715e"># → (C * FH * FW, FN)</span>
        self<span style="color:#f92672">.</span>db <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>sum(dout, axis<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>) <span style="color:#75715e"># → (FN)</span>

        <span style="color:#75715e"># Filter (weight) derivative shaping</span>
        <span style="color:#75715e"># (C * FH * FW, FN) → (FN, C * FH * FW) → (FN, C, FH, FW)</span>
        self<span style="color:#f92672">.</span>dW <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>dW<span style="color:#f92672">.</span>transpose(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">0</span>)<span style="color:#f92672">.</span>reshape(FN, C, FH, FW)

        <span style="color:#75715e">#Result (gradient) shaping</span>
        <span style="color:#75715e"># (N * OH * OW, C * FH * FW) → (N, C, H, W)</span>
        dx <span style="color:#f92672">=</span> col2im(dcol_x, self<span style="color:#f92672">.</span>x<span style="color:#f92672">.</span>shape, FH, FW, self<span style="color:#f92672">.</span>stride, self<span style="color:#f92672">.</span>pad)
    
        <span style="color:#66d9ef">return</span> dx
</code></pre></div><p>Next is the implementation of the Pooling layer. This is also full of comments.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python:pooling.py" data-lang="python:pooling.py"><span style="color:#75715e"># coding: utf-8</span>
<span style="color:#f92672">import</span> os
<span style="color:#f92672">import</span> sys
<span style="color:#f92672">import</span> numpy <span style="color:#f92672">as</span> np
sys<span style="color:#f92672">.</span>path<span style="color:#f92672">.</span>append(os<span style="color:#f92672">.</span>pardir) <span style="color:#75715e"># add parent directory to path</span>
<span style="color:#f92672">from</span> common.util <span style="color:#f92672">import</span> im2col, col2im


<span style="color:#66d9ef">class</span> <span style="color:#a6e22e">Pooling</span>:
    <span style="color:#66d9ef">def</span> __init__(self, pool_h, pool_w, stride<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, pad<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>):
        <span style="color:#e6db74">&#34;&#34;&#34;Pooling layer
</span><span style="color:#e6db74">        
</span><span style="color:#e6db74">        Args:
</span><span style="color:#e6db74">            pool_h (int): Height of pooling area
</span><span style="color:#e6db74">            pool_w (int): Width of pooling area
</span><span style="color:#e6db74">            stride (int, optional): stride, default is 1.
</span><span style="color:#e6db74">            pad (int, optional): padding, default 0.
</span><span style="color:#e6db74">        &#34;&#34;&#34;</span>
        self<span style="color:#f92672">.</span>pool_h <span style="color:#f92672">=</span> pool_h
        self<span style="color:#f92672">.</span>pool_w <span style="color:#f92672">=</span> pool_w
        self<span style="color:#f92672">.</span>stride <span style="color:#f92672">=</span> stride
        self<span style="color:#f92672">.</span>pad <span style="color:#f92672">=</span> pad

        self<span style="color:#f92672">.</span>x <span style="color:#f92672">=</span> None <span style="color:#75715e"># Forward propagation input required for back propagation</span>
        self<span style="color:#f92672">.</span>arg_max <span style="color:#f92672">=</span> None <span style="color:#75715e"># Position of each col_x line used for forward propagation, which is required for back propagation</span>

    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x):
        <span style="color:#e6db74">&#34;&#34;&#34; forward propagation
</span><span style="color:#e6db74">        
</span><span style="color:#e6db74">        Args:
</span><span style="color:#e6db74">            x (numpy.ndarray): Input, shape is (N, C, H, W).
</span><span style="color:#e6db74">            
</span><span style="color:#e6db74">        Returns:
</span><span style="color:#e6db74">            numpy.ndarray: Output, shape is (N, C, OH, OW).
</span><span style="color:#e6db74">        &#34;&#34;&#34;</span>
        N, C, H, W <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>shape <span style="color:#75715e"># N: number of data, C: number of channels, H: height, W: width</span>

        <span style="color:#75715e"># Output size calculation</span>
        <span style="color:#66d9ef">assert</span> (H<span style="color:#f92672">-</span>self<span style="color:#f92672">.</span>pool_h) <span style="color:#f92672">%</span>self<span style="color:#f92672">.</span>stride <span style="color:#f92672">==</span> <span style="color:#ae81ff">0</span>,<span style="color:#e6db74">&#39;OH is indivisible! &#39;</span>
        <span style="color:#66d9ef">assert</span> (W<span style="color:#f92672">-</span>self<span style="color:#f92672">.</span>pool_w) <span style="color:#f92672">%</span>self<span style="color:#f92672">.</span>stride <span style="color:#f92672">==</span> <span style="color:#ae81ff">0</span>,<span style="color:#e6db74">&#39;OW is indivisible! &#39;</span>
        OH <span style="color:#f92672">=</span> int((H<span style="color:#f92672">-</span>self<span style="color:#f92672">.</span>pool_h) <span style="color:#f92672">/</span> self<span style="color:#f92672">.</span>stride <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span>)
        OW <span style="color:#f92672">=</span> int((W<span style="color:#f92672">-</span>self<span style="color:#f92672">.</span>pool_w) <span style="color:#f92672">/</span> self<span style="color:#f92672">.</span>stride <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span>)

        <span style="color:#75715e"># Expand and format input data</span>
        <span style="color:#75715e"># (N, C, H, W) → (N * OH * OW, C * PH * PW)</span>
        col_x <span style="color:#f92672">=</span> im2col(x, self<span style="color:#f92672">.</span>pool_h, self<span style="color:#f92672">.</span>pool_w, self<span style="color:#f92672">.</span>stride, self<span style="color:#f92672">.</span>pad)
        <span style="color:#75715e"># (N * OH * OW, C * PH * PW) → (N * OH * OW * C, PH * PW)</span>
        col_x <span style="color:#f92672">=</span> col_x<span style="color:#f92672">.</span>reshape(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, self<span style="color:#f92672">.</span>pool_h <span style="color:#f92672">*</span> self<span style="color:#f92672">.</span>pool_w)

        <span style="color:#75715e"># Calculate output</span>
        <span style="color:#75715e"># (N * OH * OW * C, PH * PW) → (N * OH * OW * C)</span>
        out <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>max(col_x, axis<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)

        <span style="color:#75715e">#Result formatting</span>
        <span style="color:#75715e"># (N * OH * OW * C) → (N, OH, OW, C) → (N, C, OH, OW)</span>
        out <span style="color:#f92672">=</span> out<span style="color:#f92672">.</span>reshape(N, OH, OW, C)<span style="color:#f92672">.</span>transpose(<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>)

        <span style="color:#75715e">#Save for back propagation</span>
        self<span style="color:#f92672">.</span>x <span style="color:#f92672">=</span> x
        self<span style="color:#f92672">.</span>arg_max <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>argmax(col_x, axis<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>) <span style="color:#75715e"># col_x Position of maximum value of each line (index)</span>

        <span style="color:#66d9ef">return</span> out

    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">backward</span>(self, dout):
        <span style="color:#e6db74">&#34;&#34;&#34; backpropagation
</span><span style="color:#e6db74">        
</span><span style="color:#e6db74">        Args:
</span><span style="color:#e6db74">            dout (numpy.ndarray): The differential value and shape transmitted from the layer on the right are (N, C, OH, OW).
</span><span style="color:#e6db74">        
</span><span style="color:#e6db74">        Returns:
</span><span style="color:#e6db74">            numpy.ndarray: derivative (slope), shape is (N, C, H, W).
</span><span style="color:#e6db74">        &#34;&#34;&#34;</span>
        <span style="color:#75715e"># Shape differential value from right layer</span>
        <span style="color:#75715e"># (N, C, OH, OW) → (N, OH, OW, C)</span>
        dout <span style="color:#f92672">=</span> dout<span style="color:#f92672">.</span>transpose(<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">1</span>)

        <span style="color:#75715e"># Initialize the col for the derivative of the result with 0</span>
        <span style="color:#75715e"># (N * OH * OW * C, PH * PW)</span>
        pool_size <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>pool_h <span style="color:#f92672">*</span> self<span style="color:#f92672">.</span>pool_w
        dcol_x <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>zeros((dout<span style="color:#f92672">.</span>size, pool_size))

        <span style="color:#75715e"># Set the differential value of dout (= doutmanma) only at the position that was adopted as the maximum value during forward propagation</span>
        <span style="color:#75715e"># The position of the value not adopted during forward propagation remains 0 at initialization.</span>
        <span style="color:#75715e"># (Same as processing when x is greater than 0 and x is 0 or less in ReLU)</span>
        <span style="color:#66d9ef">assert</span> dout<span style="color:#f92672">.</span>size <span style="color:#f92672">==</span> self<span style="color:#f92672">.</span>arg_max<span style="color:#f92672">.</span>size,<span style="color:#e6db74">&#39;Does not match the number of col_x lines during forward propagation&#39;</span>
        dcol_x[np<span style="color:#f92672">.</span>arange(self<span style="color:#f92672">.</span>arg_max<span style="color:#f92672">.</span>size), self<span style="color:#f92672">.</span>arg_max<span style="color:#f92672">.</span>flatten()] <span style="color:#f92672">=</span> \
            dout<span style="color:#f92672">.</span>flatten()

        <span style="color:#75715e">#Shape the derivative of the result 1</span>
        <span style="color:#75715e"># (N * OH * OW * C, PH * PW) → (N, OH, OW, C, PH * PW)</span>
        dcol_x <span style="color:#f92672">=</span> dcol_x<span style="color:#f92672">.</span>reshape(dout<span style="color:#f92672">.</span>shape <span style="color:#f92672">+</span> (pool_size,)) <span style="color:#75715e"># The last&#39;,&#39; indicates a tuple of 1 element</span>

        <span style="color:#75715e">#Shape the derivative of the result 2</span>
        <span style="color:#75715e"># (N, OH, OW, C, PH * PW) → (N * OH * OW, C * PH * PW)</span>
        dcol_x <span style="color:#f92672">=</span> dcol_x<span style="color:#f92672">.</span>reshape(
            dcol_x<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>] <span style="color:#f92672">*</span> dcol_x<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">1</span>] <span style="color:#f92672">*</span> dcol_x<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">2</span>], <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>
        )

        <span style="color:#75715e">#Shaping the derivative of the result 3</span>
        <span style="color:#75715e"># (N * OH * OW, C * PH * PW) → (N, C, H, W)</span>
        dx <span style="color:#f92672">=</span> col2im(
            dcol_x, self<span style="color:#f92672">.</span>x<span style="color:#f92672">.</span>shape, self<span style="color:#f92672">.</span>pool_h, self<span style="color:#f92672">.</span>pool_w, self<span style="color:#f92672">.</span>stride, self<span style="color:#f92672">.</span>pad
        )

        <span style="color:#66d9ef">return</span> dx
</code></pre></div><h2 id="75-cnn-implementation">7.5 CNN implementation</h2>
<p>Implement CNN by combining the previous implementations.</p>
<h3 id="1-implementation-of-each-layer">(1) Implementation of each layer</h3>
<p>First, I will organize the input and output in this network.</p>
<table>
<thead>
<tr>
<th align="left">Layer</th>
<th align="center">Input/output shape</th>
<th align="center">Mounting shape</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left"></td>
<td align="center">$ (Batch size N, number of channels CH, image height H, width W) $</td>
<td align="center">$ (100, 1, 28, 28) $</td>
</tr>
<tr>
<td align="left">:one: Convolution</td>
<td align="center">↓</td>
<td></td>
</tr>
<tr>
<td align="left"></td>
<td align="center">$ (Batch size N, Number of filters FN, Output height OH, Width OW) $</td>
<td align="center">$ (100, 30, 24, 24) $</td>
</tr>
<tr>
<td align="left">:two: ReLU</td>
<td align="center">↓</td>
<td></td>
</tr>
<tr>
<td align="left"></td>
<td align="center">$ (Batch size N, Number of filters FN, Output height OH, Width OW) $</td>
<td align="center">$ (100, 30, 24, 24) $</td>
</tr>
<tr>
<td align="left">:three: Pooling</td>
<td align="center">↓</td>
<td></td>
</tr>
<tr>
<td align="left"></td>
<td align="center">$ (Batch size N, number of filters FN, output height OH, width OW) $</td>
<td align="center">$ (100, 30, 12, 12) $</td>
</tr>
<tr>
<td align="left">:four: Affine</td>
<td align="center">↓</td>
<td></td>
</tr>
<tr>
<td align="left"></td>
<td align="center">$ (Batch size N, hidden layer size) $</td>
<td align="center">$ (100, 100) $</td>
</tr>
<tr>
<td align="left">:five: ReLU</td>
<td align="center">↓</td>
<td></td>
</tr>
<tr>
<td align="left"></td>
<td align="center">$ (Batch size N, hidden layer size) $</td>
<td align="center">$ (100, 100) $</td>
</tr>
<tr>
<td align="left">:six: Affine</td>
<td align="center">↓</td>
<td></td>
</tr>
<tr>
<td align="left"></td>
<td align="center">$ (Batch size N, final output size) $</td>
<td align="center">$ (100, 10) $</td>
</tr>
<tr>
<td align="left">:seven: Softmax</td>
<td align="center">↓</td>
<td></td>
</tr>
<tr>
<td align="left"></td>
<td align="center">$ (Batch size N, final output size) $</td>
<td align="center">$ (100, 10) $</td>
</tr>
</tbody>
</table>
<p>The implementation of Convlolution layer and Pooling layer is as described above.The Affine layer needs some modifications to the previous implementation. Previous <a href="https://qiita.com/segavvy/items/8707e4e65aa7fa357d8a#562-%E3%83%90%E3%83%83%E3%83%81%E7%89%25Whenitwasimplementedwith88affine%E3%83%AC%E3%82%A4%E3%83%A4">5.6.2 Batch Affine Layer</a>,theinputwastwo-dimensional($batchsizeN$,imagesize),butthistime,thefourthTheinputoftheAffinelayeris4dimensions(numberofbatches$N$,numberoffilters$FN$,$OH$ofpoolingresults,$OW$),soitisnecessarytosupportit.Onpage152ofthebook,thereisaprovisothat&quot;TheimplementationofAffineincommon/layers.pyisanimplementationthattakesintoaccountthecasewheretheinputdataisatensor(four-dimensionaldata)&rdquo;. I left it without knowing it, but it means that I was supposed to use it this time.</p>
<p>Below is an implementation of the Affine layer that supports input of 3 dimensions or more.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python:affine.py" data-lang="python:affine.py"><span style="color:#75715e"># coding: utf-8</span>
<span style="color:#f92672">import</span> numpy <span style="color:#f92672">as</span> np


<span style="color:#66d9ef">class</span> <span style="color:#a6e22e">Affine</span>:

    <span style="color:#66d9ef">def</span> __init__(self, W, b):
        <span style="color:#e6db74">&#34;&#34;&#34;Affine layer
</span><span style="color:#e6db74">        
</span><span style="color:#e6db74">        Args:
</span><span style="color:#e6db74">            W (numpy.ndarray): Weight
</span><span style="color:#e6db74">            b (numpy.ndarray): bias
</span><span style="color:#e6db74">        &#34;&#34;&#34;</span>
        self<span style="color:#f92672">.</span>W <span style="color:#f92672">=</span> W <span style="color:#75715e"># weight</span>
        self<span style="color:#f92672">.</span>b <span style="color:#f92672">=</span> b <span style="color:#75715e"># bias</span>
        self<span style="color:#f92672">.</span>x <span style="color:#f92672">=</span> None <span style="color:#75715e"># input (after 2D conversion)</span>
        self<span style="color:#f92672">.</span>dW <span style="color:#f92672">=</span> None <span style="color:#75715e"># Weight derivative</span>
        self<span style="color:#f92672">.</span>db <span style="color:#f92672">=</span> None <span style="color:#75715e"># derivative of bias</span>
        self<span style="color:#f92672">.</span>original_x_shape <span style="color:#f92672">=</span> None <span style="color:#75715e"># Original input shape (for 3D or more input)</span>

    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x):
        <span style="color:#e6db74">&#34;&#34;&#34; forward propagation
</span><span style="color:#e6db74">        
</span><span style="color:#e6db74">        Args:
</span><span style="color:#e6db74">            x (numpy.ndarray): input
</span><span style="color:#e6db74">            
</span><span style="color:#e6db74">        Returns:
</span><span style="color:#e6db74">            numpy.ndarray: output
</span><span style="color:#e6db74">        &#34;&#34;&#34;</span>
        <span style="color:#75715e"># Make 3D or higher (tensor) input 2D</span>
        self<span style="color:#f92672">.</span>original_x_shape <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>shape <span style="color:#75715e"># Since we need to save the shape and backpropagate it back</span>
        x <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>reshape(x<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>], <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>)
        self<span style="color:#f92672">.</span>x <span style="color:#f92672">=</span> x

        <span style="color:#75715e"># Calculate output</span>
        out <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>dot(x, self<span style="color:#f92672">.</span>W) <span style="color:#f92672">+</span> self<span style="color:#f92672">.</span>b

        <span style="color:#66d9ef">return</span> out

    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">backward</span>(self, dout):
        <span style="color:#e6db74">&#34;&#34;&#34; backpropagation
</span><span style="color:#e6db74">        
</span><span style="color:#e6db74">        Args:
</span><span style="color:#e6db74">            dout (numpy.ndarray): differential value transmitted from the right layer
</span><span style="color:#e6db74">
</span><span style="color:#e6db74">        Returns:
</span><span style="color:#e6db74">            numpy.ndarray: derivative value
</span><span style="color:#e6db74">        &#34;&#34;&#34;</span>
        <span style="color:#75715e">#Differential value calculation</span>
        dx <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>dot(dout, self<span style="color:#f92672">.</span>W<span style="color:#f92672">.</span>T)
        self<span style="color:#f92672">.</span>dW <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>dot(self<span style="color:#f92672">.</span>x<span style="color:#f92672">.</span>T, dout)
        self<span style="color:#f92672">.</span>db <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>sum(dout, axis<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)

        <span style="color:#75715e">#Return to original shape</span>
        dx <span style="color:#f92672">=</span> dx<span style="color:#f92672">.</span>reshape(<span style="color:#f92672">*</span>self<span style="color:#f92672">.</span>original_x_shape)
        <span style="color:#66d9ef">return</span> dx
</code></pre></div><p>The ReLU layer and Softmax layer are the same as the previous implementation, but will be listed again.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python:relu.py" data-lang="python:relu.py"><span style="color:#75715e"># coding: utf-8</span>


<span style="color:#66d9ef">class</span> <span style="color:#a6e22e">ReLU</span>:
    <span style="color:#66d9ef">def</span> __init__(self):
        <span style="color:#e6db74">&#34;&#34;&#34;ReLU layer
</span><span style="color:#e6db74">        &#34;&#34;&#34;</span>
        self<span style="color:#f92672">.</span>mask <span style="color:#f92672">=</span> None

    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x):
        <span style="color:#e6db74">&#34;&#34;&#34; forward propagation
</span><span style="color:#e6db74">        
</span><span style="color:#e6db74">        Args:
</span><span style="color:#e6db74">            x (numpy.ndarray): input
</span><span style="color:#e6db74">            
</span><span style="color:#e6db74">        Returns:
</span><span style="color:#e6db74">            numpy.ndarray: output
</span><span style="color:#e6db74">        &#34;&#34;&#34;</span>
        self<span style="color:#f92672">.</span>mask <span style="color:#f92672">=</span> (x <span style="color:#f92672">&lt;=</span> <span style="color:#ae81ff">0</span>)
        out <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>copy()
        out[self<span style="color:#f92672">.</span>mask] <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>

        <span style="color:#66d9ef">return</span> out

    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">backward</span>(self, dout):
        <span style="color:#e6db74">&#34;&#34;&#34; backpropagation
</span><span style="color:#e6db74">        
</span><span style="color:#e6db74">        Args:
</span><span style="color:#e6db74">            dout (numpy.ndarray): differential value transmitted from the right layer
</span><span style="color:#e6db74">        
</span><span style="color:#e6db74">        Returns:
</span><span style="color:#e6db74">            numpy.ndarray: derivative value
</span><span style="color:#e6db74">        &#34;&#34;&#34;</span>
        dout[self<span style="color:#f92672">.</span>mask] <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
        dx <span style="color:#f92672">=</span> dout

        <span style="color:#66d9ef">return</span> dx
</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python:softmax_with_loss.py" data-lang="python:softmax_with_loss.py"><span style="color:#75715e"># coding: utf-8</span>
<span style="color:#f92672">from</span> functions <span style="color:#f92672">import</span> softmax, cross_entropy_error


<span style="color:#66d9ef">class</span> <span style="color:#a6e22e">SoftmaxWithLoss</span>:
    <span style="color:#66d9ef">def</span> __init__(self):
        <span style="color:#e6db74">&#34;&#34;&#34;Softmax-with-Loss layer
</span><span style="color:#e6db74">        &#34;&#34;&#34;</span>
        self<span style="color:#f92672">.</span>loss <span style="color:#f92672">=</span> None <span style="color:#75715e"># loss</span>
        self<span style="color:#f92672">.</span>y <span style="color:#f92672">=</span> None <span style="color:#75715e"># softmax output</span>
        self<span style="color:#f92672">.</span>t <span style="color:#f92672">=</span> None <span style="color:#75715e"># Teacher data (one-hot vector)</span>

    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x, t):
        <span style="color:#e6db74">&#34;&#34;&#34; forward propagation
</span><span style="color:#e6db74">        
</span><span style="color:#e6db74">        Args:
</span><span style="color:#e6db74">            x (numpy.ndarray): input
</span><span style="color:#e6db74">            t (numpy.ndarray): Teacher data
</span><span style="color:#e6db74">
</span><span style="color:#e6db74">        Returns:
</span><span style="color:#e6db74">            float: cross entropy error
</span><span style="color:#e6db74">        &#34;&#34;&#34;</span>
        self<span style="color:#f92672">.</span>t <span style="color:#f92672">=</span> t
        self<span style="color:#f92672">.</span>y <span style="color:#f92672">=</span> softmax(x)
        self<span style="color:#f92672">.</span>loss <span style="color:#f92672">=</span> cross_entropy_error(self<span style="color:#f92672">.</span>y, self<span style="color:#f92672">.</span>t)

        <span style="color:#66d9ef">return</span> self<span style="color:#f92672">.</span>loss

    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">backward</span>(self, dout<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>):
        <span style="color:#e6db74">&#34;&#34;&#34; backpropagation
</span><span style="color:#e6db74">        
</span><span style="color:#e6db74">        Args:
</span><span style="color:#e6db74">            dout (float, optional): Differential value transmitted from the right layer. The default is 1.
</span><span style="color:#e6db74">
</span><span style="color:#e6db74">        Returns:
</span><span style="color:#e6db74">            numpy.ndarray: derivative value
</span><span style="color:#e6db74">        &#34;&#34;&#34;</span>
        batch_size <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>t<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>] <span style="color:#75715e"># number of batches</span>
        dx <span style="color:#f92672">=</span> (self<span style="color:#f92672">.</span>y<span style="color:#f92672">-</span>self<span style="color:#f92672">.</span>t) <span style="color:#f92672">*</span> (dout <span style="color:#f92672">/</span> batch_size)

        <span style="color:#66d9ef">return</span> dx
</code></pre></div><p>The functions required for implementing the softmax layer are the same as before, but will be listed again. The functions that are not used this time are deleted.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python:functions.py" data-lang="python:functions.py"><span style="color:#75715e"># coding: utf-8</span>
<span style="color:#f92672">import</span> numpy <span style="color:#f92672">as</span> np


<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">softmax</span>(x):
    <span style="color:#e6db74">&#34;&#34;&#34; softmax function
</span><span style="color:#e6db74">    
</span><span style="color:#e6db74">    Args:
</span><span style="color:#e6db74">        x (numpy.ndarray): input
</span><span style="color:#e6db74">    
</span><span style="color:#e6db74">    Returns:
</span><span style="color:#e6db74">        numpy.ndarray: output
</span><span style="color:#e6db74">    &#34;&#34;&#34;</span>
    <span style="color:#75715e"># In case of batch processing, x is a 2D array of (number of batches, 10).</span>
    <span style="color:#75715e"># In this case, you have to use broadcast to successfully calculate for each image.</span>
    <span style="color:#75715e"># Here, np.max() and np.sum() are calculated with axis=-1 so that they can be used in both one and two dimensions.</span>
    <span style="color:#75715e"># Keep dimensions with keepdims=True so that you can broadcast as is.</span>
    c <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>max(x, axis<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>, keepdims<span style="color:#f92672">=</span>True)
    exp_a <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>exp(x<span style="color:#f92672">-</span>c) <span style="color:#75715e"># Overflow protection</span>
    sum_exp_a <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>sum(exp_a, axis<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>, keepdims<span style="color:#f92672">=</span>True)
    y <span style="color:#f92672">=</span> exp_a <span style="color:#f92672">/</span> sum_exp_a
    <span style="color:#66d9ef">return</span> y


<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">cross_entropy_error</span>(y, t):
    <span style="color:#e6db74">&#34;&#34;&#34;Calculation of cross entropy error
</span><span style="color:#e6db74">    
</span><span style="color:#e6db74">    Args:
</span><span style="color:#e6db74">        y (numpy.ndarray): Neural network output
</span><span style="color:#e6db74">        t (numpy.ndarray): correct answer label
</span><span style="color:#e6db74">    
</span><span style="color:#e6db74">    Returns:
</span><span style="color:#e6db74">        float: cross entropy error
</span><span style="color:#e6db74">    &#34;&#34;&#34;</span>

    <span style="color:#75715e">#Shape the shape when there is only one data (1 line per data)</span>
    <span style="color:#66d9ef">if</span> y<span style="color:#f92672">.</span>ndim <span style="color:#f92672">==</span> <span style="color:#ae81ff">1</span>:
        t <span style="color:#f92672">=</span> t<span style="color:#f92672">.</span>reshape(<span style="color:#ae81ff">1</span>, t<span style="color:#f92672">.</span>size)
        y <span style="color:#f92672">=</span> y<span style="color:#f92672">.</span>reshape(<span style="color:#ae81ff">1</span>, y<span style="color:#f92672">.</span>size)

    <span style="color:#75715e"># Calculate error and normalize by batch number</span>
    batch_size <span style="color:#f92672">=</span> y<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>]
    <span style="color:#66d9ef">return</span> <span style="color:#f92672">-</span>np<span style="color:#f92672">.</span>sum(t <span style="color:#f92672">*</span> np<span style="color:#f92672">.</span>log(y <span style="color:#f92672">+</span> <span style="color:#ae81ff">1e-7</span>)) <span style="color:#f92672">/</span> batch_size
</code></pre></div><h3 id="2-optimizer-implementation">(2) Optimizer implementation</h3>
<p>For the optimizer that optimizes parameters, see [6.1 Parameter Update](<a href="https://qiita.com/segavvy/items/ca4ac4c9ee1a126bff41#61-%E3%83%91%E3%83%A9%E3%83%25(A1%E3%83%BC%E3%82%BF%E3%81%AE%E6%9B%B4%E6%96%B0)">https://qiita.com/segavvy/items/ca4ac4c9ee1a126bff41#61-%E3%83%91%E3%83%A9%E3%83%(A1%E3%83%BC%E3%82%BF%E3%81%AE%E6%9B%B4%E6%96%B0)</a> was skipping the implementation, so I decided to use it this time AdaGrad I tried to implement. It is almost the same as the code in the book.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python:ada_grad.py" data-lang="python:ada_grad.py"><span style="color:#75715e"># coding: utf-8</span>
<span style="color:#f92672">import</span> numpy <span style="color:#f92672">as</span> np


<span style="color:#66d9ef">class</span> <span style="color:#a6e22e">AdaGrad</span>:

    <span style="color:#66d9ef">def</span> __init__(self, lr<span style="color:#f92672">=</span><span style="color:#ae81ff">0.01</span>):
        <span style="color:#e6db74">&#34;&#34;&#34; Parameter optimization with AdaGrad
</span><span style="color:#e6db74">        
</span><span style="color:#e6db74">        Args:
</span><span style="color:#e6db74">            lr (float, optional): Learning factor, default 0.01.
</span><span style="color:#e6db74">        &#34;&#34;&#34;</span>self<span style="color:#f92672">.</span>lr <span style="color:#f92672">=</span> lr
        self<span style="color:#f92672">.</span>h <span style="color:#f92672">=</span> None <span style="color:#75715e"># Sum of squares of the gradient so far</span>

    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">update</span>(self, params, grads):
        <span style="color:#e6db74">&#34;&#34;&#34; parameter update
</span><span style="color:#e6db74">        
</span><span style="color:#e6db74">        Args:
</span><span style="color:#e6db74">            params (dict): Dictionary of parameters to update, key is&#39;W1&#39;,&#39;b1&#39;, etc.
</span><span style="color:#e6db74">            grads (dict): dictionary of gradients corresponding to params
</span><span style="color:#e6db74">        &#34;&#34;&#34;</span>

        <span style="color:#75715e">#h initialization</span>
        <span style="color:#66d9ef">if</span> self<span style="color:#f92672">.</span>h <span style="color:#f92672">is</span> None:
            self<span style="color:#f92672">.</span>h <span style="color:#f92672">=</span> {}
            <span style="color:#66d9ef">for</span> key, val <span style="color:#f92672">in</span> params<span style="color:#f92672">.</span>items():
                self<span style="color:#f92672">.</span>h[key] <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>zeros_like(val)

        <span style="color:#75715e"># Update</span>
        <span style="color:#66d9ef">for</span> key <span style="color:#f92672">in</span> params<span style="color:#f92672">.</span>keys():

            <span style="color:#75715e">#h update</span>
            self<span style="color:#f92672">.</span>h[key] <span style="color:#f92672">+=</span> grads[key] <span style="color:#f92672">**</span> <span style="color:#ae81ff">2</span>

            <span style="color:#75715e"># Parameter update, last 1e-7 avoid division by 0</span>
            params[key] <span style="color:#f92672">-=</span> self<span style="color:#f92672">.</span>lr <span style="color:#f92672">*</span> grads[key] <span style="color:#f92672">/</span> (np<span style="color:#f92672">.</span>sqrt(self<span style="color:#f92672">.</span>h[key]) <span style="color:#f92672">+</span> <span style="color:#ae81ff">1e-7</span>)
</code></pre></div><h3 id="3-cnn-implementation">(3) CNN implementation</h3>
<p>CNN was previously <a href="https://qiita.com/segavvy/items/8707e4e65aa7fa357d8a#572-%E8%AA%A4%E5%B7%AE%E9%80%86%E4%BC%9D%E6%92%AD%E6%B3%95%E3%81%AB%E5%AF%BE%E5%BF%9C%E3%81%97%E3%81%9F%E3%83%8B%E3%83%A5%E3%83%BC%E3%83%A9%E3%83%AB%E3%83%8D%E3%83%83%E3%83%88%E3%83%AF%E3%83%BC%E3%82%AF%E3%81%AE%E5%AE%9F%E8%A3%85">5.7.2 Implementation of neural network corresponding to backpropagation method</a> based on the <code>TwoLayerNet</code> I implemented it according to the description.</p>
<p>Note that the code in the book uses <code>OrderedDict</code>, but just like last time, here we use a normal <code>dict</code>. This is because from Python 3.7, the insertion order of <code>dict</code> objects is saved <sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>. Also, I stumbled upon the implementation of <code>accuracy</code>, so I will explain it later.</p>
<p>Below is the implementation of CNN.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python:simple_conv_net.py" data-lang="python:simple_conv_net.py"><span style="color:#75715e"># coding: utf-8</span>
<span style="color:#f92672">import</span> numpy <span style="color:#f92672">as</span> np
<span style="color:#f92672">from</span> affine <span style="color:#f92672">import</span> Affine
<span style="color:#f92672">from</span> convolution <span style="color:#f92672">import</span> Convolution
<span style="color:#f92672">from</span> pooling <span style="color:#f92672">import</span> Pooling
<span style="color:#f92672">from</span> relu <span style="color:#f92672">import</span> ReLU
<span style="color:#f92672">from</span> softmax_with_loss <span style="color:#f92672">import</span> SoftmaxWithLoss


<span style="color:#66d9ef">class</span> <span style="color:#a6e22e">SimpleConvNet</span>:

    <span style="color:#66d9ef">def</span> __init__(
        self, input_dim<span style="color:#f92672">=</span>(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">28</span>, <span style="color:#ae81ff">28</span>),
        conv_param<span style="color:#f92672">=</span>{<span style="color:#e6db74">&#39;filter_num&#39;</span>: <span style="color:#ae81ff">30</span>,<span style="color:#e6db74">&#39;filter_size&#39;</span>: <span style="color:#ae81ff">5</span>,<span style="color:#e6db74">&#39;pad&#39;</span>: <span style="color:#ae81ff">0</span>,<span style="color:#e6db74">&#39;stride&#39;</span>: <span style="color:#ae81ff">1</span>},
        hidden_size<span style="color:#f92672">=</span><span style="color:#ae81ff">100</span>, output_size<span style="color:#f92672">=</span><span style="color:#ae81ff">10</span>, weight_init_std<span style="color:#f92672">=</span><span style="color:#ae81ff">0.01</span>
    ):
        <span style="color:#e6db74">&#34;&#34;&#34; Simple convolutional neural network
</span><span style="color:#e6db74">        
</span><span style="color:#e6db74">        Args:
</span><span style="color:#e6db74">            input_dim (tuple, optional): Input data shape, default is (1, 28, 28).
</span><span style="color:#e6db74">            conv_param (dict, optional): convolutional layer hyperparameters,
</span><span style="color:#e6db74">                The default is {&#39;filter_num&#39;:30,&#39;filter_size&#39;:5,&#39;pad&#39;:0,&#39;stride&#39;:1}.
</span><span style="color:#e6db74">            hidden_size (int, optional): Number of hidden layer neurons, default 100.
</span><span style="color:#e6db74">            output_size (int, optional): Number of neurons in the output layer, default 10.
</span><span style="color:#e6db74">            weight_init_std (float, optional): Tuning parameter for initial value of weight. The default is 0.01.
</span><span style="color:#e6db74">        &#34;&#34;&#34;</span>
        <span style="color:#75715e"># Extract hyperparameters of convolutional layer</span>
        filter_num <span style="color:#f92672">=</span> conv_param[<span style="color:#e6db74">&#39;filter_num&#39;</span>] <span style="color:#75715e"># number of filters</span>
        filter_size <span style="color:#f92672">=</span> conv_param[<span style="color:#e6db74">&#39;filter_size&#39;</span>] <span style="color:#75715e"># Filter size (same height and width)</span>
        filter_stride <span style="color:#f92672">=</span> conv_param[<span style="color:#e6db74">&#39;stride&#39;</span>] <span style="color:#75715e"># stride</span>
        filter_pad <span style="color:#f92672">=</span> conv_param[<span style="color:#e6db74">&#39;pad&#39;</span>] <span style="color:#75715e"># padding</span>
        
        <span style="color:#75715e"># Hyper parameter of pooling layer is fixed</span>
        pool_size <span style="color:#f92672">=</span> <span style="color:#ae81ff">2</span> <span style="color:#75715e"># size (same height and width)</span>
        pool_stride <span style="color:#f92672">=</span> <span style="color:#ae81ff">2</span> <span style="color:#75715e"># stride</span>
        pool_pad <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span> <span style="color:#75715e"># padding</span>

        <span style="color:#75715e"># Calculate the size of input data</span>
        input_ch <span style="color:#f92672">=</span> input_dim[<span style="color:#ae81ff">0</span>] <span style="color:#75715e"># number of input data channels</span>
        <span style="color:#66d9ef">assert</span> input_dim[<span style="color:#ae81ff">1</span>] <span style="color:#f92672">==</span> input_dim[<span style="color:#ae81ff">2</span>],<span style="color:#e6db74">&#39;The input data has the same height and width! &#39;</span>
        input_size <span style="color:#f92672">=</span> input_dim[<span style="color:#ae81ff">1</span>] <span style="color:#75715e"># size of input data</span>
        
        <span style="color:#75715e"># Calculate output size of convolutional layer</span>
        <span style="color:#66d9ef">assert</span> (input_size <span style="color:#f92672">+</span> <span style="color:#ae81ff">2</span> <span style="color:#f92672">*</span> filter_pad<span style="color:#f92672">-</span>filter_size) \
            <span style="color:#f92672">%</span> filter_stride <span style="color:#f92672">==</span> <span style="color:#ae81ff">0</span>,<span style="color:#e6db74">&#39;The output size of the convolutional layer is indivisible! &#39;</span>
        conv_output_size <span style="color:#f92672">=</span> int(
            (input_size <span style="color:#f92672">+</span> <span style="color:#ae81ff">2</span> <span style="color:#f92672">*</span> filter_pad<span style="color:#f92672">-</span>filter_size) <span style="color:#f92672">/</span> filter_stride <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span>
        )

        <span style="color:#75715e"># Output size calculation of pooling layer</span>
        <span style="color:#66d9ef">assert</span> (conv_output_size<span style="color:#f92672">-</span>pool_size) <span style="color:#f92672">%</span>pool_stride <span style="color:#f92672">==</span> <span style="color:#ae81ff">0</span>, \
            <span style="color:#e6db74">&#39;The output size of the pooling layer is indivisible! &#39;</span>
        pool_output_size_one <span style="color:#f92672">=</span> int(
            (conv_output_size<span style="color:#f92672">-</span>pool_size) <span style="color:#f92672">/</span> pool_stride <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span> <span style="color:#75715e"># height/width size</span>
        )
        pool_output_size <span style="color:#f92672">=</span> filter_num <span style="color:#f92672">*</span> \
            pool_output_size_one <span style="color:#f92672">*</span> pool_output_size_one <span style="color:#75715e"># total size of all filters</span>

        <span style="color:#75715e"># Weight initialization</span>
        self<span style="color:#f92672">.</span>params <span style="color:#f92672">=</span> {}
        <span style="color:#75715e"># Convolutional layer</span>
        self<span style="color:#f92672">.</span>params[<span style="color:#e6db74">&#39;W1&#39;</span>] <span style="color:#f92672">=</span> weight_init_std <span style="color:#f92672">*</span> \
            np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>randn(filter_num, input_ch, filter_size, filter_size)
        self<span style="color:#f92672">.</span>params[<span style="color:#e6db74">&#39;b1&#39;</span>] <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>zeros(filter_num)
        <span style="color:#75715e"># Affine Layer 1</span>
        self<span style="color:#f92672">.</span>params[<span style="color:#e6db74">&#39;W2&#39;</span>] <span style="color:#f92672">=</span> weight_init_std <span style="color:#f92672">*</span> \
            np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>randn(pool_output_size, hidden_size)
        self<span style="color:#f92672">.</span>params[<span style="color:#e6db74">&#39;b2&#39;</span>] <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>zeros(hidden_size)
        <span style="color:#75715e"># Affine Layer 2</span>
        self<span style="color:#f92672">.</span>params[<span style="color:#e6db74">&#39;W3&#39;</span>] <span style="color:#f92672">=</span> weight_init_std <span style="color:#f92672">*</span> \
            np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>randn(hidden_size, output_size)
        self<span style="color:#f92672">.</span>params[<span style="color:#e6db74">&#39;b3&#39;</span>] <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>zeros(output_size)
            
        <span style="color:#75715e"># Layer generation</span>
        self<span style="color:#f92672">.</span>layers <span style="color:#f92672">=</span> {} <span style="color:#75715e"># Since Python 3.7, the storage order of the dictionary is retained, so OrderedDict is not required.</span>
        <span style="color:#75715e"># Convolutional layer</span>
        self<span style="color:#f92672">.</span>layers[<span style="color:#e6db74">&#39;Conv1&#39;</span>] <span style="color:#f92672">=</span> Convolution(
            self<span style="color:#f92672">.</span>params[<span style="color:#e6db74">&#39;W1&#39;</span>], self<span style="color:#f92672">.</span>params[<span style="color:#e6db74">&#39;b1&#39;</span>], filter_stride, filter_pad
        )
        self<span style="color:#f92672">.</span>layers[<span style="color:#e6db74">&#39;Relu1&#39;</span>] <span style="color:#f92672">=</span> ReLU()
        self<span style="color:#f92672">.</span>layers[<span style="color:#e6db74">&#39;Pool1&#39;</span>] <span style="color:#f92672">=</span> Pooling(
            pool_size, pool_size, pool_stride, pool_pad
        )
        <span style="color:#75715e"># Affine Layer 1</span>
        self<span style="color:#f92672">.</span>layers[<span style="color:#e6db74">&#39;Affine1&#39;</span>] <span style="color:#f92672">=</span> \
            Affine(self<span style="color:#f92672">.</span>params[<span style="color:#e6db74">&#39;W2&#39;</span>], self<span style="color:#f92672">.</span>params[<span style="color:#e6db74">&#39;b2&#39;</span>])
        self<span style="color:#f92672">.</span>layers[<span style="color:#e6db74">&#39;Relu2&#39;</span>] <span style="color:#f92672">=</span> ReLU()
        <span style="color:#75715e"># Affine Layer 2</span>
        self<span style="color:#f92672">.</span>layers[<span style="color:#e6db74">&#39;Affine2&#39;</span>] <span style="color:#f92672">=</span> \
            Affine(self<span style="color:#f92672">.</span>params[<span style="color:#e6db74">&#39;W3&#39;</span>], self<span style="color:#f92672">.</span>params[<span style="color:#e6db74">&#39;b3&#39;</span>])
    
        self<span style="color:#f92672">.</span>lastLayer <span style="color:#f92672">=</span> SoftmaxWithLoss()

    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">predict</span>(self, x):
        <span style="color:#e6db74">&#34;&#34;&#34; Neural network inference
</span><span style="color:#e6db74">        
</span><span style="color:#e6db74">        Args:
</span><span style="color:#e6db74">            x (numpy.ndarray): Input to neural network
</span><span style="color:#e6db74">        
</span><span style="color:#e6db74">        Returns:
</span><span style="color:#e6db74">            numpy.ndarray: neural network output
</span><span style="color:#e6db74">        &#34;&#34;&#34;</span>
        <span style="color:#75715e"># Propagate layer forwardfor layer in self.layers.values():</span>
            x <span style="color:#f92672">=</span> layer<span style="color:#f92672">.</span>forward(x)

        <span style="color:#66d9ef">return</span> x

    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">loss</span>(self, x, t):
        <span style="color:#e6db74">&#34;&#34;&#34; Loss function value calculation
</span><span style="color:#e6db74">        
</span><span style="color:#e6db74">        Args:
</span><span style="color:#e6db74">            x (numpy.ndarray): Input to neural network
</span><span style="color:#e6db74">            t (numpy.ndarray): correct answer label
</span><span style="color:#e6db74">
</span><span style="color:#e6db74">        Returns:
</span><span style="color:#e6db74">            float: value of loss function
</span><span style="color:#e6db74">        &#34;&#34;&#34;</span>
        <span style="color:#75715e"># Inference</span>
        y <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>predict(x)

        <span style="color:#75715e"># Softmax-with-Loss Layer forward calculation</span>
        loss <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>lastLayer<span style="color:#f92672">.</span>forward(y, t)

        <span style="color:#66d9ef">return</span> loss

    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">accuracy</span>(self, x, t, batch_size<span style="color:#f92672">=</span><span style="color:#ae81ff">100</span>):
        <span style="color:#e6db74">&#34;&#34;&#34; Calculation of recognition accuracy
</span><span style="color:#e6db74">        batch_size is the batch size at the time of calculation. When trying to calculate a large amount of data at once
</span><span style="color:#e6db74">        im2col eats too much memory and thrashing occurs, so it doesn&#39;t work.
</span><span style="color:#e6db74">        The thing for avoiding it.
</span><span style="color:#e6db74">
</span><span style="color:#e6db74">        Args:
</span><span style="color:#e6db74">            x (numpy.ndarray): Input to neural network
</span><span style="color:#e6db74">            t (numpy.ndarray): Correct answer label (one-hot)
</span><span style="color:#e6db74">            batch_size (int), optional): Batch size when calculating, default 100.
</span><span style="color:#e6db74">        
</span><span style="color:#e6db74">        Returns:
</span><span style="color:#e6db74">            float: recognition accuracy
</span><span style="color:#e6db74">        &#34;&#34;&#34;</span>
        <span style="color:#75715e">#Calculate the number of divisions</span>
        batch_num <span style="color:#f92672">=</span> max(int(x<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>] <span style="color:#f92672">/</span> batch_size), <span style="color:#ae81ff">1</span>)

        <span style="color:#75715e"># Split</span>
        x_list <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array_split(x, batch_num, <span style="color:#ae81ff">0</span>)
        t_list <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array_split(t, batch_num, <span style="color:#ae81ff">0</span>)

        <span style="color:#75715e"># Process in divided units</span>
        correct_num <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span> <span style="color:#75715e"># total number of correct answers</span>
        <span style="color:#66d9ef">for</span> (sub_x, sub_t) <span style="color:#f92672">in</span> zip(x_list, t_list):
            <span style="color:#66d9ef">assert</span> sub_x<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>] <span style="color:#f92672">==</span> sub_t<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>],<span style="color:#e6db74">&#39;Is the division boundary shifted? &#39;</span>
            y <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>predict(sub_x)
            y <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>argmax(y, axis<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
            t <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>argmax(sub_t, axis<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
            correct_num <span style="color:#f92672">+=</span> np<span style="color:#f92672">.</span>sum(y <span style="color:#f92672">==</span> t)
        
        <span style="color:#75715e">#Calculation of recognition accuracy</span>
        <span style="color:#66d9ef">return</span> correct_num <span style="color:#f92672">/</span> x<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>]

    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">gradient</span>(self, x, t):
        <span style="color:#e6db74">&#34;&#34;&#34; Gradient for weight parameter is calculated by error backpropagation method
</span><span style="color:#e6db74">        
</span><span style="color:#e6db74">         Args:
</span><span style="color:#e6db74">            x (numpy.ndarray): Input to neural network
</span><span style="color:#e6db74">            t (numpy.ndarray): correct answer label
</span><span style="color:#e6db74">        
</span><span style="color:#e6db74">        Returns:
</span><span style="color:#e6db74">            dictionary: dictionary that stores the gradient
</span><span style="color:#e6db74">        &#34;&#34;&#34;</span>
        <span style="color:#75715e">#Propagation</span>
        self<span style="color:#f92672">.</span>loss(x, t) <span style="color:#75715e"># Propagate forward to calculate the loss value</span>

        <span style="color:#75715e">#Backpropagation</span>
        dout <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>lastLayer<span style="color:#f92672">.</span>backward()
        <span style="color:#66d9ef">for</span> layer <span style="color:#f92672">in</span> reversed(list(self<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>values())):
            dout <span style="color:#f92672">=</span> layer<span style="color:#f92672">.</span>backward(dout)

        <span style="color:#75715e"># Extract differential value of each layer</span>
        grads <span style="color:#f92672">=</span> {}
        grads[<span style="color:#e6db74">&#39;W1&#39;</span>] <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>layers[<span style="color:#e6db74">&#39;Conv1&#39;</span>]<span style="color:#f92672">.</span>dW
        grads[<span style="color:#e6db74">&#39;b1&#39;</span>] <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>layers[<span style="color:#e6db74">&#39;Conv1&#39;</span>]<span style="color:#f92672">.</span>db
        grads[<span style="color:#e6db74">&#39;W2&#39;</span>] <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>layers[<span style="color:#e6db74">&#39;Affine1&#39;</span>]<span style="color:#f92672">.</span>dW
        grads[<span style="color:#e6db74">&#39;b2&#39;</span>] <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>layers[<span style="color:#e6db74">&#39;Affine1&#39;</span>]<span style="color:#f92672">.</span>db
        grads[<span style="color:#e6db74">&#39;W3&#39;</span>] <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>layers[<span style="color:#e6db74">&#39;Affine2&#39;</span>]<span style="color:#f92672">.</span>dW
        grads[<span style="color:#e6db74">&#39;b3&#39;</span>] <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>layers[<span style="color:#e6db74">&#39;Affine2&#39;</span>]<span style="color:#f92672">.</span>db

        <span style="color:#66d9ef">return</span> grads
</code></pre></div><p>The stumbling block in this implementation is the <code>accuracy</code>, which is omitted from the book.</p>
<p>The recognition accuracy is calculated in 1-epoch units during learning, but with the code written in Chapter 4, 60,000 pieces of training data were thrown in at a stretch to obtain recognition accuracy. However, if you do the same thing this time, it seems that the expansion of <code>im2col</code> consumes a large amount of memory, and in my VM with 4GB of memory, it stops with thrashing <sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup> :sweat:</p>
<p>But the source of the book still consumes less memory and works fine in my environment. It is strange, so when I traced the source, it was divided and processed internally. That&rsquo;s why I also manage and divide inside. In addition, try using <a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.array_split.html#numpy-array-split"><code> numpy.array_split</code></a> to implement the split. It was.</p>
<h3 id="4-implementation-of-learning">(4) Implementation of learning</h3>
<p>Learning is based on the previous [5.7.4 Learning using Back Propagation](<a href="https://qiita.com/segavvy/items/8707e4e65aa7fa357d8a#574-%E8%AA%A4%E5%B7%AE%E9%80%86%E4%BC%9D%E6%92%AD%E6%B3%95%E3%82%92%E4%BD%BF%E3%81%A3%E3%81%9F%E5%AD%25Implementedbasedon(A6%E7%BF%92)">https://qiita.com/segavvy/items/8707e4e65aa7fa357d8a#574-%E8%AA%A4%E5%B7%AE%E9%80%86%E4%BC%9D%E6%92%AD%E6%B3%95%E3%82%92%E4%BD%BF%E3%81%A3%E3%81%9F%E5%AD%Implementedbasedon(A6%E7%BF%92)</a>. Below are some points.</p>
<ul>
<li>Unlike the previous time, the input image this time is (1, 28, 28), so it is necessary to specify <code>flatten=False</code> when reading the MNIST data with <code>load_mnist</code>.</li>
<li>Hyper parameter <code>learning_rate</code> was made small for AdaGrad and tried a few times to get to <code>0.06</code>.</li>
<li>The number of updates is set to <code>6000</code> (10 epochs) because the recognition accuracy of test data stabilizes relatively quickly.</li>
<li>In the previous source, the display of the number of updates was off by one, and the display of the recognition accuracy of the first time was also not after the update but after the update once, so it was corrected.</li>
</ul>
<p>Below is the implementation of learning.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python:mnist.py" data-lang="python:mnist.py"><span style="color:#75715e"># coding: utf-8</span>
<span style="color:#f92672">import</span> os
<span style="color:#f92672">import</span> sys
<span style="color:#f92672">import</span> matplotlib.pylab <span style="color:#f92672">as</span> plt
<span style="color:#f92672">import</span> numpy <span style="color:#f92672">as</span> np
<span style="color:#f92672">from</span> ada_grad <span style="color:#f92672">import</span> AdaGrad
<span style="color:#f92672">from</span> simple_conv_net <span style="color:#f92672">import</span> SimpleConvNet
sys<span style="color:#f92672">.</span>path<span style="color:#f92672">.</span>append(os<span style="color:#f92672">.</span>pardir) <span style="color:#75715e"># add parent directory to path</span>
<span style="color:#f92672">from</span> dataset.mnist <span style="color:#f92672">import</span> load_mnist


<span style="color:#75715e"># Load MNIST training and test data</span>
(x_train, t_train), (x_test, t_test) <span style="color:#f92672">=</span> \
    load_mnist(normalize<span style="color:#f92672">=</span>True, flatten<span style="color:#f92672">=</span>False, one_hot_label<span style="color:#f92672">=</span>True)

<span style="color:#75715e"># Hyper parameter setting</span>
iters_num <span style="color:#f92672">=</span> <span style="color:#ae81ff">6000</span> <span style="color:#75715e"># Number of updates</span>
batch_size <span style="color:#f92672">=</span> <span style="color:#ae81ff">100</span> <span style="color:#75715e"># batch size</span>
learning_rate <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.06</span> <span style="color:#75715e"># Learning rate, assuming AdaGrad</span>

train_size <span style="color:#f92672">=</span> x_train<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>] <span style="color:#75715e"># size of training data</span>
iter_per_epoch <span style="color:#f92672">=</span> max(int(train_size <span style="color:#f92672">/</span> batch_size), <span style="color:#ae81ff">1</span>) <span style="color:#75715e"># 1 number of iterations per epoch</span>

<span style="color:#75715e"># Simple convolutional neural network generation</span>
network <span style="color:#f92672">=</span> SimpleConvNet(
    input_dim<span style="color:#f92672">=</span>(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">28</span>, <span style="color:#ae81ff">28</span>),
    conv_param<span style="color:#f92672">=</span>{<span style="color:#e6db74">&#39;filter_num&#39;</span>: <span style="color:#ae81ff">30</span>,<span style="color:#e6db74">&#39;filter_size&#39;</span>: <span style="color:#ae81ff">5</span>,<span style="color:#e6db74">&#39;pad&#39;</span>: <span style="color:#ae81ff">0</span>,<span style="color:#e6db74">&#39;stride&#39;</span>: <span style="color:#ae81ff">1</span>},
    hidden_size<span style="color:#f92672">=</span><span style="color:#ae81ff">100</span>, output_size<span style="color:#f92672">=</span><span style="color:#ae81ff">10</span>, weight_init_std<span style="color:#f92672">=</span><span style="color:#ae81ff">0.01</span>
)

<span style="color:#75715e"># Optimizer generation</span>
optimizer <span style="color:#f92672">=</span> AdaGrad(learning_rate) <span style="color:#75715e"># AdaGrad</span>

<span style="color:#75715e"># Confirmation of recognition accuracy before learning</span>
train_acc <span style="color:#f92672">=</span> network<span style="color:#f92672">.</span>accuracy(x_train, t_train)
test_acc <span style="color:#f92672">=</span> network<span style="color:#f92672">.</span>accuracy(x_test, t_test)
train_loss_list <span style="color:#f92672">=</span> [] <span style="color:#75715e"># Destination for the transition of loss function values</span>
train_acc_list <span style="color:#f92672">=</span> [train_acc] <span style="color:#75715e"># Destination of transition of recognition accuracy for training data</span>
test_acc_list <span style="color:#f92672">=</span> [test_acc] <span style="color:#75715e"># Storage location of transition of recognition accuracy for test data</span>
<span style="color:#66d9ef">print</span>(f<span style="color:#e6db74">&#39; before learning [training data recognition accuracy]{train_acc:.4f} [test data recognition accuracy]{test_acc:.4f}&#39;</span>)

<span style="color:#75715e"># Start learning</span>
<span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(iters_num):

    <span style="color:#75715e"># Mini batch generation</span>
    batch_mask <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>choice(train_size, batch_size, replace<span style="color:#f92672">=</span>False)
    x_batch <span style="color:#f92672">=</span> x_train[batch_mask]
    t_batch <span style="color:#f92672">=</span> t_train[batch_mask]

    <span style="color:#75715e">#Gradient calculation</span>
    grads <span style="color:#f92672">=</span> network<span style="color:#f92672">.</span>gradient(x_batch, t_batch)

    <span style="color:#75715e"># Update weight parameter</span>
    optimizer<span style="color:#f92672">.</span>update(network<span style="color:#f92672">.</span>params, grads)
    
    <span style="color:#75715e"># Loss function value calculation</span>
    loss <span style="color:#f92672">=</span> network<span style="color:#f92672">.</span>loss(x_batch, t_batch)
    train_loss_list<span style="color:#f92672">.</span>append(loss)

    <span style="color:#75715e">#1 Calculation of recognition accuracy for each epoch</span>
    <span style="color:#66d9ef">if</span> (i <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span>)<span style="color:#f92672">%</span> iter_per_epoch <span style="color:#f92672">==</span> <span style="color:#ae81ff">0</span>:
        train_acc <span style="color:#f92672">=</span> network<span style="color:#f92672">.</span>accuracy(x_train, t_train)test_acc <span style="color:#f92672">=</span> network<span style="color:#f92672">.</span>accuracy(x_test, t_test)
        train_acc_list<span style="color:#f92672">.</span>append(train_acc)
        test_acc_list<span style="color:#f92672">.</span>append(test_acc)

        <span style="color:#75715e"># Progress display</span>
        <span style="color:#66d9ef">print</span>(
            f<span style="color:#e6db74">&#39;[epoch] {(i + 1) // iter_per_epoch:&gt;2} &#39;</span>
            f<span style="color:#e6db74">&#39;[number of updates] {i + 1:&gt;5} [value of loss function] {loss:.4f} &#39;</span>
            f<span style="color:#e6db74">&#39;[Recognition accuracy of training data]{train_acc:.4f} [Recognition accuracy of test data]{test_acc:.4f}&#39;</span>
        )

<span style="color:#75715e"># Draw the transition of the value of the loss function</span>
x <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>arange(len(train_loss_list))
plt<span style="color:#f92672">.</span>plot(x, train_loss_list, label<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;loss&#39;</span>)
plt<span style="color:#f92672">.</span>xlabel(<span style="color:#e6db74">&#39;iteration&#39;</span>)
plt<span style="color:#f92672">.</span>ylabel(<span style="color:#e6db74">&#39;loss&#39;</span>)
plt<span style="color:#f92672">.</span>xlim(left<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)
plt<span style="color:#f92672">.</span>ylim(<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">2.5</span>)
plt<span style="color:#f92672">.</span>show()

<span style="color:#75715e">#Draw the transition of recognition accuracy of training data and test data</span>
x2 <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>arange(len(train_acc_list))
plt<span style="color:#f92672">.</span>plot(x2, train_acc_list, label<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;train acc&#39;</span>)
plt<span style="color:#f92672">.</span>plot(x2, test_acc_list, label<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;test acc&#39;</span>, linestyle<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;--&#39;</span>)
plt<span style="color:#f92672">.</span>xlabel(<span style="color:#e6db74">&#39;epochs&#39;</span>)
plt<span style="color:#f92672">.</span>ylabel(<span style="color:#e6db74">&#39;accuracy&#39;</span>)
plt<span style="color:#f92672">.</span>xlim(left<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)
plt<span style="color:#f92672">.</span>ylim(<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1.0</span>)
plt<span style="color:#f92672">.</span>legend(loc<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;lower right&#39;</span>)
plt<span style="color:#f92672">.</span>show()
</code></pre></div><h3 id="5-execution-result">(5) Execution result</h3>
<p>The following is the execution result. It took about an hour in my environment.</p>
<pre><code>Before learning [Recognition accuracy of training data] 0.0909 [Recognition accuracy of test data] 0.0909
[Epoch] 1 [Number of updates] 600 [Value of loss function] 0.0699 [Recognition accuracy of training data] 0.9784 [Recognition accuracy of test data] 0.9780
[Epoch] 2 [Number of updates] 1200 [Loss function value] 0.0400 [Recognition accuracy of training data] 0.9844 [Recognition accuracy of test data] 0.9810
[Epoch] 3 [Number of updates] 1800 [Loss function value] 0.0362 [Training data recognition accuracy] 0.9885 [Test data recognition accuracy] 0.9853
[Epoch] 4 [Number of updates] 2400 [Value of loss function] 0.0088 [Recognition accuracy of training data] 0.9907 [Recognition accuracy of test data] 0.9844
[Epoch] 5 [Number of updates] 3000 [Value of loss function] 0.0052 [Recognition accuracy of training data] 0.9926 [Recognition accuracy of test data] 0.9851
[Epoch] 6 [Number of updates] 3600 [Value of loss function] 0.0089 [Recognition accuracy of training data] 0.9932 [Recognition accuracy of test data] 0.9850
[Epoch] 7 [Number of updates] 4200 [Loss function value] 0.0029 [Training data recognition accuracy] 0.9944 [Test data recognition accuracy] 0.9865
[Epoch] 8 [Number of updates] 4800 [Value of loss function] 0.0023 [Recognition accuracy of training data] 0.9954 [Recognition accuracy of test data] 0.9873
[Epoch] 9 [Number of updates] 5400 [Loss function value] 0.0051 [Recognition accuracy of training data] 0.9959 [Recognition accuracy of test data] 0.9860
[Epoch] 10 [Number of updates] 6000 [Loss function value] 0.0037 [Recognition accuracy of training data] 0.9972 [Recognition accuracy of test data] 0.9860
</code></pre><p><img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/139624/554c9411-8e9e-7af4-17e6-f7c18fbe641a.png" alt="Screenshot 2020-02-11 17.03.04.png">
![Screenshot 2020-02-11 17.03.20.png](<a href="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/139624/fe120e0b-f2a9-703e-7519-(995576c9d79d.png)">https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/139624/fe120e0b-f2a9-703e-7519-(995576c9d79d.png)</a>
As a result, the recognition accuracy of the training data was 99.72%, and the recognition accuracy of the test data was 98.60%. One epoch already exceeds the previous recognition accuracy. 7 Since the recognition accuracy of test data has not changed from around the epoch, it may have been only to proceed with over-learning after that. Even so, the accuracy of 98.60% with a simple CNN is amazing.</p>
<p>I also tried running the source of the book, but for some reason the calculation of recognition accuracy per epoch is very fast. Mysteriously, I was able to sample with the <code>evaluate_sample_num_per_epoch</code> parameter of the <code>Trainer</code> class, and I was able to calculate the training image and test image only with the first 1,000 images. unfair! :unamused:</p>
<h2 id="76-cnn-visualization">7.6 CNN visualization</h2>
<p>It&rsquo;s amazing that the necessary filters such as edge and blob extraction are automatically created. It is very interesting that the level of abstraction increases as you stack layers.</p>
<h2 id="77-typical-cnn">7.7 Typical CNN</h2>
<p>Big data and GPUs are said to have contributed greatly to the development of deep learning, but I think the major point is the spread of the cloud, which makes it possible to use enormous machine resources at low cost.</p>
<p>Also, as a complete digression, I heard that LeNet&rsquo;s proposal was made in 1998, 20 years ago, and I was deeply moved by it. It was a more recent impression in 1998. I don&rsquo;t want to get old :sweat:</p>
<h2 id="78-summary">7.8 Summary</h2>
<p>It was a bit difficult to implement, but thanks to CNN I was able to understand.
That&rsquo;s it for this chapter. If you have any mistakes, I would appreciate it if you could point me out.</p>
<p>(To other chapters of this memo: <a href="https://qiita.com/segavvy/items/1945aa1a0f91a1473555">Chapter 1</a>/<a href="https://qiita.com/segavvy/items/d8e9e70437e35083a459">Chapter2</a>/<a href="https://qiita.com/segavvy/items/6d79d0c3b4367869f4ea">Chapter3</a>/<a href="https://qiita.com/segavvy/items/bdad9fcda2f0da918e7c">Chapter4</a>/<a href="https://qiita.com/segavvy/items/8707e4e65aa7fa357d8a">Chapter5</a>/<a href="https://qiita.com/segavvy/items/ca4ac4c9ee1a126bff41">Chapter6</a>/Chapter7/<a href="https://qiita.com/segavvy/items/3eb6ea0ea2af689696">Chapter8</a>)/<a href="https://qiita.com/segavvy/items/4e8c36cac9c6f3543ffd">Summary</a>))</p>
<section class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1" role="doc-endnote">
<p><a href="https://web.archive.org/web/20130228002415/http://www.microsoft.com/japan/presspass/detail.aspx?newsid=3491">Changes in long-sound notation at the end of foreign words in katakana for Microsoft products and services</a>(*Sincetherearenopagesleftatthattime,<a href="https://en.wikipedia.org/wiki/%E9%95%B7%E9%9F%25(B3%E7%AC%A6)isalsoalinktotheWaybackMachineoftheInternetArchivewhichisalsolinkedfrom">Wikipedia&gt;LongNote</a> <a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2" role="doc-endnote">
<p>See &ldquo;Improving Python&rsquo;s data model&rdquo; in <a href="https://docs.python.org/en/3/whatsnew/3.7.html">What&rsquo;s New In Python 3.7</a>. <a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3" role="doc-endnote">
<p>Thrashing is a phenomenon that occurs when there is insufficient memory, and it may be inoperable for each OS, which is troublesome. If you are interested in memory management of OS, please post <a href="https://qiita.com/segavvy/items/9a9f8aa5cc4e6760307a">Memory management introduction: 01</a> I posted before! :grin: <a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</section>

    </article>
  </div>

  
  
  
  <script>
  window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
  ga('create', 'UA-169005401-1', 'auto');
  ga('send', 'pageview');
  </script>
  <script async src='https://www.google-analytics.com/analytics.js'></script>
  

  

  
<link rel="stylesheet" type="text/css" href="//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.0/cookieconsent.min.css" />
<script src="//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.0/cookieconsent.min.js"></script>
<script>
window.addEventListener("load", function(){
window.cookieconsent.initialise({
  "palette": {
    "popup": {
      "background": "#216942",
      "text": "#b2d192"
    },
    "button": {
      "background": "#afed71"
    }
  }
})});
</script>

</body>

</html>
