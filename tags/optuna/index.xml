<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Optuna on Memo Tut</title>
    <link>https://memotut.com/tags/optuna/</link>
    <description>Recent content in Optuna on Memo Tut</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 01 Dec 2019 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://memotut.com/tags/optuna/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>I don&#39;t want to search for high para because it is IQ1 (how to use lightgbm_tuner)</title>
      <link>https://memotut.com/i-dont-want-to-search-for-high-para-because-it-is-iq1-how-to-use-lightgbm_tuner-dee7f/</link>
      <pubDate>Sun, 01 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://memotut.com/i-dont-want-to-search-for-high-para-because-it-is-iq1-how-to-use-lightgbm_tuner-dee7f/</guid>
      <description>This article is posted on [IQ1&#39;s 2nd Advent Calendar 2019](https://adventar.org/calendars/4711).  IQ1 machine learning For machine learning, data preprocessing and model hyper search are inevitable. However, I have an IQ of 1, so I don&amp;rsquo;t want to do it if I can do preprocessing of data and high-para search. If you put the preprocessing of data into this modern gradient boosting tree system algorithm (such as lightgbm), you can handle the missing values as they are and you do not have to preprocess the categorical variables, so it will be a world relatively friendly to IQ1.</description>
    </item>
    
  </channel>
</rss>