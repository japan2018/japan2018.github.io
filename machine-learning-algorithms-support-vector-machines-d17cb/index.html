<!doctype html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  
  <meta name="generator" content="Hugo 0.72.0" />

  
  <meta name="description" content="Some description">
  

  
  <link rel="apple-touch-icon" sizes="180x180" href="https://memotut.com/apple-touch-icon.png">

  
  <link rel="icon" type="image/png" sizes="32x32" href="https://memotut.com/favicon-32x32.png">

  
  <link rel="icon" type="image/png" sizes="16x16" href="https://memotut.com/favicon-16x16.png">

  
  <link rel="manifest" href="https://memotut.com/site.webmanifest">

  
  <link rel="mask-icon" href="https://memotut.com/safari-pinned-tab.svg" color="#5bbad5">

  <meta name="msapplication-TileColor" content="#da532c">

  <meta name="theme-color" content="#ffffff">

  
  <link rel="stylesheet" href="https://memotut.com/css/bootstrap.min.css" />

  
  <title>Machine learning algorithms (support vector machines) | Memo Tut</title>
  

  <style>
body {
  min-width: 300px;
}

.custom-navbar {
  margin-bottom: 1em;
  height: 60px;
}

.custom-navbar a {
  display: inline-block; 
  padding: 18px 0;
  margin-right: 1em; 
  font-weight: bold; 
}

.custom-navbar a:hover,
.custom-navbar a:focus {
  text-decoration: none; 
}

@media print {
  .custom-navbar {
    display: none;
  }
}

article {
  padding-bottom: 1em;
}

img {
  max-width: 100%;
}


body {
  background-color: #fff;
}



body {
  color: #212529;
}



a {
  color: #007bff;
}



a:hover,
a:focus {
  color: #0056b3;
}



.custom-navbar {
  background-color: #212529;
}



.custom-navbar a {
  color: rgba(255, 255, 255, 0.75);
}



.custom-navbar a:hover,
.custom-navbar a:focus {
  color: rgba(255, 255, 255, 1);
}



.container {
  max-width: 800px;
}



pre {
  display: block;
  padding: 9.5px;
  word-break: break-all;
  word-wrap: break-word;
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 4px;
}

pre code {
  padding: 0;
  font-size: inherit;
  color: inherit; 
  white-space: pre-wrap;
  background-color: transparent;
  border: none;
  border-radius: 0;
}

code {
  padding: 2px 4px;
  color: inherit; 
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 4px;
  font-size: .9em;
}



blockquote,
.blockquote {
  padding: 10px 20px;
  margin: 0 0 20px;
  font-size: 1em;
  border-left: 5px solid #6c757d;
}

</style>
</head>

<body>
  <nav class="custom-navbar">
  <div class="container">
    
    <a href="/">Posts</a>
    
    <a href="/tags/">Tags</a>
    
    <a href="/about/">About</a>
    
    <a href="/index.xml">RSS</a>
    
  </div>
</nav>
  
  <div class="container">
    <article>
      <h1>Machine learning algorithms (support vector machines)</h1>
<p>
  <small class="text-secondary">
  
  
  Mar 15, 2020
  </small>
  

<small><code><a href="https://memotut.com/tags/python">Python</a></code></small>


<small><code><a href="https://memotut.com/tags/machine-learning"> machine learning</a></code></small>


<small><code><a href="https://memotut.com/tags/data-science"> data science</a></code></small>

</p>
<pre><code>#Introduction
</code></pre>
<p>Step-by-step about the theory, the implementation in python, and the analysis using scikit-learn of the algorithm taken up in &ldquo;<a href="https://qiita.com/hiro88hyo/items/c00dcf8f083ba3d76af8">Classification of machine learning</a>&rdquo; Learn with. I wrote it for personal learning, so I would like you to take a look at any mistakes.</p>
<p>This time about <strong>Support Vector Machine</strong>. Despite its old history, it is a popular technique in the field of machine learning. It has a high generalization performance, and was the strongest algorithm until it was broken into deep learning by <a href="http://www.image-net.org/challenges/LSVRC/2012/">ILSVRC2012</a>.Itfeelsprettygoodtorememberthis(vocabulary).</p>
<p>The following sites were referenced this time. Thank you very much. It seems that there are many other good books, so please refer to that as well.</p>
<ul>
<li><a href="https://qiita.com/amber_kshz/items/6a9f8b6dd857edffce58">Support Vector Machine (SVM) of PRML Chapter 7 implemented in Python</a></li>
<li><a href="https://qiita.com/hirokuyu/items/d3d90465fc7fa133e668">SVM (Support Vector Machine)</a></li>
<li><a href="https://qiita.com/sloganjp/items/81816616c22e1f788208">Understanding Support Vector Machine</a></li>
<li><a href="https://drumato.hatenablog.com/entry/2018/11/17/234633">Easily understand SVM.</a></li>
<li><a href="https://www.slideshare.net/mknh1122/svm-13623887">SVM (Support Vector Machine)</a></li>
<li><a href="https://www.hellocybernetics.tech/entry/2017/01/24/201702">Calculate and understand Support Vector Machine by hand</a></li>
<li><a href="https://www.slideshare.net/sleepy_yoshi/smo-svm">SMO thorough introduction</a></li>
<li><a href="https://qiita.com/shuhei_f/items/5c4ff6ed278eb1747a6f">I implemented SMO with Python + NumPy</a></li>
</ul>
<h1 id="theory">Theory</h1>
<p>I tried to understand the theory firmly, but since it was very complicated, there was an <a href="https://qiita.com/amber_kshz/items/6a9f8b6dd857edffce58">entry that touched the theory as hard as possible</a> , I will only describe the essence.</p>
<h2 id="rough-overview">Rough overview</h2>
<p>Support Vector Machine is a supervised binary classifier such as Perceptron or Logistic Regression. By introducing the concept of support vector, it becomes possible to classify even classification problems (XOR etc.) that have high generalization performance for unknown data and cannot perform linear classification by using the kernel method.
Roughly speaking, it can be said that it is a classifier that brings margin maximization and kernel method to Perceptron.</p>
<h2 id="theory-1">Theory</h2>
<p>Consider the situation where binary classification is necessary for the following two feature quantities. The blue and red circles are the training data, and the blue point is 1 (<strong>positive example</strong>) and the red point is -1 (<strong>negative example</strong>). The green line drawn on the border is set as $$y=ax+b$$. In each of the positive and negative examples, the point closest to the green line is called <strong>support vector</strong>, and the distance from the support vector to the border is called <strong>margin</strong>.</p>
<img width="439" alt="svm_1.png" src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/18497/ecf6d3f4-2768-5b3c-0544-90e58ed813ec.png">
<p>Support vector machines have the problem of finding $a$ and $b$ that maximize this margin for the teacher data.
In reality, not only is this case convenient, but there are many cases where it does not divide properly, but first we will consider this simple case.</p>
<p>The reason why the support vector machine can classify well in various cases is that it can classify unknown data nicely thanks to this margin maximization and that it can be classified by using the data near the boundary surface. There seems to be a point that it does not affect the value so much.</p>
<h3 id="initial-setting">Initial setting</h3>
<p>$\boldsymbol{x}=(x_0, x_1, \cdots, x_{N-1})$ for N teacher data and $\boldsymbol{t}=(t_0, t_1, \cdots, t_{ for classification labels N-1})$, and the boundary expression (called the discriminant function) is $g(x)=\boldsymbol{w}^Tx+w_0$. $\boldsymbol{w}$ is the weight vector of $\boldsymbol{x}$.</p>
<h3 id="maximize-margin">Maximize margin</h3>
<p>The distance between $y=ax+b$ and a point $x_n$ is $$\frac{|ax+b|}{\sqrt{a^2}$$(<a href="https://mathtrain.jp/tentotyokusen">Reference</a>),butthinkaboutthiswith$g(x)$.Thedistance$|r|$betweenacertainpoint$x_n$and$g(x)$canbeexpressedas$$|r|=\frac{|g(x)|}{|w|}$$.Thelabel$t_n$is$|t_n|=1$and$t_ng(x)&gt;=0$,so$|r|=\frac{t_n(\boldsymbol{w}^Tx_n+w_0) }{|w|}$$.</p>
<p>To maximize $|r_{min}|$ at $x_{min}$ closest to $g(x)$, $$|r_{min}|=\frac{t_{min}( \boldsymbol{w}^Tx_{min}+w_0)}{|w|}$$ is the maximum for $\boldsymbol{w}$ and $w_0$.</p>
<h3 id="constraints">constraints</h3>
<p>A smaller denominator is better for maximizing the above formula, but it becomes indefinite when it becomes 0 (the solution cannot be uniquely determined), so a constraint is added.
Assuming that $g(x)=1$ in the positive example and $g(x)=-1$ in the negative example, $$t_ng(x)=t_n(\boldsymbol{w}^Tx_n+w_0 ) \geq 1$$ will be obtained.
This maximizes the margin between the boundary and the nearest point $t_ng(x_{min})=1$, $\frac{1}{|w|}$, thus minimizing $|w|$ All you have to do is There is no problem in replacing $\frac{1}{2}|w|^2$ by minimizing it in consideration of differentiating later.</p>
<p>To summarize, $$\frac{1}{2}|w|^2$$ is minimized under the condition of $$t_n(\boldsymbol{w}^Tx_n+w_0) \geq 1$$$\ We will ask for boldsymbol{w}$ and $w_0$.</p>
<h3 id="lagranges-undetermined-multiplier-method">Lagrange&rsquo;s undetermined multiplier method</h3>
<p>In order to minimize a function with constraints such as the above equation, we use a method called <a href="https://mathtrain.jp/mlm">Lagrange&rsquo;s undetermined multiplier method</a>. This uses the theory called the <strong>dual problem</strong>, which means that solving the rewritten problem means solving the original problem.</p>
<p>Maximum $f(x)=\frac{1}{2}|w|^2$ under constraint expression $h(x)=t_n(\boldsymbol{w}^Tx_n+w_0)-1$ Lagrangian function $L(w, w_0, \lambda)=f(w, w_0)-\sum_{i=1}^{N}\lambda_ih(w , w_0)$$ is defined.</p>
<pre><code class="language-math" data-lang="math">L(w,w_0,\lambda)=\frac{1}{2}|w|^2-\sum_{i=1}^{N}\lambda_i \{ t_i(\boldsymbol{w}^Tx_i+w_0 )-1\}
</code></pre><p>So, if we partially differentiate $w$ and $w_0$ and set the partial derivative to 0, the Lagrangian function will be an expression with only $\lambda$,</p>
<pre><code class="language-math" data-lang="math">L(\lambda)=\sum_{n=1}^{N}\lambda_n-\frac{1}{2}\sum_{n=1}^{N}\sum_{m=1}^{N} \lambda_n\lambda_mt_nt_mx_n^Tx_m
</code></pre><p>Is derived. The constraint is</p>
<pre><code class="language-math" data-lang="math">\lambda_n\geq 0 \\
\sum_{i=1}^N\lambda_it_i=0
</code></pre><p>is. Now we can replace it with the problem of finding $\lambda$ that maximizes $L(\lambda)$.</p>
<p>Partially differentiating the above equation by $w$,
$$w=\sum_{i=1}^{N}\lambda_it_ix_i$$ can be obtained.</p>
<p>Now that we have $w$, we want $w_0$. $$g(x)=\boldsymbol{w}^Tx+w_0=\sum_{i=1}^{N}\lambda_it_ix_ix+w_0$$, and the margin with the support vector is 1, so all support For vectors,</p>
<pre><code class="language-math" data-lang="math">t_n(\sum_{m} \lambda_mt_mx_mx_n+w_0)=1
</code></pre><p>Holds. Actually transformed</p>
<pre><code class="language-math" data-lang="math">w_0=\frac{1}{N_M}\sum_{n}(t_n-\sum_{m}\lambda_mt_mx_mx_n)
</code></pre><p>Ask as. To summarize, after finding $\lambda$,</p>
<pre><code class="language-math" data-lang="math">w=\sum_{i=1}^{N}\lambda_it_ix_i \\
w_0=\frac{1}{N_M}\sum_{n}(t_n-\sum_{m}\lambda_mt_mx_mx_n)
</code></pre><p>Is finally calculated. So how do you find that $\lambda$? Actually, it can be obtained using a method called SMO (Sequential Minimal Optimization).</p>
<h3 id="smo">SMO</h3>
<p>First, I will re-post the problem to be solved.</p>
<pre><code class="language-math" data-lang="math">L(\lambda)=\sum_{n=1}^{N}\lambda_n-\frac{1}{2}\sum_{n=1}^{N}\sum_{m=1}^{N} \lambda_n\lambda_mt_nt_mx_n^Tx_m \\
</code></pre><p>Constraint</p>
<pre><code class="language-math" data-lang="math">\lambda_n\geq 0 ,\sum_{i=1}^N\lambda_it_i=0
</code></pre><p>In this way, the problem of maximizing the constrained $L(\lambda)$ can be solved using SMO for $\lambda$.</p>
<p><a href="https://en.wikipedia.org/wiki/%E9%80%90%E6%AC%A1%E6%9C%80%E5%B0%8F%E5%95%8F%E9%A1Accordingto%8C%E6%9C%80%E9%81%A9%E5%8C%96%E6%B3%95">Wikipedia</a>, SMO is a <strong>sequential minimum problem optimization method</strong> in Japanese. It is a method of iteratively approaching the solution like the gradient method, selecting any two variables and repeating until convergence.
These arbitrary 2 variables are called Working Set, and the key is how to select these 2 variables.</p>
<p>As a flow,</p>
<ul>
<li>Select variable $\lambda_1$ that violates the KKT condition* Determine $\lambda_2$</li>
<li>Update $\lambda_1, \lambda_2$</li>
</ul>
<p>The above process is repeated until there is no variable that violates the KKT condition.</p>
<h4 id="kkt-condition">KKT condition</h4>
<p>The Karush-Kuhn-Tucker condition, hereinafter KKT condition, is the optimum condition that the first derivative should satisfy.</p>
<p>Actually, the KKT condition is also used in the above Lagrange undetermined multiplier method,</p>
<p>(1) $\frac{\partial{L}}{\partial{w}}=0$
(2) $\frac{\partial{L}}{\partial{w_0}}=0$
(3) $t_n(\boldsymbol{w}^Tx_n+w_0) \geq 1$
(4) $\lambda_n \geq 0$
(5) $\lambda_n(t_n(\boldsymbol{w}^Tx_n+w_0)-1) = 0$</p>
<p>It is a condition. Especially, the fifth condition is called &ldquo;complementary condition&rdquo;.</p>
<h4 id="check-kkt-condition-violation-and-decide-one-variable">Check KKT condition violation and decide one variable</h4>
<p>Complementary condition $$\lambda_n(t_n(\boldsymbol{w}^Tx_n+w_0)-1) = 0$$
(1) If $\lambda_n=0$, $t_n(\boldsymbol{w}^Tx_n+w_0) \geq 1$
(2) If $\lambda_n&gt; 0$, $t_n(\boldsymbol{w}^Tx_n+w_0) = 1$</p>
<p>Will be introduced, so we have to choose $\lambda$ that does not satisfy this. On the contrary, suppose that the solution is found when $\lambda$ disappears. The $\lambda$ selected here is set to $\lambda_1$.</p>
<h4 id="determine-another-variable">Determine another variable</h4>
<p>Select the other variable in the following order. First, we need to find $\boldsymbol{w}$ and $w_0$ at the current $\lambda$.
The error function at this time is $$E_n=(\boldsymbol{w}^Tx_n+w_0)-t_n$$
will do.</p>
<dl>
  <dt>(1) Select to maximize the update amount of variables</dt>
    Select $n$ that maximizes the error from $E_1$ in the case of <dd>$\lambda_1$. </dd>
  <dt>(2) Does not exist on the boundary</dt>
    <dd>$t_n(\boldsymbol{w}^Tx_n+w_0) = any point that is not 1$
  <dt>(3) Remaining</dt>
</dl>
<h4 id="update-variables">Update variables</h4>
<p>Although $\lambda_1$ and $\lambda_2$ to be updated are decided, there is a linear constraint $$\sum_{i=1}^N\lambda_it_i=0$$ when updating, so update under this condition. There is a need. That is, if one of the $\lambda$ is updated, the other $\lambda$ must be adjusted. If the updated $\lambda_1$ and $\lambda_2$ are $\lambda_1^{new}$ and $\lambda_2^{new}$ respectively</p>
<pre><code class="language-math" data-lang="math">\lambda_1^{new}t_1+\lambda_2^{new}t_2=\lambda_1t_1+\lambda_2t_2
</code></pre><p>Will be. This is divided into cases of $t_1=t_2$ and $t_1 \ne t_2$,</p>
<pre><code class="language-math" data-lang="math">\lambda_1^{new}=\lambda_1+\frac{t_1(E_2-E_1)}{x_1^2+x_1x_2+x_2^2} \\
\lambda_2^{new}=\lambda_2+t_1t_2(\lambda_1-\lambda_1^{new})
</code></pre><p>To get Actually, the process called clipping is required to find the possible range of $\lambda_1$ and $\lambda_2$, but I was exhausted.</p>
<p>#python implementation
First of all, I&rsquo;ve exhausted myself in the theory, and since it has become too long, I will only implement scikit-learn. sorry. Someday I will definitely implement it myself.</p>
<h2 id="implementation-of-scikit-learn">Implementation of scikit-learn</h2>
<p>The python implementation is very simple. Use <a href="https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html">sklearn.svm.LinearSVC</a> to perform classification using support vector machines with scikit-learn. .. For the sake of clarity, we will use a sample in which 50 positive examples and 50 negative examples are properly separated.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> numpy <span style="color:#f92672">as</span> np
<span style="color:#f92672">import</span> pandas <span style="color:#f92672">as</span> pd
<span style="color:#f92672">import</span> matplotlib.pyplot <span style="color:#f92672">as</span> plt

<span style="color:#f92672">%</span>matplotlib inline

<span style="color:#f92672">from</span> sklearn <span style="color:#f92672">import</span> svm

fig, ax <span style="color:#f92672">=</span> plt<span style="color:#f92672">.</span>subplots()
    
x1_1<span style="color:#f92672">=</span>np<span style="color:#f92672">.</span>ones(<span style="color:#ae81ff">50</span>)<span style="color:#f92672">+</span><span style="color:#ae81ff">10</span><span style="color:#f92672">*</span>np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>random(<span style="color:#ae81ff">50</span>)
x1_2<span style="color:#f92672">=</span>np<span style="color:#f92672">.</span>ones(<span style="color:#ae81ff">50</span>)<span style="color:#f92672">+</span><span style="color:#ae81ff">10</span><span style="color:#f92672">*</span>np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>random(<span style="color:#ae81ff">50</span>)
x2_1<span style="color:#f92672">=-</span>np<span style="color:#f92672">.</span>ones(<span style="color:#ae81ff">50</span>)<span style="color:#f92672">-</span><span style="color:#ae81ff">10</span><span style="color:#f92672">*</span>np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>random(<span style="color:#ae81ff">50</span>)
x2_2<span style="color:#f92672">=-</span>np<span style="color:#f92672">.</span>ones(<span style="color:#ae81ff">50</span>)<span style="color:#f92672">-</span><span style="color:#ae81ff">10</span><span style="color:#f92672">*</span>np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>random(<span style="color:#ae81ff">50</span>)

x1 <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>c_[x1_1,x1_2]
x2 <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>c_[x2_1,x2_2]
y <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array(np<span style="color:#f92672">.</span>r_[np<span style="color:#f92672">.</span>ones(<span style="color:#ae81ff">50</span>), <span style="color:#f92672">-</span>np<span style="color:#f92672">.</span>ones(<span style="color:#ae81ff">50</span>)])

model <span style="color:#f92672">=</span> svm<span style="color:#f92672">.</span>LinearSVC()
model<span style="color:#f92672">.</span>fit(np<span style="color:#f92672">.</span>array(np<span style="color:#f92672">.</span>r_[x1,x2]), y)

ax<span style="color:#f92672">.</span>scatter(x1[:,<span style="color:#ae81ff">0</span>],x1[:,<span style="color:#ae81ff">1</span>],marker<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;o&#39;</span>,color<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;g&#39;</span>,s<span style="color:#f92672">=</span><span style="color:#ae81ff">100</span>)
ax<span style="color:#f92672">.</span>scatter(x2[:,<span style="color:#ae81ff">0</span>],x2[:,<span style="color:#ae81ff">1</span>],marker<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;s&#39;</span>,color<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;b&#39;</span>,s<span style="color:#f92672">=</span><span style="color:#ae81ff">100</span>)

w <span style="color:#f92672">=</span> model<span style="color:#f92672">.</span>coef_[<span style="color:#ae81ff">0</span>]

x_fig <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>linspace(<span style="color:#f92672">-</span><span style="color:#ae81ff">12.</span>,<span style="color:#ae81ff">12.</span>,<span style="color:#ae81ff">100</span>)
y_fig <span style="color:#f92672">=</span> [<span style="color:#f92672">-</span>w[<span style="color:#ae81ff">0</span>]<span style="color:#f92672">/</span>w[<span style="color:#ae81ff">1</span>]<span style="color:#f92672">*</span>xi<span style="color:#f92672">-</span>model<span style="color:#f92672">.</span>intercept_<span style="color:#f92672">/</span>w[<span style="color:#ae81ff">1</span>] <span style="color:#66d9ef">for</span> xi <span style="color:#f92672">in</span> x_fig]
ax<span style="color:#f92672">.</span>plot(x_fig, y_fig)

plt<span style="color:#f92672">.</span>show()
</code></pre></div><img width="372" alt="svm_2.png" src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/18497/133734f5-491d-96ec-9ef4-8f0b258baf55.png">
<p>It was classified like this. That&rsquo;s normal.</p>
<p>#Summary
At the end, I&rsquo;ve exhausted myself, but I think I got the atmosphere (I haven&rsquo;t got it). We dealt with this time in the case of basic, linearly separable, and cases in which positive and negative examples can be properly classified (hard margin).</p>
<p>For more advanced or near-realistic support vector machines, it is necessary to consider cases where linear separation is not possible (kernel method) and cases where positive and negative examples cannot be properly classified (soft margin). Is almost the same as this time.</p>
<p>I would like to deal with that area from the next time onward.</p>
<p><strong>Addition</strong>: <a href="https://qiita.com/hiro88hyo/items/45772ea5636bda7faf02">Writed</a></p>

    </article>
  </div>

  
  
  
  <script>
  window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
  ga('create', 'UA-169005401-1', 'auto');
  ga('send', 'pageview');
  </script>
  <script async src='https://www.google-analytics.com/analytics.js'></script>
  

  

  
<link rel="stylesheet" type="text/css" href="//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.0/cookieconsent.min.css" />
<script src="//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.0/cookieconsent.min.js"></script>
<script>
window.addEventListener("load", function(){
window.cookieconsent.initialise({
  "palette": {
    "popup": {
      "background": "#216942",
      "text": "#b2d192"
    },
    "button": {
      "background": "#afed71"
    }
  }
})});
</script>

</body>

</html>
