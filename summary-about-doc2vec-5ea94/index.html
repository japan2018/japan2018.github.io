<!doctype html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  
  <meta name="generator" content="Hugo 0.72.0" />

  
  <meta name="description" content="Some description">
  

  
  <link rel="apple-touch-icon" sizes="180x180" href="https://memotut.com/apple-touch-icon.png">

  
  <link rel="icon" type="image/png" sizes="32x32" href="https://memotut.com/favicon-32x32.png">

  
  <link rel="icon" type="image/png" sizes="16x16" href="https://memotut.com/favicon-16x16.png">

  
  <link rel="manifest" href="https://memotut.com/site.webmanifest">

  
  <link rel="mask-icon" href="https://memotut.com/safari-pinned-tab.svg" color="#5bbad5">

  <meta name="msapplication-TileColor" content="#da532c">

  <meta name="theme-color" content="#ffffff">

  
  <link rel="stylesheet" href="https://memotut.com/css/bootstrap.min.css" />

  
  <title>Summary about Doc2Vec | Memo Tut</title>
  

  <style>
body {
  min-width: 300px;
}

.custom-navbar {
  margin-bottom: 1em;
  height: 60px;
}

.custom-navbar a {
  display: inline-block; 
  padding: 18px 0;
  margin-right: 1em; 
  font-weight: bold; 
}

.custom-navbar a:hover,
.custom-navbar a:focus {
  text-decoration: none; 
}

@media print {
  .custom-navbar {
    display: none;
  }
}

article {
  padding-bottom: 1em;
}

img {
  max-width: 100%;
}


body {
  background-color: #fff;
}



body {
  color: #212529;
}



a {
  color: #007bff;
}



a:hover,
a:focus {
  color: #0056b3;
}



.custom-navbar {
  background-color: #212529;
}



.custom-navbar a {
  color: rgba(255, 255, 255, 0.75);
}



.custom-navbar a:hover,
.custom-navbar a:focus {
  color: rgba(255, 255, 255, 1);
}



.container {
  max-width: 800px;
}



pre {
  display: block;
  padding: 9.5px;
  word-break: break-all;
  word-wrap: break-word;
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 4px;
}

pre code {
  padding: 0;
  font-size: inherit;
  color: inherit; 
  white-space: pre-wrap;
  background-color: transparent;
  border: none;
  border-radius: 0;
}

code {
  padding: 2px 4px;
  color: inherit; 
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 4px;
  font-size: .9em;
}



blockquote,
.blockquote {
  padding: 10px 20px;
  margin: 0 0 20px;
  font-size: 1em;
  border-left: 5px solid #6c757d;
}

</style>
</head>

<body>
  <nav class="custom-navbar">
  <div class="container">
    
    <a href="/">Posts</a>
    
    <a href="/tags/">Tags</a>
    
    <a href="/about/">About</a>
    
    <a href="/index.xml">RSS</a>
    
  </div>
</nav>
  
  <div class="container">
    <article>
      <h1>Summary about Doc2Vec</h1>
<p>
  <small class="text-secondary">
  
  
  Feb 13, 2020
  </small>
  

<small><code><a href="https://memotut.com/tags/python">Python</a></code></small>


<small><code><a href="https://memotut.com/tags/natural-language-processing"> natural language processing</a></code></small>


<small><code><a href="https://memotut.com/tags/doc2vec"> doc2vec</a></code></small>

</p>
<pre><code>#Introduction
</code></pre>
<p>This time, I studied <strong>Doc2Vec</strong> as an extension of Word2Vec.
&ldquo;Document classification&rdquo; and &ldquo;document grouping (clustering)&rdquo; are tasks that are often requested in natural language processing, but in order to implement them, a distributed representation of the document itself is required.
If you use Doc2Vec, you can directly get the distributed representation.</p>
<p>##reference
I have referred to the following to understand Doc2Vec.</p>
<ul>
<li>(doc2vec(Paragraph Vector) algorithm
](<a href="https://kitayamalab.wordpress.com/2016/12/10/doc2vecparagraph-vector-%e3%81%ae%e3%82%a2%e3%83%ab%e3%82%b4%e3%83%aa%e3%82%ba%e3%83%a0/">https://kitayamalab.wordpress.com/2016/12/10/doc2vecparagraph-vector-%e3%81%ae%e3%82%a2%e3%83%ab%e3%82%b4%e3%83%aa%e3%82%ba%e3%83%a0/</a>)</li>
<li><a href="https://arxiv.org/pdf/1405.4053.pdf">Distributed Representations of Sentences and Documents (original paper)</a></li>
<li><a href="https://deepage.net/machine_learning/2017/01/08/doc2vec.html">Doc2Vec mechanism and document similarity calculation tutorial using gensim</a></li>
<li><a href="https://blog.brainpad.co.jp/entry/2019/02/20/103508#orgbda4cf7">How to use natural language processing technology-I tried to predict the quality of papers using Doc2Vec and DAN!ー</a></li>
</ul>
<p>#Doc2Vec</p>
<h2 id="what-is-doc2vec">What is Doc2Vec</h2>
<p>Doc2Vec is a technology that converts texts of arbitrary length into fixed-length vectors.
Whereas Word2Vec was meant to get a distributed representation of words, Doc2Vec gets a distributed representation of sentences and documents. Bag-of-Words and TF-IDF are the classical methods for acquiring distributed expressions of sentences, but they have the following weak points.</p>
<ul>
<li>Does not have word order information for words in the sentence</li>
<li>Recognize synonyms as completely different and independent words</li>
</ul>
<p>Although these are methods called count-based, Doc2Vec is trying to acquire the distributed representation of sentences by using a different approach to overcome the above weak points.</p>
<p>#Doc2Vec algorithm
Doc2Vec is a generic term for the following two algorithms.</p>
<ul>
<li>PV-DM (Distributed Memory Model of Paragraph Vectors)</li>
<li>PV-DBOW(Distributed Bag of Words version of Paragraph Vector)</li>
</ul>
<p>Below is a brief description of each algorithm.</p>
<p>###PV-DM
PV-DM is an algorithm corresponding to CBOW of Word2Vec.
Acquire a distributed expression of sentences while solving the task of predicting the next word by passing the sentence id and multiple words.
According to my research, it seems that the following steps are being taken for learning.</p>
<ol>
<li>Prepare a vector of sentences and a vector of words sampled in part from the document
Combine the vectors prepared in 2.1 in the middle layer (average or concatenation, selectable in gensim)</li>
<li>Predict the next word following the sampled word</li>
<li>Sentence vector and middle layer → Update weight of output layer</li>
</ol>
<p>The image figure is as follows. (Quoted from the original paper described in &ldquo;Reference&rdquo;)</p>
<p>![Screenshot 2020-02-12 20.41.34.png](<a href="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/395230/0e99ee3d-4f07-e44c-71a7-(83df8a798d63.png)">https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/395230/0e99ee3d-4f07-e44c-71a7-(83df8a798d63.png)</a></p>
<p>###PV-DBOW
PV-DBOW is an algorithm corresponding to Word2Vec&rsquo;s skip-gram.
PV-DBOW is faster than PV-DM because it does not need to use word vectors for learning. However, PV-DMOW is more accurate than PV-DMOW because it ignores word order during learning.</p>
<p>According to my research, it seems that the following steps are being taken.</p>
<ol>
<li>Sample any number of words from the same sentence</li>
<li>Optimize sentence vector and weight of middle layer → output layer to predict sampled words</li>
</ol>
<p>The image figure is as follows. (Quoted from the original paper described in &ldquo;Reference&rdquo;)</p>
<p>![Screenshot 2020-02-13 21.16.01.png](<a href="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/395230/21aca1bc-6b02-1390-6607-(a20f5b9ce7a9.png)">https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/395230/21aca1bc-6b02-1390-6607-(a20f5b9ce7a9.png)</a></p>
<hr>
<p>I briefly summarized the two algorithms above, but there were some details that I did not understand well even if I examined them. I would be grateful if anyone could understand in the comments. .. ..</p>
<p>** Where I did not understand even after checking **</p>
<ul>
<li>What is the format of the document vector given as the input at first, is there a problem with recognizing that only the sentence id is passed?</li>
<li>As a result of this learning, where is the distributed expression (Paragraph Vector) of the sentence that can be finally obtained from? (Word2Vec uses the weight vector that transforms the input to the middle layer as the distributed expression of words)</li>
</ul>
<p>#Create Doc2vec model using library
In the following, we will actually create a model of Doc2Vec using the library.</p>
<h2 id="library-used">Library used</h2>
<p>gensim 3.8.1
##data set
It is possible to easily create Doc2Vec models using gensim, a python library. This time the data set will be <a href="https://www.rondhuit.com/download.html#ldcc">[livedoor news corpus]</a>.Ifyouareinterestedinthedetailsofthedatasetandthemethodofmorphologicalanalysis,pleasereferto<a href="https://qiita.com/gk/items/e49f68d7e2fed6e300ea">Postedinapreviouslypostedarticle</a>. I will.</p>
<p>In the case of Japanese, pre-processing that decomposes sentences into morpheme units is required in advance, so after decomposing all sentences into morphemes, it is put into the following data frame.</p>
<p>![Screenshot 2020-01-13 21.07.38.png](<a href="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/395230/1c6fa150-6b85-a2c6-c0f9-(3f3ca0765711.png)">https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/395230/1c6fa150-6b85-a2c6-c0f9-(3f3ca0765711.png)</a></p>
<p>The column on the far right is a sentence that is morphologically analyzed and separated into half-width spaces. Create a Doc2Vec model using this.</p>
<p>##Model learning
Create a Word2vec model using gensim. The following are the main parameters for creating a model.</p>
<table>
<thead>
<tr>
<th align="left">Parameter name</th>
<th align="left">Parameter meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">dm</td>
<td align="left">1 for PV=DM, 0 for PV-DBOW</td>
</tr>
<tr>
<td align="left">vector_size</td>
<td align="left">Specifies how many dimensions of a sentence should be converted to distributed representation</td>
</tr>
<tr>
<td align="left">window</td>
<td align="left">How many words to predict the next word (for PV-DM) or how many words to predict from the document id (for PV-DBOW)</td>
</tr>
<tr>
<td align="left">min_count</td>
<td align="left">Ignore words that occur less than the specified number of times</td>
</tr>
<tr>
<td align="left">wokers</td>
<td align="left">Number of threads used for learning</td>
</tr>
</tbody>
</table>
<p>Below is the code that creates the Doc2Vec model. You can create a model in one line as long as you can create the text to enter.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">
sentences <span style="color:#f92672">=</span> []
<span style="color:#66d9ef">for</span> text <span style="color:#f92672">in</span> df[<span style="color:#ae81ff">3</span>]:
    text_list <span style="color:#f92672">=</span> text<span style="color:#f92672">.</span>split(<span style="color:#e6db74">&#39;&#39;</span>)
    sentences<span style="color:#f92672">.</span>append(text_list)

<span style="color:#f92672">from</span> gensim.models.doc2vec <span style="color:#f92672">import</span> Doc2Vec, TaggedDocument
documents <span style="color:#f92672">=</span> [TaggedDocument(doc, [i]) <span style="color:#66d9ef">for</span> i, doc <span style="color:#f92672">in</span> enumerate(sentences)]
model <span style="color:#f92672">=</span> Doc2Vec(documents, vector_size<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>, window<span style="color:#f92672">=</span><span style="color:#ae81ff">5</span>, min_count<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, workers<span style="color:#f92672">=</span><span style="color:#ae81ff">4</span>)

</code></pre></div><h2 id="what-you-can-do-with-doc2vec">What you can do with Doc2Vec</h2>
<p>With the model of Doc2Vec, we were able to obtain a distributed representation of sentences. The distributed expression of sentences can quantitatively express the semantic distance between sentences.</p>
<p>Let&rsquo;s look at what kind of content article is similar to the following news article in the model created earlier.</p>
<pre><code>On the 19th (Sat/Japan time), the South African World Cup/Japan×Netherlands match was a good fight for the Japanese national team.
In the second half, Sneijder's goal allowed the lead to lose 0-1 without being able to turn around.
After the match, Hidetoshi Nakata, the commander of the former Japan national team who was in charge of commentary on the TV Asahi relay,
``Well, even though I lost 0-1, the team had a much better fight than the first match, especially in the latter half of the year, Japan was doing well and it was the next match. I think it was because of this,&quot; he said.
In addition, &quot;(in some cases, there is a case where the goal is sluggish), we will properly protect it and then connect our attacks.
Especially at the end, Manager Okada used attacking players early on, and his attitude will lead to the next match.
It's not that I can lose this match, it's a big thing that I saw the attitude to win.&quot; '

</code></pre><p>You can output a document similar to the one specified below.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">
<span style="color:#75715e">#Pass the id of the document and output a document close to it (5792 is the id of the above article in this case)</span>
model<span style="color:#f92672">.</span>docvecs<span style="color:#f92672">.</span>most_similar(<span style="color:#ae81ff">5792</span>)
</code></pre></div><p>The output is here. Returns a set of the document id and its cosine similarity.</p>
<pre><code>[(6084, 0.8220762014389038),
 (5838, 0.8150338530540466),
 (6910, 0.8055128455162048),
 (351, 0.8003012537956238),
 (6223, 0.7960485816001892),
 (5826, 0.7933120131492615),
 (6246, 0.7902486324310303),
 (6332, 0.7871333360671997),
 (6447, 0.7836691737174988),
 (6067, 0.7836177349090576)]]
</code></pre><p>Let&rsquo;s look at the contents of the articles with the highest similarity.</p>
<pre><code>## 6084 article contents

'Soccer U-22 Japan national team aiming for London Olympics participation.
The second qualifying round is a home and away match against one Kuwait country.
Attracting attention is Kensuke Nagai (Nagoya Grampus), Japan's ace, who has scored 8 goals in 11 U-22 matches.
The TBS &quot;S-1&quot; broadcast late on the 18th approached the expected swift FW that could run 50m in 5.8 seconds.“I don't lose,” said Nagai, “I don't know for myself, but it seems to be fast.
It's about 2 years in high school (I got faster),” says a friend of Nagai's speed legend. ..
``Run that guy,'' said Koichi Sugiyama, a teacher at the high school in Kyushu International University, who said, ``I had a through pass and the child on the opposite side was offside, so I could chase myself and become dribble. That was often the case.&quot;
Speaking of quick-footed soccer players, Masayuki Okano, who is a “fielder,” is so famous, but when asked about Okano, Nagai laughs at him, “It's not that fast.”
Nagai, who was small and not fast at the time of high school entrance, said, “I often fell and moved back.” “The timing was right, his growth was due to his physical growth and training. Maybe it's because of that,&quot; said Sugiyama, looking back at his high school's specialty-hill training using stairs and stairs. '
</code></pre><pre><code>##5838 article content

On the TBS sports program &quot;S1&quot;, which was aired on the midnight of the 29th, guest commentator Ramos Lui was angry at the current situation where the Japanese national coach could not decide.
Japan and North Korea are the only countries in the World Cup that haven't been decided, &quot;No, it's too miserable. Especially, the motivation of the selected players will decrease.
Looking at the members this time, why is Inamoto and Tamada not being selected?&quot; says Ramos, who begins to talk with a blatant look.
When asked, &quot;Mr. Ramos, why don't you do it?&quot; he said, &quot;No, I want to do it. That's definitely&quot;, and &quot;I wouldn't be chosen. It's too late. I'm really miserable. I'm lonely.&quot; '

</code></pre><p>You can see that the top two are both football topics. The learned corpus is composed of articles from 9 news sites, but 8 out of the top 10 similarities were articles from sports news sites.</p>
<p>If a sentence can be converted into a fixed-length vector expression in this way, it can be applied to various machine learning algorithms such as clustering and classification.
In the world of natural language processing, how to obtain highly expressive vector expressions is very important, and various algorithms have been developed including Doc2Vec introduced this time.</p>
<p>#Next
Let&rsquo;s try various natural language processing tasks using this Doc2Vec model.</p>

    </article>
  </div>

  
  
  
  <script>
  window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
  ga('create', 'UA-169005401-1', 'auto');
  ga('send', 'pageview');
  </script>
  <script async src='https://www.google-analytics.com/analytics.js'></script>
  

  

  
<link rel="stylesheet" type="text/css" href="//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.0/cookieconsent.min.css" />
<script src="//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.0/cookieconsent.min.js"></script>
<script>
window.addEventListener("load", function(){
window.cookieconsent.initialise({
  "palette": {
    "popup": {
      "background": "#216942",
      "text": "#b2d192"
    },
    "button": {
      "background": "#afed71"
    }
  }
})});
</script>

</body>

</html>
