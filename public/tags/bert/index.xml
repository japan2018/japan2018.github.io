<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>bert on Memo Tut</title>
    <link>https://memotut.com/tags/bert/</link>
    <description>Recent content in bert on Memo Tut</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 15 May 2020 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://memotut.com/tags/bert/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>[Python] 10 techniques to improve BERT accuracy</title>
      <link>https://memotut.com/343309257/</link>
      <pubDate>Fri, 15 May 2020 00:00:00 +0000</pubDate>
      
      <guid>https://memotut.com/343309257/</guid>
      <description>## Introduction  It has become commonplace to fine-tune and use BERT for natural language processing tasks. It is expected that there will be more and more scenes where you want to improve the accuracy even a little when doing competitions such as Kaggle or projects where accuracy requirements are tight. Therefore, we will summarize the accuracy improvement method. Classification task is assumed as a task.
Adjust number of characters You can enter up to 512 words in the learned BERT.</description>
    </item>
    
    <item>
      <title>[Python] [PyTorch] How to use BERT-fine tuning Japanese pre-trained models to solve classification problem</title>
      <link>https://memotut.com/7f3a5d859/</link>
      <pubDate>Sat, 18 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>https://memotut.com/7f3a5d859/</guid>
      <description>#Introduction  Although BERT is updating SOTA with various tasks of natural language processing, what is published by [Google] on Github is based on Tensorflow. It has been implemented. People who use PyTorch want to use the PyTorch version, but I have not created the PyTorch version, so please use the one made by HuggingFace,butwedevelopedAskthemformoreinformationastheyarenotinvolvedin!AndQA.
Although it is a BERT made by Hugging Face, there was no Japanese pre-trained model until December 2019.</description>
    </item>
    
    <item>
      <title>[Python] Creation of negative/positive classifier using BERT</title>
      <link>https://memotut.com/b01250228/</link>
      <pubDate>Wed, 25 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://memotut.com/b01250228/</guid>
      <description>This post is the 25th day of &amp;quot;Natural Language Processing Advent Calendar 2019-Qiita&amp;quot;.  siny.
In this article, I summarized the creation of a negative-positive classifier using BERT, which plays a major role in natural language processing in 2019.
#Introduction I think that knowledge about BERT has appeared in books, blogs, Qiita, etc. However, since most datasets that can be used for natural language processing are based on English, and there are not many Japanese datasets, it is quite difficult to use BERT using Japanese text.</description>
    </item>
    
    <item>
      <title>[Python] Hands-on to visualize your tweet while understanding BERT</title>
      <link>https://memotut.com/09fda4c5c/</link>
      <pubDate>Thu, 12 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://memotut.com/09fda4c5c/</guid>
      <description>This article is the 13th day article of [NTT Communications Advent Calendar 2019](https://qiita.com/advent-calendar/2019/nttcom).  Yesterday was @nitky&amp;rsquo;s article, &amp;ldquo;We deal with threat intelligence in a mood&amp;rdquo; (https://qiita.com/nitky/items/ccefd0353b74aa21e357).
#Introduction Hi, my name is yuki uchida, and I belong to the SkyWay team of NTT Communications. This article is an article that visualizes your tweet using the language model BERT used for natural language processing, which was recently applied to Google search. I will write in a hands-on format so that as many people as possible can try it out, so if you are interested, please give it a try.</description>
    </item>
    
  </channel>
</rss>