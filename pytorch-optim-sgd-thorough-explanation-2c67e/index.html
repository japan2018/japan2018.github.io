<!doctype html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  
  <meta name="generator" content="Hugo 0.72.0" />

  
  <meta name="description" content="Some description">
  

  
  <link rel="apple-touch-icon" sizes="180x180" href="https://memotut.com/apple-touch-icon.png">

  
  <link rel="icon" type="image/png" sizes="32x32" href="https://memotut.com/favicon-32x32.png">

  
  <link rel="icon" type="image/png" sizes="16x16" href="https://memotut.com/favicon-16x16.png">

  
  <link rel="manifest" href="https://memotut.com/site.webmanifest">

  
  <link rel="mask-icon" href="https://memotut.com/safari-pinned-tab.svg" color="#5bbad5">

  <meta name="msapplication-TileColor" content="#da532c">

  <meta name="theme-color" content="#ffffff">

  
  <link rel="stylesheet" href="https://memotut.com/css/bootstrap.min.css" />

  
  <title>pyTorch optim SGD thorough explanation | Memo Tut</title>
  

  <style>
body {
  min-width: 300px;
}

.custom-navbar {
  margin-bottom: 1em;
  height: 60px;
}

.custom-navbar a {
  display: inline-block; 
  padding: 18px 0;
  margin-right: 1em; 
  font-weight: bold; 
}

.custom-navbar a:hover,
.custom-navbar a:focus {
  text-decoration: none; 
}

@media print {
  .custom-navbar {
    display: none;
  }
}

article {
  padding-bottom: 1em;
}

img {
  max-width: 100%;
}


body {
  background-color: #fff;
}



body {
  color: #212529;
}



a {
  color: #007bff;
}



a:hover,
a:focus {
  color: #0056b3;
}



.custom-navbar {
  background-color: #212529;
}



.custom-navbar a {
  color: rgba(255, 255, 255, 0.75);
}



.custom-navbar a:hover,
.custom-navbar a:focus {
  color: rgba(255, 255, 255, 1);
}



.container {
  max-width: 800px;
}



pre {
  display: block;
  padding: 9.5px;
  word-break: break-all;
  word-wrap: break-word;
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 4px;
}

pre code {
  padding: 0;
  font-size: inherit;
  color: inherit; 
  white-space: pre-wrap;
  background-color: transparent;
  border: none;
  border-radius: 0;
}

code {
  padding: 2px 4px;
  color: inherit; 
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 4px;
  font-size: .9em;
}



blockquote,
.blockquote {
  padding: 10px 20px;
  margin: 0 0 20px;
  font-size: 1em;
  border-left: 5px solid #6c757d;
}

</style>
</head>

<body>
  <nav class="custom-navbar">
  <div class="container">
    
    <a href="/">Posts</a>
    
    <a href="/tags/">Tags</a>
    
    <a href="/about/">About</a>
    
    <a href="/index.xml">RSS</a>
    
  </div>
</nav>
  
  <div class="container">
    <article>
      <h1>pyTorch optim SGD thorough explanation</h1>
<p>
  <small class="text-secondary">
  
  
  Jan 27, 2020
  </small>
  

<small><code><a href="https://memotut.com/tags/python">Python</a></code></small>


<small><code><a href="https://memotut.com/tags/deep-learning"> Deep Learning</a></code></small>


<small><code><a href="https://memotut.com/tags/sgd"> SGD</a></code></small>


<small><code><a href="https://memotut.com/tags/optimizer"> Optimizer</a></code></small>


<small><code><a href="https://memotut.com/tags/pytorch"> PyTorch</a></code></small>

</p>
<pre><code>2020/1/27 Post
</code></pre>
<h1 id="0-target-audience-of-this-article">0. Target audience of this article</h1>
<ul>
<li>People who have touched python and have a good execution environment</li>
<li>People who have touched pyTorch to some extent</li>
<li>People who want to understand optimizer SGD with machine learning by pyTorch</li>
<li>People who want to use optimizer SGD of pyTorch for other than Network model (normal variables etc.)</li>
</ul>
<h1 id="1first-of-all">1.First of all</h1>
<p>Recently, the main focus of research on machine learning is the python language, because python has many libraries (called modules) for high-speed data analysis and calculation.
Among them, this time we will use a module called <strong>pyTorch</strong> and explain how to handle it in a program ** regarding the parameter updating method using stochastic gradient descent (SGD).
However, this article is like a memo to you, and it may be that you want it to be for reference only, and there may be incorrect expressions or phrases for the sake of brevity, but please understand that. I want you to.
Furthermore, I do not really explain SGD in detail, so if you are interested in it, please learn by yourself.</p>
<p>Also, in this article, we will not actually learn using Network.
If you are interested in it, please refer to the link below.</p>
<p><a href="https://qiita.com/mslive/private/8e1f9a8467fff8dfd03c">Thorough explanation of CNNs with pyTorch</a></p>
<h1 id="2-install-pytorch">2. Install pyTorch</h1>
<p>If you are using pyTorch for the first time, you have to install it with cmd, because pyTorch is not yet installed in python.
Jump to the link below, select your own environment with ``QUICK START LOCALLY&rsquo;&rsquo; at the bottom of the page, and enter the command that appears with cmd etc. (you should copy and execute the command)</p>
<p><a href="https://pytorch.org/">Pytorch official site</a></p>
<h1 id="3-special-types-provided-by-pytorch">3. Special types provided by pyTorch</h1>
<p>Just like numpy has a type called ndarray, pyTorch has a type called ``<strong>Tensor type</strong>'&rsquo;.
Like the ndarray type, it can perform matrix calculations and is very similar to each other, but the Tensor type is excellent for machine learning in that it can use a GPU.
Because machine learning uses a GPU with a high calculation speed because it requires a considerable amount of calculation.
In addition, the Tensor type makes differentiation for updating the parameters of machine learning very easy.
The key to this article is how easy this is to do.</p>
<p>Please refer to the following link for the operation and explanation of Tensor type.</p>
<p><a href="https://qiita.com/mslive/private/241bfb42d852bb801b96">What is the Tensor type of pyTorch</a></p>
<p>Please refer to the link below for how differentiation is realized.</p>
<p><a href="https://qiita.com/mathlive/items/3dcb46af2e2f0eca559a">Summary of examples that cannot be pyTorch backward</a></p>
<h1 id="4-what-is-stochastic-gradient-desent-sgd">4. What is Stochastic Gradient Desent (SGD)?</h1>
<p>This is called the stochastic gradient descent method, or simply the parameter updating method.
Mathematical explanations on this will come out as much as you want, so I will not explain it here.</p>
<h1 id="5-sgd-from-pytorch">5. SGD from pyTorch</h1>
<h3 id="5-1importing-pytorch">5-1.Importing pyTorch</h3>
<p>First, import so that you can use pyTorch.
From here, write to python file instead of cmd etc.
Use module by writing the following code.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ruby:filename.rb" data-lang="ruby:filename.rb">import torch
import torch<span style="color:#f92672">.</span>optim as optim
</code></pre></div><p>The second line ``<strong>import torch.optim as optim</strong>&rsquo;&rsquo; is a module prepared for using SGD.</p>
<h3 id="5-2optimsgd">5-2.optim.SGD</h3>
<p>First, the arguments of SGD will be explained.
The usage is written as follows.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ruby:filename.rb" data-lang="ruby:filename.rb">op <span style="color:#f92672">=</span> optim<span style="color:#f92672">.</span>SGD(params, lr<span style="color:#f92672">=</span>l, momentum<span style="color:#f92672">=</span>m, dampening<span style="color:#f92672">=</span>d, weight_decay<span style="color:#f92672">=</span>w, nesterov<span style="color:#f92672">=</span>n)
</code></pre></div><p>Description of the arguments below</p>
<ul>
<li>params: pass the parameter you want to update, this parameter must be differentiable.</li>
<li>lr: learning rate.Pass float type.</li>
<li>momentum: momentum.Pass float type.</li>
<li>dampening: Control momentum momentum.Pass float type.</li>
<li>weight_decay: How much to add the L2 norm of params as regularization.Pass a float type.</li>
<li>nesterov: Whether to apply nesterov momentum as momentum, pass True or False.</li>
</ul>
<p>This time, in order to see the behavior of SGD, unnecessary information such as momentum, dampening, weight_decay, and nestrov are left as the initial values (all are 0 or False).</p>
<h3 id="5-3-using-sgd">5-3. Using SGD</h3>
<p>The program itself is very simple, and we will show an example of calculation as a preliminary.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ruby:filename.rb" data-lang="ruby:filename.rb">x <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>tensor(<span style="color:#ae81ff">5</span><span style="color:#f92672">.</span><span style="color:#ae81ff">0</span>, requires_grad <span style="color:#f92672">=</span> <span style="color:#66d9ef">True</span>)
c <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>tensor(<span style="color:#ae81ff">3</span><span style="color:#f92672">.</span><span style="color:#ae81ff">0</span>, requires_grad <span style="color:#f92672">=</span> <span style="color:#66d9ef">True</span>)
b <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>tensor(<span style="color:#ae81ff">2</span><span style="color:#f92672">.</span><span style="color:#ae81ff">0</span>)
d <span style="color:#f92672">=</span> <span style="color:#ae81ff">4</span><span style="color:#f92672">.</span><span style="color:#ae81ff">0</span>
y <span style="color:#f92672">=</span> c<span style="color:#f92672">*</span><span style="color:#ae81ff">3</span><span style="color:#f92672">*</span>torch<span style="color:#f92672">.</span>exp(x) <span style="color:#f92672">+</span> b<span style="color:#f92672">*</span>x <span style="color:#f92672">+</span> d
print(y)

<span style="color:#f92672">-</span> <span style="color:#f92672">-----------</span> <span style="color:#66d9ef">Output</span> below <span style="color:#f92672">---------------</span>
tensor(<span style="color:#ae81ff">1349</span><span style="color:#f92672">.</span><span style="color:#ae81ff">7185</span>, grad_fn<span style="color:#f92672">=&lt;</span><span style="color:#66d9ef">AddBackward0</span><span style="color:#f92672">&gt;</span>)
</code></pre></div><p>If you write this program by formula</p>
<pre><code class="language-math" data-lang="math">y = 3 c e^{x} + bx + d
</code></pre><p>At $x = 5$, $c = 3$, $b = 2$, $d = 4$.
Also, ``<strong>requires_grad = True</strong>&rsquo;&rsquo; is set so that differential calculation can be performed only for variables <strong>x</strong> and <strong>c</strong>.
That is why we pass SGD to update these two variables.
Here is an example.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ruby:filename.rb" data-lang="ruby:filename.rb">op <span style="color:#f92672">=</span> optim<span style="color:#f92672">.</span>SGD(<span style="color:#f92672">[</span>x,c<span style="color:#f92672">]</span>, lr<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span><span style="color:#f92672">.</span><span style="color:#ae81ff">0</span>)
</code></pre></div><p>Note that the argument passing to the <strong>params</strong> part of SGD is &ldquo;<strong>[x,c]</strong>&rdquo;, but the parameters passed in this way are list &ldquo;<strong>[ ]</strong>&rdquo;. I have to cover it up.
Of course, this is also done when there is only one parameter variable **.
In addition, when creating a Network by machine learning etc., the parameters of the model may be inserted, but in that case the parentheses are not necessary.
To elaborate a bit, params expects iteration to come as an argument, so we don&rsquo;t need it because the model parameters are in the form of iteration.
For an example of inserting a model, see the explanation of CNN introduced above.</p>
<p>Also, this variable <strong>op</strong> has the SGD function now, but when you actually output it, the contents of SGD will appear.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ruby:filename.rb" data-lang="ruby:filename.rb">print(op)

<span style="color:#f92672">-</span> <span style="color:#f92672">-----------</span> <span style="color:#66d9ef">Output</span> below <span style="color:#f92672">---------------</span>
<span style="color:#66d9ef">SGD</span> (
<span style="color:#66d9ef">Parameter</span> <span style="color:#66d9ef">Group</span> <span style="color:#ae81ff">0</span>
    <span style="color:#e6db74">dampening</span>: <span style="color:#ae81ff">0</span>
    <span style="color:#e6db74">lr</span>: <span style="color:#ae81ff">1</span><span style="color:#f92672">.</span><span style="color:#ae81ff">0</span>
    <span style="color:#e6db74">momentum</span>: <span style="color:#ae81ff">0</span>
    <span style="color:#e6db74">nesterov</span>: <span style="color:#66d9ef">False</span>
    <span style="color:#e6db74">weight_decay</span>: <span style="color:#ae81ff">0</span>
)
</code></pre></div><p>The actual parameter differentiation is as follows.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ruby:filename.rb" data-lang="ruby:filename.rb">y<span style="color:#f92672">.</span>backward()
print(x<span style="color:#f92672">.</span>grad)
print(c<span style="color:#f92672">.</span>grad)

<span style="color:#f92672">-</span> <span style="color:#f92672">-----------</span> <span style="color:#66d9ef">Output</span> below <span style="color:#f92672">---------------</span>
tensor(<span style="color:#ae81ff">1337</span><span style="color:#f92672">.</span><span style="color:#ae81ff">7185</span>)
tensor(<span style="color:#ae81ff">445</span><span style="color:#f92672">.</span><span style="color:#ae81ff">2395</span>)
</code></pre></div><p>The differential value of each variable can be viewed by setting <code>**variable name.grad**''. In this way, </code><strong>y.backward()</strong>&rsquo;&rsquo; automatically differentiates the variables related to the final output <strong>y</strong>.
Then, update the parameters as follows.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ruby:filename.rb" data-lang="ruby:filename.rb">op<span style="color:#f92672">.</span>step()
print(x)
print(c)

<span style="color:#f92672">-</span> <span style="color:#f92672">-----------</span> <span style="color:#66d9ef">Output</span> below <span style="color:#f92672">---------------</span>
tensor(<span style="color:#f92672">-</span><span style="color:#ae81ff">1332</span><span style="color:#f92672">.</span><span style="color:#ae81ff">7185</span>, requires_grad<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
tensor(<span style="color:#f92672">-</span><span style="color:#ae81ff">442</span><span style="color:#f92672">.</span><span style="color:#ae81ff">2395</span>, requires_grad<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</code></pre></div><p>In this way, ``<strong>op.step()</strong>&rsquo;&rsquo; is used to update using the differential information of each variable.
In other words, you can see that the memory of the actual variables <strong>x</strong> and <strong>c</strong> and the memory of those values that SGD has are the same.
The update formula under the current conditions is as follows.</p>
<pre><code class="language-math" data-lang="math">x \leftarrow x-\eta\frac{\partial y}{\partial x}
</code></pre><p>$\eta$ is the learning rate and is now 1.0.
If we rewrite the above equation with the values of the actual variable and the derivative,</p>
<pre><code class="language-math" data-lang="math">- 1332.7185 \leftarrow 5.0-1.0\times 1337.7185
</code></pre><p>It certainly does.</p>
<h3 id="5-4-rewriting-the-contents-of-sgd">5-4. Rewriting the contents of SGD</h3>
<p>Although the variable <strong>op</strong> with the SGD function was output above, detailed information such as parameters was not output.
The detailed output can be done as follows.
However, the following program is performed with the data before the differential calculation and the update performed above (variables x and c are the same as before).</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ruby:filename.rb" data-lang="ruby:filename.rb">print(op<span style="color:#f92672">.</span>param_groups)

<span style="color:#f92672">-</span> <span style="color:#f92672">-----------</span> <span style="color:#66d9ef">Output</span> below <span style="color:#f92672">---------------</span>
<span style="color:#f92672">[</span>{<span style="color:#e6db74">&#39;params&#39;</span>: <span style="color:#f92672">[</span>tensor(<span style="color:#ae81ff">5</span><span style="color:#f92672">.</span>, requires_grad<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>), tensor(<span style="color:#ae81ff">3</span><span style="color:#f92672">.</span>, requires_grad<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)<span style="color:#f92672">]</span>,
  <span style="color:#e6db74">&#39;lr&#39;</span>: <span style="color:#ae81ff">1</span><span style="color:#f92672">.</span><span style="color:#ae81ff">0</span>,
  <span style="color:#e6db74">&#39;momentum&#39;</span>: <span style="color:#ae81ff">0</span>,
  <span style="color:#e6db74">&#39;dampening&#39;</span>: <span style="color:#ae81ff">0</span>,
  <span style="color:#e6db74">&#39;weight_decay&#39;</span>: <span style="color:#ae81ff">0</span>,
  <span style="color:#e6db74">&#39;nesterov&#39;</span>: <span style="color:#66d9ef">False</span>}<span style="color:#f92672">]</span>
</code></pre></div><p>If you do this, you can see the detailed parameters of SGD, and you can see that the information is in the list type &ldquo;<strong>[ ]</strong>&rdquo;.
In other words, if you want to manipulate the contents, do the following.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ruby:filename.rb" data-lang="ruby:filename.rb">op<span style="color:#f92672">.</span>param_groups<span style="color:#f92672">[</span><span style="color:#ae81ff">0</span><span style="color:#f92672">][</span><span style="color:#e6db74">&#39;lr&#39;</span><span style="color:#f92672">]</span> <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span><span style="color:#f92672">.</span><span style="color:#ae81ff">1</span>
print(op)

<span style="color:#f92672">-</span> <span style="color:#f92672">-----------</span> <span style="color:#66d9ef">Output</span> below <span style="color:#f92672">---------------</span>
<span style="color:#66d9ef">SGD</span> (
<span style="color:#66d9ef">Parameter</span> <span style="color:#66d9ef">Group</span> <span style="color:#ae81ff">0</span>
    <span style="color:#e6db74">dampening</span>: <span style="color:#ae81ff">0</span>
    <span style="color:#e6db74">lr</span>: <span style="color:#ae81ff">0</span><span style="color:#f92672">.</span><span style="color:#ae81ff">1</span>
    <span style="color:#e6db74">momentum</span>: <span style="color:#ae81ff">0</span>
    <span style="color:#e6db74">nesterov</span>: <span style="color:#66d9ef">False</span>
    <span style="color:#e6db74">weight_decay</span>: <span style="color:#ae81ff">0</span>
)
</code></pre></div><p>Now, the learning rate <strong>lr</strong> has been rewritten to 0.1.
If you actually output it normally, you can see that it was rewritten.</p>
<h3 id="5-5-rewriting-the-variable-you-want-to-update-sgd-side">5-5. Rewriting the variable you want to update (SGD side)</h3>
<p>As above, let&rsquo;s rewrite the information of the variable <strong>x</strong> using the parameter rewriting of SGD.First the parameters are</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ruby:filename.rb" data-lang="ruby:filename.rb">print(op<span style="color:#f92672">.</span>param_groups<span style="color:#f92672">[</span><span style="color:#ae81ff">0</span><span style="color:#f92672">][</span><span style="color:#e6db74">&#39;params&#39;</span><span style="color:#f92672">]</span>)

<span style="color:#f92672">-</span> <span style="color:#f92672">-----------</span> <span style="color:#66d9ef">Output</span> below <span style="color:#f92672">---------------</span>
<span style="color:#f92672">[</span>tensor(<span style="color:#ae81ff">5</span><span style="color:#f92672">.</span>, requires_grad<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>), tensor(<span style="color:#ae81ff">3</span><span style="color:#f92672">.</span>, requires_grad<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)<span style="color:#f92672">]</span>
</code></pre></div><p>Is.
Now, rewrite the 0th element on the variable <strong>x</strong> side of this.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ruby:filename.rb" data-lang="ruby:filename.rb">op<span style="color:#f92672">.</span>param_groups<span style="color:#f92672">[</span><span style="color:#ae81ff">0</span><span style="color:#f92672">][</span><span style="color:#e6db74">&#39;params&#39;</span><span style="color:#f92672">][</span><span style="color:#ae81ff">0</span><span style="color:#f92672">]</span> <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>tensor(<span style="color:#ae81ff">10</span><span style="color:#f92672">.</span>, requires_grad<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
print(op<span style="color:#f92672">.</span>param_groups)

<span style="color:#f92672">-</span> <span style="color:#f92672">-----------</span> <span style="color:#66d9ef">Output</span> below <span style="color:#f92672">---------------</span>
<span style="color:#f92672">[</span>{<span style="color:#e6db74">&#39;params&#39;</span>: <span style="color:#f92672">[</span>tensor(<span style="color:#ae81ff">10</span><span style="color:#f92672">.</span>, requires_grad<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>), tensor(<span style="color:#ae81ff">3</span><span style="color:#f92672">.</span>, requires_grad<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)<span style="color:#f92672">]</span>,
  <span style="color:#e6db74">&#39;lr&#39;</span>: <span style="color:#ae81ff">0</span><span style="color:#f92672">.</span><span style="color:#ae81ff">1</span>,
  <span style="color:#e6db74">&#39;momentum&#39;</span>: <span style="color:#ae81ff">0</span>,
  <span style="color:#e6db74">&#39;dampening&#39;</span>: <span style="color:#ae81ff">0</span>,
  <span style="color:#e6db74">&#39;weight_decay&#39;</span>: <span style="color:#ae81ff">0</span>,
  <span style="color:#e6db74">&#39;nesterov&#39;</span>: <span style="color:#66d9ef">False</span>}<span style="color:#f92672">]</span>
</code></pre></div><p>Certainly it was rewritten.
<strong>But let&rsquo;s look at the actual value of the variable.</strong></p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ruby:filename.rb" data-lang="ruby:filename.rb">print(x)

<span style="color:#f92672">-</span> <span style="color:#f92672">-----------</span> <span style="color:#66d9ef">Output</span> below <span style="color:#f92672">---------------</span>
tensor(<span style="color:#ae81ff">5</span><span style="color:#f92672">.</span>, requires_grad<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</code></pre></div><p>The body has not been rewritten at all.
When I actually update it with backward()</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ruby:filename.rb" data-lang="ruby:filename.rb">y<span style="color:#f92672">.</span>backward()
op<span style="color:#f92672">.</span>step()
print(x)
print(x<span style="color:#f92672">.</span>grad)
print(op<span style="color:#f92672">.</span>param_groups)

<span style="color:#f92672">-</span> <span style="color:#f92672">-----------</span> <span style="color:#66d9ef">Output</span> below <span style="color:#f92672">---------------</span>
tensor(<span style="color:#ae81ff">5</span><span style="color:#f92672">.</span>, requires_grad<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
tensor(<span style="color:#ae81ff">1337</span><span style="color:#f92672">.</span><span style="color:#ae81ff">7185</span>)
<span style="color:#f92672">[</span>{<span style="color:#e6db74">&#39;params&#39;</span>: <span style="color:#f92672">[</span>tensor(<span style="color:#ae81ff">10</span><span style="color:#f92672">.</span>, requires_grad<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>),
   tensor(<span style="color:#f92672">-</span><span style="color:#ae81ff">41</span><span style="color:#f92672">.</span><span style="color:#ae81ff">5240</span>, requires_grad<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)<span style="color:#f92672">]</span>,
  <span style="color:#e6db74">&#39;lr&#39;</span>: <span style="color:#ae81ff">0</span><span style="color:#f92672">.</span><span style="color:#ae81ff">1</span>,
  <span style="color:#e6db74">&#39;momentum&#39;</span>: <span style="color:#ae81ff">0</span>,
  <span style="color:#e6db74">&#39;dampening&#39;</span>: <span style="color:#ae81ff">0</span>,
  <span style="color:#e6db74">&#39;weight_decay&#39;</span>: <span style="color:#ae81ff">0</span>,
  <span style="color:#e6db74">&#39;nesterov&#39;</span>: <span style="color:#66d9ef">False</span>}<span style="color:#f92672">]</span>
</code></pre></div><p>Note that the learning rate lr is 0.1, so the value of the variable c is different from that in the above example.
In this way, neither the value of the variable x nor the value of x held by SGD is updated.
Moreover, <strong>x.grad</strong> is the calculation when the variable <strong>x</strong> is 5.
The cause of this is because the memory location is different when assigning ``<strong>torch.tensor(10., requires_grad=True)</strong>'&rsquo;.
Be careful when changing the value of a variable.</p>
<h3 id="5-5-rewriting-the-variable-you-want-to-update-normal-variable-side">5-5. Rewriting the variable you want to update (normal variable side)</h3>
<p>It was found above that if the SGD variable information is rewritten, the variables and SGD parameters will not interact with each other.
He said the reason was a memory mismatch.
Now let&rsquo;s rewrite the variable.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ruby:filename.rb" data-lang="ruby:filename.rb">op <span style="color:#f92672">=</span> optim<span style="color:#f92672">.</span>SGD(<span style="color:#f92672">[</span>x,c<span style="color:#f92672">]</span>, lr<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span><span style="color:#f92672">.</span><span style="color:#ae81ff">0</span>)
print(op<span style="color:#f92672">.</span>param_groups)

<span style="color:#f92672">-</span> <span style="color:#f92672">-----------</span> <span style="color:#66d9ef">Output</span> below <span style="color:#f92672">---------------</span>
<span style="color:#f92672">[</span>{<span style="color:#e6db74">&#39;params&#39;</span>: <span style="color:#f92672">[</span>tensor(<span style="color:#ae81ff">5</span><span style="color:#f92672">.</span>, requires_grad<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>), tensor(<span style="color:#ae81ff">3</span><span style="color:#f92672">.</span>, requires_grad<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)<span style="color:#f92672">]</span>,
  <span style="color:#e6db74">&#39;lr&#39;</span>: <span style="color:#ae81ff">1</span><span style="color:#f92672">.</span><span style="color:#ae81ff">0</span>,
  <span style="color:#e6db74">&#39;momentum&#39;</span>: <span style="color:#ae81ff">0</span>,
  <span style="color:#e6db74">&#39;dampening&#39;</span>: <span style="color:#ae81ff">0</span>,
  <span style="color:#e6db74">&#39;weight_decay&#39;</span>: <span style="color:#ae81ff">0</span>,
  <span style="color:#e6db74">&#39;nesterov&#39;</span>: <span style="color:#66d9ef">False</span>}<span style="color:#f92672">]</span>
</code></pre></div><p>For this variable <strong>x</strong></p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ruby:filename.rb" data-lang="ruby:filename.rb">x <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>tensor(<span style="color:#ae81ff">10</span><span style="color:#f92672">.</span>, requires_grad<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
print(x)

<span style="color:#f92672">-</span> <span style="color:#f92672">-----------</span> <span style="color:#66d9ef">Output</span> below <span style="color:#f92672">---------------</span>
tensor(<span style="color:#ae81ff">10</span><span style="color:#f92672">.</span>, requires_grad<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</code></pre></div><p>And.
Here again if you look at the details of the variable <strong>op</strong></p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ruby:filename.rb" data-lang="ruby:filename.rb">print(op<span style="color:#f92672">.</span>param_groups)

<span style="color:#f92672">-</span> <span style="color:#f92672">-----------</span> <span style="color:#66d9ef">Output</span> below <span style="color:#f92672">---------------</span>
<span style="color:#f92672">[</span>{<span style="color:#e6db74">&#39;params&#39;</span>: <span style="color:#f92672">[</span>tensor(<span style="color:#ae81ff">5</span><span style="color:#f92672">.</span>, requires_grad<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>), tensor(<span style="color:#ae81ff">3</span><span style="color:#f92672">.</span>, requires_grad<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)<span style="color:#f92672">]</span>,
  <span style="color:#e6db74">&#39;lr&#39;</span>: <span style="color:#ae81ff">1</span><span style="color:#f92672">.</span><span style="color:#ae81ff">0</span>,
  <span style="color:#e6db74">&#39;momentum&#39;</span>: <span style="color:#ae81ff">0</span>,
  <span style="color:#e6db74">&#39;dampening&#39;</span>: <span style="color:#ae81ff">0</span>,
  <span style="color:#e6db74">&#39;weight_decay&#39;</span>: <span style="color:#ae81ff">0</span>,
  <span style="color:#e6db74">&#39;nesterov&#39;</span>: <span style="color:#66d9ef">False</span>}<span style="color:#f92672">]</span>
</code></pre></div><p>The parameters have not changed.
Of course if you update backward() as it is,</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ruby:filename.rb" data-lang="ruby:filename.rb">y<span style="color:#f92672">.</span>backward()
op<span style="color:#f92672">.</span>step()
print(x)
print(x<span style="color:#f92672">.</span>grad)
print(op<span style="color:#f92672">.</span>param_groups)

<span style="color:#f92672">-</span> <span style="color:#f92672">-----------</span> <span style="color:#66d9ef">Output</span> below <span style="color:#f92672">---------------</span>
tensor(<span style="color:#ae81ff">10</span><span style="color:#f92672">.</span>, requires_grad<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
<span style="color:#66d9ef">None</span>
<span style="color:#f92672">[</span>{<span style="color:#e6db74">&#39;params&#39;</span>: <span style="color:#f92672">[</span>tensor(<span style="color:#f92672">-</span><span style="color:#ae81ff">1332</span><span style="color:#f92672">.</span><span style="color:#ae81ff">7185</span>, requires_grad<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>),
   tensor(<span style="color:#f92672">-</span><span style="color:#ae81ff">442</span><span style="color:#f92672">.</span><span style="color:#ae81ff">2395</span>, requires_grad<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)<span style="color:#f92672">]</span>,
  <span style="color:#e6db74">&#39;lr&#39;</span>: <span style="color:#ae81ff">1</span><span style="color:#f92672">.</span><span style="color:#ae81ff">0</span>,
  <span style="color:#e6db74">&#39;momentum&#39;</span>: <span style="color:#ae81ff">0</span>,
  <span style="color:#e6db74">&#39;dampening&#39;</span>: <span style="color:#ae81ff">0</span>,
  <span style="color:#e6db74">&#39;weight_decay&#39;</span>: <span style="color:#ae81ff">0</span>,
  <span style="color:#e6db74">&#39;nesterov&#39;</span>: <span style="color:#66d9ef">False</span>}<span style="color:#f92672">]</span>
</code></pre></div><p>Interestingly, nothing has been done to the different memory variables <strong>x</strong>, and the parameters of SGD have been updated.
The bottom line is that you should avoid overwriting the variables you want to update.</p>
<h1 id="6-a-word">6. A word</h1>
<p>This time, I explained the SGD of optimizer using pyTorch, although it is simple.
Surprisingly, there was no example of applying SGD to anything other than Network, so I will introduce it.
I think there were many points that were difficult to read, but thank you for reading.</p>

    </article>
  </div>

  
  
  
  <script>
  window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
  ga('create', 'UA-169005401-1', 'auto');
  ga('send', 'pageview');
  </script>
  <script async src='https://www.google-analytics.com/analytics.js'></script>
  

  

  
<link rel="stylesheet" type="text/css" href="//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.0/cookieconsent.min.css" />
<script src="//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.0/cookieconsent.min.js"></script>
<script>
window.addEventListener("load", function(){
window.cookieconsent.initialise({
  "palette": {
    "popup": {
      "background": "#216942",
      "text": "#b2d192"
    },
    "button": {
      "background": "#afed71"
    }
  }
})});
</script>

</body>

</html>
