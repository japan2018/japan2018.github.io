<!doctype html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  
  <meta name="generator" content="Hugo 0.72.0" />

  
  <meta name="description" content="Some description">
  

  
  <link rel="apple-touch-icon" sizes="180x180" href="https://memotut.com/apple-touch-icon.png">

  
  <link rel="icon" type="image/png" sizes="32x32" href="https://memotut.com/favicon-32x32.png">

  
  <link rel="icon" type="image/png" sizes="16x16" href="https://memotut.com/favicon-16x16.png">

  
  <link rel="manifest" href="https://memotut.com/site.webmanifest">

  
  <link rel="mask-icon" href="https://memotut.com/safari-pinned-tab.svg" color="#5bbad5">

  <meta name="msapplication-TileColor" content="#da532c">

  <meta name="theme-color" content="#ffffff">

  
  <link rel="stylesheet" href="https://memotut.com/css/bootstrap.min.css" />

  
  <title>[Python] Story that made Mel icon generator | Memo Tut</title>
  

  <style>
body {
  min-width: 300px;
}

.custom-navbar {
  margin-bottom: 1em;
  height: 60px;
}

.custom-navbar a {
  display: inline-block; 
  padding: 18px 0;
  margin-right: 1em; 
  font-weight: bold; 
}

.custom-navbar a:hover,
.custom-navbar a:focus {
  text-decoration: none; 
}

@media print {
  .custom-navbar {
    display: none;
  }
}

article {
  padding-bottom: 1em;
}

img {
  max-width: 100%;
}


body {
  background-color: #fff;
}



body {
  color: #212529;
}



a {
  color: #007bff;
}



a:hover,
a:focus {
  color: #0056b3;
}



.custom-navbar {
  background-color: #212529;
}



.custom-navbar a {
  color: rgba(255, 255, 255, 0.75);
}



.custom-navbar a:hover,
.custom-navbar a:focus {
  color: rgba(255, 255, 255, 1);
}



.container {
  max-width: 800px;
}



pre {
  display: block;
  padding: 9.5px;
  word-break: break-all;
  word-wrap: break-word;
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 4px;
}

pre code {
  padding: 0;
  font-size: inherit;
  color: inherit; 
  white-space: pre-wrap;
  background-color: transparent;
  border: none;
  border-radius: 0;
}

code {
  padding: 2px 4px;
  color: inherit; 
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 4px;
  font-size: .9em;
}



blockquote,
.blockquote {
  padding: 10px 20px;
  margin: 0 0 20px;
  font-size: 1em;
  border-left: 5px solid #6c757d;
}

</style>
</head>

<body>
  <nav class="custom-navbar">
  <div class="container">
    
    <a href="/">Posts</a>
    
    <a href="/tags/">Tags</a>
    
    <a href="/about/">About</a>
    
    <a href="/index.xml">RSS</a>
    
  </div>
</nav>
  
  <div class="container">
    <article>
      <h1>[Python] Story that made Mel icon generator</h1>
<p>
  <small class="text-secondary">
  
  
  Feb 14, 2020
  </small>
  

<small><code><a href="https://memotut.com/tags/python">Python</a></code></small>


<small><code><a href="https://memotut.com/tags/machine-learning"> machine learning</a></code></small>


<small><code><a href="https://memotut.com/tags/gan"> GAN</a></code></small>


<small><code><a href="https://memotut.com/tags/image-generation"> image generation</a></code></small>


<small><code><a href="https://memotut.com/tags/melville"> Melville</a></code></small>

</p>
<pre><code>#Introduction
</code></pre>
<p>Do you know this icon?
<img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/584592/bc60d0f3-1654-c0ac-94d4-40cdd1fff3c0.jpeg" width=30%></p>
<p>Yes, it is the icon of the famous <a href="https://twitter.com/V_Melville">Melville</a>.
It is known that there are many people who have Melville draw their favorite characters, etc. and use it as a thumbnail for twitter, and it has gained great support.
The icon drawn by this person is often called &ldquo;Mel icon&rdquo; due to its unique style.
Examples of typical Mel icons</p>
<div>
<img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/584592/49ed3e86-7c45-c778-8bbd-5b6ba154906b.jpg" width=30%>
<img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/584592/2367a0f1-0cb1-aa98-87bf-ce308173f980.jpg" width=30%>
</div>
<p>(Those of <a href="https://twitter.com/yukata_yu">Yukatayu</a> and <a href="https://twitter.com/shun_skycrew">Shun</a>, respectively. (As of February 19, 2020))</p>
<p>I also want an icon like this! ! ! ! ! ! So I made Mel Icon Generator by machine learning.
In this article, I would like to briefly introduce the method used for it.</p>
<h2 id="what-is-gan">What is GAN</h2>
<p>We use a method called GAN (Generative adversarial networks) for the generation.</p>
<p><img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/584592/249bd7ea-0776-9b0f-7c39-2caf47c8c51f.jpeg" alt="is20tech001zu004-1.jpg">
<a href="https://www.imagazine.co.jp/gan%EF%BC%9A%E6%95%B5%E5%AF%BE%E7%9A%84%E7%94%9F%E6 %88%90%E3%83%8D%E3%83%83%E3%83%88%E3%83%AF%E3%83%BC%E3%82%AF%E3%81%A8%E3%81 %AF%E4%BD%95%E3%81%8B%E3%80%80%EF%BD%9E%E3%80%8C%E6%95%99%E5%B8%AB/">Figure Quotation Original</a></p>
<p>In this method, a neural network (Generator) that generates an image and a neural network (Discriminator) that distinguishes whether the input data is a mel icon or not are combined. The Generator tries to deceive the Discriminator by creating an image that resembles the Mel icon as much as possible, and the Discriminator learns to identify the image more accurately. As the two neural networks train each other, the Generator will be able to generate images close to mel icons.</p>
<h2 id="data-set-collection">Data set collection</h2>
<p>In order for Generator to be able to generate images like Mel icons and Discriminator to be able to identify whether the input images are Mel icons or not, bring in as many real Mel icons as possible and teach It is necessary to create a data set that becomes data and use this for learning.
So I went over twitter, found the thumbnail of Mel icon and saved it, and I got more than 100 sheets. Use this for learning.</p>
<p>##Generator creation
Ask the Generator to look at the Mel icon prepared earlier and learn to generate an image like that.
The image to be generated is 64 x 64 pixels and the color is 3 channels of rgb.
If the Generator generates similar data every time, learning will not work well, so it is necessary to be able to generate as many types of images as possible. Therefore, input a sequence of random numbers for image generation in Generator.
A process called &ldquo;convolutional convolution&rdquo;, which will be described later, is applied to this sequence of numbers in each convolution layer to gradually approximate an image having three channels of 64Ã—64 pixels and rgb.</p>
<h3 id="what-is-transposition-convolution">What is transposition convolution</h3>
<p>In normal convolution, as shown below, the product is taken as the output while the kernel is not prepared. In pytorch, you can implement it with torch.nn.Conv2d, for example.</p>
<img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/584592/1dcf482d-faab-1286-b338-662af39f578a.gif" width=40%>
<a href="https://postd.cc/image-scaling-using-deep-convolutional-neural-networks-part1/">Source</a>
<img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/584592/bf8e0aba-963c-beeb-a5ee-ff7d8e7fd6fc.gif" width=40%>
<a href="https://github.com/vdumoulin/conv_arithmetic">Citation source</a>
<p>On the other hand, in the transposition convolution used this time, the product with the kernel is calculated for each element, and the sum of the results is taken. The image is like expanding the target element. In pytorch, you can implement it with torch.nn.ConvTranspose2d, for example.</p>
<img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/584592/1a2713ef-964c-a4dd-2d4f-113344d0b935.gif" width=40%>
<a href="https://medium.com/apache-mxnet/transposed-convolutions-explained-with-ms-excel-52d13030c7e8">Source</a>
<p>The transposed convolutional layer and the self_attention layer (described later) are overlapped, and the last layer has 3 output channels. (corresponding to rgb respectively)
From the above contents, the outline of the Generator we are going to make is as shown in the figure below.</p>
<p><img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/584592/fc565c71-3b16-c8d8-77e7-3533553acd96.png" alt="generator_structure.png"></p>
<p>This Generator has a total of 5 transposed convolutional layers, but a layer called self_attention is sandwiched between the 3rd and 4th layers and between the 4th and 5th layers. By viewing pixels with similar values at once, it is possible to evaluate the entire image with a relatively small amount of calculation.</p>
<p>The Generator configured in this way outputs, for example, such an image if it is in an unlearned state. (The result changes depending on the sequence of random numbers you enter)
<img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/584592/8faf30ef-e715-cebd-398e-228317311c01.png" width=30%>
Since it is unlearned, it can only output something like noise. However, we will be able to output appropriate images by training each other with a neural network (Discriminator) that distinguishes whether the input data is a mel icon or not, as explained below.</p>
<h2 id="creating-discriminator">Creating Discriminator</h2>
<p>Discriminator will look at the image generated by the above Generator to see if it is a melicon. The point is to make an image recognizer.
The input image is 64 x 64 pixels, the color is 3 channels of rgb, and the output is a value (range 0 to 1) that shows how much it looks like a mel icon.
As a configuration, five ordinary convolution layers are stacked, and a self_attention layer is sandwiched between the third and fourth layers and between the fourth and fifth layers. It is as follows when illustrated.</p>
<p><img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/584592/bf3db1ec-e91b-a0c6-0700-93f740c9f3db.png" alt="discriminator_structure.png"></p>
<h2 id="learning-method-and-error-function">Learning method and error function</h2>
<p>Discriminator and Generator learning methods are as follows.</p>
<h3 id="learning-discriminator">Learning Discriminator</h3>
<p>Discriminator returns a number 0 to 1 indicating how much a melicon looks like when an image is input.
First, input the real Mel icon and set the output (value from 0 to 1) at that time as $d_{real}$.
Next, enter a random number into the Generator and have it generate an image. When this image is input to Discriminator, a value between 0 and 1 is returned as well. Call this $d_{fake}$.
By inputting $d_{real}$ and $d_{fake}$ thus generated into the loss function described below, the value used for error propagation is obtained.</p>
<h4 id="loss-function">Loss function</h4>
<p>One of GAN&rsquo;s methods, SAGAN&rsquo;s &ldquo;hinge version of the adversarial loss&rdquo; uses a loss function as described below. Briefly, this function uses $l_{i}$ and $l_{i}^{\prime}$ as correct labels and $y_{i}$ and $y_{i}^{\prime}$ from Discriminator When the output value, $M$ is the number of data per mini-batch</p>
<pre><code class="language-math" data-lang="math">- \frac{1}{M}\sum_{i=1}^{M}(l_{i}min(0,-1+y_{i})+(1-l_{i}^{\prime} )min(0,-1-y_{i}^{\prime}))
</code></pre><p>It is expressed as <sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>
This time $y_{i}=d_{real}$, $y_{i}^{\prime}=d_{fake}$, $l_{i}=1$ (representing 100% Mel Icon) , $l_{i}^{\prime}=0$ (representing that it is not an absolute mel icon)</p>
<pre><code class="language-math" data-lang="math">- \frac{1}{M}\sum_{i=1}^{M}(min(0,-1+d_{real})+min(0,-1-d_{fake}))
```will do. This is the loss function of Discriminator used this time. Adam was used as the error propagation optimization method, and the learning rate was set to 0.0004, and Adam's first and second moments (exponential decay rate used for moment estimation) were set to 0.0 and 0.9, respectively.

### Learning Generator
When a sequence of random numbers is input, the Generator will generate an image while trying to make it look like a mel icon.
First, input the sequence $z_{i}$ made up of random numbers to the Generator and get an image. Input it to Discriminator and output a value that shows how much it looks like a mel icon. Call this $r_{i}$.
#### Loss function
In SAGAN's &quot;hinge version of the adversarial loss&quot;, the generator loss function is defined as:

```math
- \frac{1}{M}\sum_{i=1}^{M}r_{i}
</code></pre><p>It seems that SAGAN is empirically known to work well with this definition. <sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>
Considering that $M$ is the number of data per mini-batch, the actual discriminator judgment result is used as it is. I was a little surprised at this, but how about it?
Adam was used as the error propagation optimization method, and the learning rate was set to 0.0001, and the first and second moments of Adam were set to 0.0 and 0.9, respectively. (Same as Discriminator except learning rate)</p>
<p>##Overall picture
It is a reprint of the image introduced above, but GAN is constructed by combining the Generator and Discriminator created earlier in this way.
<img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/584592/249bd7ea-0776-9b0f-7c39-2caf47c8c51f.jpeg" alt="is20tech001zu004-1.jpg"></p>
<h2 id="generate-now">Generate now</h2>
<p>Learning is performed by using the collected real Mel icons, and the Generator is made to generate Mel icons. The number of data $M$ per mini-batch is set to 5. The result is as follows.
<img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/584592/dfbc4434-1402-6f83-0337-9a71bdd1ce62.png" alt="generated_img64_remastered.png">
__awesome! ! ! ! ! ! ! ! ! ! ! __
__ Impressed! ! ! ! ! ! ! ! ! ! ! __
For comparison, an example of the input data is shown on the upper side and the actually generated image is shown on the lower side. In addition, the generation result changes each time it is executed.
Personally, I was quite surprised at what I could do with source code that was not that long. GAN is really great! ! ! ! ! ! ! !
##Task
I have created something that can do such a great thing, but there are still points that I have not solved.</p>
<ul>
<li>Mode collapse
Someone pointed out on twitter, but when you look at the generation results, you can see that although five sheets were supposed to be generated using random numbers, they all have the same character image. This phenomenon is called mode collapse.
This time, I was learning with 5 mini batches and 3000 epochs, so I thought that this was the cause of over-learning, but even with the number of epochs reduced to about 200, the same image is generated as follows. Will be done.
<img width="1027" alt="generated.png" src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/584592/54fc669f-54ba-8faf-7b0c-ca2c16e720ee.png">
(This is the output when self_attention has not been applied yet, but with about 200 epochs, you will get something like this.)
Certainly, a slightly different one is generated each time it is executed compared to when the number of epochs is 3000, but it seems that the image looks similar at first glance and the quality is definitely due to the small number of epochs in the first place. You can see that it is getting low.
Considering that there are about 60,000 handwritten number images in the MNIST example, it is possible to increase the data set by nearly 59,900, but it is virtually impossible because Mr. Melville is such a large amount.
Mode collapse, difficult.</li>
</ul>
<p>##Source code
The code I wrote can be found in this repository.
<a href="https://github.com/zassou65535/image_generator">https://github.com/zassou65535/image_generator</a></p>
<h2 id="summary">Summary</h2>
<p>GAN is a great technique. Despite the mode collapse, we were able to make a very close to Mel icon with a dataset of only 100 sheets. Let&rsquo;s all generate images with GAN.</p>
<p>##bonus
The following image came out when I simply averaged all the Mel icons I collected.
<img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/584592/acd938e8-e9b2-934a-9954-39f8c9f41e68.png" width=30%></p>
<h2 id="references">References</h2>
<section class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1" role="doc-endnote">
<p><a href="https://www.amazon.co.jp/%E3%81%A4%E3%81%8F%E3%82%8A%E3%81%AA%E3%81 %8C%E3%82%89%E5%AD%A6%E3%81%B6-PyTorch%E3%81%AB%E3%82%88%E3%82%8B%E7%99%BA%E5%B1 %95%E3%83%87%E3%82%A3%E3%83%BC%E3%83%97%E3%83%A9%E3%83%BC%E3%83%8B%E3%83%B3 %E3%82%B0-%E5%B0%8F%E5%B7%9D%E9%9B%84%E5%A4%AA%E9%83%8E/dp/4839970254">Learn While Making-Development with PyTorch Deep learning </a> <a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</section>

    </article>
  </div>

  
  
  
  <script>
  window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
  ga('create', 'UA-169005401-1', 'auto');
  ga('send', 'pageview');
  </script>
  <script async src='https://www.google-analytics.com/analytics.js'></script>
  

  

  
<link rel="stylesheet" type="text/css" href="//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.0/cookieconsent.min.css" />
<script src="//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.0/cookieconsent.min.js"></script>
<script>
window.addEventListener("load", function(){
window.cookieconsent.initialise({
  "palette": {
    "popup": {
      "background": "#216942",
      "text": "#b2d192"
    },
    "button": {
      "background": "#afed71"
    }
  }
})});
</script>

</body>

</html>
