<!doctype html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  
  <meta name="generator" content="Hugo 0.72.0" />

  
  <meta name="description" content="Some description">
  

  
  <link rel="apple-touch-icon" sizes="180x180" href="https://japan2018.github.io/apple-touch-icon.png">

  
  <link rel="icon" type="image/png" sizes="32x32" href="https://japan2018.github.io/favicon-32x32.png">

  
  <link rel="icon" type="image/png" sizes="16x16" href="https://japan2018.github.io/favicon-16x16.png">

  
  <link rel="manifest" href="https://japan2018.github.io/site.webmanifest">

  
  <link rel="mask-icon" href="https://japan2018.github.io/safari-pinned-tab.svg" color="#5bbad5">

  <meta name="msapplication-TileColor" content="#da532c">

  <meta name="theme-color" content="#ffffff">

  
  <link rel="stylesheet" href="https://japan2018.github.io/css/bootstrap.min.css" />

  
  <title>Python: unsupervised learning: non-hierarchical clustering | Some Title</title>
  

  <style>
body {
  min-width: 300px;
}

.custom-navbar {
  margin-bottom: 1em;
  height: 60px;
}

.custom-navbar a {
  display: inline-block; 
  padding: 18px 0;
  margin-right: 1em; 
  font-weight: bold; 
}

.custom-navbar a:hover,
.custom-navbar a:focus {
  text-decoration: none; 
}

@media print {
  .custom-navbar {
    display: none;
  }
}

article {
  padding-bottom: 1em;
}

img {
  max-width: 100%;
}


body {
  background-color: #fff;
}



body {
  color: #212529;
}



a {
  color: #007bff;
}



a:hover,
a:focus {
  color: #0056b3;
}



.custom-navbar {
  background-color: #212529;
}



.custom-navbar a {
  color: rgba(255, 255, 255, 0.75);
}



.custom-navbar a:hover,
.custom-navbar a:focus {
  color: rgba(255, 255, 255, 1);
}



.container {
  max-width: 800px;
}



pre {
  display: block;
  padding: 9.5px;
  word-break: break-all;
  word-wrap: break-word;
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 4px;
}

pre code {
  padding: 0;
  font-size: inherit;
  color: inherit; 
  white-space: pre-wrap;
  background-color: transparent;
  border: none;
  border-radius: 0;
}

code {
  padding: 2px 4px;
  color: inherit; 
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 4px;
  font-size: .9em;
}



blockquote,
.blockquote {
  padding: 10px 20px;
  margin: 0 0 20px;
  font-size: 1em;
  border-left: 5px solid #6c757d;
}

</style>
</head>

<body>
  <nav class="custom-navbar">
  <div class="container">
    
    <a href="/">Posts</a>
    
    <a href="/tags/">Tags</a>
    
    <a href="/about/">About</a>
    
    <a href="/index.xml">RSS</a>
    
  </div>
</nav>
  
  <div class="container">
    <article>
      <h1>Python: unsupervised learning: non-hierarchical clustering</h1>
<p>
  <small class="text-secondary">
  
  
  May 2, 2020
  </small>
  

<small><code><a href="https://japan2018.github.io/tags/python">Python</a></code></small>


<small><code><a href="https://japan2018.github.io/tags/machine-learning"> machine learning</a></code></small>


<small><code><a href="https://japan2018.github.io/tags/unsupervised-learning"> unsupervised learning</a></code></small>

</p>
<pre><code># Clustering techniques
</code></pre>
<h2 id="hierarchical-clustering">Hierarchical clustering</h2>
<p>Hierarchical clustering is to find the most similar combination in the data
It is a method of making clusters in order, and is characterized by having a hierarchical structure during the process.</p>
<p>See the figure below for a concrete example.
There are 5 data points A,B,C,D,E. The closest of these 5 data
We will make one cluster.</p>
<pre><code>A, B, C, D, E
→ (A,B), C,D,E
</code></pre><p>In this case, the two points A and B are the closest points in this combination.
Since it was judged by calculation, we made one cluster (A,B).</p>
<p>Next, consider the newly created cluster as one data point and repeat this process.</p>
<pre><code>→ (A,B), (C,D), E
→ (A,B,C,D), E
→ (A, B, C, D, E)
</code></pre><p>Finally, when you have reached the cluster that collects all the data, you are finished.
To express which cluster the data points were grouped into
As shown in the right figure below</p>
<pre><code>It is a dendrogram.
</code></pre><p><img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/430767/5f018a4d-a175-0fe3-4fb6-06baa0b84afa.png" alt="image.png"></p>
<h2 id="non-hierarchical-clustering">Non-hierarchical clustering</h2>
<p>Similar to hierarchical clustering, non-hierarchical clustering searches for similar characteristics in the data.
Creates a cluster but does not have a hierarchical structure.</p>
<p>Given the data, the developer decides in advance how many clusters to divide
A cluster is created from the data for that few minutes.</p>
<p>However, the optimal number of clusters has not been decided for each data.
Since it does not have a hierarchical structure, it requires less computation than hierarchical clustering.
This is an effective method when the amount of data is large.</p>
<p>A typical method of non-hierarchical clustering is</p>
<pre><code>The k-means method.
</code></pre><h1 id="k-means-method">k-means method</h1>
<h2 id="data-collection">Data collection</h2>
<p>Clusters are generated in the data by the specified number of clusters</p>
<pre><code>I would like to introduce you to the make_blobs function in sklearn.datasets.
</code></pre><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># Import sklearn.datasets make_blobs function</span>
<span style="color:#f92672">from</span> sklearn.datasets <span style="color:#f92672">import</span> make_blobs
<span style="color:#75715e">#X is the (x,y) of one plot, and Y is the cluster number to which the plot belongs</span>
X,Y <span style="color:#f92672">=</span> make_blobs(n_samples<span style="color:#f92672">=</span><span style="color:#ae81ff">150</span>, <span style="color:#75715e"># total number of data points</span>
               n_features<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>, <span style="color:#75715e">#Specify features (number of dimensions) default:2</span>
               centers<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>, <span style="color:#75715e"># Number of clusters</span>
               cluster_std<span style="color:#f92672">=</span><span style="color:#ae81ff">0.5</span>, <span style="color:#75715e"># standard deviation within cluster</span>
               shuffle<span style="color:#f92672">=</span>True, <span style="color:#75715e"># shuffle samples</span>
               random_state<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>) <span style="color:#75715e"># specify the state of the random number generator</span>
</code></pre></div><p>With the above code, X is the data point and Y is the label of the cluster to which the data point belongs.</p>
<h2 id="about-k-means-method">About k-means method</h2>
<p>The “k-means method” is a typical non-hierarchical clustering.</p>
<p>The k-means method is a method that can divide the data into n clusters with equal variance.
Each cluster is assigned a mean μ, which is the centroid of the data.</p>
<p>This center of gravity is called the &ldquo;centroid.&rdquo; To divide into clusters of equal variance</p>
<pre><code>We use an index called &quot;SSE&quot;.
</code></pre><p>SSE is the sum of squares of the difference between the data points contained in each cluster and the centroid.
(Corresponding to dispersion)
The k-means method chooses centroids to minimize and equalize this SSE over all clusters.</p>
<p>The k-means algorithm has three steps.</p>
<ol>
<li>
<p>First, kk (arbitrary number) data points are extracted from the data group, and the kk points are used as the initial centroid. After initializing the centroid, repeat the two steps.</p>
</li>
<li>
<p>Assign all data points to the nearest centroid respectively.</p>
</li>
<li>
<p>Next, calculate the centroid of the data set assigned to each kk centroid, and update the centroid as a new centroid.</p>
</li>
</ol>
<p>After each step 3, calculate the distance between the previous centroid and the new centroid.
When the distance becomes small to some extent, the above iterative process ends.
In other words, it iterates until the centroid is almost stationary when updated.</p>
<p>An example of data clustered by the k-means method is shown below.</p>
<p><img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/430767/ba984fd2-d0ba-cc07-5d4a-3a7a4cb45961.png" alt="image.png"></p>
<h2 id="sklearns-kmeans-library">sklearn&rsquo;s KMeans library</h2>
<pre><code>The KMeans class in sklearn.cluster is
Find the cluster from the data and assign a cluster number to each data.
</code></pre><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># Import KMeans class of sklearn.cluster</span>
<span style="color:#f92672">from</span> sklearn.cluster <span style="color:#f92672">import</span> KMeans

km <span style="color:#f92672">=</span> KMeans(n_clusters<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>, <span style="color:#75715e"># number of clusters</span>
            init<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;random&#34;</span>, <span style="color:#75715e"># Randomly set initial value of centroid default: &#34;k-means++&#34;</span>
            n_init<span style="color:#f92672">=</span><span style="color:#ae81ff">10</span>, <span style="color:#75715e"># number of executions of k-means using different centroid initial values</span>
            max_iter<span style="color:#f92672">=</span><span style="color:#ae81ff">300</span>, <span style="color:#75715e"># maximum number of times to repeat the k-means algorithm</span>
            tol<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-04</span>, <span style="color:#75715e"># Relative tolerance for judging convergence</span>
            random_state<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>) <span style="color:#75715e"># Random number generation initialization</span>

Y_km <span style="color:#f92672">=</span> km<span style="color:#f92672">.</span>fit_predict(X) <span style="color:#75715e"># Pass data with clusters present and find cluster number for each sample</span>
</code></pre></div><p>Find the specified number of clusters from the data with the above code
The cluster number is automatically stored in Y_km for each sample.
There are various other functions in the KMeans class.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># Perform clustering calculation</span>
km<span style="color:#f92672">.</span>fit(X[, y])
<span style="color:#75715e"># Perform clustering calculation, convert X to the metric space used for analysis, and return</span>
km<span style="color:#f92672">.</span>fit_transform(X[, y])

<span style="color:#75715e"># Returns the parameter used for calculation</span>
km<span style="color:#f92672">.</span>get_params([deep])
Returns the cluster number to which the <span style="color:#75715e">#X sample belongs</span>
km<span style="color:#f92672">.</span>predict(X)
<span style="color:#75715e"># Set parameter</span>
km<span style="color:#f92672">.</span>set_params(<span style="color:#f92672">**</span>params)
Convert <span style="color:#75715e"># X to the metric space used for analysis and return</span>
km<span style="color:#f92672">.</span>transform(X[, y])
</code></pre></div><h2 id="about-sse">About SSE</h2>
<p>One of the performance evaluation functions for clustering</p>
<pre><code>There is SSE (intra-cluster error sum of squares)
</code></pre><p>The performance of various k-means clustering can be evaluated by using SSE.
The SSE formula is omitted, but the SSE value indicates how far apart the values are in the cluster.</p>
<pre><code>With sklearn, you can get the value of SSE through the KMeans class's inertia_ attribute.
</code></pre><p>SSE is the sum of how much each data deviates from the center of gravity of the cluster to which it belongs (variance)
It can be said that the smaller the SSE value, the better the clustering.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e">#Access the sum of squared errors within a cluster</span>
<span style="color:#66d9ef">print</span> (<span style="color:#e6db74">&#34;Distortion: </span><span style="color:#e6db74">%.2f</span><span style="color:#e6db74">&#34;</span><span style="color:#f92672">%</span> km<span style="color:#f92672">.</span>inertia_)
</code></pre></div><h2 id="elbow-method">Elbow method</h2>
<p>There is a problem such as how to decide the number of clusters specified by k-means clustering.</p>
<p>There is a helpful method to determine this number of clusters. this is</p>
<pre><code>Called the elbow method
Plot how SSE changes as you increase the number of clusters
It is a method to determine the number of k-means clusters from the result.
</code></pre><p>As you can see by executing the code below, there is a point that the value of SSE bends sharply.
The number of clusters at this time can be regarded as optimal.</p>
<p>Because the shape of the plot looks like the elbow is bent
It is called the elbow method.</p>
<p>However, in reality, it looks beautiful as a result of the problem.
It&rsquo;s hard to get an elbow plot that makes the graph fall.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> matplotlib.pyplot <span style="color:#f92672">as</span> plt
<span style="color:#f92672">import</span> numpy <span style="color:#f92672">as</span> np
<span style="color:#f92672">import</span> pandas <span style="color:#f92672">as</span> pd
<span style="color:#f92672">from</span> sklearn.cluster <span style="color:#f92672">import</span> KMeans
<span style="color:#f92672">from</span> sklearn.datasets <span style="color:#f92672">import</span> make_blobs

<span style="color:#75715e">#Generate sample data</span>
X, Y <span style="color:#f92672">=</span> make_blobs(n_samples<span style="color:#f92672">=</span><span style="color:#ae81ff">150</span>, n_features<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>, centers<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>,
                  cluster_std<span style="color:#f92672">=</span><span style="color:#ae81ff">0.5</span>, shuffle<span style="color:#f92672">=</span>True, random_state<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)

distortions <span style="color:#f92672">=</span> []
<span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">11</span>): <span style="color:#75715e"># Calculate the number of clusters 1-10 at once</span>
    km <span style="color:#f92672">=</span> KMeans(n_clusters<span style="color:#f92672">=</span>i,
                Select cluster centers by init<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;k-means++&#34;</span>, <span style="color:#75715e"># k-means++ method</span>
                n_init<span style="color:#f92672">=</span><span style="color:#ae81ff">10</span>,
                max_iter<span style="color:#f92672">=</span><span style="color:#ae81ff">300</span>,
                random_state<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)
    km<span style="color:#f92672">.</span>fit(X) <span style="color:#75715e"># Perform clustering</span>
    distortions<span style="color:#f92672">.</span>append(km<span style="color:#f92672">.</span>inertia_) <span style="color:#75715e"># km.fit gives km.inertia_</span>

<span style="color:#75715e"># Graph plot</span>
plt<span style="color:#f92672">.</span>plot(range(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">11</span>), distortions, marker<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;o&#34;</span>)
plt<span style="color:#f92672">.</span>xticks(np<span style="color:#f92672">.</span>arange(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">11</span>, <span style="color:#ae81ff">1</span>))
plt<span style="color:#f92672">.</span>xlabel(<span style="color:#e6db74">&#34;Number of clusters&#34;</span>)
plt<span style="color:#f92672">.</span>ylabel(<span style="color:#e6db74">&#34;Distortion&#34;</span>)
plt<span style="color:#f92672">.</span>show()
<span style="color:#e6db74">``</span><span style="color:#960050;background-color:#1e0010">`!</span>[image<span style="color:#f92672">.</span>png](https:<span style="color:#f92672">//</span>qiita<span style="color:#f92672">-</span>image<span style="color:#f92672">-</span>store<span style="color:#f92672">.</span>s3<span style="color:#f92672">.</span>ap<span style="color:#f92672">-</span>northeast<span style="color:#f92672">-</span><span style="color:#ae81ff">1.</span>amazonaws<span style="color:#f92672">.</span>com<span style="color:#f92672">/</span><span style="color:#ae81ff">0</span><span style="color:#f92672">/</span><span style="color:#ae81ff">430767</span><span style="color:#f92672">/</span>a37702db<span style="color:#f92672">-</span><span style="color:#ae81ff">799</span>a<span style="color:#f92672">-</span>b265<span style="color:#f92672">-</span><span style="color:#ae81ff">4</span>b7a<span style="color:#f92672">-</span><span style="color:#ae81ff">718</span>b0bd9f434<span style="color:#f92672">.</span>png)

<span style="color:#75715e"># DBSCAN</span>

<span style="color:#75715e">## DBSCAN algorithm</span>

Another non<span style="color:#f92672">-</span>hierarchical clustering algorithm <span style="color:#66d9ef">for</span> the k<span style="color:#f92672">-</span>means method

</code></pre></div><p>There is &ldquo;DBSCAN&rdquo;.</p>
<pre><code>
The algorithm of &quot;DBSCAN&quot; is to find the high density (where data is agglomerated) locations in the cluster.
Capture from low density areas separately. It demonstrates its true value when there is a bias in cluster size and shape.

In the k-means method, clustering was performed so that data was collected as much as possible in the center of the cluster.
Therefore, the cluster inevitably takes a shape close to a circle (sphere).
Effective when the size and shape of the clusters are not biased
Good clustering tends to be impossible for data with uneven cluster size and shape.

&quot;DBSCAN&quot; defines two parameters.

</code></pre><p>min_samples and eps.</p>
<pre><code>
The &quot;DBSCAN&quot; algorithm classifies data points into the following three types:

</code></pre><ol>
<li>
<p>Core point
Data points when there are min_sample number of data within radius eps of certain data</p>
</li>
<li>
<p>Border point
Data that is not the core point but is within the radius eps from the core point</p>
</li>
<li>
<p>Noise point
Data points that do not meet either</p>
</li>
</ol>
<pre><code>
A cluster is formed from a collection of core points.

Border points are assigned to the cluster to which the closest core point belongs.

Thus, in the algorithm of &quot;DBSCAN&quot;
By classifying all data into three data
Be able to classify biased data and non-average clusters
You can also remove noise properly.

&quot;DBSCAN&quot; can use DBSCAN class of sklearn.cluster.
As the main parameters

</code></pre><p>Specify the distance calculation method with eps, min_sample, and metric.</p>
<pre><code>
```python
from sklearn.cluster import DBSCAN
db = DBSCAN(eps=0.2,
            min_samples=5,
            metric=&quot;euclidean&quot;)
Y_db = db.fit_predict(X)
</code></pre><p>While the k-means method is weak for data with complicated shapes like this time, DBSCAN can cluster arbitrary shapes.</p>

    </article>
  </div>

  
  
  
  <script>
  window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
  ga('create', 'UA-123456789-1', 'auto');
  ga('send', 'pageview');
  </script>
  <script async src='https://www.google-analytics.com/analytics.js'></script>
  

  

  
<link rel="stylesheet" type="text/css" href="//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.0/cookieconsent.min.css" />
<script src="//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.0/cookieconsent.min.js"></script>
<script>
window.addEventListener("load", function(){
window.cookieconsent.initialise({
  "palette": {
    "popup": {
      "background": "#216942",
      "text": "#b2d192"
    },
    "button": {
      "background": "#afed71"
    }
  }
})});
</script>

</body>

</html>
