<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title> Reinforcement Learning on Some Title</title>
    <link>https://japan2018.github.io/tags/reinforcement-learning/</link>
    <description>Recent content in  Reinforcement Learning on Some Title</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 19 May 2020 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://japan2018.github.io/tags/reinforcement-learning/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Reinforcement Learning: Speed up Value Iteration</title>
      <link>https://japan2018.github.io/reinforcement-learning-speed-up-value-iteration-fe140/</link>
      <pubDate>Tue, 19 May 2020 00:00:00 +0000</pubDate>
      
      <guid>https://japan2018.github.io/reinforcement-learning-speed-up-value-iteration-fe140/</guid>
      <description>#Introduction  In recent years, the field of model-free deep reinforcement learning has been actively studied due to the success of AlphaGo and DQN. These algorithms are one of the effective approaches when the state-action space is very large or when mathematical modeling of dynamics is difficult. However, in many of the problems encountered in reality, mathematical modeling of the environment is relatively easy, and the state-action space can be reduced by devising it in many cases.</description>
    </item>
    
    <item>
      <title>Learn while making! Deep reinforcement learning_1</title>
      <link>https://japan2018.github.io/learn-while-making-deep-reinforcement-learning_1-fc266/</link>
      <pubDate>Wed, 18 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://japan2018.github.io/learn-while-making-deep-reinforcement-learning_1-fc266/</guid>
      <description># Deep Reinforcement Learning-Practical Programming with Pytorch-  I am Harima, a 1st year master of science graduate student. I will summarize my learning contents in a memo. I&amp;rsquo;m sorry it&amp;rsquo;s hard to see. I would like to know what you do not understand.
 Implementation code (GitHub) https://github.com/YutaroOgawa/Deep-Reinforcement-Learning-Book
 Chap.2 Let&amp;rsquo;s implement reinforcement learning for maze tasks 2.1 How to use Python 2.2 Implement mazes and agents import numpy as np import matplotlib.</description>
    </item>
    
    <item>
      <title>Reinforcement learning 11 Try OpenAI acrobot with ChainerRL.</title>
      <link>https://japan2018.github.io/reinforcement-learning-11-try-openai-acrobot-with-chainerrl.-ff53c/</link>
      <pubDate>Sat, 16 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>https://japan2018.github.io/reinforcement-learning-11-try-openai-acrobot-with-chainerrl.-ff53c/</guid>
      <description>It is a prerequisite that you have achieved up to 10 reinforcement learning.  If you google with openai acrobot, Acrobot-v1 will come out. I do not know v1 or v0 well, so I will investigate before the magic modification. userfolder/anaconda3/envs/chainer/lib/python3.7/site-packages/gym With VS Code. A full search with CartPole found CartPole-v0 and CartPole-v1. Mumu? For acrobot, only Acrobot-v1. In CartPole created earlier, I tried to replace CartPole-v0 with CartPole-v1. The difficulty level seems to have increased.</description>
    </item>
    
  </channel>
</rss>