<!doctype html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  
  <meta name="generator" content="Hugo 0.72.0" />

  
  <meta name="description" content="Some description">
  

  
  <link rel="apple-touch-icon" sizes="180x180" href="https://memotut.com/apple-touch-icon.png">

  
  <link rel="icon" type="image/png" sizes="32x32" href="https://memotut.com/favicon-32x32.png">

  
  <link rel="icon" type="image/png" sizes="16x16" href="https://memotut.com/favicon-16x16.png">

  
  <link rel="manifest" href="https://memotut.com/site.webmanifest">

  
  <link rel="mask-icon" href="https://memotut.com/safari-pinned-tab.svg" color="#5bbad5">

  <meta name="msapplication-TileColor" content="#da532c">

  <meta name="theme-color" content="#ffffff">

  
  <link rel="stylesheet" href="https://memotut.com/css/bootstrap.min.css" />

  
  <title>[Python] Hands-on to visualize your tweet while understanding BERT | Memo Tut</title>
  

  <style>
body {
  min-width: 300px;
}

.custom-navbar {
  margin-bottom: 1em;
  height: 60px;
}

.custom-navbar a {
  display: inline-block; 
  padding: 18px 0;
  margin-right: 1em; 
  font-weight: bold; 
}

.custom-navbar a:hover,
.custom-navbar a:focus {
  text-decoration: none; 
}

@media print {
  .custom-navbar {
    display: none;
  }
}

article {
  padding-bottom: 1em;
}

img {
  max-width: 100%;
}


body {
  background-color: #fff;
}



body {
  color: #212529;
}



a {
  color: #007bff;
}



a:hover,
a:focus {
  color: #0056b3;
}



.custom-navbar {
  background-color: #212529;
}



.custom-navbar a {
  color: rgba(255, 255, 255, 0.75);
}



.custom-navbar a:hover,
.custom-navbar a:focus {
  color: rgba(255, 255, 255, 1);
}



.container {
  max-width: 800px;
}



pre {
  display: block;
  padding: 9.5px;
  word-break: break-all;
  word-wrap: break-word;
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 4px;
}

pre code {
  padding: 0;
  font-size: inherit;
  color: inherit; 
  white-space: pre-wrap;
  background-color: transparent;
  border: none;
  border-radius: 0;
}

code {
  padding: 2px 4px;
  color: inherit; 
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 4px;
  font-size: .9em;
}



blockquote,
.blockquote {
  padding: 10px 20px;
  margin: 0 0 20px;
  font-size: 1em;
  border-left: 5px solid #6c757d;
}

</style>
</head>

<body>
  <nav class="custom-navbar">
  <div class="container">
    
    <a href="/">Posts</a>
    
    <a href="/tags/">Tags</a>
    
    <a href="/about/">About</a>
    
    <a href="/index.xml">RSS</a>
    
  </div>
</nav>
  
  <div class="container">
    <article>
      <h1>[Python] Hands-on to visualize your tweet while understanding BERT</h1>
<p>
  <small class="text-secondary">
  
  
  Dec 12, 2019
  </small>
  

<small><code><a href="https://memotut.com/tags/python">Python</a></code></small>


<small><code><a href="https://memotut.com/tags/nlp">NLP</a></code></small>


<small><code><a href="https://memotut.com/tags/machinelearning">MachineLearning</a></code></small>


<small><code><a href="https://memotut.com/tags/python3">Python3</a></code></small>


<small><code><a href="https://memotut.com/tags/bert">bert</a></code></small>

</p>
<pre><code>This article is the 13th day article of [NTT Communications Advent Calendar 2019](https://qiita.com/advent-calendar/2019/nttcom).
</code></pre>
<p>Yesterday was @nitky&rsquo;s article, &ldquo;We deal with threat intelligence in a mood&rdquo; (<a href="https://qiita.com/nitky/items/ccefd0353b74aa21e357)">https://qiita.com/nitky/items/ccefd0353b74aa21e357)</a>.</p>
<p>#Introduction
Hi, my name is yuki uchida, and I belong to the SkyWay team of NTT Communications.
This article is an article that visualizes your tweet using the language model <code>BERT</code> used for natural language processing, which was recently applied to Google search.
I will write in a hands-on format so that as many people as possible can try it out, so if you are interested, please give it a try.</p>
<p><a href="https://wired.jp/2019/11/14/google-search-advancing-grade-reading/">Google&rsquo;s search engine&rsquo;s biggest leap in the past 5 years&rsquo;</a></p>
<h2 id="what-is-bert">What is BERT?</h2>
<p>BERT is a natural language model announced by Google in 2018, and has achieved impact accuracy in many natural language tasks. This precision was amazing, and it gained great popularity in the natural language processing area.
(By the way, in the area of natural language processing, a natural language model called Word2Vec was announced in 2014, which made a lot of noise, but this is also a paper from Google.)
Since many people have already explained the details of this BERT, I will put some links.</p>
<p><a href="https://qiita.com/Kosuke-Szk/items/4b74b5cce84f423b7125">Move general-purpose language expression model BERT in Japanese (PyTorch)</a>
<a href="https://qiita.com/Kosuke-Szk/items/d49e2127bf95a1a8e19f">Understanding the internal operation of the general-purpose language expression model BERT</a></p>
<p>What can we do with this BERT this time? To understand that intuitively, let&rsquo;s open the following site!</p>
<p><a href="https://transformer.huggingface.co/">https://transformer.huggingface.co/</a></p>
<p>On this site, you can try the library called <code>transformers (formerly pytorch-pretrained-bert)</code> provided by <code>huggingface</code> online.
This time, let&rsquo;s demonstrate by selecting <code>gpt</code>, which is the same language model as BERT.
<img width="414" alt="Write_With_Transformer.png" src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/124611/bce986e7-357d-cd4a-21d9-8f241462500e.png"></p>
<p>When you select it, it will switch to the screen like Word like below where you can enter sentences.</p>
<p>In this state, enter some text and then press the tab button.
You can see that the sentences are displayed as three candidates. If you select this sentence, it will be added to the sentence.
<Img width = "823" alt = "68747470733a2f2f71696974612d696d6167652d73746f72652e73332e61702d6e6f727468656173742d312e616d617a6f6e6177732e636f6d2f302f3132343631312f33346433643539312d323735372d323363312d613638612d6362303766333964303031372e706e67.png" src = "https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/124611/ac9d61ee-114f-d935-98fe -fd659f5bd302.png"></p>
<p>This demo allows you to try <strong>Sentence Generation</strong>.</p>
<p><strong>Sentence generation</strong> is a technology that has been studied for a long time in the natural language processing area. By using the <strong>natural language model</strong>, we were able to make the computer understand the <strong>text information</strong> well, and we were able to do <strong>natural sentence generation</strong> in this way.</p>
<p>If you try to generate the text a few times, the result will be quite interesting.
<code>I have never met anyone who did not find it useful or useful for others .It was originally released as an open source project</code></p>
<blockquote>
<img width="707" alt=" screenshot 2019-12-12 21.05.18.png" src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/124611/b79bc18d-4ad4-b63d-0bd0-64b6c5a61167.png">
</blockquote>
<p><em>(Sentences that do not seem to be transmitted (laughs)</em></p>
<p>The model chosen on this demo site this time is the model called <code>GPT</code>, but the successor model, <code>GPT2</code>, was left open to the public for a while due to fear that the proponent OpenAI would abuse it. (Currently publicly available, you can also try it on the demo site)</p>
<p><a href="https://jp.techcrunch.com/2019/02/18/2019-02-17-openai-text-generator-dangerous/">Techcrunch: OpenAI has developed a very good text generator, but I think it is too dangerous to release it as it is</a></p>
<p>Thus, **better natural language models improve the accuracy of many natural language processing tasks. **</p>
<h2 id="install-transformers">Install transformers</h2>
<p>We will use the <code>huggingface</code> library, which has been renamed from <code>pytorch-pretrained-bert</code> to <code>transformers</code>.
The demo site I used earlier is an online demo of this <code>transformers</code>.
By using this library, you can easily call BERT in combination with <code>pytorch(or tensorflow)</code>.</p>
<p><a href="https://github.com/huggingface/transformers">https://github.com/huggingface/transformers</a></p>
<p>In this hands-on, I&rsquo;ll call <code>BERT</code> using <code>pytorch</code> instead of <code>transformers</code>.
If you haven&rsquo;t installed <code>pytorch</code> and <code>transformers</code> yet, install it with the following command.</p>
<p><code>pip install torch torchvision</code>
<code>pip install transformers</code></p>
<h2 id="apply-hidden-words-using-berts-english-learned-model">Apply hidden words using BERT&rsquo;s English-learned model</h2>
<p>This time, I will be pulling a BERT model that supports Japanese.
If you want to read in English, you don&rsquo;t need to prepare anything and you can read it with the following code.
<code>model = BertForMasked.from_pretrained('bert-base-uncased')</code></p>
<p>**Because it takes a little time to use the Japanese learned model, let&rsquo;s use the BERT English learned model that can be easily called before using the Japanese learned model once. **</p>
<p>First, import the necessary libraries. Create <code>test.py</code> and describe as follows.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> torch
<span style="color:#f92672">from</span> transformers <span style="color:#f92672">import</span> BertTokenizer, BertForMaskedLM
<span style="color:#f92672">import</span> numpy <span style="color:#f92672">as</span> np
</code></pre></div><p>Next, set any simple English sentences.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">
text <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;How many lakes are there in Japan.&#34;</span>
</code></pre></div><p>Now, let&rsquo;s do word splitting using <code>BertTokenizer</code>, a BERT-specific separating tool.
Paste the code below.
Word-by-word breaks are a necessary step in determining where the computer breaks into words.
(In English, you can basically divide each word just by separating it with a space.)</p>
<p><strong>(Note: Start and end with [CLS] [SEP].)</strong></p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python:test.py" data-lang="python:test.py"><span style="color:#75715e">## At the stage of importing the library, BertTokenizer is also loaded.</span>
tokenizer <span style="color:#f92672">=</span> BertTokenizer<span style="color:#f92672">.</span>from_pretrained(<span style="color:#e6db74">&#39;bert-base-uncased&#39;</span>)
tokenized_text <span style="color:#f92672">=</span> tokenizer<span style="color:#f92672">.</span>tokenize(text)
tokenized_text<span style="color:#f92672">.</span>insert(<span style="color:#ae81ff">0</span>, <span style="color:#e6db74">&#34;[CLS]&#34;</span>)
tokenized_text<span style="color:#f92672">.</span>append(<span style="color:#e6db74">&#34;[SEP]&#34;</span>)
<span style="color:#75715e"># [&#39;[CLS]&#39;,&#39;how&#39;,&#39;many&#39;,&#39;lakes&#39;,&#39;are&#39;,&#39;there&#39;,&#39;in&#39;,&#39;japan&#39;,&#39;.&#39;,&#39;[SEP]&#39;]</span>
</code></pre></div><p>By doing this, I was able to divide each word.</p>
<p>**Next, select the word you want to hide. BERT will try to guess the word. **
This time I want to hide <strong>are</strong>. Describe the process of replacing this word with <code>[MASK]</code>.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python:test.py" data-lang="python:test.py">masked_index <span style="color:#f92672">=</span> <span style="color:#ae81ff">4</span>
tokenized_text[masked_index] <span style="color:#f92672">=</span><span style="color:#e6db74">&#39;[MASK]&#39;</span>
<span style="color:#75715e"># [&#39;[CLS]&#39;,&#39;how&#39;,&#39;many&#39;,&#39;lakes&#39;,&#39;[MASK]&#39;,&#39;there&#39;,&#39;in&#39;,&#39;japan&#39;,&#39;.&#39;,&#39;[SEP]&#39;]</span>
</code></pre></div><p>As a result, <strong>are</strong> has been superseded by <strong>[MASK]</strong>. Now I don&rsquo;t know what the word is here.</p>
<p>Let&rsquo;s give this text to BERT and predict the word <strong>[MASK]</strong>! ! !</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python:test.py" data-lang="python:test.py"><span style="color:#75715e">## Instead of passing the text as it is to BERT, convert it with a dictionary and set it as id.</span>
tokens <span style="color:#f92672">=</span> tokenizer<span style="color:#f92672">.</span>convert_tokens_to_ids(tokenized_text)
tokens_tensor <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>tensor([tokens])
<span style="color:#75715e">## Load BERT. It may take a while here</span>
model <span style="color:#f92672">=</span> BertForMaskedLM<span style="color:#f92672">.</span>from_pretrained(<span style="color:#e6db74">&#39;bert-base-uncased&#39;</span>)
model<span style="color:#f92672">.</span>eval()

<span style="color:#66d9ef">with</span> torch<span style="color:#f92672">.</span>no_grad():
    outputs <span style="color:#f92672">=</span> model(tokens_tensor)
    predictions <span style="color:#f92672">=</span> outputs[<span style="color:#ae81ff">0</span>]
<span style="color:#75715e">## Extract the prediction result of the word that is masked_index, and output the prediction result top5</span>
_, predict_indexes <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>topk(predictions[<span style="color:#ae81ff">0</span>, masked_index], k<span style="color:#f92672">=</span><span style="color:#ae81ff">5</span>)predict_tokens <span style="color:#f92672">=</span> tokenizer<span style="color:#f92672">.</span>convert_ids_to_tokens(predict_indexes<span style="color:#f92672">.</span>tolist())
<span style="color:#66d9ef">print</span>(predict_tokens)
<span style="color:#75715e"># [&#39;are&#39;,&#39;were&#39;,&#39;lie&#39;,&#39;out&#39;,&#39;is&#39;]</span>
</code></pre></div><p>As a result, as a result of predicting hidden words, TOP5 became <code>['are','were','lie','out','is']</code>. As you can see from this result, **BERT was able to guess the hidden word. ** It is amazing!</p>
<p>This time, I used <strong>BertForMaskedLM</strong> to predict hidden words. Other things you can easily try are: Please, try it.</p>
<ol>
<li>BertForNextSentencePrediction</li>
<li>BertForSequenceClassification</li>
<li>BertForTokenClassification</li>
<li>BertForQuestionAnswering</li>
</ol>
<h2 id="pulling-berts-japanese-learned-models">Pulling BERT&rsquo;s Japanese-learned models</h2>
<p>Well then, here&rsquo;s the real thing.
<strong>Visualize what type of tweet you are using with BERT</strong>
First, prepare a BERT that supports Japanese.
Originally, I would like to do BERT pre-learning by myself, but BERT takes a very long time to learn.
Therefore, [Kurohashi/Kawara Lab.&rsquo;s HP at Kyoto University] that distributes the learned models. (<a href="http://nlp.ist.i.kyoto-u.ac.jp/index.php?BERT">http://nlp.ist.i.kyoto-u.ac.jp/index.php?BERT</a> Go to %E6%97%A5%E6%9C%AC%E8%AA%9EPretrained%E3%83%A2%E3%83%87%E3%83%AB) to download your model.</p>
<p>It takes about 30 days to learn BERT, so it&rsquo;s really appreciated that you can feel free to try it if you publish it like this&hellip;</p>
<blockquote>
<p>30 epoch (1GPU (using GeForce GTX 1080 Ti) takes about 1 day for 1 epoch, so about 30 days for pretraining)</p>
</blockquote>
<p>Unzip the downloaded file and put it in the same location as the python file. (It is good to create a bert folder as follows and store it there)</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python:test.py" data-lang="python:test.py"><span style="color:#f92672">import</span> torch
<span style="color:#f92672">from</span> transformers <span style="color:#f92672">import</span> BertTokenizer, BertModel, BertForMaskedLM
<span style="color:#f92672">import</span> numpy <span style="color:#f92672">as</span> np
model <span style="color:#f92672">=</span> BertModel<span style="color:#f92672">.</span>from_pretrained(<span style="color:#e6db74">&#39;bert/Japanese_L-12_H-768_A-12_E-30_BPE_transformers&#39;</span>)
bert_tokenizer <span style="color:#f92672">=</span> BertTokenizer(<span style="color:#e6db74">&#34;bert/Japanese_L-12_H-768_A-12_E-30_BPE_transformers/vocab.txt&#34;</span>,
                               do_lower_case<span style="color:#f92672">=</span>False, do_basic_tokenize<span style="color:#f92672">=</span>False)
</code></pre></div><p>Now, let&rsquo;s try to use the BERT model once.</p>
<h3 id="try-to-find-hidden-words-in-japanese-bert">Try to find hidden words in Japanese BERT</h3>
<p>I would like to reuse the code I did earlier, but this time I&rsquo;m dealing with Japanese rather than English, so I can&rsquo;t split the text with <code>BertTokenizer</code>. (Since the dictionary function that replaces words with id is used, read it as above)
This time, use <code>Juman</code> to do word-dividing and word-division.
Since pip install is necessary to handle juman, enter the following command to install
<code>pip install pyknp</code></p>
<p>Now that the installation is complete, let&rsquo;s use this <code>Juman</code> to write words into words.
The target sentence is &ldquo;I like playing soccer with my friends,&rdquo; as I did in English.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python:test.py" data-lang="python:test.py"><span style="color:#f92672">from</span> pyknp <span style="color:#f92672">import</span> Juman
jumanpp <span style="color:#f92672">=</span> Juman()
text <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;I like playing soccer with my friends&#34;</span>
result <span style="color:#f92672">=</span> jumanpp<span style="color:#f92672">.</span>analysis(text)
tokenized_text <span style="color:#f92672">=</span> [mrph<span style="color:#f92672">.</span>midasi <span style="color:#66d9ef">for</span> mrph <span style="color:#f92672">in</span> result<span style="color:#f92672">.</span>mrph_list()]
<span style="color:#75715e"># [&#34;I&#34;, &#34;wa&#34;, &#34;friend&#34;, &#34;and&#34;, &#34;soccer&#34;, &#34;to&#34;, &#34;do&#34;, &#34;koto&#34;, &#34;wa&#34;, &#34;like&#34;]</span>
</code></pre></div><p>From here, the prediction will be performed in the same way as the English version.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python:test.py" data-lang="python:test.py">
tokenized_text<span style="color:#f92672">.</span>insert(<span style="color:#ae81ff">0</span>,<span style="color:#e6db74">&#39;[CLS]&#39;</span>)
tokenized_text<span style="color:#f92672">.</span>append(<span style="color:#e6db74">&#39;[SEP]&#39;</span>)
masked_index <span style="color:#f92672">=</span> <span style="color:#ae81ff">5</span>
tokenized_text[masked_index] <span style="color:#f92672">=</span><span style="color:#e6db74">&#39;[MASK]&#39;</span>
<span style="color:#66d9ef">print</span>(tokenized_text)
<span style="color:#75715e"># [&#39;[CLS]&#39;,&#39;Boku&#39;,&#39;wa&#39;,&#39;friend&#39;,&#39;and&#39;,&#39;[MASK]&#39;,&#39;do&#39;,&#39;do&#39;,&#39;koto&#39;,&#39;wa&#39;,&#39;like&#39; ,&#39;[SEP]&#39;]</span>
tokens <span style="color:#f92672">=</span> bert_tokenizer<span style="color:#f92672">.</span>convert_tokens_to_ids(tokenized_text)
tokens_tensor <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>tensor([tokens])
model<span style="color:#f92672">.</span>eval()


<span style="color:#66d9ef">with</span> torch<span style="color:#f92672">.</span>no_grad():
    outputs <span style="color:#f92672">=</span> model(tokens_tensor)
    predictions <span style="color:#f92672">=</span> outputs[<span style="color:#ae81ff">0</span>]

_, predict_indexes <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>topk(predictions[<span style="color:#ae81ff">0</span>, masked_index], k<span style="color:#f92672">=</span><span style="color:#ae81ff">5</span>)
predict_tokens <span style="color:#f92672">=</span> bert_tokenizer<span style="color:#f92672">.</span>convert_ids_to_tokens(predict_indexes<span style="color:#f92672">.</span>tolist())
<span style="color:#66d9ef">print</span>(predict_tokens)
<span style="color:#75715e"># [&#39;Talk&#39;,&#39;Work&#39;,&#39;Kiss&#39;,&#39;Game&#39;,&#39;Soccer&#39;]</span>
</code></pre></div><p>In the Japanese version of BERT, I made a prediction by hiding soccer from &ldquo;&ldquo;I like playing soccer with friends.&rdquo; It became <code>['story','work','kiss','game','soccer']</code>.
**Soccer wasn&rsquo;t number one, but you can feel the other answers are correct. **</p>
<p>I was able to confirm that the Japanese version of BERT works here.</p>
<h2 id="preparing-your-tweet">Preparing your tweet</h2>
<p>Now, prepare the tweets for visualization.
Please access the page below and click on <code>Twitter Data</code>.
<a href="https://twitter.com/settings/account">https://twitter.com/settings/account</a></p>
<p>If the screen below appears, enter the password and then press <code>Request archive</code>.
When the preparation is completed and the mail arrives, it will change to <code>Download archive</code> and you can download it.
<img width="1052" alt="Twitter data ___Twitter.png" src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/124611/8b5c73e8-dfa4-a3ac-bca2-12ce691e7b10.png">
<img width="588" alt="Twitter data ___Twitter.png" src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/124611/5081fafa-4b8d-18ba-e5ce-95e5e3451a38.png"></p>
<p>When the download is complete, unzip it and check the contents. As below. It is OK if a lot of Javascript files and <code>tweet.js</code> exist.</p>
<p>&lt;img width=&quot;552&rdquo; alt=&quot;yuki-u_ and _ edit &ldquo;Hands-on to visualize your tweet without understanding BERT&rdquo; _-_Qiita.png&rdquo; src=&quot;https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/124611/337704b4-479b-0a14-7ef2-8bdacdbdd8bf.png&quot;&gt;</p>
<h2 id="convert-your-tweet-to-csv">Convert your tweet to csv</h2>
<p>All of the current tweet history is stored in tweet.js and it is a little hard to handle, so let&rsquo;s convert it to csv.
You can convert tweet.js to csv on the following sites.</p>
<p>**However, as described in this site, if you are worried about using this tool, please convert it to csv by another method. **
<a href="https://r17n.page/2019/10/22/tweet-js-loader-introduction/">I made a tool &ldquo;tweet.js loader&rdquo; that reads tweet.js of Twitter data and displays all tweet history</a></p>
<img width="1244" alt="tweet_js_loader_-_ all tweets history_display er.png" src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/124611/daf358d4-dcf9-0d52-ad44-6d35e1127220.png">
<img width="1402" alt="tweet_js_loader_-_ all tweets history_display er.png" src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/124611/56fcf0a9-7f9e-8abb-56a8-078bba1aeef0.png">
<p>Click the <code>CSV output</code> button to download CSV.</p>
<p>It is OK if the conversion to csv is completed and you can download it. (let&rsquo;s name it tweets.csv)</p>
<h2 id="convert-tweets-into-text-vectors">Convert tweets into text vectors</h2>
<p>Now, let&rsquo;s use BERT to convert this tweet.csv tweet into a sentence vector.
<strong>The text vector is a vectorized version of the text</strong>.
Let&rsquo;s check what kind of tweet is being made by pouring the sentence vector converted by BERT into a visualization tool.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python:test.py" data-lang="python:test.py"><span style="color:#f92672">import</span> pandas <span style="color:#f92672">as</span> pd
<span style="color:#f92672">import</span> re
tweets_df <span style="color:#f92672">=</span> pd<span style="color:#f92672">.</span>read_csv(<span style="color:#e6db74">&#34;./tweets.csv&#34;</span>)
tweets_df[<span style="color:#e6db74">&#34;text&#34;</span>] <span style="color:#f92672">=</span> tweets_df[<span style="color:#e6db74">&#34;text&#34;</span>]<span style="color:#f92672">.</span>astype(str) <span style="color:#75715e"># leave it as a string</span>
<span style="color:#75715e">## Declare an array to store the result after the text vector conversion and an array to save the original tweet</span>
vectors <span style="color:#f92672">=</span> []
tweets <span style="color:#f92672">=</span> []
<span style="color:#66d9ef">for</span> tweet <span style="color:#f92672">in</span> tweets_df[<span style="color:#e6db74">&#34;text&#34;</span>]:
    tweet <span style="color:#f92672">=</span> re<span style="color:#f92672">.</span>sub(<span style="color:#e6db74">&#39;</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&#39;</span>, <span style="color:#e6db74">&#34;&#34;</span>, tweet) <span style="color:#75715e">#Delete line feed character</span>
    strip_tweet <span style="color:#f92672">=</span> re<span style="color:#f92672">.</span>sub(<span style="color:#e6db74">r</span><span style="color:#e6db74">&#39;[︰-＠]&#39;</span>, <span style="color:#e6db74">&#34;&#34;</span>, tweet) <span style="color:#75715e">#Delete full-width symbol</span>
    <span style="color:#66d9ef">try</span>:<span style="color:#66d9ef">if</span> len(strip_tweet) <span style="color:#f92672">&gt;</span><span style="color:#ae81ff">3</span>: <span style="color:#75715e"># because too few words may not give a good vector</span>
            vector <span style="color:#f92672">=</span> compute_vector(
                strip_tweet, model, bert_tokenizer, juman_tokenizer)
            vectors<span style="color:#f92672">.</span>append(vector)
            tweets<span style="color:#f92672">.</span>append(tweet)
    <span style="color:#66d9ef">except</span> <span style="color:#a6e22e">Exception</span> <span style="color:#66d9ef">as</span> e:
        <span style="color:#66d9ef">continue</span>
<span style="color:#75715e">## Put the converted text vector in tsv. (Visualization tool requests tsv, so set it to tsv)</span>
pd<span style="color:#f92672">.</span>DataFrame(tweets)<span style="color:#f92672">.</span>to_csv(<span style="color:#e6db74">&#39;./tweets_text.tsv&#39;</span>, index<span style="color:#f92672">=</span>False, header<span style="color:#f92672">=</span>None))
pd<span style="color:#f92672">.</span>DataFrame(vectors)<span style="color:#f92672">.</span>to_csv(<span style="color:#e6db74">&#39;./tweets_vector.tsv&#39;</span>, sep<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;</span><span style="color:#ae81ff">\t</span><span style="color:#e6db74">&#39;</span>, index<span style="color:#f92672">=</span>False, header<span style="color:#f92672">=</span>None))
</code></pre></div><p>The <code>compute_vector</code> that appears here is the process of converting to a sentence vector using the BERT model.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python:test.py" data-lang="python:test.py"><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">compute_vector</span>(text, model, bert_tokenizer, juman_tokenizer):
    use_model <span style="color:#f92672">=</span> model
    tokens <span style="color:#f92672">=</span> juman_tokenizer<span style="color:#f92672">.</span>tokenize(text)
    bert_tokens <span style="color:#f92672">=</span> bert_tokenizer<span style="color:#f92672">.</span>tokenize(<span style="color:#e6db74">&#34; &#34;</span><span style="color:#f92672">.</span>join(tokens))
    ids <span style="color:#f92672">=</span> bert_tokenizer<span style="color:#f92672">.</span>convert_tokens_to_ids(
        [<span style="color:#e6db74">&#34;[CLS]&#34;</span>] <span style="color:#f92672">+</span> bert_tokens[:<span style="color:#ae81ff">126</span>] <span style="color:#f92672">+</span> [<span style="color:#e6db74">&#34;[SEP]&#34;</span>])
    tokens_tensor <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>tensor(ids)<span style="color:#f92672">.</span>reshape(<span style="color:#ae81ff">1</span>, <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>)
    use_model<span style="color:#f92672">.</span>eval()
    <span style="color:#66d9ef">with</span> torch<span style="color:#f92672">.</span>no_grad():
        all_encoder_layers, _ <span style="color:#f92672">=</span> use_model(tokens_tensor)
    pooling_layer <span style="color:#f92672">=</span> <span style="color:#f92672">-</span><span style="color:#ae81ff">2</span>
    embedding <span style="color:#f92672">=</span> all_encoder_layers[<span style="color:#ae81ff">0</span>][pooling_layer]<span style="color:#f92672">.</span>numpy()
    <span style="color:#75715e"># embedding = all_encoder_layers[0].numpy()</span>
    <span style="color:#75715e"># return np.mean(embedding, axis=0)</span>
    <span style="color:#66d9ef">return</span> embedding
</code></pre></div><p>If you check the file saved by this process, there should be <code>tweets_vector.tsv</code> where the text vector is saved with tab delimiters and <code>tweets_text.tsv</code> where the original tweet is saved. is.</p>
<img width="1074" alt="tweets_vector_tsv_—_AIResearch.png" src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/124611/b1ecfa1d-5be9-1fc5-3848-4ce4049679fd.png">
<img width="571" alt="tweets_text_tsv_—_AIResearch.png" src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/124611/0e5b69d0-2bba-a832-1bb6-ae53111c7043.png">
<h2 id="try-to-visualize">Try to visualize</h2>
<p>Now you are ready for visualization.
Let&rsquo;s visualize these sentence vectors using EmbeddingProjector! ! ! ! !</p>
<p><a href="http://projector.tensorflow.org/">http://projector.tensorflow.org/</a></p>
<p>When you access it, the distribution of words in Word2Vec should be displayed as shown below.
<img width="1676" alt="Embedding_projector_-_visualization_of_high-dimensional_data.png" src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/124611/15612157-1bdb-3cac-89d3-57d8580ba7b9.png"></p>
<p>Since I want to see the distribution of my tweets this time, let&rsquo;s use the <code>tweets_vector.tsv</code> and <code>tweets_text.tsv</code> generated earlier as the data source.
Press the Load button and select <code>tweets_vector.tsv</code> for the first and <code>tweets_text.tsv</code> for the second.
<img width="1698" alt="Embedding_projector_-_visualization_of_high-dimensional_data.png" src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/124611/d7efc1fb-26e1-06a7-6d12-201be9f774f1.png"></p>
<img width="1679" alt="Embedding_projector_-_visualization_of_high-dimensional_data.png" src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/124611/f503fa1d-83cc-df92-f65d-0716339a3f0a.png">
<p>**You should be able to visualize your tweet. **</p>
<p>Originally, the text vector converted by BERT is 768 dimensions, so it cannot be displayed in 3D like this, but since PCA (one of the dimension compression methods) is automatically performed, Is displayed. Not only PCA but also T-SNE and UMAP can be selected for dimensional compression.</p>
<img width="1679" alt="Embedding_projector_-_visualization_of_high-dimensional_data.png" src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/124611/af810f7a-4846-d5b5-dc77-df8035392e38.png">
<p>If you select one point, a sentence similar to that sentence will be displayed. This time, the top 10 similarity is displayed.</p>
<img width="1138" alt="Screenshots 2019-12-12 1.19.36.png" src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/124611/d7ad961f-9a1e-f74c-4faf-959464cfcfe5.png">
<p>**Sentences related to study and papers have been selected as similar sentences! ! It seems that the similarity of sentences can be calculated reasonably well. **</p>
<p>If you drop it in two dimensions, it will have such a distribution.
<img width="1059" alt="Embedding_projector_-__visualization_of_high-dimensional_data.png" src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/124611/f791d9c9-2ad6-140f-dfa8-53bb992e1db4.png"></p>
<p>**Originally, the 768-dimensional one is forcibly dropped into the 2nd and 3rd dimensions, so the distribution is not clearly divided. **
** As a result of doing PCA, the explanation was 25% in the top 2 positions, 30% in 3rd place, and finally 50% using 10th place, so this is also a convincing result. **</p>
<h2 id="summary">Summary</h2>
<p>This time, it was a hands-on where I tried to visualize my tweet while understanding BERT.
By visualizing the distribution of your own tweets, you will be able to understand to some extent what kind of tweets you are making. In my case, I was biased towards programming and negative tweets, and I found that &ldquo;I&rsquo;m such a person from an objective perspective&hellip;&rdquo;.
I regret that I didn&rsquo;t have time to use sentence pieces, so I&rsquo;ll give you an article using sentence pieces again.</p>
<p>Natural language processing often does not give easy-to-understand results, but visualizing it gives us new discoveries. If you find this article interesting, please try it out with your own tweets.</p>
<p>Also, if you follow <a href="https://twitter.com/yuki_wtz">twitter</a>, you will mutter about natural language processing and recommendation system, and write articles.</p>
<p>That&rsquo;s it for my article.
Tomorrow is @Mahito&rsquo;s article. looking forward to!</p>

    </article>
  </div>

  
  
  
  <script>
  window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
  ga('create', 'UA-169005401-1', 'auto');
  ga('send', 'pageview');
  </script>
  <script async src='https://www.google-analytics.com/analytics.js'></script>
  

  

  
<link rel="stylesheet" type="text/css" href="//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.0/cookieconsent.min.css" />
<script src="//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.0/cookieconsent.min.js"></script>
<script>
window.addEventListener("load", function(){
window.cookieconsent.initialise({
  "palette": {
    "popup": {
      "background": "#216942",
      "text": "#b2d192"
    },
    "button": {
      "background": "#afed71"
    }
  }
})});
</script>

</body>

</html>
