<!doctype html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  
  <meta name="generator" content="Hugo 0.72.0" />

  
  <meta name="description" content="Some description">
  

  
  <link rel="apple-touch-icon" sizes="180x180" href="https://memotut.com/apple-touch-icon.png">

  
  <link rel="icon" type="image/png" sizes="32x32" href="https://memotut.com/favicon-32x32.png">

  
  <link rel="icon" type="image/png" sizes="16x16" href="https://memotut.com/favicon-16x16.png">

  
  <link rel="manifest" href="https://memotut.com/site.webmanifest">

  
  <link rel="mask-icon" href="https://memotut.com/safari-pinned-tab.svg" color="#5bbad5">

  <meta name="msapplication-TileColor" content="#da532c">

  <meta name="theme-color" content="#ffffff">

  
  <link rel="stylesheet" href="https://memotut.com/css/bootstrap.min.css" />

  
  <title>[Python] Introduction to effect verification was written in Python | Memo Tut</title>
  

  <style>
body {
  min-width: 300px;
}

.custom-navbar {
  margin-bottom: 1em;
  height: 60px;
}

.custom-navbar a {
  display: inline-block; 
  padding: 18px 0;
  margin-right: 1em; 
  font-weight: bold; 
}

.custom-navbar a:hover,
.custom-navbar a:focus {
  text-decoration: none; 
}

@media print {
  .custom-navbar {
    display: none;
  }
}

article {
  padding-bottom: 1em;
}

img {
  max-width: 100%;
}


body {
  background-color: #fff;
}



body {
  color: #212529;
}



a {
  color: #007bff;
}



a:hover,
a:focus {
  color: #0056b3;
}



.custom-navbar {
  background-color: #212529;
}



.custom-navbar a {
  color: rgba(255, 255, 255, 0.75);
}



.custom-navbar a:hover,
.custom-navbar a:focus {
  color: rgba(255, 255, 255, 1);
}



.container {
  max-width: 800px;
}



pre {
  display: block;
  padding: 9.5px;
  word-break: break-all;
  word-wrap: break-word;
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 4px;
}

pre code {
  padding: 0;
  font-size: inherit;
  color: inherit; 
  white-space: pre-wrap;
  background-color: transparent;
  border: none;
  border-radius: 0;
}

code {
  padding: 2px 4px;
  color: inherit; 
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 4px;
  font-size: .9em;
}



blockquote,
.blockquote {
  padding: 10px 20px;
  margin: 0 0 20px;
  font-size: 1em;
  border-left: 5px solid #6c757d;
}

</style>
</head>

<body>
  <nav class="custom-navbar">
  <div class="container">
    
    <a href="/">Posts</a>
    
    <a href="/tags/">Tags</a>
    
    <a href="/about/">About</a>
    
    <a href="/index.xml">RSS</a>
    
  </div>
</nav>
  
  <div class="container">
    <article>
      <h1>[Python] Introduction to effect verification was written in Python</h1>
<p>
  <small class="text-secondary">
  
  
  Jan 3, 2020
  </small>
  

<small><code><a href="https://memotut.com/tags/python">Python</a></code></small>


<small><code><a href="https://memotut.com/tags/causal-inference"> causal inference</a></code></small>

</p>
<pre><code># TL;DR
</code></pre>
<ul>
<li>(Almost) reproduced the R source code of the book &ldquo;Introduction to Effect Verification: Basics of Causal Reasoning/Econometrics for Correct Comparison&rdquo; in Python.</li>
<li><a href="https://github.com/nekoumei/cibook-python">https://github.com/nekoumei/cibook-python</a></li>
<li>In this article, the library is mainly for R! But I will explain the parts that are not so in Python</li>
</ul>
<h1 id="introduction-of-books">Introduction of books</h1>
<p><a href="https://www.amazon.co.jp/dp/B0834JN23Y">https://www.amazon.co.jp/dp/B0834JN23Y</a>
<img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/203205/bc09efdd-8abc-0ae8-20fc-defe4af5e4b8.png" alt="image.png"></p>
<p>I think it&rsquo;s quick to see it because the table of contents is on Amazon. ..
It&rsquo;s a very good book. How to get rid of bias to make accurate decisions? Various causal reasoning methods (propensity score/DiD/RDD, etc.) are introduced with R source code implementation.
How can we use causal reasoning to verify the effectiveness of real problems throughout? It was written from the viewpoint, and I felt that it was very practical.</p>
<h1 id="written-in-python">Written in Python</h1>
<p><a href="https://github.com/nekoumei/cibook-python">https://github.com/nekoumei/cibook-python</a>
I have created a Jupyter Notebook corresponding to the original R source code (<a href="https://github.com/ghmagazine/cibook">https://github.com/ghmagazine/cibook</a> ).
In addition, for this Python implementation, the graph visualization library uses plotly.express except for some parts.
Plotly graphs cannot be displayed with Notebook rendering on Github, so if you want to check online, please check the Github Pages in the README. (Displaying html under images)
Regarding plotly.express, the following articles will be very helpful in Japanese.
<a href="https://qiita.com/hanon/items/d8cbe25aa8f3a9347b0b">Summary of basic drawing method of PlotlyExpress, the de facto standard of Python drawing library in the Reiwa era</a></p>
<p>#Key points in writing in Python</p>
<h2 id="regression-analysis">regression analysis</h2>
<p>I use OLS and WLS from statsmodels.</p>
<p>(Excerpt from ch2_regression.ipynb)</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e">## Multiple regression with biased data</span>
y <span style="color:#f92672">=</span> biased_df<span style="color:#f92672">.</span>spend
In <span style="color:#75715e"># R lm, categorical variables are automatically converted to dummy variables, so reproduce them</span>
X <span style="color:#f92672">=</span> pd<span style="color:#f92672">.</span>get_dummies(biased_df[[<span style="color:#e6db74">&#39;treatment&#39;</span>,<span style="color:#e6db74">&#39;recency&#39;</span>,<span style="color:#e6db74">&#39;channel&#39;</span>,<span style="color:#e6db74">&#39;history&#39;</span>]], columns<span style="color:#f92672">=</span>[<span style="color:#e6db74">&#39;channel&#39;</span>], drop_first<span style="color:#f92672">=</span>True)
X <span style="color:#f92672">=</span> sm<span style="color:#f92672">.</span>add_constant(X)
results <span style="color:#f92672">=</span> sm<span style="color:#f92672">.</span>OLS(y, X)<span style="color:#f92672">.</span>fit()
nonrct_mreg_coef <span style="color:#f92672">=</span> results<span style="color:#f92672">.</span>summary()<span style="color:#f92672">.</span>tables[<span style="color:#ae81ff">1</span>]
nonrct_mreg_coef
</code></pre></div><p>There are two differences here with the R lm function.
(1) It seems that lm automatically converts categorical variables into dummy variables. Since sm.OLS cannot do this automatically, I am using <code>pd.get_dummies()</code>.
â‘¡ With lm, the bias term is automatically added. This is also added manually. (Reference: <a href="https://blog.amedama.jp/entry/2016/12/23/193452">Statistics: Multiple regression analysis using Python and R</a>)</p>
<h2 id="read-data-set-in-rdata-format">Read data set in RData format</h2>
<p>Some datasets used in the implementation are published as R packages such as <a href="https://github.com/itamarcaspi/experimentdatar">experimentdatar</a>, or datasets published in RData format. there is.
~~ There is no way to read these in Python only. ~~ (Please let me know if you have any)
In the comment section, we were taught by upura-san!
<a href="https://qiita.com/nekoumei/items/648726e89d05cba6f432#comment-0ea9751e3f01b27b0adb">https://qiita.com/nekoumei/items/648726e89d05cba6f432#comment-0ea9751e3f01b27b0adb</a>
The .rda file is a package called rdata and can be read without R.
(Excerpt from ch2_voucher.ipynb)</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">parsed <span style="color:#f92672">=</span> rdata<span style="color:#f92672">.</span>parser<span style="color:#f92672">.</span>parse_file(<span style="color:#e6db74">&#39;../data/vouchers.rda&#39;</span>)
converted <span style="color:#f92672">=</span> rdata<span style="color:#f92672">.</span>conversion<span style="color:#f92672">.</span>convert(parsed)
vouchers <span style="color:#f92672">=</span> converted[<span style="color:#e6db74">&#39;vouchers&#39;</span>]
</code></pre></div><p>It is as follows when reading the data using R via rpy2 and converting it to pandas DataFrame.
(Excerpt from ch2_voucher.ipynb)</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">from</span> rpy2.robjects <span style="color:#f92672">import</span> r, pandas2ri
<span style="color:#f92672">from</span> rpy2.robjects.packages <span style="color:#f92672">import</span> importr
pandas2ri<span style="color:#f92672">.</span>activate()
experimentdatar <span style="color:#f92672">=</span> importr(<span style="color:#e6db74">&#39;experimentdatar&#39;</span>)
vouchers <span style="color:#f92672">=</span> r[<span style="color:#e6db74">&#39;vouchers&#39;</span>]
</code></pre></div><p>As described in ch2_voucher.ipynb, R packages are installed in advance in the R interactive environment.
Also, the dataset used in ch3_lalonde.ipynb is a .dta file.
This can be read by pandas read_stata().
(Excerpt from ch3_lalonde.ipynb)</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">cps1_data <span style="color:#f92672">=</span> pd<span style="color:#f92672">.</span>read_stata(<span style="color:#e6db74">&#39;https://users.nber.org/~rdehejia/data/cps_controls.dta&#39;</span>)
cps3_data <span style="color:#f92672">=</span> pd<span style="color:#f92672">.</span>read_stata(<span style="color:#e6db74">&#39;https://users.nber.org/~rdehejia/data/cps_controls3.dta&#39;</span>)
nswdw_data <span style="color:#f92672">=</span> pd<span style="color:#f92672">.</span>read_stata(<span style="color:#e6db74">&#39;https://users.nber.org/~rdehejia/data/nsw_dw.dta&#39;</span>)
</code></pre></div><h2 id="propensity-score-matching-nearest-neighbor-matching">Propensity score matching (nearest neighbor matching)</h2>
<p>There are several matching methods for propensity score matching, but the book seems to use MatchIt&rsquo;s nearest neighbor matching.
I didn&rsquo;t seem to have a good library for Python, so I implemented it in a straightforward manner.
(Excerpt from ch3_pscore.ipynb)</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">get_matched_dfs_using_propensity_score</span>(X, y, random_state<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>):
    <span style="color:#75715e"># Calculate propensity score</span>
    ps_model <span style="color:#f92672">=</span> LogisticRegression(solver<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;lbfgs&#39;</span>, random_state<span style="color:#f92672">=</span>random_state)<span style="color:#f92672">.</span>fit(X, y)
    ps_score <span style="color:#f92672">=</span> ps_model<span style="color:#f92672">.</span>predict_proba(X)[:, <span style="color:#ae81ff">1</span>]
    all_df <span style="color:#f92672">=</span> pd<span style="color:#f92672">.</span>DataFrame({<span style="color:#e6db74">&#39;treatment&#39;</span>: y,<span style="color:#e6db74">&#39;ps_score&#39;</span>: ps_score})
    treatments <span style="color:#f92672">=</span> all_df<span style="color:#f92672">.</span>treatment<span style="color:#f92672">.</span>unique()
    <span style="color:#66d9ef">if</span> len(treatments) <span style="color:#f92672">!=</span> <span style="color:#ae81ff">2</span>:
        <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#39;You can only match 2 groups. Be sure to express 2 groups with [0, 1].&#39;</span>)
        <span style="color:#66d9ef">raise</span> <span style="color:#a6e22e">ValueError</span>
    <span style="color:#75715e"># treatment == 1 is group 1, treatment == 0 is group 2. It should be an estimate of ATT because it extracts group2 that matches group1</span>
    group1_df <span style="color:#f92672">=</span> all_df[all_df<span style="color:#f92672">.</span>treatment<span style="color:#f92672">==</span><span style="color:#ae81ff">1</span>]<span style="color:#f92672">.</span>copy()
    group1_indices <span style="color:#f92672">=</span> group1_df<span style="color:#f92672">.</span>index
    group1_df <span style="color:#f92672">=</span> group1_df<span style="color:#f92672">.</span>reset_index(drop<span style="color:#f92672">=</span>True)
    group2_df <span style="color:#f92672">=</span> all_df[all_df<span style="color:#f92672">.</span>treatment<span style="color:#f92672">==</span><span style="color:#ae81ff">0</span>]<span style="color:#f92672">.</span>copy()
    group2_indices <span style="color:#f92672">=</span> group2_df<span style="color:#f92672">.</span>index
    group2_df <span style="color:#f92672">=</span> group2_df<span style="color:#f92672">.</span>reset_index(drop<span style="color:#f92672">=</span>True)

    <span style="color:#75715e"># Threshold is the standard deviation * 0.2 of the overall propensity score</span>
    threshold <span style="color:#f92672">=</span> all_df<span style="color:#f92672">.</span>ps_score<span style="color:#f92672">.</span>std() <span style="color:#f92672">*</span> <span style="color:#ae81ff">0.2</span>

    matched_group1_dfs <span style="color:#f92672">=</span> []
    matched_group2_dfs <span style="color:#f92672">=</span> []
    _group1_df <span style="color:#f92672">=</span> group1_df<span style="color:#f92672">.</span>copy()
    _group2_df <span style="color:#f92672">=</span> group2_df<span style="color:#f92672">.</span>copy()

    <span style="color:#66d9ef">while</span> True:
        Find <span style="color:#f92672">and</span> match <span style="color:#ae81ff">1</span> nearest point <span style="color:#66d9ef">with</span> <span style="color:#75715e"># Nearest Neighbors</span>
        neigh <span style="color:#f92672">=</span> NearestNeighbors(n_neighbors<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
        neigh<span style="color:#f92672">.</span>fit(_group1_df<span style="color:#f92672">.</span>ps_score<span style="color:#f92672">.</span>values<span style="color:#f92672">.</span>reshape(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>))
        distances, indices <span style="color:#f92672">=</span> neigh<span style="color:#f92672">.</span>kneighbors(_group2_df<span style="color:#f92672">.</span>ps_score<span style="color:#f92672">.</span>values<span style="color:#f92672">.</span>reshape(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>))
        <span style="color:#75715e"># Remove duplicate points</span>
        distance_df <span style="color:#f92672">=</span> pd<span style="color:#f92672">.</span>DataFrame({<span style="color:#e6db74">&#39;distance&#39;</span>: distances<span style="color:#f92672">.</span>reshape(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>),<span style="color:#e6db74">&#39;indices&#39;</span>: indices<span style="color:#f92672">.</span>reshape(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>)})
        distance_df<span style="color:#f92672">.</span>index <span style="color:#f92672">=</span> _group2_df<span style="color:#f92672">.</span>index
        distance_df <span style="color:#f92672">=</span> distance_df<span style="color:#f92672">.</span>drop_duplicates(subset<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;indices&#39;</span>)
        <span style="color:#75715e"># Delete records that exceed threshold</span>
        distance_df <span style="color:#f92672">=</span> distance_df[distance_df<span style="color:#f92672">.</span>distance <span style="color:#f92672">&lt;</span>threshold]
        <span style="color:#66d9ef">if</span> len(distance_df) <span style="color:#f92672">==</span> <span style="color:#ae81ff">0</span>:
            <span style="color:#66d9ef">break</span>
        <span style="color:#75715e"># Extract and delete matching records</span>
        group1_matched_indices <span style="color:#f92672">=</span> _group1_df<span style="color:#f92672">.</span>iloc[distance_df[<span style="color:#e6db74">&#39;indices&#39;</span>]]<span style="color:#f92672">.</span>index<span style="color:#f92672">.</span>tolist()
        group2_matched_indices <span style="color:#f92672">=</span> distance_df<span style="color:#f92672">.</span>index
        matched_group1_dfs<span style="color:#f92672">.</span>append(_group1_df<span style="color:#f92672">.</span>loc[group1_matched_indices])matched_group2_dfs<span style="color:#f92672">.</span>append(_group2_df<span style="color:#f92672">.</span>loc[group2_matched_indices])
        _group1_df <span style="color:#f92672">=</span> _group1_df<span style="color:#f92672">.</span>drop(group1_matched_indices)
        _group2_df <span style="color:#f92672">=</span> _group2_df<span style="color:#f92672">.</span>drop(group2_matched_indices)

    <span style="color:#75715e">#Return matching records</span>
    group1_df<span style="color:#f92672">.</span>index <span style="color:#f92672">=</span> group1_indices
    group2_df<span style="color:#f92672">.</span>index <span style="color:#f92672">=</span> group2_indices
    matched_df <span style="color:#f92672">=</span> pd<span style="color:#f92672">.</span>concat([
        group1_df<span style="color:#f92672">.</span>iloc[pd<span style="color:#f92672">.</span>concat(matched_group1_dfs)<span style="color:#f92672">.</span>index],
        group2_df<span style="color:#f92672">.</span>iloc[pd<span style="color:#f92672">.</span>concat(matched_group2_dfs)<span style="color:#f92672">.</span>index]
    ])<span style="color:#f92672">.</span>sort_index()
    matched_indices <span style="color:#f92672">=</span> matched_df<span style="color:#f92672">.</span>index

    <span style="color:#66d9ef">return</span> X<span style="color:#f92672">.</span>loc[matched_indices], y<span style="color:#f92672">.</span>loc[matched_indices]
</code></pre></div><p>It is repeated to match 1 point in the treatment group with 1 point in the control group that has the closest tendency score. At that time, a threshold is set so that only pairs whose distance is within 0.2 times std are extracted.
The details about this were detailed in <a href="https://www.slideshare.net/okumurayasuyuki/ss-43780294">Concept of trend score and its practice</a>.
I am comparing with the matching result by MatchIt in ch3_pscore.ipynb, but it is not a perfect match. It is generally good because the conclusion does not change and the covariates are well balanced (see below).
Again, since there is no convenient library like R&rsquo;s cobalt love.plot(), I visualize it myself.
(From <a href="https://nekoumei.github.io/cibook-python/images/ch3_plot1.html">images/ch3_plot1.html</a>)
![Screenshot 2020-01-03 16.37.23.png](<a href="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/203205/90c32fef-08f1-a568-b9a0-(5778d9cacc28.png)">https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/203205/90c32fef-08f1-a568-b9a0-(5778d9cacc28.png)</a></p>
<h2 id="inverse-probability-weighted-estimation-ipw">Inverse probability weighted estimation (IPW)</h2>
<p>The good thing about IPW is its simple implementation.
This is also compared with WeightIt, but it seems to be almost correct.
(Excerpt from ch3_pscore.ipynb)</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">get_ipw</span>(X, y, random_state<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>):
    <span style="color:#75715e"># Calculate propensity score</span>
    ps_model <span style="color:#f92672">=</span> LogisticRegression(solver<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;lbfgs&#39;</span>, random_state<span style="color:#f92672">=</span>random_state)<span style="color:#f92672">.</span>fit(X, y)
    ps_score <span style="color:#f92672">=</span> ps_model<span style="color:#f92672">.</span>predict_proba(X)[:, <span style="color:#ae81ff">1</span>]
    all_df <span style="color:#f92672">=</span> pd<span style="color:#f92672">.</span>DataFrame({<span style="color:#e6db74">&#39;treatment&#39;</span>: y,<span style="color:#e6db74">&#39;ps_score&#39;</span>: ps_score})
    treatments <span style="color:#f92672">=</span> all_df<span style="color:#f92672">.</span>treatment<span style="color:#f92672">.</span>unique()
    <span style="color:#66d9ef">if</span> len(treatments) <span style="color:#f92672">!=</span> <span style="color:#ae81ff">2</span>:
        <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#39;You can only match 2 groups. Be sure to express 2 groups with [0, 1].&#39;</span>)
        <span style="color:#66d9ef">raise</span> <span style="color:#a6e22e">ValueError</span>
    <span style="color:#75715e"># treatment == 1 is group 1, treatment == 0 is group 2.</span>
    group1_df <span style="color:#f92672">=</span> all_df[all_df<span style="color:#f92672">.</span>treatment<span style="color:#f92672">==</span><span style="color:#ae81ff">1</span>]<span style="color:#f92672">.</span>copy()
    group2_df <span style="color:#f92672">=</span> all_df[all_df<span style="color:#f92672">.</span>treatment<span style="color:#f92672">==</span><span style="color:#ae81ff">0</span>]<span style="color:#f92672">.</span>copy()
    group1_df[<span style="color:#e6db74">&#39;weight&#39;</span>] <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span> <span style="color:#f92672">/</span> group1_df<span style="color:#f92672">.</span>ps_score
    group2_df[<span style="color:#e6db74">&#39;weight&#39;</span>] <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span> <span style="color:#f92672">/</span> (<span style="color:#ae81ff">1</span><span style="color:#f92672">-</span>group2_df<span style="color:#f92672">.</span>ps_score)
    weights <span style="color:#f92672">=</span> pd<span style="color:#f92672">.</span>concat([group1_df, group2_df])<span style="color:#f92672">.</span>sort_index()[<span style="color:#e6db74">&#39;weight&#39;</span>]<span style="color:#f92672">.</span>values
    <span style="color:#66d9ef">return</span> weights
</code></pre></div><h2 id="causalimpact">CausalImpact</h2>
<p>As described in ch4_did.ipynb, the following two libraries are used for comparison.</p>
<ul>
<li>dafiti/causalinpact: <a href="https://github.com/dafiti/causalimpact">https://github.com/dafiti/causalimpact</a></li>
<li>tcassou/causal_impact: <a href="https://github.com/tcassou/causal_impact">https://github.com/tcassou/causal_impact</a></li>
</ul>
<p>Since output is beautiful, I usually use dafiti&rsquo;s causalimpact, but it was causal_impact of tcassou that the estimation result was close to the book (R&rsquo;s main family causalimpact).
Both of them seem to estimate with the state space model of statsmodels, but I did not understand the difference in implementation a bit. Please let me know ..
In both cases, the estimation error is much smaller than the R implementation. Maybe it is not so good that there are many covariates.</p>
<h3 id="dafiticausalimpact-plot">dafiti/causalimpact plot</h3>
<p><img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/203205/442ce4a4-36c4-61f2-b919-632f90aae240.png" alt="image.png"></p>
<h3 id="tcassoucausal_impact-plot">tcassou/causal_impact plot</h3>
<p><img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/203205/e2254c1a-dffa-65ec-94d4-a323c507a0f3.png" alt="image.png"></p>
<h2 id="regressive-discontinuous-design-rdd">Regressive discontinuous design (RDD)</h2>
<p>With R, you can run it quickly with rddtools, but not with Python.
First, check the regression equation used in rdd_reg_lm of rddtools.
(Reference: <a href="https://cran.r-project.org/web/packages/rddtools/rddtools.pdf">https://cran.r-project.org/web/packages/rddtools/rddtools.pdf</a> P23)</p>
<pre><code class="language-math" data-lang="math">Y = Î± + Ï„D + Î²_1(X-c) + Î²_2D(X-c) + Îµ
</code></pre><p>As an aside, I was thinking that RDD was a way to create regression models for the left and right cut-off values, and take the difference between the estimated values at the cut-off values. It is represented by a formula. I wonder if they are the same in meaning.
Here, D is a binary variable that has a value of 1 if X is after the cut-off value and 0 if it is before. The coef of D is the value you want to confirm as the effect size.
Also, c is the cut-off value.
Implement based on the above. The implementation also refers to the rddtools source code. (<a href="https://github.com/MatthieuStigler/RDDtools/blob/master/RDDtools/R/model.matrix.RDD.R">https://github.com/MatthieuStigler/RDDtools/blob/master/RDDtools/R/model.matrix.RDD.R</a>)
(Excerpt from ch5_rdd.ipynb)</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">RDDRegression</span>:
<span style="color:#75715e"># Reproduce rdd_reg_lm of R package rddtools</span>
<span style="color:#75715e"># Reference: https://cran.r-project.org/web/packages/rddtools/rddtools.pdf P23</span>
    <span style="color:#66d9ef">def</span> __init__(self, cut_point, degree<span style="color:#f92672">=</span><span style="color:#ae81ff">4</span>):
        self<span style="color:#f92672">.</span>cut_point <span style="color:#f92672">=</span> cut_point
        self<span style="color:#f92672">.</span>degree <span style="color:#f92672">=</span> degree
        
    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">_preprocess</span>(self, X):
        X <span style="color:#f92672">=</span> X<span style="color:#f92672">-</span>threshold_value
        X_poly <span style="color:#f92672">=</span> PolynomialFeatures(degree<span style="color:#f92672">=</span>self<span style="color:#f92672">.</span>degree, include_bias<span style="color:#f92672">=</span>False)<span style="color:#f92672">.</span>fit_transform(X)
        D_df <span style="color:#f92672">=</span> X<span style="color:#f92672">.</span>applymap(<span style="color:#66d9ef">lambda</span> x: <span style="color:#ae81ff">1</span> <span style="color:#66d9ef">if</span> x <span style="color:#f92672">&gt;=</span> <span style="color:#ae81ff">0</span> <span style="color:#66d9ef">else</span> <span style="color:#ae81ff">0</span>)
        X <span style="color:#f92672">=</span> pd<span style="color:#f92672">.</span>DataFrame(X_poly, columns<span style="color:#f92672">=</span>[f<span style="color:#e6db74">&#39;X^{i+1}&#39;</span> <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(X_poly<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">1</span>])])
        X[<span style="color:#e6db74">&#39;D&#39;</span>] <span style="color:#f92672">=</span> D_df
        <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(X_poly<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">1</span>]):
            X[f<span style="color:#e6db74">&#39;D_X^{i+1}&#39;</span>] <span style="color:#f92672">=</span> X_poly[:, i] <span style="color:#f92672">*</span> X[<span style="color:#e6db74">&#39;D&#39;</span>]
        <span style="color:#66d9ef">return</span> X
    
    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">fit</span>(self, X, y):
        X <span style="color:#f92672">=</span> X<span style="color:#f92672">.</span>copy()
        X <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>_preprocess(X)
        self<span style="color:#f92672">.</span>X <span style="color:#f92672">=</span> X
        self<span style="color:#f92672">.</span>y <span style="color:#f92672">=</span> y
        X <span style="color:#f92672">=</span> sm<span style="color:#f92672">.</span>add_constant(X)
        self<span style="color:#f92672">.</span>model <span style="color:#f92672">=</span> sm<span style="color:#f92672">.</span>OLS(y, X)
        self<span style="color:#f92672">.</span>results <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>model<span style="color:#f92672">.</span>fit()
        coef <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>results<span style="color:#f92672">.</span>summary()<span style="color:#f92672">.</span>tables[<span style="color:#ae81ff">1</span>]
        self<span style="color:#f92672">.</span>coef <span style="color:#f92672">=</span> pd<span style="color:#f92672">.</span>read_html(coef<span style="color:#f92672">.</span>as_html(), header<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>, index_col<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)[<span style="color:#ae81ff">0</span>]
        
    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">predict</span>(self, X):
        X <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>_preprocess(X)
        X <span style="color:#f92672">=</span> sm<span style="color:#f92672">.</span>add_constant(X)
        <span style="color:#66d9ef">return</span> self<span style="color:#f92672">.</span>model<span style="color:#f92672">.</span>predict(self<span style="color:#f92672">.</span>results<span style="color:#f92672">.</span>params, X)
</code></pre></div><p>PolynomialFeatures of scikit-learn is used as a preprocessing for polynomial regression.
(From <a href="https://nekoumei.github.io/cibook-python/images/ch5_plot2_3.html">images/ch5_plot2_3.html</a>)
<img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/203205/c673d2f5-f5b9-c301-7275-ec5f3068f24d.png" alt="newplot (1).png">
I was able to make a good estimate. As you can see from the Notebook, the estimation of the effect size is almost the same as the book. Was good.</p>
<h2 id="nonparametric-rdd-rdestimate">nonparametric RDD (RDestimate)</h2>
<p>I couldn&rsquo;t do it&hellip; It&rsquo;s almost part of the &ldquo;(almost) reproduced in Python&rdquo; at the beginning.RDestimate uses the method of Imbens and Kalyanaraman (2012) Optimal Bandwidth Choice for the Regression Discontinuity Estimator to select the optimal bandwidth, but I&rsquo;m not sure how that bandwidth is estimated.
In ch5_rdd.ipynb, I tried to find the optimal bandwidth for MSE when I changed the bandwidth roughly, but it doesn&rsquo;t seem to work very well.
(From <a href="https://nekoumei.github.io/cibook-python/images/ch5_plot4.html">ch5_plot4.html</a>)
<img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/203205/87caed7a-9e4a-1efb-fc1f-883021bba003.png" alt="newplot (3).png">
Hmm. It seems that it is impossible to predict outside the bandwidth if the bandwidth is narrowed and estimated, but how do you actually do it?
Or maybe I don&rsquo;t have to worry so much because I&rsquo;m only interested in estimating cut-off neighborhoods.</p>
<h1 id="in-conclusion">in conclusion</h1>
<p>If there is something wrong with your understanding or implementation, or something strange, please let me know.
<a href="https://twitter.com/nekoumei">Twitter</a></p>

    </article>
  </div>

  
  
  
  <script>
  window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
  ga('create', 'UA-169005401-1', 'auto');
  ga('send', 'pageview');
  </script>
  <script async src='https://www.google-analytics.com/analytics.js'></script>
  

  

  
<link rel="stylesheet" type="text/css" href="//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.0/cookieconsent.min.css" />
<script src="//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.0/cookieconsent.min.js"></script>
<script>
window.addEventListener("load", function(){
window.cookieconsent.initialise({
  "palette": {
    "popup": {
      "background": "#216942",
      "text": "#b2d192"
    },
    "button": {
      "background": "#afed71"
    }
  }
})});
</script>

</body>

</html>
