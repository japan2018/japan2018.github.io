<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title> NLP on Memo Tut</title>
    <link>https://memotut.com/tags/nlp/</link>
    <description>Recent content in  NLP on Memo Tut</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 10 May 2020 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://memotut.com/tags/nlp/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>[Natural language processing] I tried to visualize the comments of each member in the Slack community</title>
      <link>https://memotut.com/natural-language-processing-i-tried-to-visualize-the-comments-of-each-member-in-the-slack-community-dca47/</link>
      <pubDate>Sun, 10 May 2020 00:00:00 +0000</pubDate>
      
      <guid>https://memotut.com/natural-language-processing-i-tried-to-visualize-the-comments-of-each-member-in-the-slack-community-dca47/</guid>
      <description>## About this article  In this article, I will introduce a method to visualize what each member says in the Slack community with Wordcloud.
The source code can be found here [https://github.com/sota0121/slack-msg-analysis) :octocat:
I would also like to read: [Natural language processing] I tried to visualize the topics raised this week in the Slack community
table of contents  Usage and output example Get message from Slack Pre-processing: table creation/cleaning/morphological analysis/normalization/stopword removal Pre-processing: Extracting important words (tf-idf) Visualization processing with Wordcloud Bonus  *I would like to summarize the preprocessing in another article in the future</description>
    </item>
    
    <item>
      <title>[Natural language processing] I tried to visualize the topics raised this week in the Slack community</title>
      <link>https://memotut.com/natural-language-processing-i-tried-to-visualize-the-topics-raised-this-week-in-the-slack-community-41630/</link>
      <pubDate>Sat, 09 May 2020 00:00:00 +0000</pubDate>
      
      <guid>https://memotut.com/natural-language-processing-i-tried-to-visualize-the-topics-raised-this-week-in-the-slack-community-41630/</guid>
      <description>## About this article  In this article, I will introduce a method of using Wordcloud to visualize what topics were raised within a certain period (here one week) of the Slack community.
The source code can be found here [https://github.com/sota0121/slack-msg-analysis) :octocat:
I would also like to read: [Natural language processing] I tried to visualize the comments of each member in the Slack community
table of contents  Usage and output example Get message from Slack Pre-processing: message mart table creation Pre-treatment: Cleaning Pre-processing: Morphological analysis (Janome) Pre-processing: Normalization Pre-processing: Stop word removal Pre-processing: Extract important words (tf-idf) Visualization processing with Wordcloud Bonus  *I would like to summarize the preprocessing in another article in the future</description>
    </item>
    
    <item>
      <title>Implemented AllenNLP linkage function in Japanese analysis tool Konoha</title>
      <link>https://memotut.com/implemented-allennlp-linkage-function-in-japanese-analysis-tool-konoha-f1d29/</link>
      <pubDate>Sun, 03 May 2020 00:00:00 +0000</pubDate>
      
      <guid>https://memotut.com/implemented-allennlp-linkage-function-in-japanese-analysis-tool-konoha-f1d29/</guid>
      <description>### Introduction  &amp;lt;img src=&amp;quot;https://github-link-card.s3.ap-northeast-1.amazonaws.com/himkt/konoha.png&amp;quot;width=&amp;quot;460px&amp;quot;&amp;gt;
We are developing a morphological analysis library called konoha. We have implemented the AllenNLP integration for this library, so this time we will introduce it. By using this function, Japanese text can be passed to the allennlp train command without any preprocessing such as segmentation.
AllenNLP AllenNLP is a natural language processing library developed by Allen Institute for Artificial Intelligence. AllenNLp is a very powerful tool, but if you want to handle Japanese data, it may be necessary to perform pre-processing that performs morphological analysis beforehand.</description>
    </item>
    
    <item>
      <title>Easily learn 100 language processing knocks 2020 with Google Colaboratory</title>
      <link>https://memotut.com/easily-learn-100-language-processing-knocks-2020-with-google-colaboratory-e4694/</link>
      <pubDate>Sat, 18 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>https://memotut.com/easily-learn-100-language-processing-knocks-2020-with-google-colaboratory-e4694/</guid>
      <description># 100 language processing What is knock 2020  100 language processing knock 2020
It is a collection of problems on the above site. I quote from the site.
 100 Language Processing Knock is a collection of problems aimed at learning programming, data analysis, and research skills in a fun way while tackling practical and exciting challenges.
 I am very grateful to be able to publish such wonderful things for free.</description>
    </item>
    
    <item>
      <title>100 language processing knocks 2020 version released! What has changed</title>
      <link>https://memotut.com/100-language-processing-knocks-2020-version-released-what-has-changed-3c179/</link>
      <pubDate>Wed, 08 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>https://memotut.com/100-language-processing-knocks-2020-version-released-what-has-changed-3c179/</guid>
      <description>#Introduction  100 knocks of language processing that have been popular for a long time as a collection of problems that you can enjoy learning the basics of natural language processing, its 2020 version was released on 4/6 ! This is the first revision in five years. 2015 version but those who are interested, those who feel sorry that the 15th edition Qiita article is no longer useful, 15th edition For the person who was doing halfway, but the 20-year edition is coming out and it seems to be heartbreaking, I will summarize what has changed.</description>
    </item>
    
    <item>
      <title>Challenges for large-scale models such as BERT</title>
      <link>https://memotut.com/challenges-for-large-scale-models-such-as-bert-e72d8/</link>
      <pubDate>Tue, 25 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>https://memotut.com/challenges-for-large-scale-models-such-as-bert-e72d8/</guid>
      <description># Breakthrough model of natural language processing-BERT  BERT[BidirectionalEncoderRepresentationsfromTransformers]wasannouncedtoGoogle&amp;rsquo;steaminthefallof2018.Thisistheresultoflearningamodelinarevolutionarywayforaverylargenetworkwithalargeamountofdatausingatransformerarchitecture.PleaserefertoThisarticle for learning method and accuracy.
In the field of machine learning where open source and free exchange of information are important, new ideas are published as papers at arxiv.org, and models are shared on github. Doing is the basic way. As soon as new ideas are published, they can be referenced and reused by machine learning researchers and development teams around the world.</description>
    </item>
    
    <item>
      <title>Introduction of ALBERT (using MeCab&#43;Sentencepiece) model that learned large-scale Japanese business news corpus</title>
      <link>https://memotut.com/introduction-of-albert-using-mecab-sentencepiece-model-that-learned-large-scale-japanese-business-news-corpus-b41dc/</link>
      <pubDate>Mon, 17 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>https://memotut.com/introduction-of-albert-using-mecab-sentencepiece-model-that-learned-large-scale-japanese-business-news-corpus-b41dc/</guid>
      <description>#Introduction  Previously, Japanese BERT pre-learned modelandXLNetpre-learnedmodel It is the stockmark Morinaga who posted the introduction articles such as. Thank you for reading the article on model release to many people.
This time, we will release the pre-learned Japanese model of ALBERT. Now, while various pre-learned models have been proposed, why do you publish ALBERT Japanese model?ALBERT is not just SOTA as described as A Lite BERT , Because it is a model that makes the BERT lighter while maintaining and improving accuracy.</description>
    </item>
    
    <item>
      <title>A story of trying to reproduce Katsuno Isono who does not react to inconvenient things by natural language processing.</title>
      <link>https://memotut.com/a-story-of-trying-to-reproduce-katsuno-isono-who-does-not-react-to-inconvenient-things-by-natural-language-processing.-57eb5/</link>
      <pubDate>Sun, 16 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>https://memotut.com/a-story-of-trying-to-reproduce-katsuno-isono-who-does-not-react-to-inconvenient-things-by-natural-language-processing.-57eb5/</guid>
      <description>#Introduction  COTOHA API is a Japanese natural language processing API. It is an API that has various functions and is quite fun to use even a free account can be used 1000/day.
COTOHA API
By the way, do you know Isono-kun? Yes, this is Katsuno Isono from a certain national anime.
The name of this Isono-kun is called something in the play, but when it&amp;rsquo;s inconvenient, he doesn&amp;rsquo;t hear or reacts so-called &amp;ldquo;human-like&amp;rdquo;.</description>
    </item>
    
    <item>
      <title>Whether the COTOHA API can understand Manzai</title>
      <link>https://memotut.com/whether-the-cotoha-api-can-understand-manzai-2b81b/</link>
      <pubDate>Sat, 15 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>https://memotut.com/whether-the-cotoha-api-can-understand-manzai-2b81b/</guid>
      <description>#Introduction  This article has been entered in Qiita x COTOHA API Present Project.
What is #COTOHA API? It is a service provided by NTT Communications so that APIs can easily use natural language processing technology and speech recognition/synthesis technology that make use of the research results of the NTT Group for over 40 years.
Natural language processing such as syntactic analysis and proper expression extraction, and characteristic &amp;ldquo;summary&amp;rdquo; and &amp;ldquo;user attribute estimation&amp;rdquo; can also be used.</description>
    </item>
    
    <item>
      <title>Youtube live It analyzes the sentiment of the comment and judges the emo part from Youtube LIve.</title>
      <link>https://memotut.com/youtube-live-it-analyzes-the-sentiment-of-the-comment-and-judges-the-emo-part-from-youtube-live.-fa198/</link>
      <pubDate>Fri, 14 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>https://memotut.com/youtube-live-it-analyzes-the-sentiment-of-the-comment-and-judges-the-emo-part-from-youtube-live.-fa198/</guid>
      <description># TL;DR  It is a suggestion and experiment that it is possible to extract &amp;ldquo;emo&amp;rdquo; and &amp;ldquo;highlight&amp;rdquo; with better accuracy than just the number of comments per hour by performing sentiment analysis of comments on Youtube live. ..
In fact, we were able to get the result that the accuracy is likely to improve compared to just looking at the number of comments.
#Introduction
Last time, I borrowed COTOHA API and tried to think freely, not practical Article.</description>
    </item>
    
    <item>
      <title>Miya is not enough for our communication.</title>
      <link>https://memotut.com/miya-is-not-enough-for-our-communication.-e63f6/</link>
      <pubDate>Thu, 13 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>https://memotut.com/miya-is-not-enough-for-our-communication.-e63f6/</guid>
      <description>#TL;DR  By using the COTOHA API, we have proposed a poem that is close to what you want to convey from the Hyakunin Isshu. As a gateway to more &amp;ldquo;elegant&amp;rdquo; communication, you can bridge between engineers and non-engineers. (May be)
| What I want to convey | Proposed Waka | Meaning | |&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;- &amp;mdash;&amp;mdash;&amp;mdash;|&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;- &amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;|&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;- &amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;ndash; &amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;ndash; &amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;| | I feel very lonely to hear that I will be transferred.</description>
    </item>
    
    <item>
      <title>Me, NER, Flair</title>
      <link>https://memotut.com/me-ner-flair-efda0/</link>
      <pubDate>Tue, 24 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://memotut.com/me-ner-flair-efda0/</guid>
      <description>This article is on the 23rd day of [MYJLab Advent Calendar](https://qiita.com/advent-calendar/2019/myjlab).  Introduction Hello. This is marutaku from MYJLab m1. This time, I had the opportunity to use a technique called proper expression extraction, so I would like to introduce the library used at that time.
About proper expression extraction (NER) Named Entity Recognition is a technology that acquires proper nouns such as company names and place names with labels such as company and place names.</description>
    </item>
    
    <item>
      <title>I made a library that divides Japanese sentences nicely</title>
      <link>https://memotut.com/i-made-a-library-that-divides-japanese-sentences-nicely-35359/</link>
      <pubDate>Mon, 16 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://memotut.com/i-made-a-library-that-divides-japanese-sentences-nicely-35359/</guid>
      <description>#Introduction  Recently, the development of natural language processing technology has been remarkable and is being applied in various fields. Although I often do business using natural language processing technology and AI, the most troublesome (but important) work is related to various preprocessing.
The main pre-processing that you will perform for most tasks are:
 Cleaning  Removes noise in text such as HTML tags and symbols   Normalization  Full-width/half-width and unification of capital letters and small letters   ** sentence segmentation **  Detects sentence breaks and splits   Wordization  Split sentence into a sequence of words   Remove stopword  Remove unnecessary words for the task you want to solve    I mainly use Python, but I didn&amp;rsquo;t have a suitable library for ** Japanese sentence breaks **, and I had to write similar code every time.</description>
    </item>
    
    <item>
      <title>Hands-on to visualize your tweet while understanding BERT</title>
      <link>https://memotut.com/hands-on-to-visualize-your-tweet-while-understanding-bert-09fda/</link>
      <pubDate>Thu, 12 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://memotut.com/hands-on-to-visualize-your-tweet-while-understanding-bert-09fda/</guid>
      <description>This article is the 13th day article of [NTT Communications Advent Calendar 2019](https://qiita.com/advent-calendar/2019/nttcom).  Yesterday was @nitky&amp;rsquo;s article, &amp;ldquo;We deal with threat intelligence in a mood&amp;rdquo; (https://qiita.com/nitky/items/ccefd0353b74aa21e357).
#Introduction Hi, my name is yuki uchida, and I belong to the SkyWay team of NTT Communications. This article is an article that visualizes your tweet using the language model BERT used for natural language processing, which was recently applied to Google search. I will write in a hands-on format so that as many people as possible can try it out, so if you are interested, please give it a try.</description>
    </item>
    
    <item>
      <title>Morphological analysis tool installation (MeCab,Juman&#43;&#43;,Janome,GiNZA)</title>
      <link>https://memotut.com/morphological-analysis-tool-installation-mecabjuman-janomeginza-fdd58/</link>
      <pubDate>Mon, 09 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://memotut.com/morphological-analysis-tool-installation-mecabjuman-janomeginza-fdd58/</guid>
      <description>A morphological analysis tool is indispensable for Japanese text processing, but it was troublesome to check the installation method of the tool each time, so the following 4 installations are summarized.   MeCab Juman++ Janome GiNZA  Installation environment: Ubuntu 18.04
MeCab Quoted from official site
 MeCab is an open source morphological analysis engine developed through the Joint Research Unit Project of Kyoto University Graduate School of Informatics-Nippon Telegraph and Telephone Corporation, Communication Science Laboratories.</description>
    </item>
    
    <item>
      <title>I tried MindMeld for the first time</title>
      <link>https://memotut.com/i-tried-mindmeld-for-the-first-time-9e9d8/</link>
      <pubDate>Wed, 04 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://memotut.com/i-tried-mindmeld-for-the-first-time-9e9d8/</guid>
      <description>This article was posted as the 10th day of the Cisco Systems Japan Advent Calendar 2019 by Cisco colleagues.  Click here for past calendars 2017 edition: https://qiita.com/advent-calendar/2017/cisco
 -&amp;gt; Distribute policy tag with LISP, tried TrustSec with CML/VIRL 2018 version: https://qiita.com/advent-calendar/2018/cisco -&amp;gt; I tried using Cisco&amp;rsquo;s SaaS monitoring and behavior detection tool for multi-cloud environment monitoring  This year, the 3rd year since joining the company (Advent Calendar has also participated for 3 consecutive years!</description>
    </item>
    
    <item>
      <title>I made a library konoha to switch the tokenizer to a nice one.</title>
      <link>https://memotut.com/i-made-a-library-konoha-to-switch-the-tokenizer-to-a-nice-one.-bb9ff/</link>
      <pubDate>Thu, 14 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>https://memotut.com/i-made-a-library-konoha-to-switch-the-tokenizer-to-a-nice-one.-bb9ff/</guid>
      <description># TL; DR  I will introduce konoha, a library for tokenizing sentences. (Formerly tiny_tokenizer) You can use it like ↓. What?
from konoha import WordTokenizer sentence = &amp;#34;I am studying natural language processing&amp;#34; tokenizer = WordTokenizer(&amp;#39;MeCab&amp;#39;) print(tokenizer.tokenize(sentence)) # -&amp;gt; [natural, language, processing, study, study, do, do, masu] tokenizer = WordTokenizer(&amp;#39;Kytea&amp;#39;) print(tokenizer.tokenize(sentence)) # -&amp;gt; [natural, language, processing, study, study, te, i, ma, su] tokenizer = WordTokenizer(&amp;#39;Sentencepiece&amp;#39;, model_path=&amp;#34;data/model.spm&amp;#34;) print(tokenizer.tokenize(sentence)) # -&amp;gt; [, natural, language, processing, studying, doing, doing] Introduction: What is a tokenizer?</description>
    </item>
    
  </channel>
</rss>