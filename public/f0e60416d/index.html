<!doctype html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  
  <meta name="generator" content="Hugo 0.72.0" />

  
  <meta name="description" content="Some description">
  

  
  <link rel="apple-touch-icon" sizes="180x180" href="https://memotut.com/apple-touch-icon.png">

  
  <link rel="icon" type="image/png" sizes="32x32" href="https://memotut.com/favicon-32x32.png">

  
  <link rel="icon" type="image/png" sizes="16x16" href="https://memotut.com/favicon-16x16.png">

  
  <link rel="manifest" href="https://memotut.com/site.webmanifest">

  
  <link rel="mask-icon" href="https://memotut.com/safari-pinned-tab.svg" color="#5bbad5">

  <meta name="msapplication-TileColor" content="#da532c">

  <meta name="theme-color" content="#ffffff">

  
  <link rel="stylesheet" href="https://memotut.com/css/bootstrap.min.css" />

  
  <title>[Python] Deep learning made it easy to see the time-lapse of physical changes | Memo Tut</title>
  

  <style>
body {
  min-width: 300px;
}

.custom-navbar {
  margin-bottom: 1em;
  height: 60px;
}

.custom-navbar a {
  display: inline-block; 
  padding: 18px 0;
  margin-right: 1em; 
  font-weight: bold; 
}

.custom-navbar a:hover,
.custom-navbar a:focus {
  text-decoration: none; 
}

@media print {
  .custom-navbar {
    display: none;
  }
}

article {
  padding-bottom: 1em;
}

img {
  max-width: 100%;
}


body {
  background-color: #fff;
}



body {
  color: #212529;
}



a {
  color: #007bff;
}



a:hover,
a:focus {
  color: #0056b3;
}



.custom-navbar {
  background-color: #212529;
}



.custom-navbar a {
  color: rgba(255, 255, 255, 0.75);
}



.custom-navbar a:hover,
.custom-navbar a:focus {
  color: rgba(255, 255, 255, 1);
}



.container {
  max-width: 800px;
}



pre {
  display: block;
  padding: 9.5px;
  word-break: break-all;
  word-wrap: break-word;
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 4px;
}

pre code {
  padding: 0;
  font-size: inherit;
  color: inherit; 
  white-space: pre-wrap;
  background-color: transparent;
  border: none;
  border-radius: 0;
}

code {
  padding: 2px 4px;
  color: inherit; 
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 4px;
  font-size: .9em;
}



blockquote,
.blockquote {
  padding: 10px 20px;
  margin: 0 0 20px;
  font-size: 1em;
  border-left: 5px solid #6c757d;
}

</style>
</head>

<body>
  <nav class="custom-navbar">
  <div class="container">
    
    <a href="/">Posts</a>
    
    <a href="/tags/">Tags</a>
    
    <a href="/about/">About</a>
    
    <a href="/index.xml">RSS</a>
    
  </div>
</nav>
  
  <div class="container">
    <article>
      <h1>[Python] Deep learning made it easy to see the time-lapse of physical changes</h1>
<p>
  <small class="text-secondary">
  
  
  Mar 10, 2020
  </small>
  

<small><code><a href="https://memotut.com/tags/python">Python</a></code></small>


<small><code><a href="https://memotut.com/tags/deeplearning"> DeepLearning</a></code></small>


<small><code><a href="https://memotut.com/tags/muscle-training"> Muscle training</a></code></small>


<small><code><a href="https://memotut.com/tags/deep-learning"> Deep learning</a></code></small>


<small><code><a href="https://memotut.com/tags/pytorch"> PyTorch</a></code></small>

</p>
<pre><code>#Introduction
</code></pre>
<p>&ldquo;Self-portraits (body)&rdquo; that many trainees (people who love muscle training) have become accustomed. It&rsquo;s blissful to take a picture of your pumped body after training and look back later. In addition, if you display the captured image in animation like a time lapse, you can understand that muscle growth is more like a hand!
This article uses deep learning to drastically make it easier to see the timelapse of the body.</p>
<h2 id="first-from-the-result">First from the result</h2>
<p>![ezgif.com-optimize (3).gif](<a href="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/22648/830a231f-59aa-a858-914c-ca8923b4f85d.(gif)">https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/22648/830a231f-59aa-a858-914c-ca8923b4f85d.(gif)</a>
Physical changes from 2017/12 to 2020/3</p>
<p>*The image is cropped and compressed due to the data size.</p>
<h1 id="table-of-contents">table of contents</h1>
<ul>
<li><a href="#1Manualcorrection">1. Manual correction</a>
<ul>
<li><a href="#1-1displayasitis">1-1. Display as it is</a></li>
<li><a href="#1-2Fixedposition">1-2. Fixed position</a>
<ul>
<li><a href="#1-2-1Nipplenavelcoordinategivingtool">1-2-1. Nipple navel coordinate giving tool</a></li>
<li><a href="#1-2-2VideoCreation">1-2-2. Video Creation</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#2Automaticcorrectionusingdeeplearning">2. Automatic correction using deep learning</a></li>
<li><a href="#2-1Annotationdatacreation">2-1. Annotation data creation</a></li>
<li><a href="#2-2Learning">2-2. Learning</a></li>
<li><a href="#2-3Applicationtounknownimage">2-3. Application to unknown image</a></li>
<li><a href="#2-4Post-processing">2-4. Post-processing</a>
<ul>
<li><a href="#2-4-1Truncateiftheoutputvalueofeachpixelisbelowthethreshold">2-4-1. Truncate if the output value of each pixel is below the threshold</a></li>
</ul>
</li>
<li><a href="#2-4-2Divideobject">2-4-2. Divide object</a></li>
<li>[2-4-3. If there are 4 or more clusters, select 3 in descending order of area and discard the rest](#2-4-3Incaseof4ormoreclusters,indescendingorderofarea(Selectthreeanddiscardtherest)</li>
<li><a href="#2-4-4Findingthecenterofgravityofeachcluster">2-4-4. Finding the center of gravity of each cluster</a></li>
<li>[2-4-5. Sort in ascending order of x-coordinate of centroid of each cluster (right nipple → navel → left nipple)](#2-4-5Sortinascendingorderofx-coordinateofcentroidofeachclusterRight(Nipplenavelleftnipple)</li>
<li><a href="#2-5result">2-5. Result</a></li>
<li><a href="#Summary">Summary</a></li>
</ul>
<h2 id="overview">Overview</h2>
<p>We created a time lapse from the captured images. However, I am worried about the gap between the images, so I manually corrected it and created a smooth time-lapse. In addition, in order to save the labor of manual work, we performed automatic correction using deep learning.</p>
<h1 id="1-manual-correction">1. Manual correction</h1>
<h2 id="1-1-display-as-is">1-1. Display as is</h2>
<p>For the time being, I will make a time lapse that just switches the images as they are.</p>
<pre><code class="language-python:" data-lang="python:">
You can make a video with #opencv, but
# To make an mp4 file that can be played on discord in the environment of google colab,
It was easy to use # skvideo.
import skvideo.io

def create_video(imgs, out_video_path, size_wh):
  video = []
  vid_out = skvideo.io.FFmpegWriter(out_video_path,
      inputdict={
          &quot;-r&quot;: &quot;10&quot;
      },
      outputdict={
          &quot;-r&quot;: &quot;10&quot;
      })
  
  for img in imgs:
    img = cv2.resize(img, size_wh)
    vid_out.writeFrame(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))

  vid_out.close()

imgs = load_images(&quot;images_dir&quot;)
create_video(imgs, &quot;video.mp4&quot;, (w,h))
</code></pre><p>The results are as follows.</p>
<p><img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/22648/64ec20aa-30ef-6112-a428-26d38dc65d89.gif" alt="ezgif.com-crop.gif"></p>
<p>I can&rsquo;t concentrate on my child (body) because I am worried about the gap.</p>
<h2 id="1-2-fixed-position">1-2. Fixed position</h2>
<p>I want to somehow easily resolve this gap. If I set a reference point somewhere on my body and fix it, I thought, &ldquo;, and within about 0.1 seconds, I reached the solution of &ldquo;nipple&rdquo; and &ldquo;navel&rdquo;.
The following describes how to fix the nipple and navel.</p>
<h3 id="1-2-1-nipple-navel-coordinate-attachment-tool">1-2-1. Nipple navel coordinate attachment tool</h3>
<p>First, create a tool to add UV coordinates to the nipple and navel. It may be possible to use cvat etc., but when I estimated the time to master it and the time to make the tool by myself, I decided that it was faster to make it this time, so I made it.</p>
<p>The specification of the tool is that if you specify a folder, the images will be displayed consecutively, so for each image, click the nipple and navel 3 points and output the clicked coordinates to a csv file. Become. I used tkinter for the GUI (the source is short-lived and omitted).</p>
<ul>
<li>For the annotation data for deep learning that will be used later, I think that it is better to handle it if the image and annotation data are 1:1. But this time I didn&rsquo;t make it in order to finish it quickly.</li>
</ul>
<h3 id="1-2-2-video-creation">1-2-2. Video creation</h3>
<p>The nipple and navel are fixed by affine transformation according to the first image.</p>
<pre><code class="language-python:" data-lang="python:">def p3affine_img(img, src_p, dst_p):
    h, w, ch = img.shape
    pts1 = np.float32([src_p[0],src_p[1],src_p[2]])
    pts2 = np.float32([dst_p[0],dst_p[1],dst_p[2]])
    M = cv2.getAffineTransform(pts1,pts2)
    dst = cv2.warpAffine(img,M,(h, w))
    return dst


df = read_annotationd() # omitted

imgs = []
src_p = None
for index, row in df.iterrows():
    img = cv2.imread(row.file)
    dst_p = [[row.p1x, row.p1y], # left nipple
              [row.p2x, row.p2y], # right nipple
              [row.p3x, row.p3y]] # Navel
    if src_p is None:
      src_p = dst_p
    else:
      img = p3affine_img(img, dst_p, src_p)
    
    imgs.append(img)

write_video(imgs) # omitted

</code></pre><p>The results are as follows.</p>
<p><img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/22648/ee02e849-5d0b-c9ad-9077-f458dec8f584.gif" alt="ezgif.com-optimize.gif"></p>
<p>I was able to make the timelapse as expected, happy. Not in **! **</p>
<p>The number of sheets with the coordinates this time is 120 (the period is from September 9, 2019 to March 2020). However, I still have 281 images that I have taken since 2017/12 and have not added coordinates. Furthermore, it is necessary to do muscle training for the next few decades, that is, to continue assigning coordinates for decades. Even just imagined, cortisol is secreted and it becomes catabolic. I thought about supplementing sugar to solve this.</p>
<p>Yeah, ~~ let&rsquo;s go to the gym~~ deep learning.</p>
<h1 id="2-automatic-correction-using-deep-learning">2. Automatic correction using deep learning</h1>
<p>Create a model that estimates the position of the &ldquo;nipple&rdquo; and &ldquo;navel&rdquo;. If this is realized, then just apply the affine transformation as before. The detection of nipples and navel is approached as a segmentation task. Keypoint detection such as posture estimation seems to be better, but I personally have more experience with segmentation tasks, so I chose that.</p>
<p>The data set is as follows. From 2019/9 to 2020/3, the coordinates are already assigned, so this will be used for the training image and the verification image, and the coordinates will be automatically calculated for the remaining period.</p>
<p><img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/22648/c513a865-419c-8cf9-de26-69291b3eff27.png" alt="image.png"></p>
<h2 id="2-1-annotation-data-creation">2-1. Annotation data creation</h2>
<p>It may be possible to solve with 4 classes of &ldquo;right nipple&rdquo;, &ldquo;left nipple&rdquo;, &ldquo;navel&rdquo; and &ldquo;background&rdquo;, but this time we have classified into 2 classes of &ldquo;right nipple/left nipple/navel&rdquo; and &ldquo;background&rdquo;. I thought it would be easy to classify them on a rule basis if only 3 points could be detected.
Now, make a mask image immediately. Based on the coordinate data created earlier, make the coordinate point a little larger and fill it with 1. Other than that, the background is set to 0.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">for</span> index, row <span style="color:#f92672">in</span> df<span style="color:#f92672">.</span>iterrows():
  file <span style="color:#f92672">=</span> row<span style="color:#f92672">.</span>file
  mask <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>zeros((img_h, img_w), dtype<span style="color:#f92672">=</span>np<span style="color:#f92672">.</span>uint8)
  mask <span style="color:#f92672">=</span> cv2<span style="color:#f92672">.</span>circle(mask,(row<span style="color:#f92672">.</span>p1x, row<span style="color:#f92672">.</span>p1y,), <span style="color:#ae81ff">15</span>, (<span style="color:#ae81ff">1</span>), <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>)
  mask <span style="color:#f92672">=</span> cv2<span style="color:#f92672">.</span>circle(mask,(row<span style="color:#f92672">.</span>p2x, row<span style="color:#f92672">.</span>p2y,), <span style="color:#ae81ff">15</span>, (<span style="color:#ae81ff">1</span>), <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>)
  mask <span style="color:#f92672">=</span> cv2<span style="color:#f92672">.</span>circle(mask,(row<span style="color:#f92672">.</span>p3x, row<span style="color:#f92672">.</span>p3y,), <span style="color:#ae81ff">15</span>, (<span style="color:#ae81ff">1</span>), <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>)
  save_img(mask, row<span style="color:#f92672">.</span>file) <span style="color:#75715e"># Omitted</span>
</code></pre></div><p>Visually (1 is white, 0 is black) and the data is as follows.</p>
<p><img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/22648/fd256663-32d3-4654-c532-e5745629628e.png" alt="image.png"></p>
<p>Make these paired with the body image.</p>
<h2 id="2-2-learning">2-2. Learning</h2>
<p>For learning, I used DeepLab v3 (torchvision). We divided 120 images into 8:2 for training and verification. Although the number is quite small, we did not expand the data for the following reasons.</p>
<ul>
<li>Physical images are taken with the same camera</li>
<li>The camera posture and lighting environment are consistent between images to some extent</li>
</ul>
<p>However, I think that it is better to expand the data originally (it is not tedious).```python: dataset class/learning related functions
class MaskDataset(Dataset):
def <strong>init</strong>(self, imgs_dir, masks_dir, scale=1, transforms=None):
self.imgs_dir = imgs_dir
self.masks_dir = masks_dir</p>
<pre><code>self.imgs = list(sorted(glob.glob(os.path.join(imgs_dir, &quot;*.jpg&quot;))))
self.msks = list(sorted(glob.glob(os.path.join(masks_dir, &quot;*.png&quot;))))
self.transforms = transforms
self.scale = scale
</code></pre>
<p>def <strong>len</strong>(self):
return len(self.imgs_dir)</p>
<p>@classmethod
def preprocess(cls, pil_img, scale):</p>
<pre><code># It seems fine to use grayscale, but I don't like it
# pil_img = pil_img.convert(&quot;L&quot;)

w, h = pil_img.size
newW, newH = int(scale * w), int(scale * h)
pil_img = pil_img.resize((newW, newH))

img_nd = np.array(pil_img)

if len(img_nd.shape) == 2:
  img_nd = np.expand_dims(img_nd, axis=2)

# HWC to CHW
img_trans = img_nd.transpose((2, 0, 1))
if img_trans.max() &gt;1:
    img_trans = img_trans / 255

return img_trans
</code></pre>
<p>def <strong>getitem</strong>(self, i):</p>
<pre><code>mask_file = self.msks[i]
img_file = self.imgs[i]

mask = Image.open(mask_file)
img = Image.open(img_file)

img = self.preprocess(img, self.scale)
mask = self.preprocess(mask, self.scale)

item = {&quot;image&quot;: torch.from_numpy(img), &quot;mask&quot;: torch.from_numpy(mask)}
if self.transforms:
  item = self.transforms(item)
return item
</code></pre>
<p>from torchvision.models.segmentation.deeplabv3 import DeepLabHead</p>
<p>def create_deeplabv3(num_classes):
model = models.segmentation.deeplabv3_resnet101(pretrained=True, progress=True)
model.classifier = DeepLabHead(2048, num_classes)</p>
<h1 id="it-seems-fine-to-use-grayscale-but-i-dont-like-it">It seems fine to use grayscale, but I don&rsquo;t like it</h1>
<p>#model.backbone.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)</p>
<p>return model</p>
<p>def train_model(model, criterion, optimizer, dataloaders, device, num_epochs=25, print_freq=1):
since = time.time()</p>
<p>best_model_wts = copy.deepcopy(model.state_dict())
best_loss = 1e15</p>
<p>for epoch in range(num_epochs):
print(&lsquo;Epoch {}/{}'.format(epoch+1, num_epochs))
print('-&rsquo; * 10)</p>
<pre><code>loss_history = {&quot;train&quot;: [], &quot;val&quot;: []}

for phase in [&quot;train&quot;, &quot;val&quot;]:
    
  if phase == &quot;train&quot;:
    model.train()
  else:
    model.eval()

  for sample in tqdm(iter(dataloaders[phase])):
    imgs = sample[&quot;image&quot;].to(device, dtype=torch.float)
    msks = sample[&quot;mask&quot;].to(device, dtype=torch.float)

    optimizer.zero_grad()

    with torch.set_grad_enabled(phase == &quot;train&quot;):
      outputs = model(imgs)
      loss = criterion(outputs[&quot;out&quot;], msks)

      if phase == &quot;train&quot;:
        loss.backward()
        optimizer.step()

  epoch_loss = np.float(loss.data)
  if (epoch + 1) %print_freq == 0:
    print(&quot;Epoch: [%d/%d], Loss: %.4f&quot; %(epoch+1, num_epochs, epoch_loss))
    loss_history[phase].append(epoch_loss)

  # deep copy the model
  if phase == &quot;val&quot; and epoch_loss &lt;best_loss:
    best_loss = epoch_loss
    best_model_wts = copy.deepcopy(model.state_dict())
</code></pre>
<p>time_elapsed = time.time()-since
print(&ldquo;Training complete in {:.0f}m {:.0f}s&rdquo;.format(time_elapsed // 60, time_elapsed %60))
print(&ldquo;Best val Acc: {:4f}&quot;.format(best_loss))</p>
<p>model.load_state_dict(best_model_wts)</p>
<p>return model, loss_history</p>
<pre><code>
```python: learning execution

dataset = MaskDataset(&quot;images_dir&quot;, &quot;masks_dir&quot;, 0.5, transforms=None)

# Separate for training and verification
val_percent= 0.2
batch_size=4
n_val = int(len(dataset) * val_percent)
n_train = len(dataset)-n_val
train, val = random_split(dataset, [n_train, n_val])
train_loader = DataLoader(train, batch_size=batch_size, shuffle=True, num_workers=8, pin_memory=True, drop_last=True ).
val_loader = DataLoader(val, batch_size=batch_size, shuffle=False, num_workers=8, pin_memory=True, drop_last=True ).

dataloaders = {&quot;train&quot;: train_loader, &quot;val&quot;: val_loader}

device = torch.device(&quot;cuda:0&quot; if torch.cuda.is_available() else &quot;cpu&quot;)

When using #BCEWithLogitsLoss, specify 1 if it is a binary classification
num_classes = 1

model = create_deeplabv3(num_classes)

#for pre trained
#model.load_state_dict(torch.load(&quot;model.pth&quot;))

model.to(device)

# The background is overwhelmingly large, so adjust with pos_weight
criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(10000.0).to(device))

params = [p for p in model.parameters() if p.requires_grad]

#optimizer = torch.optim.SGD(params, lr=0.005,momentum=0.9, weight_decay=0.0005)
optimizer = optim.Adam(params)

total_epoch = 50

model, loss_dict = train_model(model, criterion, optimizer, dataloaders, device, total_epoch)
</code></pre><p>This time, the learning has converged to some extent when turning about 50 epochs.</p>
<h2 id="2-3-application-to-unknown-images">2-3. Application to unknown images</h2>
<p>ㆍAs a result, it was generally good and 3 points responded properly, but occasionally there were also the following results (heat map expression).</p>
<p><img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/22648/d6e748e9-5c19-6388-472e-8295474f809f.png" alt="image.png"></p>
<p>Of course, there are never two left nipples, so the small point at the top right is the False Positive.
By the way, there was no False Negative.</p>
<h2 id="2-4-post-processing">2-4. Post-processing</h2>
<p>From the inference result, we will do the following in post processing.</p>
<ol>
<li>Truncate if the output value of each pixel is below the threshold</li>
<li>Divide the object</li>
<li>If there are 4 or more clusters, select 3 in descending order of area and discard the rest.</li>
<li>Find the center of gravity of each cluster</li>
<li>Sort the x-coordinates of the center of gravity of each cluster in ascending order (right nipple → navel → left nipple)</li>
</ol>
<h3 id="2-4-1-truncate-if-the-output-value-of-each-pixel-is-below-the-threshold-valuetruncate-all-pixels-with-a-definite-accuracy-for-further-processing-this-threshold-is-empirically-set-to-0995">2-4-1. Truncate if the output value of each pixel is below the threshold valueTruncate all pixels with a definite accuracy for further processing. This threshold is empirically set to 0.995.</h3>
<h3 id="2-4-2-object-division">2-4-2. Object division</h3>
<p>Use cv2.connectedComponents for object division (divided into clusters). For details, please refer to <a href="https://www.pynote.info/entry/opencv-connected-components-labeling">How to Label Connected Components with OpenCV-connectedComponents-pynote</a>.</p>
<h3 id="2-4-3-if-there-are-4-or-more-clusters-select-3-in-descending-order-of-area-and-discard-the-rest">2-4-3. If there are 4 or more clusters, select 3 in descending order of area and discard the rest.</h3>
<p>From the example, it was found that the area of False Positive outside the nipple and navel is small. Therefore, we will choose three with a large area. Actually, I feel that such a countermeasure is not very robust, but this time it worked, so I will adopt it.</p>
<h3 id="2-4-4-find-the-center-of-gravity-of-each-cluster">2-4-4. Find the center of gravity of each cluster</h3>
<p>Use cv2.moments to find the center of gravity of each cluster. For details, please refer to <a href="https://cvtech.cc/pycvmoment/">Calculating the center of gravity with Python+OpenCV-Introduction to CV image analysis</a>.</p>
<h3 id="2-4-5-sort-the-x-coordinates-of-the-center-of-gravity-of-each-cluster-in-ascending-order-right-nipple--navel--left-nipple">2-4-5. Sort the x-coordinates of the center of gravity of each cluster in ascending order (right nipple → navel → left nipple)</h3>
<p>Because points need to correspond when performing affine transformation, it is necessary to unify the coordinate order of the nipple and navel between images. All of the images this time are taken upright, and there is no doubt that the nipple → navel → nipple will appear in the horizontal axis direction, so simply rearrange them by the x coordinate.</p>
<pre><code class="language-python:" data-lang="python:">
#3 points detected from mask
def triangle_pt(heatmask, thresh=0.995):
  mask = heatmask.copy()

  # 2-4-1. Truncate if the output value of each pixel is below the threshold value
  mask[mask&gt;thresh] = 255
  mask[mask&lt;=thresh] = 0
  mask = mask.astype(np.uint8)
  # 2-4-2. Divide the object
  nlabels, labels = cv2.connectedComponents(mask)

  pt = []
  if nlabels != 4:

    #If less, do nothing
    #I really want to lower the threshold, but it's a pain
    if nlabels &lt;4:
      return None
    
    # 2-4-3. If there are 4 or more clusters, select 3 in descending order of area and discard the rest
    elif nlabels &gt;4:
      sum_px = []
      for i in range(1, nlabels):
        sum_px.append((labels==i).sum())
      # +1 for background
      indices = [x+1 for x in np.argsort(-np.array(sum_px))[:3]]

  else:
    indices = [x for x in range(1, nlabels)]

  # 2-4-4. Find the center of gravity of each cluster
  for i in indices:
    base = np.zeros_like(mask, dtype=np.uint8)
    base[labels==i] = 255
    mu = cv2.moments(base, False)
    x,y= int(mu[&quot;m10&quot;]/mu[&quot;m00&quot;]), int(mu[&quot;m01&quot;]/mu[&quot;m00&quot;])
    pt.append([x,y])

  # 2-4-5. Sort the clusters in ascending order of the center of gravity (right nipple → navel → left nipple)
  sort_key = lambda v: v[0]
  pt.sort(key=sort_key)
  return np.array(pt)


def correct_img(model, device, in_dir, out_dir,
                draw_heatmap=True, draw_triangle=True, correct=True):

  imgs = []

  base_3p = None
  model.eval()
  with torch.no_grad():
    imglist = sorted(glob.glob(os.path.join(in_dir, &quot;*.jpg&quot;)))
    
    for idx, img_path in enumerate(imglist):

      # Batch size 1 because it's troublesome
      full_img = Image.open(img_path)
      img = torch.from_numpy(BasicDataset.preprocess(full_img, 0.5))
      img = img.unsqueeze(0)
      img = img.to(device=device, dtype=torch.float32)

      output = model(img)[&quot;out&quot;]
      probs = torch.sigmoid(output)
      probs = probs.squeeze(0)

      tf = transforms.Compose(
                [
                    transforms.ToPILImage(),
                    transforms.Resize(full_img.size[0]),
                    transforms.ToTensor()
                ]
            )
      
      probs = tf(probs.cpu())
      full_mask = probs.squeeze().cpu().numpy()

      full_img = np.asarray(full_img).astype(np.uint8)
      full_img = cv2.cvtColor(full_img, cv2.COLOR_RGB2BGR)

      # Triangle
      triangle = triangle_pt(full_mask)
      if draw_triangle and triangle is not None:
        cv2.drawContours(full_img, [triangle], 0, (0, 0, 255), 5)

      #Heat map
      if draw_heatmap:
        full_mask = (full_mask*255).astype(np.uint8)
        jet = cv2.applyColorMap(full_mask, cv2.COLORMAP_JET)

        alpha = 0.7
        full_img = cv2.addWeighted(full_img, alpha, jet, 1-alpha, 0)

      #Affine transformation
      if correct:
        if base_3p is None and triangle is not None:
          base_3p = triangle
        elif triangle is not None:
          full_img = p3affine_img(full_img, triangle, base_3p)

      if out_dir is not None:
        cv2.imwrite(os.path.join(out_dir, os.path.basename(img_path)), full_img)

      imgs.append(full_img)

  return imgs

imgs = correct_img(model, device,
                   &quot;images_dir&quot;, None,
                    draw_heatmap=False, draw_triangle=False, correct=True)

</code></pre><h2 id="2-5-result">2-5. Result</h2>
<p>The time lapse just before correction is as follows.</p>
<p>![ezgif.com-optimize (1).gif](<a href="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/22648/23034e65-e8ad-9ab7-4a3f-d7f9d1dd2829.(gif)">https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/22648/23034e65-e8ad-9ab7-4a3f-d7f9d1dd2829.(gif)</a></p>
<p>The corrected time lapse is as follows.</p>
<p>![ezgif.com-optimize (2).gif](<a href="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/22648/5ae7d8d8-fbaf-6721-0b42-1a6875b8cc8d.(gif)">https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/22648/5ae7d8d8-fbaf-6721-0b42-1a6875b8cc8d.(gif)</a></p>
<p>#Summary</p>
<p>Deep learning was used to detect the nipple and navel, and the image was automatically corrected to make the time lapse dramatically easier to see. This made me more motivated for training.
Of course, I think some people might think, &ldquo;Isn&rsquo;t it possible to use such a non-deep CV?&rdquo;, but in my case I would like to give a barbell if I have time to think about the rules. It feels like I&rsquo;ve solved it with brute force.
All development was done with google colab except for the coordinate assignment tool, 3150 uu!
The challenges are:</p>
<ul>
<li>I&rsquo;m not sure if it will work with other people&rsquo;s bodies (hopefully I&rsquo;ll let them learn)</li>
<li>Not compatible when the overall size is large (reference points other than nipples and navel are required)</li>
<li>Shadow removal</li>
<li>App release (I want to do this!!!)</li>
</ul>
<p>However, as cortisol is secreted, don&rsquo;t worry about it being too hard!</p>
<p>Then, have a fun muscle training life!</p>

    </article>
  </div>

  
  
  
  <script>
  window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
  ga('create', 'UA-169005401-1', 'auto');
  ga('send', 'pageview');
  </script>
  <script async src='https://www.google-analytics.com/analytics.js'></script>
  

  

  
<link rel="stylesheet" type="text/css" href="//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.0/cookieconsent.min.css" />
<script src="//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.0/cookieconsent.min.js"></script>
<script>
window.addEventListener("load", function(){
window.cookieconsent.initialise({
  "palette": {
    "popup": {
      "background": "#216942",
      "text": "#b2d192"
    },
    "button": {
      "background": "#afed71"
    }
  }
})});
</script>

</body>

</html>
