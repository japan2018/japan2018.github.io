<!doctype html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  
  <meta name="generator" content="Hugo 0.72.0" />

  
  <meta name="description" content="Some description">
  

  
  <link rel="apple-touch-icon" sizes="180x180" href="https://memotut.com/apple-touch-icon.png">

  
  <link rel="icon" type="image/png" sizes="32x32" href="https://memotut.com/favicon-32x32.png">

  
  <link rel="icon" type="image/png" sizes="16x16" href="https://memotut.com/favicon-16x16.png">

  
  <link rel="manifest" href="https://memotut.com/site.webmanifest">

  
  <link rel="mask-icon" href="https://memotut.com/safari-pinned-tab.svg" color="#5bbad5">

  <meta name="msapplication-TileColor" content="#da532c">

  <meta name="theme-color" content="#ffffff">

  
  <link rel="stylesheet" href="https://memotut.com/css/bootstrap.min.css" />

  
  <title>Hashing algorithm for determining the same image | Memo Tut</title>
  

  <style>
body {
  min-width: 300px;
}

.custom-navbar {
  margin-bottom: 1em;
  height: 60px;
}

.custom-navbar a {
  display: inline-block; 
  padding: 18px 0;
  margin-right: 1em; 
  font-weight: bold; 
}

.custom-navbar a:hover,
.custom-navbar a:focus {
  text-decoration: none; 
}

@media print {
  .custom-navbar {
    display: none;
  }
}

article {
  padding-bottom: 1em;
}

img {
  max-width: 100%;
}


body {
  background-color: #fff;
}



body {
  color: #212529;
}



a {
  color: #007bff;
}



a:hover,
a:focus {
  color: #0056b3;
}



.custom-navbar {
  background-color: #212529;
}



.custom-navbar a {
  color: rgba(255, 255, 255, 0.75);
}



.custom-navbar a:hover,
.custom-navbar a:focus {
  color: rgba(255, 255, 255, 1);
}



.container {
  max-width: 800px;
}



pre {
  display: block;
  padding: 9.5px;
  word-break: break-all;
  word-wrap: break-word;
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 4px;
}

pre code {
  padding: 0;
  font-size: inherit;
  color: inherit; 
  white-space: pre-wrap;
  background-color: transparent;
  border: none;
  border-radius: 0;
}

code {
  padding: 2px 4px;
  color: inherit; 
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 4px;
  font-size: .9em;
}



blockquote,
.blockquote {
  padding: 10px 20px;
  margin: 0 0 20px;
  font-size: 1em;
  border-left: 5px solid #6c757d;
}

</style>
</head>

<body>
  <nav class="custom-navbar">
  <div class="container">
    
    <a href="/">Posts</a>
    
    <a href="/tags/">Tags</a>
    
    <a href="/about/">About</a>
    
    <a href="/index.xml">RSS</a>
    
  </div>
</nav>
  
  <div class="container">
    <article>
      <h1>Hashing algorithm for determining the same image</h1>
<p>
  <small class="text-secondary">
  
  
  Nov 18, 2019
  </small>
  

<small><code><a href="https://memotut.com/tags/python">Python</a></code></small>


<small><code><a href="https://memotut.com/tags/image-processing"> image processing</a></code></small>


<small><code><a href="https://memotut.com/tags/phash"> pHash</a></code></small>

</p>
<pre><code>#Introduction
</code></pre>
<p>When creating a machine learning dataset based on images collected from the internet, it is necessary to delete duplicate images. It is still good if there are duplicate images in the training data, but if there are duplicate images between the training data and the test data, so-called leakage will occur.</p>
<p>The simplest way to detect duplicate images is to use the hash value of a file such as MD5. However, the hash value of the file is just a hash of the binary string of the image file, and it will change even if the storage format and compression parameters are changed even for the same image, leading to detection failure.</p>
<p>Therefore, in this article, we will introduce algorithms that hash the characteristics of images themselves, and also look at the characteristics of these hashing algorithms through simple experiments.</p>
<h1 id="image-hashing-algorithm">Image hashing algorithm</h1>
<h2 id="average-hash-ahash">Average Hash (aHash)</h2>
<p>It is a hash value based on the image features (luminance pattern) and can be calculated by a simple algorithm. The specific procedure is as follows.</p>
<ol>
<li>Reduce the image to 8x8 pixels.</li>
<li>Further convert to gray scale.</li>
<li>Obtain the average pixel value.</li>
<li>For each 8x8 pixels, binarize (0 or 1) depending on whether it is higher or lower than the average value.</li>
<li>For a binary sequence, arrange in a certain order such as raster scan order to obtain a 64-bit hash.</li>
</ol>
<p>aHash has the advantage that the algorithm is simple and the calculation is fast. On the other hand, it also has the drawback of being inflexible. For example, the hash value of a gamma-corrected image is far from the original image.</p>
<h2 id="perseptual-hash-phash">Perseptual Hash (pHash)</h2>
<p>While aHash used the pixel values themselves, pHash uses the Discrete Cosine Transform (DCT) of the image. DCT is one of the methods to convert signals such as images to the frequency domain, and is used for JPEG compression, for example. With JPEG compression, the amount of data is reduced by DCTing the image and extracting only the low-frequency components that are easy for humans to perceive.</p>
<p>Similar to JPGE compression, pHash focuses on the low frequency components in the DCT of the image and hashes them. By doing so, features that are easy for humans to perceive can be preferentially extracted, and hashing that is robust against parallel movement of the image and changes in brightness can be considered.</p>
<ol>
<li>Reduce the image. Make it larger than 8x8 (eg 32x32).</li>
<li>Convert to gray scale.</li>
<li>Perform DCT.</li>
<li>Take out only the low frequency component 8x8.</li>
<li>Calculate the average value of low frequency components excluding DC components.</li>
<li>Binarize depending on whether it is higher or lower than the average value.</li>
<li>Get a 64-bit hash by arranging it in a certain order such as raster scan order.</li>
</ol>
<h2 id="other-xhash">Other xHash</h2>
<p>There seems to be various variations other than aHash and pHash. Some people have benchmarks <sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>.</p>
<p>#Experiment
Apply various processing to the image and compare the hash value with the original image. Calculate aHash and pHash respectively as hash values.</p>
<p>We also try a method that regards the output as a hash for the layer immediately before the final layer of ResNet50. This method was adopted in a paper <sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup> <sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup>.</p>
<h2 id="code">code</h2>
<p>OpenCV is used to calculate aHash and pHash, but there seems to be a library called ImageHash. In aHash and pHash, Hamming distance can be used to compare hash values. The range is <code>[0, 64]</code>. In order to match this range, in hash comparison using ResNet50, the cosine similarity is calculated and then converted to the above range.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-Python" data-lang="Python"><span style="color:#f92672">import</span> copy
<span style="color:#f92672">import</span> pprint

<span style="color:#f92672">import</span> cv2.cv2 <span style="color:#f92672">as</span> cv2
<span style="color:#f92672">import</span> numpy <span style="color:#f92672">as</span> np
<span style="color:#f92672">from</span> keras <span style="color:#f92672">import</span> models
<span style="color:#f92672">from</span> keras.applications.resnet50 <span style="color:#f92672">import</span> ResNet50, preprocess_input
<span style="color:#f92672">from</span> sklearn.metrics.pairwise <span style="color:#f92672">import</span> cosine_similarity


<span style="color:#66d9ef">class</span> <span style="color:#a6e22e">ImagePairGenerator</span>(object):
    <span style="color:#e6db74">&#34;&#34;&#34;
</span><span style="color:#e6db74">    Class that generates image pairs for experiment
</span><span style="color:#e6db74">    &#34;&#34;&#34;</span>

    <span style="color:#66d9ef">def</span> __init__(self, img: np<span style="color:#f92672">.</span>ndarray):
        self<span style="color:#f92672">.</span>_img <span style="color:#f92672">=</span> img
        self<span style="color:#f92672">.</span>_processings <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>_prepare_processings()

    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">_prepare_processings</span>(self):
        h, w, _ <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>_img<span style="color:#f92672">.</span>shape
        <span style="color:#75715e"># lenna Position and size for cropping around the face of the image</span>
        org <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array([<span style="color:#ae81ff">128</span>, <span style="color:#ae81ff">128</span>])
        size <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array([<span style="color:#ae81ff">256</span>, <span style="color:#ae81ff">256</span>])
        <span style="color:#75715e"># kind (processing description), img1, img2</span>
        processings <span style="color:#f92672">=</span> [
            (<span style="color:#e6db74">&#39;Identical&#39;</span>,
             <span style="color:#66d9ef">lambda</span> x: x,
             <span style="color:#66d9ef">lambda</span> x: x),
            (<span style="color:#e6db74">&#39;Grayscale conversion&#39;</span>,
             <span style="color:#66d9ef">lambda</span> x: x,
             <span style="color:#66d9ef">lambda</span> x: cv2<span style="color:#f92672">.</span>cvtColor(
                 cv2<span style="color:#f92672">.</span>cvtColor(x, cv2<span style="color:#f92672">.</span>COLOR_BGR2GRAY), cv2<span style="color:#f92672">.</span>COLOR_GRAY2BGR)),
            <span style="color:#f92672">*</span>list(map(<span style="color:#66d9ef">lambda</span> s:
                      (Reduce to f<span style="color:#e6db74">&#39;1/{s:2}&#39;</span>,
                       <span style="color:#66d9ef">lambda</span> x: x,
                       <span style="color:#66d9ef">lambda</span> x: cv2<span style="color:#f92672">.</span>resize(x, (w <span style="color:#f92672">//</span> s, h <span style="color:#f92672">//</span> s))),
                      np<span style="color:#f92672">.</span>power(<span style="color:#ae81ff">2</span>, range(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">5</span>)))),
            <span style="color:#f92672">*</span>list(map(<span style="color:#66d9ef">lambda</span> s:
                      (f<span style="color:#e6db74">&#39; smoothing (kernel size = {s:2}&#39;</span>,
                       <span style="color:#66d9ef">lambda</span> x: x,
                       <span style="color:#66d9ef">lambda</span> x: cv2<span style="color:#f92672">.</span>blur(x, (s, s))),
                      (<span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">5</span>, <span style="color:#ae81ff">7</span>, <span style="color:#ae81ff">9</span>, <span style="color:#ae81ff">11</span>])),
            <span style="color:#f92672">*</span>list(map(<span style="color:#66d9ef">lambda</span> s:
                      (f<span style="color:#e6db74">&#39; Insert text (fontScale = {s})&#39;</span>,
                       <span style="color:#66d9ef">lambda</span> x: x,
                       <span style="color:#66d9ef">lambda</span> x: cv2<span style="color:#f92672">.</span>putText(x,<span style="color:#e6db74">&#39;Text&#39;</span>, org<span style="color:#f92672">=</span>(<span style="color:#ae81ff">10</span>, <span style="color:#ae81ff">30</span><span style="color:#f92672">*</span>s),
                                             fontFace<span style="color:#f92672">=</span>cv2<span style="color:#f92672">.</span>FONT_HERSHEY_SIMPLEX,
                                             fontScale<span style="color:#f92672">=</span>s,
                                             color<span style="color:#f92672">=</span>(<span style="color:#ae81ff">255</span>, <span style="color:#ae81ff">255</span>, <span style="color:#ae81ff">255</span>),
                                             thickness<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span><span style="color:#f92672">*</span>s,
                                             lineType<span style="color:#f92672">=</span>cv2<span style="color:#f92672">.</span>LINE_AA)),
                      range(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">8</span>))),
            <span style="color:#f92672">*</span>list(map(<span style="color:#66d9ef">lambda</span> q:
                      (f<span style="color:#e6db74">&#39;JPEG compression (quality = {q})&#39;</span>,
                       <span style="color:#66d9ef">lambda</span> x: x,
                       <span style="color:#66d9ef">lambda</span> x: img_encode_decode(x, q)),
                      range(<span style="color:#ae81ff">10</span>, <span style="color:#ae81ff">100</span>, <span style="color:#ae81ff">10</span>))),
            <span style="color:#f92672">*</span>list(map(<span style="color:#66d9ef">lambda</span> gamma:
                      (f<span style="color:#e6db74">&#39; gamma correction (gamma = {gamma})&#39;</span>,
                       <span style="color:#66d9ef">lambda</span> x: x,
                       <span style="color:#66d9ef">lambda</span> x: img_gamma(x, gamma)),
                      [<span style="color:#ae81ff">0.2</span>, <span style="color:#ae81ff">0.5</span>, <span style="color:#ae81ff">0.8</span>, <span style="color:#ae81ff">1.2</span>, <span style="color:#ae81ff">1.5</span>, <span style="color:#ae81ff">2.0</span>])),
            <span style="color:#f92672">*</span>list(map(<span style="color:#66d9ef">lambda</span> d:
                      (f<span style="color:#e6db74">&#39; translation ({d:2} pixels)&#39;</span>,
                       <span style="color:#66d9ef">lambda</span> x: img_crop(x, org, size),
                       <span style="color:#66d9ef">lambda</span> x: img_crop(x, org <span style="color:#f92672">+</span> d, size)),
                      np<span style="color:#f92672">.</span>power(<span style="color:#ae81ff">2</span>, range(<span style="color:#ae81ff">7</span>)))),
        ]
        <span style="color:#66d9ef">return</span> processings

    <span style="color:#66d9ef">def</span> __iter__(self):
        <span style="color:#66d9ef">for</span> kind, p1, p2 <span style="color:#f92672">in</span> self<span style="color:#f92672">.</span>_processings:
            <span style="color:#66d9ef">yield</span> (kind,
                   p1(copy<span style="color:#f92672">.</span>deepcopy(self<span style="color:#f92672">.</span>_img)),
                   p2(copy<span style="color:#f92672">.</span>deepcopy(self<span style="color:#f92672">.</span>_img)))


<span style="color:#66d9ef">class</span> <span style="color:#a6e22e">ResNet50Hasher</span>(object):
    <span style="color:#e6db74">&#34;&#34;&#34;
</span><span style="color:#e6db74">    Class for outputting the final layer of ResNet50 as a hash value
</span><span style="color:#e6db74">    &#34;&#34;&#34;</span>
    _input_size <span style="color:#f92672">=</span> <span style="color:#ae81ff">224</span>

    <span style="color:#66d9ef">def</span> __init__(self):
        self<span style="color:#f92672">.</span>_model <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>_prepare_model()

    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">_prepare_model</span>(self):
        resnet50 <span style="color:#f92672">=</span> ResNet50(include_top<span style="color:#f92672">=</span>False, weights<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;imagenet&#39;</span>,
                            input_shape<span style="color:#f92672">=</span>(self<span style="color:#f92672">.</span>_input_size, self<span style="color:#f92672">.</span>_input_size, <span style="color:#ae81ff">3</span>),
                            pooling<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;avg&#39;</span>)
        model <span style="color:#f92672">=</span> models<span style="color:#f92672">.</span>Sequential()
        model<span style="color:#f92672">.</span>add(resnet50)
        <span style="color:#66d9ef">return</span> modeldef compute(self, img: np<span style="color:#f92672">.</span>ndarray) <span style="color:#f92672">-&gt;</span> np<span style="color:#f92672">.</span>ndarray:
        img_arr <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array([
            cv2<span style="color:#f92672">.</span>resize(img, (self<span style="color:#f92672">.</span>_input_size, self<span style="color:#f92672">.</span>_input_size))
        ])
        img_arr <span style="color:#f92672">=</span> preprocess_input(img_arr)
        embeddings <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>_model<span style="color:#f92672">.</span>predict(img_arr)
        <span style="color:#66d9ef">return</span> embeddings

    <span style="color:#a6e22e">@staticmethod</span>
    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">compare</span>(x1: np<span style="color:#f92672">.</span>ndarray, x2: np<span style="color:#f92672">.</span>ndarray):
        <span style="color:#e6db74">&#34;&#34;&#34;
</span><span style="color:#e6db74">        Calculate the cosine similarity. The range is [0, 1].
</span><span style="color:#e6db74">        According to the Hamming distance comparing aHash and pHash,
</span><span style="color:#e6db74">        Convert to the range [0, 64].
</span><span style="color:#e6db74">        &#34;&#34;&#34;</span>
        cs <span style="color:#f92672">=</span> cosine_similarity(x1, x2)
        distance <span style="color:#f92672">=</span> <span style="color:#ae81ff">64</span> <span style="color:#f92672">+</span> (<span style="color:#ae81ff">0</span><span style="color:#f92672">-</span><span style="color:#ae81ff">64</span>) <span style="color:#f92672">*</span> ((cs<span style="color:#f92672">-</span><span style="color:#ae81ff">0</span>) <span style="color:#f92672">/</span> (<span style="color:#ae81ff">1</span><span style="color:#f92672">-</span><span style="color:#ae81ff">0</span>))
        <span style="color:#66d9ef">return</span> distance<span style="color:#f92672">.</span>ravel()[<span style="color:#ae81ff">0</span>] <span style="color:#75715e"># np.array -&gt; float</span>


<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">img_crop</span>(img: np<span style="color:#f92672">.</span>ndarray, org: np<span style="color:#f92672">.</span>ndarray, size: np<span style="color:#f92672">.</span>ndarray) <span style="color:#f92672">-&gt;</span> np<span style="color:#f92672">.</span>ndarray:
    <span style="color:#e6db74">&#34;&#34;&#34;
</span><span style="color:#e6db74">    Crop any area from the image.
</span><span style="color:#e6db74">    &#34;&#34;&#34;</span>
    y, x <span style="color:#f92672">=</span> org
    h, w <span style="color:#f92672">=</span> size
    <span style="color:#66d9ef">return</span> img[y:y <span style="color:#f92672">+</span> h, x:x <span style="color:#f92672">+</span> w, :]


<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">img_encode_decode</span>(img: np<span style="color:#f92672">.</span>ndarray, quality<span style="color:#f92672">=</span><span style="color:#ae81ff">90</span>) <span style="color:#f92672">-&gt;</span> np<span style="color:#f92672">.</span>ndarray:
    <span style="color:#e6db74">&#34;&#34;&#34;
</span><span style="color:#e6db74">    Reproduce the deterioration of Jpeg compression.
</span><span style="color:#e6db74">    Reference: https://qiita.com/ka10ryu1/items/5fed6b4c8f29163d0d65
</span><span style="color:#e6db74">    &#34;&#34;&#34;</span>
    encode_param <span style="color:#f92672">=</span> [int(cv2<span style="color:#f92672">.</span>IMWRITE_JPEG_QUALITY), quality]
    _, enc_img <span style="color:#f92672">=</span> cv2<span style="color:#f92672">.</span>imencode(<span style="color:#e6db74">&#39;.jpg&#39;</span>, img, encode_param)
    dec_img <span style="color:#f92672">=</span> cv2<span style="color:#f92672">.</span>imdecode(enc_img, cv2<span style="color:#f92672">.</span>IMREAD_COLOR)
    <span style="color:#66d9ef">return</span> dec_img


<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">img_gamma</span>(img: np<span style="color:#f92672">.</span>ndarray, gamma<span style="color:#f92672">=</span><span style="color:#ae81ff">0.5</span>) <span style="color:#f92672">-&gt;</span> np<span style="color:#f92672">.</span>ndarray:
    <span style="color:#e6db74">&#34;&#34;&#34;
</span><span style="color:#e6db74">    Gamma correct.
</span><span style="color:#e6db74">    Reference: https://www.dogrow.net/python/blog99/
</span><span style="color:#e6db74">    &#34;&#34;&#34;</span>
    lut <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>empty((<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">256</span>), np<span style="color:#f92672">.</span>uint8)
    <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">256</span>):
        lut[<span style="color:#ae81ff">0</span>, i] <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>clip(pow(i <span style="color:#f92672">/</span> <span style="color:#ae81ff">255.0</span>, gamma) <span style="color:#f92672">*</span> <span style="color:#ae81ff">255.0</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">255</span>)

    <span style="color:#66d9ef">return</span> cv2<span style="color:#f92672">.</span>LUT(img, lut)


<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">image_hashing_test</span>():
    image_path <span style="color:#f92672">=</span><span style="color:#e6db74">&#39;resources/lena_std.tif&#39;</span>
    img <span style="color:#f92672">=</span> cv2<span style="color:#f92672">.</span>imread(image_path, cv2<span style="color:#f92672">.</span>IMREAD_COLOR)
    h, w, _ <span style="color:#f92672">=</span> img<span style="color:#f92672">.</span>shape

    hashers <span style="color:#f92672">=</span> [
        (<span style="color:#e6db74">&#39;aHash&#39;</span>, cv2<span style="color:#f92672">.</span>img_hash<span style="color:#f92672">.</span>AverageHash_create()),
        (<span style="color:#e6db74">&#39;pHash&#39;</span>, cv2<span style="color:#f92672">.</span>img_hash<span style="color:#f92672">.</span>PHash_create()),
        (<span style="color:#e6db74">&#39;ResNet&#39;</span>, ResNet50Hasher())
    ]

    pairs <span style="color:#f92672">=</span> ImagePairGenerator(img)

    result_dict <span style="color:#f92672">=</span> {}
    <span style="color:#66d9ef">for</span> pair_kind, img1, img2 <span style="color:#f92672">in</span> pairs:
        result_dict[pair_kind] <span style="color:#f92672">=</span> {}
        <span style="color:#66d9ef">for</span> hasher_kind, hasher <span style="color:#f92672">in</span> hashers:
            hash1 <span style="color:#f92672">=</span> hasher<span style="color:#f92672">.</span>compute(img1)
            hash2 <span style="color:#f92672">=</span> hasher<span style="color:#f92672">.</span>compute(img2)
            distance <span style="color:#f92672">=</span> hasher<span style="color:#f92672">.</span>compare(hash1, hash2)
            result_dict[pair_kind][hasher_kind] <span style="color:#f92672">=</span> distance
        <span style="color:#75715e"># Visually check the images (only when the shapes are the same. It is troublesome to align them)</span>
        <span style="color:#66d9ef">if</span> img1<span style="color:#f92672">.</span>shape <span style="color:#f92672">==</span> img2<span style="color:#f92672">.</span>shape:
            window_name <span style="color:#f92672">=</span> pair_kind
            cv2<span style="color:#f92672">.</span>imshow(window_name, cv2<span style="color:#f92672">.</span>hconcat((img1, img2)))
            cv2<span style="color:#f92672">.</span>waitKey()
            cv2<span style="color:#f92672">.</span>destroyWindow(window_name)

    pprint<span style="color:#f92672">.</span>pprint(result_dict)


<span style="color:#66d9ef">if</span> __name__ <span style="color:#f92672">==</span><span style="color:#e6db74">&#39;__main__&#39;</span>:
    image_hashing_test()
</code></pre></div><h2 id="result">result</h2>
<pre><code>{'Same': {'ResNet': 0.0,'aHash': 0.0,'pHash': 0.0},
 'Grayscale': {'ResNet': 14.379967,'aHash': 0.0,'pHash': 0.0},
 'Reduce to 1/2': {'ResNet': 1.2773285,'aHash': 3.0,'pHash': 1.0},
 'Reduce to 1/4': {'ResNet': 6.5748253,'aHash': 4.0,'pHash': 1.0},
 'Reduce to 1/8': {'ResNet': 18.959282,'aHash': 7.0,'pHash': 3.0},
 'Reduce to 1/16': {'ResNet': 34.8299,'aHash': 12.0,'pHash': 0.0},
 'JPEG compression (quality = 10)': {'ResNet': 6.4169083,'aHash': 2.0,'pHash': 0.0},
 'JPEG compression (quality = 20)': {'ResNet': 2.6065674,'aHash': 1.0,'pHash': 0.0},
 'JPEG compression (quality = 30)': {'ResNet': 1.8446579,'aHash': 0.0,'pHash': 0.0},
 'JPEG compression (quality = 40)': {'ResNet': 1.2492218,'aHash': 0.0,'pHash': 1.0},
 'JPEG compression (quality = 50)': {'ResNet': 1.0534592,'aHash': 0.0,'pHash': 0.0},
 'JPEG compression (quality = 60)': {'ResNet': 0.99293137,'aHash': 0.0,'pHash': 0.0},
 'JPEG compression (quality = 70)': {'ResNet': 0.7313309,'aHash': 0.0,'pHash': 0.0},
 'JPEG compression (quality = 80)': {'ResNet': 0.58068085,'aHash': 0.0,'pHash': 0.0},
 'JPEG compression (quality = 90)': {'ResNet': 0.354187,'aHash': 0.0,'pHash': 0.0},
 'Gamma correction (gamma = 0.2)': {'ResNet': 16.319721,'aHash': 2.0,'pHash': 1.0},
 'Gamma correction (gamma = 0.5)': {'ResNet': 4.2003975,'aHash': 2.0,'pHash': 0.0},
 'Gamma correction (gamma = 0.8)': {'ResNet': 0.48334503,'aHash': 0.0,'pHash': 0.0},
 'Gamma correction (gamma = 1.2)': {'ResNet': 0.381176,'aHash': 0.0,'pHash': 1.0},
 'Gamma correction (gamma = 1.5)': {'ResNet': 1.7187691,'aHash': 2.0,'pHash': 1.0},
 'Gamma correction (gamma = 2.0)': {'ResNet': 4.074257,'aHash': 6.0,'pHash': 2.0},
 'Insert text (fontScale = 1)': {'ResNet': 0.7838249,'aHash': 0.0,'pHash': 0.0},
 'Insert text (fontScale = 2)': {'ResNet': 1.0911484,'aHash': 0.0,'pHash': 1.0},
 'Insert text (fontScale = 3)': {'ResNet': 2.7721176,'aHash': 0.0,'pHash': 2.0},
 'Insert text (fontScale = 4)': {'ResNet': 4.646305,'aHash': 0.0,'pHash': 4.0},
 'Insert text (fontScale = 5)': {'ResNet': 8.435852,'aHash': 2.0,'pHash': 3.0},
 'Insert text (fontScale = 6)': {'ResNet': 11.267036,'aHash': 6.0,'pHash': 3.0},
 'Insert text (fontScale = 7)': {'ResNet': 15.272251,'aHash': 2.0,'pHash': 7.0},
 'Smoothing (kernel size = 3': {'ResNet': 1.3798943,'aHash': 2.0,'pHash': 0.0},
 'Smoothing (kernel size = 5': {'ResNet': 3.1528091,'aHash': 4.0,'pHash': 1.0},
 'Smoothing (kernel size = 7': {'ResNet': 4.903698,'aHash': 4.0,'pHash': 1.0},
 'Smoothing (kernel size = 9': {'ResNet': 6.8400574,'aHash': 4.0,'pHash': 1.0},
 'Smoothing (kernel size = 11': {'ResNet': 9.477722,'aHash': 5.0,'pHash': 2.0},'Translation (1 pixels)': {'ResNet': 0.47764206,'aHash': 6.0,'pHash': 0.0},
 'Translation (2 pixels)': {'ResNet': 0.98942566,'aHash': 10.0,'pHash': 3.0},
 'Translation (4 pixels)': {'ResNet': 1.475399,'aHash': 15.0,'pHash': 5.0},
 'Translation (8 pixels)': {'ResNet': 2.587471,'aHash': 20.0,'pHash': 13.0},
 'Translation (16 pixels)': {'ResNet': 3.1883087,'aHash': 25.0,'pHash': 21.0},
 'Translation (32 pixels)': {'ResNet': 4.8445663,'aHash': 23.0,'pHash': 31.0},
 'Translation (64 pixels)': {'ResNet': 9.34531,'aHash': 28.0,'pHash': 30.0}}
</code></pre><p>*For the output actually obtained, the order is changed and the indent is adjusted to improve the visibility.</p>
<h2 id="consideration">Consideration</h2>
<p>Please note that ResNet uses only what was pre-learned with ImageNet. In other words, by preparing learning data, it is possible to acquire networks with characteristics different from those described below.</p>
<p>Also, the tendency may change depending on the image. For practical use, it is better to evaluate with a fairly proper data set.</p>
<ul>
<li>Identical: Obviously, the distance is 0 for all hashing methods.</li>
<li>Grayscale: In aHash and pHash, since it is grayscale at the beginning of processing, the distance from the original image is 0.</li>
<li>Reduction: In ResNet and aHash, the distance increases according to the scale, but pHash is relatively robust.</li>
<li>JPEG compression: It&rsquo;s interesting that ResNet seems to be proportional to the compression ratio. On the other hand, aHash and pHash are relatively robust. A simple hashing algorithm (high and low encoding of the mean) may be robust against compression degradation.</li>
<li>Gamma correction: aHash and pHash seem to be more robust than ResNet.</li>
<li>Text insertion: aHash seems relatively robust. It is possible that the average of pixel values in the image has changed according to the color of the inserted text, and the encoding result has not changed. On the other hand, in pHash and ResNet, the distance increases with the size of the text.</li>
<li>Smoothing: pHash is relatively robust. This is probably because we are only focusing on the low frequency components.</li>
<li>Translation: All hashes are separated by the amount of translation. Is ResNet relatively robust?</li>
</ul>
<p>#Summary
We introduced a method to hash images. In addition, we measured the distance between the image with some processing and the original image and observed the characteristics of each hashing method. Among the algorithms compared in this article, we can observe that pHash is robust against image scaling, compression degradation, and smoothing.</p>
<p>I recently learned about xHash type algorithms, but I think they are simple and based on good ideas. I think it&rsquo;s a relatively dead algorithm, but it would be nice to be able to use it in the right place.</p>
<p>#Reference</p>
<ul>
<li>
<p>Looks Like It-The Hacker Factor Blog
<a href="http://www.hackerfactor.com/blog/index.php?/archives/432-Looks-Like-It.html">http://www.hackerfactor.com/blog/index.php?/archives/432-Looks-Like-It.html</a></p>
</li>
<li>
<p>Try to compare images using perceptual hash (phash)
<a href="http://hideack.hatenablog.com/entry/2015/03/16/194336">http://hideack.hatenablog.com/entry/2015/03/16/194336</a></p>
</li>
</ul>
<section class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1" role="doc-endnote">
<p>C. Zauner, &ldquo;<a href="https://www.phash.org/docs/pubs/thesis_zauner.pdf">Implementation and Benchmarking of Perceptual Image Hash Functions</a>,&rdquo; Upper Austria University of Applied Sciences, Hagenberg Campus , 2010. <a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2" role="doc-endnote">
<p><a href="http://pic2recipe.csail.mit.edu/">Recipe1M+: A Dataset for Learning Cross-Modal Embeddings for Cooking Recipes and Food Images-MIT</a> <a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3" role="doc-endnote">
<p>Actually, when I was reading this paper, I thought, &ldquo;Is there an easier way to identify duplicates?&rdquo; <a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</section>

    </article>
  </div>

  
  
  
  <script>
  window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
  ga('create', 'UA-169005401-1', 'auto');
  ga('send', 'pageview');
  </script>
  <script async src='https://www.google-analytics.com/analytics.js'></script>
  

  

  
<link rel="stylesheet" type="text/css" href="//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.0/cookieconsent.min.css" />
<script src="//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.0/cookieconsent.min.js"></script>
<script>
window.addEventListener("load", function(){
window.cookieconsent.initialise({
  "palette": {
    "popup": {
      "background": "#216942",
      "text": "#b2d192"
    },
    "button": {
      "background": "#afed71"
    }
  }
})});
</script>

</body>

</html>
