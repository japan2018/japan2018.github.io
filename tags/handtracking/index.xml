<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title> handtracking on Memo Tut</title>
    <link>https://memotut.com/tags/handtracking/</link>
    <description>Recent content in  handtracking on Memo Tut</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 19 Dec 2019 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://memotut.com/tags/handtracking/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>[Python] Case where hand recognition has reached a practical level</title>
      <link>https://memotut.com/3e43be59d/</link>
      <pubDate>Thu, 19 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://memotut.com/3e43be59d/</guid>
      <description>In HandTracking of MediaPipe (Google), I will write about the gesture implementation that is being talked about in Issue.  It worked like this Seeing is believing that the picture is worth a thousand words. MediaPipe With the announcement of MediaPipe at CVPR 2019 in June this year, Hand Tracking has been released as one of the applications. MediaPipe doesn&amp;rsquo;t require you to write code called ML pipeline A framework for building ML applications using only visualization tools.</description>
    </item>
    
  </channel>
</rss>