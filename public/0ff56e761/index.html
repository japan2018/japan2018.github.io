<!doctype html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  
  <meta name="generator" content="Hugo 0.72.0" />

  
  <meta name="description" content="Some description">
  

  
  <link rel="apple-touch-icon" sizes="180x180" href="https://memotut.com/apple-touch-icon.png">

  
  <link rel="icon" type="image/png" sizes="32x32" href="https://memotut.com/favicon-32x32.png">

  
  <link rel="icon" type="image/png" sizes="16x16" href="https://memotut.com/favicon-16x16.png">

  
  <link rel="manifest" href="https://memotut.com/site.webmanifest">

  
  <link rel="mask-icon" href="https://memotut.com/safari-pinned-tab.svg" color="#5bbad5">

  <meta name="msapplication-TileColor" content="#da532c">

  <meta name="theme-color" content="#ffffff">

  
  <link rel="stylesheet" href="https://memotut.com/css/bootstrap.min.css" />

  
  <title>[Python] [Introduction] Reinforcement learning | Memo Tut</title>
  

  <style>
body {
  min-width: 300px;
}

.custom-navbar {
  margin-bottom: 1em;
  height: 60px;
}

.custom-navbar a {
  display: inline-block; 
  padding: 18px 0;
  margin-right: 1em; 
  font-weight: bold; 
}

.custom-navbar a:hover,
.custom-navbar a:focus {
  text-decoration: none; 
}

@media print {
  .custom-navbar {
    display: none;
  }
}

article {
  padding-bottom: 1em;
}

img {
  max-width: 100%;
}


body {
  background-color: #fff;
}



body {
  color: #212529;
}



a {
  color: #007bff;
}



a:hover,
a:focus {
  color: #0056b3;
}



.custom-navbar {
  background-color: #212529;
}



.custom-navbar a {
  color: rgba(255, 255, 255, 0.75);
}



.custom-navbar a:hover,
.custom-navbar a:focus {
  color: rgba(255, 255, 255, 1);
}



.container {
  max-width: 800px;
}



pre {
  display: block;
  padding: 9.5px;
  word-break: break-all;
  word-wrap: break-word;
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 4px;
}

pre code {
  padding: 0;
  font-size: inherit;
  color: inherit; 
  white-space: pre-wrap;
  background-color: transparent;
  border: none;
  border-radius: 0;
}

code {
  padding: 2px 4px;
  color: inherit; 
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 4px;
  font-size: .9em;
}



blockquote,
.blockquote {
  padding: 10px 20px;
  margin: 0 0 20px;
  font-size: 1em;
  border-left: 5px solid #6c757d;
}

</style>
</head>

<body>
  <nav class="custom-navbar">
  <div class="container">
    
    <a href="/">Posts</a>
    
    <a href="/tags/">Tags</a>
    
    <a href="/about/">About</a>
    
    <a href="/index.xml">RSS</a>
    
  </div>
</nav>
  
  <div class="container">
    <article>
      <h1>[Python] [Introduction] Reinforcement learning</h1>
<p>
  <small class="text-secondary">
  
  
  May 18, 2020
  </small>
  

<small><code><a href="https://memotut.com/tags/python">Python</a></code></small>


<small><code><a href="https://memotut.com/tags/deeplearning"> DeepLearning</a></code></small>


<small><code><a href="https://memotut.com/tags/reinforcement-learning"> reinforcement learning</a></code></small>


<small><code><a href="https://memotut.com/tags/ai"> AI</a></code></small>

</p>
<pre><code>&lt;script async src=&quot;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_CHTML&quot;&gt;&lt;/script&gt;
</code></pre>
<script type="text/x-mathjax-config">
 MathJax.Hub.Config({
 tex2jax: {
 inlineMath: [["\\(","\\)"] ],
 displayMath: [['$$','$$'], ["\\[","\\]"]]
 }
 });
</script>
<p>This is a summary of a quick study of reinforcement learning.
I hope it will be a reference for beginners.</p>
<h2 id="what-is-reinforcement-learning">What is reinforcement learning?</h2>
<p>The position of reinforcement learning is like this.</p>
<img width="500" alt="Reinforcement learning" src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/138735/976147cb-db37-7a94-6f45-4b1314d9d487.png">
<pre><code>【the term】

- Supervised learning
- Input and output as teacher data
- Regression and classification problems
- Unsupervised learning
- No teacher data
- Extract data features and transform expressions
</code></pre><p>What we will do in reinforcement learning is
We will “learn behaviors that maximize future value”.</p>
<h3 id="reinforcement-learning-model">Reinforcement learning model</h3>
<p>The basic mechanism of reinforcement learning is as follows.</p>
<img width="500" alt="Model for reinforcement learning" src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/138735/021e1fde-89e3-25fe-89d3-2bf23eee613d.png">
<p>$t$ below indicates an optional step</p>
<ul>
<li>Agent (decision maker): The person who makes decisions and learns</li>
<li>Environment: What the agent interacts with</li>
<li>State: Situation given to the agent after the environment reflects the agent&rsquo;s behavior, $s_t$</li>
<li>Action: $a_t$</li>
<li>Reward: $r_t$</li>
<li>Policy: $π_t(s, a)$: Action strategy represented by probability distribution. Indicates the probability of taking a certain action in any state</li>
</ul>
<p>First, given a certain &ldquo;state&rdquo; $s_t$ from the environment
→ The &ldquo;agent&rdquo; selects &ldquo;action&rdquo; $a_t$ according to &ldquo;policy&rdquo; $\pi_t(s,a)$
→ In the next step, get &ldquo;reward&rdquo; $r_{t+1}$ and &ldquo;state&rdquo; $s_{t+1}$ as feedback from &ldquo;Environment&rdquo;</p>
<p>It is the flow.</p>
<h3 id="markov-decision-process-mdp">Markov Decision Process (MDP)</h3>
<p>Thus in reinforcement learning</p>
<blockquote>
<p>Next state $ s_{t+1}$ (probability of) is determined only by current state $ s_{t}$ and selected action $a_{t}$</p>
<p>Reward $r_{t+1}$ (probability of) is determined only by next state $s_{t+1}$, current state $s_{t}$ and selected action $a_{t}$ thing</p>
</blockquote>
<p><a href="https://www.slideshare.net/pfi/nlp2018-introduction-of-deep-reinforcement-learning">Citation: Introduction of Deep Reinforcement Learning</a></p>
<p>The discussion will proceed on the premise.
This premise is called the Markov decision process (MDP)</p>
<h2 id="reinforcement-learning-solution">Reinforcement learning solution</h2>
<p>Reinforcement learning is modeled as shown in the figure above, but what is the point of interest here?</p>
<p>Reinforcement learning was &ldquo;learning behaviors that maximize future value,&rdquo; so agents want to know how to maximize the rewards they will get in the future.</p>
<p>In addition, it is natural to multiply the reward obtained in the future by the discount rate of $γ(0 \leqq γ \leqq 1)$ to express it as the present value. From the above, the sum of a series of rewards (cumulative reward) is expressed as follows.</p>
<p>$$ R = \sum_{t=0}^{∞}γ^tr_{t+1} $$</p>
<p>In reinforcement learning, there is a motivation to find a method that maximizes this expected value.</p>
<p>There are two major methods for seeking such a measure.</p>
<h2 id="value-function-base">Value function base</h2>
<p>It is a method of estimating the value function and indirectly seeking the optimal policy.</p>
<p>By using a function that expresses how good the value of a certain state (the value that is unknown until you actually act) is to seek a measure that will take the action of moving to a state of high value.</p>
<p>So what does a &ldquo;value function&rdquo; look like?</p>
<p>Ultimately, you should be able to choose the action that maximizes the value of taking a certain action in a certain state while taking a certain policy.
→ Therefore, we want to find a function that represents &ldquo;the value of taking a certain action in a certain state while taking a certain measure&rdquo;.
→ It seems that we should pay attention only to a specific &ldquo;action&rdquo; in the function that represents &ldquo;the value in a certain state when taking a certain policy&rdquo;.
→ Then, what is the value function that represents “state”? It&rsquo;s likely to be represented by the expected value of the cumulative reward obtained in certain situations as above
→ Therefore, the function that represents &ldquo;the value in a certain state when a certain measure is taken&rdquo; is expressed by the following formula (state-value function).</p>
<p>$$ V^{\pi}(s) = \mathbb{E}\bigl[ \sum_{i=1}{\gamma^{i-1}r_{t+i}} \bigr| s\bigr] $$</p>
<p>→ Therefore, the function that expresses the &ldquo;value of taking a certain action in a certain state while taking a certain measure&rdquo; is expressed by the following formula (action value function).</p>
<p>$$ Q^{\pi}(s,a) = \mathbb{E}\bigl[ \sum_{i=1}{\gamma^{i-1}r_{t+i}} \bigr| s, a\bigr] $$</p>
<p>The story up to this point starts from the point that &ldquo;the value of taking a certain action in a certain state, if you know the action, the action that maximizes the value will be selected&rdquo;. It is likely that the objective of learning will be fulfilled if the optimal policy is a policy that achieves a certain thing (function that can take the action of obtaining the highest value).</p>
<p>The optimal state-value function can be expressed by the following &ldquo;optimal Bellman equation&rdquo;.
(Here, I will not explain what is the optimal Bellman equation and why it is such a transformation.)</p>
<p>$$ V^\ast(s) = \underset{a}{\max}\bigr[R_{(s,a)}{+}\gamma{\sum{p(s'|s, a)}{ V^\ast(s&rsquo;)}} \bigr] $$</p>
<ul>
<li>$ s'$: &ldquo;state&rdquo; in the next step</li>
<li>$ p $: State transition probability, probability of transitioning to a certain &ldquo;state&rdquo;</li>
</ul>
<h3 id="if-the-environment-parameters-are-known">If the environment parameters are known</h3>
<p>When considering the above optimal Bellman equation, if $R$ and $p$ are known, the value of $V$ can be updated by the following steps.</p>
<blockquote>
<ol>
<li>
<p>Initialize V(s) to an appropriate value (such as zero) for all states s</p>
</li>
<li>
<p>Calculate the following expression and update the value for all V(s)</p>
</li>
<li>
<p>Repeat step 2 until the values converge</p>
</li>
</ol>
</blockquote>
<p><a href="%5Bhttps://qiita.com/Hironsan/items/56f6c0b2f4cfd28dd906#%E4%BE%A1%E5%80%A4%E5%8F%8D%E5%BE%A9%E6%B3%95value-iteration%5D(https://qiita.com/Hironsan/items/56f6c0b2f4cfd28dd906#valueiterationmethodvalue-iteration)">Reference: Reinforcement learning with Python</a></p>
<h3 id="if-you-dont-know-the-environment-parameters">If you don&rsquo;t know the environment parameters</h3>
<p>In this case, it is necessary to actually perform &ldquo;action&rdquo; and sample the information.</p>
<p>In the value function base, the purpose is to derive the optimal state value function (and thus the optimal behavioral value function), and this can be obtained by the value iteration method when the environmental information is sufficient. If it is insufficient, it is necessary to collect the information first, so it is necessary to obtain $p$ information and update $V$ using a method such as Monte Carlo method.</p>
<p>(Note that even if you use Monte Carlo method etc., you can only get information by approximation. If you get perfect p, you can use the value iteration method at that point.)</p>
<p>Below are some value-function-based approaches.</p>
<h4 id="monte-carlo-method">Monte Carlo method</h4>
<p>We will repeatedly sample episodes (until the decision of winning or losing in Othello or shogi etc.) and use it to estimate the value function.</p>
<p>That is, the value function is updated as follows.</p>
<p>$$ V(s_t) \leftarrow V(s_t)+\alpha[R_t-V(s_t)] $$</p>
<ul>
<li>$R_t$: reward at t</li>
<li>$\alpha$: learning rate</li>
</ul>
<h4 id="td-method">TD method</h4>
<p>While the Monte Carlo method estimated the value function after the episode was completed, the TD method updates the value function at each step.
In the Monte Carlo method, the value function is updated so that the difference between $R_T$ and $V(S_t)$ becomes small (as shown in the above equation). Then, in the TD method, Become.</p>
<p>$$ V(s_t) \leftarrow V(s_t)+\alpha[r_{t+1}+\gamma V(s_{t+1}) -V(s_t)] $$</p>
<ul>
<li>$\gamma$: Discount rate</li>
</ul>
<p>In other words, &ldquo;sum of reward and value function in the next step&rdquo; is expressed as the value of the value function obtained from the sample, and it is updated by the difference between it and the &ldquo;current value function&rdquo;.</p>
<p>❇︎ When using the Monte Carlo method or the TD method, there is a risk of taking a local solution due to the possibility of biased sampling, and the following methods are used to avoid this.</p>
<ul>
<li>ε-greedy policy
<ul>
<li>A method of taking a random action at a certain probability ε (&lt; 1) instead of taking an action based on the action value function Q</li>
</ul>
</li>
<li>Softmax policy
<ul>
<li>Make it easier for you to choose an action that is highly estimated by the action value function Q. For those that are not, it is difficult to choose, but it does not mean that they are not chosen at all.</li>
</ul>
</li>
</ul>
<h4 id="q-learning">Q learning</h4>
<p>The TD method is applied to the estimation of the action value function and uses the ε-greedy policy.</p>
<p>The update formula looks like this:</p>
<p>$$ Q(s_t,a_t) \leftarrow Q(s,a)+\alpha[r_{t+1}+\gamma max Q(s_{t+1},a_{t+1})-Q(s_t, a_t)] $$</p>
<p>Looking at the above formula, it is updated by the value function that maximizes the value of the value function.
This means that instead of using the action policy (action policy by the ε-greedy policy) that is actually taken when proceeding with an episode, the maximum value is used to update the estimated value. Such a method is called policy off type.</p>
<pre><code>【the term】

Policy on, Policy off
- It is said that the policy to be specified when updating the value function is the policy to be evaluated (the policy that was actually used), and is not policy-type, if it is not (for example, the action that maximizes the value function) When using a policy) is called a policy off type.
</code></pre><h4 id="sarsa">SARSA</h4>
<p>It has the same characteristics as Q learning as follows.* Application of TD method to action value function</p>
<ul>
<li>Use ε-greedy policy</li>
</ul>
<p>However, the difference is that it is a policy-on method.
Let&rsquo;s look at the update formula.</p>
<p>$$ Q(s_t,a_t) \leftarrow Q(s,a)+\alpha[r_{t+1}+\gamma Q(s_{t+1},a_{t+1})-Q(s_t, a_t)] $$</p>
<p>As you can see from this formula, the action policy (action policy by ε-greedy policy) used when proceeding with an episode is used at the time of updating, and such a method is called policy-on type.</p>
<h4 id="dqn">DQN</h4>
<p>In DQN, a behavioral value function is approximated using a neural network, and methods such as Experience replay and double network are used to stabilize learning.</p>
<h5 id="experience-replay">Experience replay</h5>
<p>If the sampled information is used as it is, the information will remain strongly correlated as a time series, and learning will not be successful. To prevent this, we collect samples and take some of them out for batch learning.</p>
<h5 id="double-network">Double network</h5>
<p>In Q-learning, the value of max was used when updating the value function, but this may cause certain behavioral values to be overestimated (especially if there are extremely valuable behaviors). In order to prevent this, it is a method of creating and learning a network for performing value evaluation different from the main.</p>
<p>| | Function update timing | Policy on/off | Features |
| &mdash;&mdash;&mdash;&mdash;&ndash; | &mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&ndash; | &mdash;&mdash;&mdash;&mdash;- &mdash;&mdash;&mdash;- | &mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash; &mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash; |
| Monte Carlo method | At the end of trial | On/off types are available | Local solution may be taken. <br />Since the sampling is repeated from the start to the end of the episode, it is necessary to wait until the end of the episode to be able to calculate. |
TD method | step by step |-| Local solution may be taken. <br />Because it is updated at each step (=does not wait for the completion of the episode), the accuracy is low in the initial stage compared with the Monte Carlo method that estimates the entire episode. |
Q learning | Step by step | Policy off | Application of TD method to action value function<br /> Utilizing ε-greedy policy <br /> Policy off |
SARSA | Step by step | Policy on | Application of TD method to action value function<br /> Utilizing ε-greedy policy <br /> Policy on |
| DQN | Step by step | Policy off | Approximate the optimal action value function using a neural network. |</p>
<h2 id="policy-function-base">Policy function base</h2>
<p>It is a method of improving the current policy, which directly seeks the optimum policy (without going through the value function).
In the value function base, the &ldquo;value function&rdquo; was updated, whereas in the policy function base, the parameters of the policy are updated by a gradient-based method to improve the policy itself.</p>
<h3 id="policy-gradient-method">Policy gradient method</h3>
<p>The objective function to be considered on the basis of the policy function is $J(\theta)$.
This is expressed using the cumulative reward, Q function, and state value function, and the purpose is to maximize this.</p>
<p>In the policy gradient method, there is a motivation to know how the change of the parameter $\theta$ affects the objective function, and the results are used to update $\theta$ using an arbitrary learning rate.
Here, if the policy model is $\pi_\theta(s,a)$, the training of its parameter $\theta$ can be expressed as follows. ($\delta$ is the learning rate)</p>
<p>$$ \theta \leftarrow \theta + \delta\nabla_\theta J(\theta) $$</p>
<p>Also, &ldquo;how the change of the parameter $\theta$ affects the objective function&rdquo; can be expressed as a derivative, which is expressed by the policy gradient theorem as follows.
(I will not explain the policy gradient theorem here.)</p>
<p>$$ \begin{eqnarray}
\nabla_{\theta}J(\theta) &amp;=&amp; E_s\Bigl[ \sum_{a}Q^\pi(s,a)~\nabla_\theta(a|s;\theta) \Bigr]\<br>
&amp;=&amp; E_{(s,a)}\Bigl[ Q^{\pi_{\theta}}(s,a)~\nabla_{\theta}\log{\pi_{\theta}}(a|s ) \Bigr] \end{eqnarray}$$</p>
<p>So the policy gradient algorithm can be summarized in the following steps.</p>
<blockquote>
<p>The reinforcement learning algorithm based on this policy gradient is roughly divided into the following three procedures.
① Action based on action policy $\pi_{\theta(s,a)}$
② Evaluation of action policy $\pi_{\theta(s,a)}$
③ Update action policy $\pi_{\theta(s,a)}$</p>
</blockquote>
<p><a href="%5Bhttp://yagami12.hatenablog.com/entry/2019/02/22/210608#%E6%96%B9%E7%AD%96%E5%8B%BE%E9%85%8D%E3%81%AB%E5%9F%BA%E3%81%A5%E3%81%8F%E5%BC%B7%E5%8C%96%E5%AD%A6%E7%BF%92%E6%89%8B%E6%B3%95%5D(http://yagami12.hatenablog.com/entry/2019/02/22/210608#Reinforcementlearningmethodbasedonpolicygradient)">Quotation: Reinforcement Learning Star Bookshelf</a></p>
<p>By repeating this procedure, you will obtain the optimal strategy.</p>
<p>$Q$ appears in the formula represented by the policy gradient theorem. If the variance of this $Q$ function is large, learning will be difficult to proceed, so we may try to suppress the variance by taking the difference with a function with a similar distribution, and call this method the baseline. ..
In addition, the bias function set at the baseline is $V$ and is called the advantage function.</p>
<h3 id="main-algorithms">Main algorithms</h3>
<h4 id="reinforce">REINFORCE</h4>
<p>Looking at the formula obtained by the policy gradient theorem, $Q$ appears.
Therefore, it is necessary to find (approximate) it, but in this algorithm, sampling is performed and $Q$ is approximated by the &ldquo;mean value of reward&rdquo;.</p>
<h4 id="actor-critic">Actor-Critic</h4>
<p>While REINFORCE approximated $Q$ with the average reward, Actor-Critic will also learn and update that $Q$.</p>
<h2 id="reference">Reference</h2>
<ul>
<li><a href="https://blog.brainpad.co.jp/entry/2017/02/24/121500">Introduction to Reinforcement Learning-Basic knowledge for those who want to learn Reinforcement Learning-</a></li>
<li><a href="https://qiita.com/ishizakiiii/items/5eff79b59bce74fdca0d">I understand DQN (Deep Q Network), so I explained using Gopher&rsquo;s diagram</a></li>
<li><a href="https://www.slideshare.net/pfi/nlp2018-introduction-of-deep-reinforcement-learning">Introduction of Deep Reinforcement Learning</a></li>
<li><a href="http://yagami12.hatenablog.com/entry/2019/02/22/210608">Reinforcement Learning Star Bookshelf</a></li>
<li><a href="https://qiita.com/shionhonda/items/ec05aade07b5bea78081">Summary of Deep Reinforcement Learning Algorithm</a></li>
<li><a href="https://qiita.com/Hironsan/items/56f6c0b2f4cfd28dd906">Reinforcement learning with Python</a></li>
</ul>

    </article>
  </div>

  
  
  
  <script>
  window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
  ga('create', 'UA-169005401-1', 'auto');
  ga('send', 'pageview');
  </script>
  <script async src='https://www.google-analytics.com/analytics.js'></script>
  

  

  
<link rel="stylesheet" type="text/css" href="//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.0/cookieconsent.min.css" />
<script src="//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.0/cookieconsent.min.js"></script>
<script>
window.addEventListener("load", function(){
window.cookieconsent.initialise({
  "palette": {
    "popup": {
      "background": "#216942",
      "text": "#b2d192"
    },
    "button": {
      "background": "#afed71"
    }
  }
})});
</script>

</body>

</html>
