<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title> PyTorch on Memo Tut</title>
    <link>https://memotut.com/tags/pytorch/</link>
    <description>Recent content in  PyTorch on Memo Tut</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 03 Jun 2020 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://memotut.com/tags/pytorch/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Created a tool that visualizes mechanically from repository history in order to objectively know the productivity and soundness of the development team</title>
      <link>https://memotut.com/created-a-tool-that-visualizes-mechanically-from-repository-history-in-order-to-objectively-know-the-productivity-and-soundness-of-the-development-team-ceece/</link>
      <pubDate>Wed, 03 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>https://memotut.com/created-a-tool-that-visualizes-mechanically-from-repository-history-in-order-to-objectively-know-the-productivity-and-soundness-of-the-development-team-ceece/</guid>
      <description>#Introduction  The productivity and soundness of a software development team can be understood as an internal sensation, but it is difficult for an external person to see it. This information asymmetry has caused problems in relationships with people outside the development team.
Also, for EMs, CTOs that bundle multiple development teams and products, and visualizing each situation with objective numbers and graphs, it is important reference information when considering the overall strategy.</description>
    </item>
    
    <item>
      <title>[Implementation explanation] Japanese BERT x unsupervised learning (information maximization clustering) to classify livedoor news: Google Colaboratory (PyTorch)</title>
      <link>https://memotut.com/implementation-explanation-japanese-bert-x-unsupervised-learning-information-maximization-clustering-to-classify-livedoor-news-google-colaboratory-pytorch-6a887/</link>
      <pubDate>Thu, 21 May 2020 00:00:00 +0000</pubDate>
      
      <guid>https://memotut.com/implementation-explanation-japanese-bert-x-unsupervised-learning-information-maximization-clustering-to-classify-livedoor-news-google-colaboratory-pytorch-6a887/</guid>
      <description>In this article, we will implement and explain how clustering is performed by using the Japanese version BERT in Google Colaboratory and performing unsupervised learning information maximization clustering for livedoor news.   Using the Japanese version of BERT with Google Colaboratory, the content until vectorization of sentences, Information maximization clustering for unsupervised learning in MNIST  About this, I have explained it in the series articles so far, so please see here first.</description>
    </item>
    
    <item>
      <title>The relationship between brain science and unsupervised learning. Information maximization with unsupervised learning MNIST: Google Colabratory (PyTorch)</title>
      <link>https://memotut.com/the-relationship-between-brain-science-and-unsupervised-learning.-information-maximization-with-unsupervised-learning-mnist-google-colabratory-pytorch-e3c01/</link>
      <pubDate>Sun, 17 May 2020 00:00:00 +0000</pubDate>
      
      <guid>https://memotut.com/the-relationship-between-brain-science-and-unsupervised-learning.-information-maximization-with-unsupervised-learning-mnist-google-colabratory-pytorch-e3c01/</guid>
      <description>In this article, I will explain unsupervised learning and brain science, and IIC&#39;s implementation of high performance in MNIST for unsupervised learning.  The papers of interest in this article are: Invariant Information Clustering for Unsupervised Image Classification and Segmentation is.
In this paper, we utilize an index called mutual information to classify handwritten digit images by unsupervised learning clustering.
It is called IIC (Invariant Information Clustering).
In this article, we will discuss brain science and unsupervised learning, IIC points, and implementation examples in MNIST.</description>
    </item>
    
    <item>
      <title>10 techniques to improve BERT accuracy</title>
      <link>https://memotut.com/10-techniques-to-improve-bert-accuracy-34330/</link>
      <pubDate>Fri, 15 May 2020 00:00:00 +0000</pubDate>
      
      <guid>https://memotut.com/10-techniques-to-improve-bert-accuracy-34330/</guid>
      <description>## Introduction  It has become commonplace to fine-tune and use BERT for natural language processing tasks. It is expected that there will be more and more scenes where you want to improve the accuracy even a little when doing competitions such as Kaggle or projects where accuracy requirements are tight. Therefore, we will summarize the accuracy improvement method. Classification task is assumed as a task.
Adjust number of characters You can enter up to 512 words in the learned BERT.</description>
    </item>
    
    <item>
      <title>[Implementation explanation] How to use Japanese version BERT in Google Colaboratory (PyTorch)</title>
      <link>https://memotut.com/implementation-explanation-how-to-use-japanese-version-bert-in-google-colaboratory-pytorch-e522a/</link>
      <pubDate>Thu, 14 May 2020 00:00:00 +0000</pubDate>
      
      <guid>https://memotut.com/implementation-explanation-how-to-use-japanese-version-bert-in-google-colaboratory-pytorch-e522a/</guid>
      <description>In this article, I will explain how to use the Japanese version of BERT with Google Colaboratory.  About the BERT itself, the book I wrote last year
&amp;ldquo;Learn while making! Development deep learning with PyTorch&amp;rdquo;
Is explained in detail in.
If you want to know how BERT works, please see the above book.
Since the book only dealt with the English version, this post will explain how to use BERT in the Japanese version.</description>
    </item>
    
    <item>
      <title>Check your squat form with deep learning</title>
      <link>https://memotut.com/check-your-squat-form-with-deep-learning-bd59e/</link>
      <pubDate>Sat, 11 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>https://memotut.com/check-your-squat-form-with-deep-learning-bd59e/</guid>
      <description>**The video used in this article was recorded a long time ago, and I currently refrain from exercising at the gym. **  #Introduction This is the second edition of &amp;ldquo;Muscle Training x Deep Learning&amp;rdquo;. The first is here (Deep learning made it easier to see the time-lapse of physical changes). &amp;ldquo;Form check&amp;rdquo; that many trainees (people who love muscle training) have become habitual. You can fix your smartphone during the training, take a selfie, and look back later to know the habit of your form and improve the quality of your training afterwards!</description>
    </item>
    
    <item>
      <title>Deep learning of Raspberry Pi 3 B&#43; &amp; PyTorch for real-time classification of multiple objects in camera images</title>
      <link>https://memotut.com/deep-learning-of-raspberry-pi-3-b-pytorch-for-real-time-classification-of-multiple-objects-in-camera-images-a09b4/</link>
      <pubDate>Sun, 29 Mar 2020 00:00:00 +0000</pubDate>
      
      <guid>https://memotut.com/deep-learning-of-raspberry-pi-3-b-pytorch-for-real-time-classification-of-multiple-objects-in-camera-images-a09b4/</guid>
      <description>I got Razpai 3B+ and picamera because of university class. Since I&#39;m free, I decided to let Razpai classify using deep learning. However, instead of classifying the pictures taken in advance, it classifies the objects in the real-time image from picamera and displays them in a nice way.  It may be at the student level, but I&amp;rsquo;d appreciate it if you can refer to even a part of it.</description>
    </item>
    
    <item>
      <title>How the AI department and development team work (in my case)</title>
      <link>https://memotut.com/how-the-ai-department-and-development-team-work-in-my-case-0fc60/</link>
      <pubDate>Fri, 27 Mar 2020 00:00:00 +0000</pubDate>
      
      <guid>https://memotut.com/how-the-ai-department-and-development-team-work-in-my-case-0fc60/</guid>
      <description>In this article, I will introduce the working style of Ogawa, who belongs to the development team of the AI Technology Department.  This article is an introduction for me.
The way you work depends on the member, and also on the family structure. I am married but I have no children.
Also, different departments work differently.
In this article, I will introduce the working week for me (Yutaro Ogawa) of AI Technology Department.</description>
    </item>
    
    <item>
      <title>PyTorch C&#43;&#43; VS Python (2019 version)</title>
      <link>https://memotut.com/pytorch-c-vs-python-2019-version-274e5/</link>
      <pubDate>Tue, 24 Mar 2020 00:00:00 +0000</pubDate>
      
      <guid>https://memotut.com/pytorch-c-vs-python-2019-version-274e5/</guid>
      <description>There are many types of deep learning frameworks such as PyTorch, Tensorflow, Keras.  This time, I would like to pay attention to PyTorch, which I often use!
Did you know that C++ version is released in addition to PyTorch and Python version? This makes it easy to incorporate if you want to use Deep Learning as part of your C++ program processing!
Although it is a C++ version of PyTorch, I was wondering ** &amp;ldquo;C++ may be faster than the Python version because it is a compiled language?</description>
    </item>
    
    <item>
      <title>How to Data Augmentation with PyTorch</title>
      <link>https://memotut.com/how-to-data-augmentation-with-pytorch-9b56a/</link>
      <pubDate>Mon, 16 Mar 2020 00:00:00 +0000</pubDate>
      
      <guid>https://memotut.com/how-to-data-augmentation-with-pytorch-9b56a/</guid>
      <description>Inflating data with #PyTorch (Data Augmentation)  I will summarize the method of padding data with PyTorch. Regarding PyTorch itself, I wrote an introductory article on the blog before, so please refer to the following if you like.
Introduction to the featured deep learning framework &amp;ldquo;PyTorch&amp;rdquo;
Please refer to the following article for the reason for implementing data padding and specific examples.
Image Augmentation Method for Improving Deep Learning Accuracy by Playing with Free Materials</description>
    </item>
    
    <item>
      <title>I tried to classify MNIST by GNN (with PyTorch geometric)</title>
      <link>https://memotut.com/i-tried-to-classify-mnist-by-gnn-with-pytorch-geometric-8c825/</link>
      <pubDate>Mon, 16 Mar 2020 00:00:00 +0000</pubDate>
      
      <guid>https://memotut.com/i-tried-to-classify-mnist-by-gnn-with-pytorch-geometric-8c825/</guid>
      <description>#Introduction  Hello My name is DNA1980. Recently GNN(Graph Neural Network) is popular. I also want to follow the flow and handle graphs, but there are many graph data in the world that I do not know well. I don&amp;rsquo;t know what I&amp;rsquo;m doing when I classify&amp;hellip; Separately, I think that GNN can be applied if it can be dropped into a graph even if it does not have a graph structure from the beginning like a network, so I tried applying it to everyone&amp;rsquo;s favorite MNIST.</description>
    </item>
    
    <item>
      <title>PyTorch DataLoader is slow</title>
      <link>https://memotut.com/pytorch-dataloader-is-slow-98cb0/</link>
      <pubDate>Fri, 13 Mar 2020 00:00:00 +0000</pubDate>
      
      <guid>https://memotut.com/pytorch-dataloader-is-slow-98cb0/</guid>
      <description>In PyTorch, DataLoader(`torch.utils.data.DataLoader`) is often used to extract a mini-batch from a dataset, but when experimenting with large size data, using PyTorch&#39;s DataLoader It turns out to be very time consuming. For comparison, I made and tried an iterator that retrieves mini-batches from a dataset, but I found that Pytorch&#39;s DataLoader was much slower. In particular, this can be a bottleneck when using large size data.  [Postscript: 2020/03/23] We received a comment that the slow cause is BatchSampler used by default in DataLoader.</description>
    </item>
    
    <item>
      <title>Deep learning made it easy to see the time-lapse of physical changes</title>
      <link>https://memotut.com/deep-learning-made-it-easy-to-see-the-time-lapse-of-physical-changes-f0e60/</link>
      <pubDate>Tue, 10 Mar 2020 00:00:00 +0000</pubDate>
      
      <guid>https://memotut.com/deep-learning-made-it-easy-to-see-the-time-lapse-of-physical-changes-f0e60/</guid>
      <description>#Introduction  &amp;ldquo;Self-portraits (body)&amp;rdquo; that many trainees (people who love muscle training) have become accustomed. It&amp;rsquo;s blissful to take a picture of your pumped body after training and look back later. In addition, if you display the captured image in animation like a time lapse, you can understand that muscle growth is more like a hand! This article uses deep learning to drastically make it easier to see the timelapse of the body.</description>
    </item>
    
    <item>
      <title>I tried using PIFu that generates a 3D model of a person from one image</title>
      <link>https://memotut.com/i-tried-using-pifu-that-generates-a-3d-model-of-a-person-from-one-image-ecf40/</link>
      <pubDate>Wed, 04 Mar 2020 00:00:00 +0000</pubDate>
      
      <guid>https://memotut.com/i-tried-using-pifu-that-generates-a-3d-model-of-a-person-from-one-image-ecf40/</guid>
      <description>What is #PIFu  Image quote (left): Sumire Uesaka Official Blog Nekomori Meeting
PIFu: Pixel-Aligned Implicit Function for High-Resolution Clothed Human Digitization
If you explain roughly,
A machine learning model that generates a 3D model of a person with clothes from a single image
is.
 We introduce Pixel-aligned Implicit Function (PIFu), a highly effective implicit representation that locally aligns pixels of 2D images with the global context of their corresponding 3D object.</description>
    </item>
    
    <item>
      <title>A science university student started studying deep learning in his spare time</title>
      <link>https://memotut.com/a-science-university-student-started-studying-deep-learning-in-his-spare-time-30329/</link>
      <pubDate>Fri, 21 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>https://memotut.com/a-science-university-student-started-studying-deep-learning-in-his-spare-time-30329/</guid>
      <description>#Introduction  My specs I gave the title &amp;ldquo;Science University Student&amp;rdquo;, but I think there is a punch like &amp;ldquo;I&amp;rsquo;m actually an information student&amp;rdquo; only in these articles. If so, it will be an article that hopeless for university students who are not information systems, so I will clarify my features first. I am in the second year of university, belonging to the Department of Physics, Faculty of Science. I&amp;rsquo;m in the manufacturing circle, so programming (Python) is a bit of a bit of skill, but I&amp;rsquo;m just a beginner in deep learning**.</description>
    </item>
    
    <item>
      <title>Annotate your own data to train Mask R-CNN</title>
      <link>https://memotut.com/annotate-your-own-data-to-train-mask-r-cnn-a3be8/</link>
      <pubDate>Thu, 06 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>https://memotut.com/annotate-your-own-data-to-train-mask-r-cnn-a3be8/</guid>
      <description>To do **Instance Segmentation** (object detection + segmentation)   Own data Annotation Learn Mask R-CNN I did that, but I couldn&amp;rsquo;t find any other useful articles, so I had a hard time I will share the steps I took, but only about notes.  The explanation of Mask R-CNN is omitted. Please refer to the following. Introduction to object detection using the latest Region CNN(R-CNN) ~What is object detection? R-CNN, Fast R-CNN, Faster R-CNN, Mask R-CNN~</description>
    </item>
    
    <item>
      <title>pyTorch backward cannot be done example summary</title>
      <link>https://memotut.com/pytorch-backward-cannot-be-done-example-summary-3dcb4/</link>
      <pubDate>Mon, 27 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>https://memotut.com/pytorch-backward-cannot-be-done-example-summary-3dcb4/</guid>
      <description>2020/1/27 Post  0. Target audience of this article  People who have touched python and have a good execution environment People who have touched pyTorch to some extent People who want to know about automatic differentiation by backward with machine learning with pyTorch People who want to know that pyTorch can not be backward  1.First of all Recently, the main focus of research on machine learning is the python language, because python has many libraries (called modules) for high-speed data analysis and calculation.</description>
    </item>
    
    <item>
      <title>pyTorch optim SGD thorough explanation</title>
      <link>https://memotut.com/pytorch-optim-sgd-thorough-explanation-2c67e/</link>
      <pubDate>Mon, 27 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>https://memotut.com/pytorch-optim-sgd-thorough-explanation-2c67e/</guid>
      <description>2020/1/27 Post  0. Target audience of this article  People who have touched python and have a good execution environment People who have touched pyTorch to some extent People who want to understand optimizer SGD with machine learning by pyTorch People who want to use optimizer SGD of pyTorch for other than Network model (normal variables etc.)  1.First of all Recently, the main focus of research on machine learning is the python language, because python has many libraries (called modules) for high-speed data analysis and calculation.</description>
    </item>
    
    <item>
      <title>About torch summary can be really used when building a model with Pytorch</title>
      <link>https://memotut.com/about-torch-summary-can-be-really-used-when-building-a-model-with-pytorch-bfd8f/</link>
      <pubDate>Tue, 21 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>https://memotut.com/about-torch-summary-can-be-really-used-when-building-a-model-with-pytorch-bfd8f/</guid>
      <description>#Introduction  I am building my own model and always ask &amp;ldquo;What is the feature quantity of the input?&amp;rdquo; before connecting to the fully connected layer. If you often type print(model), you can understand the structure of the model, but you can not confirm up to the size of FeatureMap. So, the convenient one is torchsummary.
Who is #torchsummary?
Simply put, you can check the size of the feature map.</description>
    </item>
    
    <item>
      <title>[PyTorch] How to use BERT-fine tuning Japanese pre-trained models to solve classification problem</title>
      <link>https://memotut.com/pytorch-how-to-use-bert-fine-tuning-japanese-pre-trained-models-to-solve-classification-problem-7f3a5/</link>
      <pubDate>Sat, 18 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>https://memotut.com/pytorch-how-to-use-bert-fine-tuning-japanese-pre-trained-models-to-solve-classification-problem-7f3a5/</guid>
      <description>#Introduction  Although BERT is updating SOTA with various tasks of natural language processing, what is published by [Google] on Github is based on Tensorflow. It has been implemented. People who use PyTorch want to use the PyTorch version, but I have not created the PyTorch version, so please use the one made by HuggingFace,butwedevelopedAskthemformoreinformationastheyarenotinvolvedin!AndQA.
Although it is a BERT made by Hugging Face, there was no Japanese pre-trained model until December 2019.</description>
    </item>
    
    <item>
      <title>Learning Graph Convolutional Networks with PyTorch</title>
      <link>https://memotut.com/learning-graph-convolutional-networks-with-pytorch-42913/</link>
      <pubDate>Tue, 07 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>https://memotut.com/learning-graph-convolutional-networks-with-pytorch-42913/</guid>
      <description>** [Twitter](https://twitter.com/omiita_atiimo)introducesartificialintelligenceandarticleswritteninothermedia**,soifyouwanttoknowmoreaboutartificialintelligence,**Pleasefeelfreetofollowus(https://twitter.com/omiita_atiimo)! **  Learning with PyTorch Graph Convolutional Networks In this article, I will explain GCN, which has been attracting a lot of attention as a neural network that can vectorize (embed) graph structures in recent years, and PyTorch Geometric, a library that can easily use GCN. The fields of application range from biology to traffic jam prediction and recommender systems. This article is GCN creator&amp;rsquo;s blog articleand[PyTorchGeometricofficialtutorial](https://pytorch-geometric.readthedocs.io/en(/latest/index.html) is used as a reference.</description>
    </item>
    
    <item>
      <title>[kotlin] Classify images on Android (Pytorch Mobile)</title>
      <link>https://memotut.com/kotlin-classify-images-on-android-pytorch-mobile-9a24b/</link>
      <pubDate>Sat, 04 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>https://memotut.com/kotlin-classify-images-on-android-pytorch-mobile-9a24b/</guid>
      <description>#PyTorch Mobile  It appeared in October of last year (2019). I could do machine learning with Android ios such as Tensolflow Lite, but finally mobile version appeared from pytorch 1.3. It&amp;rsquo;s the best from the side of using pytorch rather than tensorflow! It can be used with android ios as well as tensorflow Lite.
Click here for details PyTorch Mobile official website: https://pytorch.org/mobile/home/
 From the official website  #What to do this time Do the tutorials introduced on the official website.</description>
    </item>
    
    <item>
      <title>Learning method with original data of CenterNet(Objects as Points)</title>
      <link>https://memotut.com/learning-method-with-original-data-of-centernetobjects-as-points-9a37c/</link>
      <pubDate>Tue, 31 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://memotut.com/learning-method-with-original-data-of-centernetobjects-as-points-9a37c/</guid>
      <description>What is #CenterNet  CenterNet is the object detection method proposed in the paper Objects as Points.
After detecting the center of the object as a feature point, the size of the width and height is predicted, so it seems that there are advantages such as lighter calculation than conventional methods.
I tried to detect the positions of tennis players, balls, and courts using CenterNet, I want to do the same thing using CenterNet .</description>
    </item>
    
    <item>
      <title>Creation of negative/positive classifier using BERT</title>
      <link>https://memotut.com/creation-of-negative-positive-classifier-using-bert-b0125/</link>
      <pubDate>Wed, 25 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://memotut.com/creation-of-negative-positive-classifier-using-bert-b0125/</guid>
      <description>This post is the 25th day of &amp;quot;Natural Language Processing Advent Calendar 2019-Qiita&amp;quot;.  siny.
In this article, I summarized the creation of a negative-positive classifier using BERT, which plays a major role in natural language processing in 2019.
#Introduction I think that knowledge about BERT has appeared in books, blogs, Qiita, etc. However, since most datasets that can be used for natural language processing are based on English, and there are not many Japanese datasets, it is quite difficult to use BERT using Japanese text.</description>
    </item>
    
    <item>
      <title>I tried implementing sentence classification by Self Attention in PyTorch</title>
      <link>https://memotut.com/i-tried-implementing-sentence-classification-by-self-attention-in-pytorch-98ff5/</link>
      <pubDate>Wed, 25 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://memotut.com/i-tried-implementing-sentence-classification-by-self-attention-in-pytorch-98ff5/</guid>
      <description>This article is from the 25th day of [Pytorch Advent Calendar 2019](https://qiita.com/advent-calendar/2019/pytorch)!  #Introduction
Previous implemented Attention in Encoder-Decoder model, but this time, I will implement sentence classification in Self Attention.
The embedded expression of sentences in Self Attention is introduced in the following paper, and is also quoted in the famous paper &amp;ldquo;Attention Is All You Need&amp;rdquo; by Transformer.
 A STRUCTURED SELF-ATTENTIVE SENTENCE EMBEDDING  This article implements the Self Attention introduced in this paper.</description>
    </item>
    
    <item>
      <title>[Note spoilers] PyTorch is going to predict a five-part bride</title>
      <link>https://memotut.com/note-spoilers-pytorch-is-going-to-predict-a-five-part-bride-b5155/</link>
      <pubDate>Sat, 21 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://memotut.com/note-spoilers-pytorch-is-going-to-predict-a-five-part-bride-b5155/</guid>
      <description>I&#39;m going to rely on the five equally divided bride (serious face)  Learn the techniques of pytorch and Deep Learning while applying a five-part bride
The pytorch code I used this time is basically here
https://github.com/yoyoyo-yo/DeepLearningMugenKnock
This is spoiler for some viewers, so if you don&amp;rsquo;t like it, please close the window immediately

&amp;gt;
Recently, the author has declared that the comic will be finished in 14 volumes, and it seemed like a great progress in terms of story.</description>
    </item>
    
    <item>
      <title>CNN (U-net, DnCNN, WIN5-RB) that restores images full of noise and graffiti, and images blurred with zoom</title>
      <link>https://memotut.com/cnn-u-net-dncnn-win5-rb-that-restores-images-full-of-noise-and-graffiti-and-images-blurred-with-zoom-2ad3d/</link>
      <pubDate>Sat, 21 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://memotut.com/cnn-u-net-dncnn-win5-rb-that-restores-images-full-of-noise-and-graffiti-and-images-blurred-with-zoom-2ad3d/</guid>
      <description>#Introduction  Deep learning is a method recently used to clean dirty images, remove graffiti from images, and restore poorly-zoomed images.
The principle is simple, just input a noisy image to the converter and output a beautiful image.
Such converters are often made up of Convolutional Neural Networks (CNN).
In this article, I will try to implement such CNN with pytorch and see the results.
There are various CNN models like this, but this time I will try with three CNN models and compare the results.</description>
    </item>
    
    <item>
      <title>For the first time after breaking up, I realized that I liked Chainer&#39;s Linear.</title>
      <link>https://memotut.com/for-the-first-time-after-breaking-up-i-realized-that-i-liked-chainers-linear.-84d4e/</link>
      <pubDate>Fri, 20 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://memotut.com/for-the-first-time-after-breaking-up-i-realized-that-i-liked-chainers-linear.-84d4e/</guid>
      <description>2020/05/12 update  https://github.com/pfnet/pytorch-pfn-extras It became possible by importing this.
===
I was notified when the major update of Chainer was announced, and I wondered if it was time to come, but I decided to touch PyTorch as soon as possible for the future, and I made the model I had made with Chainer until now PyTorch I decided to implement it again.
And immediately, I hit an error&amp;hellip;
The relevant part is the following code.</description>
    </item>
    
    <item>
      <title>I tried rewriting the MNIST code of Chainer with PyTorch &#43; Ignite</title>
      <link>https://memotut.com/i-tried-rewriting-the-mnist-code-of-chainer-with-pytorch-ignite-35baf/</link>
      <pubDate>Tue, 17 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://memotut.com/i-tried-rewriting-the-mnist-code-of-chainer-with-pytorch-ignite-35baf/</guid>
      <description># TL;DR  The impression that I rewrote the MNIST code of Chainer to PyTorch was almost the same. The difference was in the layer above Updater in Chainer and the Ignite layer in PyTorch. Moreover, when chainer-pytorch-migration is actually used, Extensions used in Chainer can also be used in Ignite, and PyTorch+Ignite can be used quite like Chainer. I think that even people who have been using Chainer can naturally get used to PyTorch+Ignite.</description>
    </item>
    
    <item>
      <title>Ver 4 to make sandwich man&#39;s control by machine learning</title>
      <link>https://memotut.com/ver-4-to-make-sandwich-mans-control-by-machine-learning-2a164/</link>
      <pubDate>Sun, 15 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://memotut.com/ver-4-to-make-sandwich-mans-control-by-machine-learning-2a164/</guid>
      <description>The fourth volume to make a sandwich man&#39;s control with deep learning! ! ! ! ! ! ! ! ! ! ! !  I&amp;rsquo;m in charge of Natural Language Processing Advent Calendar 2019 (I&amp;rsquo;m sorry for delaying one day!!!) https://qiita.com/advent-calendar/2019/nlp
I want to automatically make a control like Sandwich Man, and I want to sell it to a comedy office for 4000 trillion yen. And I want to see Sandwich Man!</description>
    </item>
    
    <item>
      <title>PyTorch Three Kingdoms (Ignite, Catalyst, Lightning)</title>
      <link>https://memotut.com/pytorch-three-kingdoms-ignite-catalyst-lightning-c32e0/</link>
      <pubDate>Sat, 14 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://memotut.com/pytorch-three-kingdoms-ignite-catalyst-lightning-c32e0/</guid>
      <description># 0. Introduction  I think that deep learning frameworks are all areas where development is very fast and exciting. While there are TensorFlow, jax, etc., there is also PFN Newstheotherday,andPyTorchislikelytobemoresolid.PerhapstherewillbemorePyTorchusersinthefuture(IwillnotmentionthisarticleiftheofficialTraineralsofoundinChainerisimplementedinPyTorch,whichthreatenstheexistenceofthisarticle).
However, while PyTorch has a high degree of freedom, the code around learning (such as around the loop of each epoch) is left to the individual, and tends to be very individual code.
Writing these codes myself is a lot of learning, and I think it&amp;rsquo;s a must if you start PyTorch.</description>
    </item>
    
    <item>
      <title>Python × Flask × PyTorch Easy construction of digit recognition web application</title>
      <link>https://memotut.com/python-flask-pytorch-easy-construction-of-digit-recognition-web-application-43397/</link>
      <pubDate>Fri, 13 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://memotut.com/python-flask-pytorch-easy-construction-of-digit-recognition-web-application-43397/</guid>
      <description>Let&#39;s create an image recognition application using Python, Flask, and PyTorch.  If you combine these three, you can make a demo app easily and explosively.
#Introduction
What is Flask A web framework for Python. Django is a popular Python web framework, but Flask sells lightweight. Compared to Django, there are few functions and extension libraries, but the code is simple due to the restrictions, and you can easily create applications.</description>
    </item>
    
    <item>
      <title>Improve PyTorch execution environment with Docker November 2019</title>
      <link>https://memotut.com/improve-pytorch-execution-environment-with-docker-november-2019-a451c/</link>
      <pubDate>Thu, 12 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://memotut.com/improve-pytorch-execution-environment-with-docker-november-2019-a451c/</guid>
      <description>## Overview   Develop a deep learning execution environment with PyTorch in a Docker container After Docker version 19.03, if you have the GPU driver of the host OS and nvidia-container-runtime, everything else will be locked in a container In most cases, you can reproduce the environment with one command  Preparation The following preparations are required.
 docker version 19.03 or later -See Official Documentation(thislinkisforUbuntu) nvidia-container-runtime -Install by referring to README.</description>
    </item>
    
    <item>
      <title>[python] Check memory consumption of variables</title>
      <link>https://memotut.com/python-check-memory-consumption-of-variables-83062/</link>
      <pubDate>Sun, 08 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://memotut.com/python-check-memory-consumption-of-variables-83062/</guid>
      <description>This article is from the 8th day of [Furukawa Lab Advent_calendar](https://qiita.com/advent-calendar/2019/flab).  This article was written by a student of Furukawa Lab as part of their studies. The content may be ambiguous or the expressions may be slightly different.
#Introduction
As axes for comparing machine learning algorithms of the same task, learning stability (dependency of initial value), whether it can be explained, (Accuracy in learning with supervision (Precision), recall (Precision), recall ( (There is also an indicator called Recall)) and there is &amp;ldquo;computation amount&amp;rdquo;.</description>
    </item>
    
    <item>
      <title>I made an AI that cuts out images with good feeling using Saliency Map</title>
      <link>https://memotut.com/i-made-an-ai-that-cuts-out-images-with-good-feeling-using-saliency-map-833d3/</link>
      <pubDate>Sun, 08 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://memotut.com/i-made-an-ai-that-cuts-out-images-with-good-feeling-using-saliency-map-833d3/</guid>
      <description>## Introduction  In this article, I will implement a method for cropping images using Saliency Map using deep learning with Python and PyTorch while reading the paper.
When it comes to images in deep learning, we often try to classify handwritten numbers and detect people, but I would like you to see that you can also do such things.
This article participates in DeNA 20 New Graduate Advent Calendar 2019-Qiita.</description>
    </item>
    
    <item>
      <title>Introduce PyTorch (GPU) with Windows10 &#43; Pipenv</title>
      <link>https://memotut.com/introduce-pytorch-gpu-with-windows10-pipenv-69b30/</link>
      <pubDate>Wed, 04 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://memotut.com/introduce-pytorch-gpu-with-windows10-pipenv-69b30/</guid>
      <description>#Introduction  While people around me were switching to PyTorch saying that they would like it better, Tensorflow officially made the Tensorboard available, so I decided to change it myself, so I wrote this article.
Target audience official
pip3 install torch===1.3.1 torchvision===0.4.2 -f https://download.pytorch.org/whl/torch_stable.html It can be introduced with, but this is for people who want to do it with pipenv install hogehoge So I don&amp;rsquo;t write about Python or CUDA, so if you are expecting that, please use the browser back!</description>
    </item>
    
    <item>
      <title>Problem that torch.cuda.is_available() of pytorch gives false</title>
      <link>https://memotut.com/problem-that-torch.cuda.is_available-of-pytorch-gives-false-7962d/</link>
      <pubDate>Tue, 03 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://memotut.com/problem-that-torch.cuda.is_available-of-pytorch-gives-false-7962d/</guid>
      <description>#Environment  ・Ubuntu 16.04 ・Python 3.7.3 ・Pytorch 1.2.0
GPU cannot be used with #pytorch
I thought about deeplearning, but it&amp;rsquo;s slow so check if you can see cuda in ipython.
In [1]: import torch In [2]: torch.cuda.is_available() Out[2]: False why? As a possible cause, I thought that NVIDIA-driver is not working,
(base) user@user:~$ nvidia-smi Tue Dec 3 11:29:10 2019 +------------------------------------------------- ----------------------------+ | NVIDIA-SMI 390.116 Driver Version: 390.116 | |-------------------------------+----------------- -----+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.</description>
    </item>
    
    <item>
      <title>Make a drawing quiz with kivy&#43;PyTorch</title>
      <link>https://memotut.com/make-a-drawing-quiz-with-kivy-pytorch-75f1b/</link>
      <pubDate>Thu, 28 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>https://memotut.com/make-a-drawing-quiz-with-kivy-pytorch-75f1b/</guid>
      <description>#Summary  This article is the second day of Docomo Advent Calendar.
Do you know the drawing quiz? It is a quiz in which the questioner draws a picture and guesses what other people are drawing. There are many apps.
If you recognize the image drawn by the user and guess it, you can play it alone, so PyTorchofthemachinelearninglibrarywillcontinue.//pytorch.org/)andkivy, a library that can create apps, will be combined.
What is # kivy?</description>
    </item>
    
    <item>
      <title>PyTorch super introduction PyTorch basics</title>
      <link>https://memotut.com/pytorch-super-introduction-pytorch-basics-df87f/</link>
      <pubDate>Thu, 21 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>https://memotut.com/pytorch-super-introduction-pytorch-basics-df87f/</guid>
      <description>#Introduction  This series is described as a summary of learning of PyTorch which is a machine learning framework of Python. This time, I have written a summary of the basics of PyTorch. It may be difficult to read as a reading material because I try to write down the main points rather than to convey it, but I hope you can use it to understand the points.
What is #PyTorch As I mentioned earlier, PyTorch is a Python machine learning framework.</description>
    </item>
    
    <item>
      <title>I checked the output specification of PyTorch Bidirectional LSTM</title>
      <link>https://memotut.com/i-checked-the-output-specification-of-pytorch-bidirectional-lstm-78a51/</link>
      <pubDate>Tue, 12 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>https://memotut.com/i-checked-the-output-specification-of-pytorch-bidirectional-lstm-78a51/</guid>
      <description>#Introduction  As stated in LSTM Reference,whenhandlingBidirectionalLSTMwithPyTorch,whendeclaringLSTMIt&amp;rsquo;sOKtospecifybidirectional=Trueinthe,anditisveryeasytohandle(KerascanjustsurroundtheLSTMwithBidrectional). However, even if I look at the reference, I don&amp;rsquo;t think much about the output when LSTM is set to Bidirectional. I didn&amp;rsquo;t quite understand the output specifications of Bidirectional LSTM in PyTorch even after a quick google, so I will briefly summarize here.
#Reference
 Bidirectional recurrent neural networks Understanding Bidirectional RNN in PyTorch   Bidirectional LSTM output question in PyTorch Understanding LSTM-along with recent trends  Specification check As you can see from References 1 and 2, you can see that the bidirectional RNN and LSTM are simple because the front and rear RNNs and LSTMs overlap.</description>
    </item>
    
    <item>
      <title>I tried to implement Attention Seq2Seq in PyTorch</title>
      <link>https://memotut.com/i-tried-to-implement-attention-seq2seq-in-pytorch-64604/</link>
      <pubDate>Fri, 08 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>https://memotut.com/i-tried-to-implement-attention-seq2seq-in-pytorch-64604/</guid>
      <description>#Introduction  Last time Following the implementation of Seq2Seq, this time I tried to implement Attention Seq2Seq which added Attention to Seq2Seq with PyTorch.
Even beginners like me can&amp;rsquo;t find the source code that implements Attention in PyTorch so easily, and there is also PyTorch Attention Tutorial.Thereis,butitseemsthatIhavenotlearnedmini-batch(?),andIwantedtotryimplementingsimplerplain(?) Attention with such feeling that it was customized for this task. I tried to implement Attention by myself. It would be greatly appreciated if we could provide some helpful information to those who are having trouble implementing Attention.</description>
    </item>
    
  </channel>
</rss>