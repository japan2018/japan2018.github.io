<!doctype html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  
  <meta name="generator" content="Hugo 0.72.0" />

  
  <meta name="description" content="Some description">
  

  
  <link rel="apple-touch-icon" sizes="180x180" href="https://memotut.com/apple-touch-icon.png">

  
  <link rel="icon" type="image/png" sizes="32x32" href="https://memotut.com/favicon-32x32.png">

  
  <link rel="icon" type="image/png" sizes="16x16" href="https://memotut.com/favicon-16x16.png">

  
  <link rel="manifest" href="https://memotut.com/site.webmanifest">

  
  <link rel="mask-icon" href="https://memotut.com/safari-pinned-tab.svg" color="#5bbad5">

  <meta name="msapplication-TileColor" content="#da532c">

  <meta name="theme-color" content="#ffffff">

  
  <link rel="stylesheet" href="https://memotut.com/css/bootstrap.min.css" />

  
  <title>Is the network since VGG really improving? The story | Memo Tut</title>
  

  <style>
body {
  min-width: 300px;
}

.custom-navbar {
  margin-bottom: 1em;
  height: 60px;
}

.custom-navbar a {
  display: inline-block; 
  padding: 18px 0;
  margin-right: 1em; 
  font-weight: bold; 
}

.custom-navbar a:hover,
.custom-navbar a:focus {
  text-decoration: none; 
}

@media print {
  .custom-navbar {
    display: none;
  }
}

article {
  padding-bottom: 1em;
}

img {
  max-width: 100%;
}


body {
  background-color: #fff;
}



body {
  color: #212529;
}



a {
  color: #007bff;
}



a:hover,
a:focus {
  color: #0056b3;
}



.custom-navbar {
  background-color: #212529;
}



.custom-navbar a {
  color: rgba(255, 255, 255, 0.75);
}



.custom-navbar a:hover,
.custom-navbar a:focus {
  color: rgba(255, 255, 255, 1);
}



.container {
  max-width: 800px;
}



pre {
  display: block;
  padding: 9.5px;
  word-break: break-all;
  word-wrap: break-word;
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 4px;
}

pre code {
  padding: 0;
  font-size: inherit;
  color: inherit; 
  white-space: pre-wrap;
  background-color: transparent;
  border: none;
  border-radius: 0;
}

code {
  padding: 2px 4px;
  color: inherit; 
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 4px;
  font-size: .9em;
}



blockquote,
.blockquote {
  padding: 10px 20px;
  margin: 0 0 20px;
  font-size: 1em;
  border-left: 5px solid #6c757d;
}

</style>
</head>

<body>
  <nav class="custom-navbar">
  <div class="container">
    
    <a href="/">Posts</a>
    
    <a href="/tags/">Tags</a>
    
    <a href="/about/">About</a>
    
    <a href="/index.xml">RSS</a>
    
  </div>
</nav>
  
  <div class="container">
    <article>
      <h1>Is the network since VGG really improving? The story</h1>
<p>
  <small class="text-secondary">
  
  
  Mar 13, 2020
  </small>
  

<small><code><a href="https://memotut.com/tags/python">Python</a></code></small>


<small><code><a href="https://memotut.com/tags/machine-learning"> machine learning</a></code></small>


<small><code><a href="https://memotut.com/tags/deeplearning"> DeepLearning</a></code></small>


<small><code><a href="https://memotut.com/tags/keras"> Keras</a></code></small>


<small><code><a href="https://memotut.com/tags/tensorflow"> TensorFlow</a></code></small>

</p>
<pre><code># 1.First of all
</code></pre>
<p>In this article, <a href="https://lukeoakdenrayner.wordpress.com/2019/09/19/ai-competitions-dont-produce-useful-models/?utm_campaign=piqcy&amp;utm_medium=email&amp;utm_source=RevueIsn'ttherecentnetworkarchitecturedefinedin%20newsletter">AI competitions don&rsquo;t produce useful models</a> just overtraining the dataset &amp; outperforming the game? The purpose of this study is to investigate the story with the wide-angle fundus dataset <sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>. Also, as an introduction, we will explain how to fine-turn from the learned weights of ImageNet<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup> using the network implemented in <code>tf.keras</code>.</p>
<p><a href="https://github.com/burokoron/StaDeep/tree/master/fine-turning">All code</a></p>
<h1 id="2-environment">2. Environment</h1>
<ul>
<li>PC specs
-CPU: Intel Core i9-9900K
-RAM: 16GB
-GPU: NVIDIA GeForce GTX 1080 Ti</li>
<li>Library
-Python: 3.7.4
-matplotlib: 3.1.1
-pandas: 0.25.1
-tqdm: 4.31.1
-pillow: 6.1.0
-scikit-learn: 0.21.3
-tensorflow-gpu: 2.0.0</li>
</ul>
<h1 id="3-ai-competitions-dont-produce-useful-models">3. AI competitions don’t produce useful models</h1>
<p><a href="https://lukeoakdenrayner.wordpress.com/2019/09/19/ai-competitions-dont-produce-useful-models/?utm_campaign=piqcy&amp;utm_medium=email&amp;utm_source=RevueAnoverviewof%20newsletter">Article (AI competitions don&rsquo;t produce useful models)</a>.
ㆍMachine learning competitions are designed so that the final test data results are not opened so that the model can be evaluated virtually only once. As a result, model gasha cannot be performed. It seems that this measure is working well, because one team can get evaluation only once. But not the whole competition. For example, if 100 teams participate, it means that they have modeled 100 times. The winning team did not necessarily have the best technique just by being lucky enough to win this gasha. Also, in some competitions, the results of the top teams are competing. When a significant difference test is performed, the results of the top teams are just errors, and a larger amount of test data must be prepared to make a significant difference. As expected, it seems that the accuracy has improved up to VGG, but after that it is a question and there is a possibility that it is just overlearning on test data.</p>
<p><img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/405329/3d734744-0c15-9479-9746-8e2cca1b20c5.png" alt="image.png"></p>
<p>This time, using the wide-angle fundus dataset <sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>, VGG16<sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup>, DenseNet121<sup id="fnref:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup>, ResNet50<sup id="fnref:5"><a href="#fn:5" class="footnote-ref" role="doc-noteref">5</a></sup>, InceptionV3<sup id="fnref:6"><a href="#fn:6" class="footnote-ref" role="doc-noteref">6</a></sup>, NASNetMobile<sup id="fnref:7"><a href="#fn:7" class="footnote-ref" role="doc-noteref">7</a></sup>, EfficientNet-B0[^8 ] To see if the network after VGG16 is really progressing.</p>
<h1 id="4-wide-angle-fundus-data-set--data-division">4. Wide-angle fundus data set &amp; data division</h1>
<p>Please refer to this <a href="https://qiita.com/burokoron/items/8c011c219b7545c50355">Article (Image classification by wide-angle fundus image dataset)</a> for the explanation of the wide-angle fundus dataset and how the data was divided. ..</p>
<h1 id="5-model-building--learning">5. Model building &amp; learning</h1>
<p>In this chapter, we will explain how to perform Fine-turning with the already implemented network &amp; ImageNet<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup> trained model as an introduction. By the way, in the wide-angle fundus dataset, the domain of the image is too different from that of ImageNet, so Fine*turning does not contribute to accuracy improvement, but it does contribute to convergence speed improvement. For details, see <a href="https://speakerdeck.com/uchi_k/lun-wen-shao-jie-yi-yong-hua-xiang-hefalsezhuan-yi-Seexue-xi-falseyou-xiao-xing-nituite-transfusion-understanding-transfer-learning-for-medical-imaging">Article (Effectiveness of transfer learning to medical images)</a>.
We will change the model building part from this <a href="https://qiita.com/burokoron/items/8c011c219b7545c50355">Article (image classification with wide-angle fundus image data set)</a>. The learning parameter changed the learning rate to <code>0.00001</code>. First, import the library.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py"><span style="color:#f92672">from</span> tensorflow.keras.applications <span style="color:#f92672">import</span> VGG16
<span style="color:#f92672">from</span> tensorflow.keras.models <span style="color:#f92672">import</span> Model
<span style="color:#f92672">from</span> tensorflow.keras.layers <span style="color:#f92672">import</span> Dense, Activation
</code></pre></div><p>Then build the network. This time I will explain using VGG16 as an example. First, load VGG16 as the base model. This time, the purpose is Fine-turnig from the learned weights of ImageNet<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>, so build a network up to the final convolution layer with <code>include_top=False</code>, and learn ImageNet<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup> with <code>weights='imagenet'</code>. Read the completed weights. Then, like <a href="https://qiita.com/burokoron/items/8c011c219b7545c50355">Article (Image classification with wide-angle fundus image data set)</a>, combine all connected layers with VGG16.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py">base_model <span style="color:#f92672">=</span> VGG16(include_top<span style="color:#f92672">=</span>False, weights<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;imagenet&#39;</span>, pooling<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;avg&#39;</span>,
                   input_shape<span style="color:#f92672">=</span>(image_size[<span style="color:#ae81ff">0</span>], image_size[<span style="color:#ae81ff">1</span>], <span style="color:#ae81ff">3</span>))
x <span style="color:#f92672">=</span> Dense(<span style="color:#ae81ff">256</span>, kernel_initializer<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;he_normal&#39;</span>)(base_model<span style="color:#f92672">.</span>output)
x <span style="color:#f92672">=</span> Dense(classes, kernel_initializer<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;he_normal&#39;</span>)(x)
outputs <span style="color:#f92672">=</span> Activation(<span style="color:#e6db74">&#39;softmax&#39;</span>)(x)
model <span style="color:#f92672">=</span> Model(inputs<span style="color:#f92672">=</span>base_model<span style="color:#f92672">.</span>inputs, outputs<span style="color:#f92672">=</span>outputs)
</code></pre></div><p>If you want to use other network such as ResNet50, you can use it by changing <code>VGG16</code> to <code>ResNet50</code>. Sample code for other networks is uploaded to <a href="https://github.com/burokoron/StaDeep/tree/master/fine-turning">here</a>.Bytheway,EfficientNet-B0hasnotbeenreleased,butsinceithasbeenimplemented,[keras-team/keras-applications&rsquo;sefficientnet.py](<a href="https://github.com/keras-team/keras-Idownloadedandused(applications/blob/master/keras_applications/efficientnet.py)">https://github.com/keras-team/keras-Idownloadedandused(applications/blob/master/keras_applications/efficientnet.py)</a>.</p>
<p>The learning result is as follows. NASNetMobile<sup id="fnref:7"><a href="#fn:7" class="footnote-ref" role="doc-noteref">7</a></sup> is the only difference between learning loss and verification loss. Although accracy is separated. EfficientNet-B0[^8] As expected, is the latest model so beautiful? is.
<img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/405329/2fdaf5cd-c927-9dad-9f0e-243c0371ff2e.png" alt="Network Comparison.png"></p>
<h1 id="6-rating">6. Rating</h1>
<p>The evaluation was performed using the F value as in this <a href="https://qiita.com/burokoron/items/8c011c219b7545c50355">Article (image classification by wide-angle fundus image data set)</a>. The results are shown below.</p>
<details><summary>Detailed results</summary><div>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py"><span style="color:#75715e">#VGG16</span>
              precision recall f1<span style="color:#f92672">-</span>score support

         AMD <span style="color:#ae81ff">0.28</span> <span style="color:#ae81ff">0.53</span> <span style="color:#ae81ff">0.37</span> <span style="color:#ae81ff">75</span>
       DR_DM <span style="color:#ae81ff">0.81</span> <span style="color:#ae81ff">0.75</span> <span style="color:#ae81ff">0.78</span> <span style="color:#ae81ff">620</span>
         Gla <span style="color:#ae81ff">0.73</span> <span style="color:#ae81ff">0.76</span> <span style="color:#ae81ff">0.74</span> <span style="color:#ae81ff">459</span>
          MH <span style="color:#ae81ff">0.12</span> <span style="color:#ae81ff">0.22</span> <span style="color:#ae81ff">0.16</span> <span style="color:#ae81ff">32</span>
      Normal <span style="color:#ae81ff">0.75</span> <span style="color:#ae81ff">0.75</span> <span style="color:#ae81ff">0.75</span> <span style="color:#ae81ff">871</span>
          RD <span style="color:#ae81ff">0.93</span> <span style="color:#ae81ff">0.77</span> <span style="color:#ae81ff">0.84</span> <span style="color:#ae81ff">176</span>
          RP <span style="color:#ae81ff">0.91</span> <span style="color:#ae81ff">0.78</span> <span style="color:#ae81ff">0.84</span> <span style="color:#ae81ff">50</span>
         RVO <span style="color:#ae81ff">0.84</span> <span style="color:#ae81ff">0.60</span> <span style="color:#ae81ff">0.70</span> <span style="color:#ae81ff">107</span>

    accuracy <span style="color:#ae81ff">0.73</span> <span style="color:#ae81ff">2390</span>
   macro avg <span style="color:#ae81ff">0.67</span> <span style="color:#ae81ff">0.65</span> <span style="color:#ae81ff">0.65</span> <span style="color:#ae81ff">2390</span>
weighted avg <span style="color:#ae81ff">0.76</span> <span style="color:#ae81ff">0.73</span> <span style="color:#ae81ff">0.74</span> <span style="color:#ae81ff">2390</span>

<span style="color:#75715e"># ResNet50</span>
              precision recall f1<span style="color:#f92672">-</span>score support

         AMD <span style="color:#ae81ff">0.26</span> <span style="color:#ae81ff">0.56</span> <span style="color:#ae81ff">0.36</span> <span style="color:#ae81ff">75</span>
       DR_DM <span style="color:#ae81ff">0.95</span> <span style="color:#ae81ff">0.61</span> <span style="color:#ae81ff">0.74</span> <span style="color:#ae81ff">620</span>
         Gla <span style="color:#ae81ff">0.78</span> <span style="color:#ae81ff">0.61</span> <span style="color:#ae81ff">0.68</span> <span style="color:#ae81ff">459</span>
          MH <span style="color:#ae81ff">0.30</span> <span style="color:#ae81ff">0.25</span> <span style="color:#ae81ff">0.27</span> <span style="color:#ae81ff">32</span>
      Normal <span style="color:#ae81ff">0.64</span> <span style="color:#ae81ff">0.84</span> <span style="color:#ae81ff">0.73</span> <span style="color:#ae81ff">871</span>RD       <span style="color:#ae81ff">0.85</span>      <span style="color:#ae81ff">0.85</span>      <span style="color:#ae81ff">0.85</span>       <span style="color:#ae81ff">176</span>
          RP       <span style="color:#ae81ff">0.64</span>      <span style="color:#ae81ff">0.88</span>      <span style="color:#ae81ff">0.74</span>        <span style="color:#ae81ff">50</span>
         RVO       <span style="color:#ae81ff">0.85</span>      <span style="color:#ae81ff">0.57</span>      <span style="color:#ae81ff">0.68</span>       <span style="color:#ae81ff">107</span>

    accuracy                           <span style="color:#ae81ff">0.71</span>      <span style="color:#ae81ff">2390</span>
   macro avg       <span style="color:#ae81ff">0.66</span>      <span style="color:#ae81ff">0.65</span>      <span style="color:#ae81ff">0.63</span>      <span style="color:#ae81ff">2390</span>
weighted avg       <span style="color:#ae81ff">0.76</span>      <span style="color:#ae81ff">0.71</span>      <span style="color:#ae81ff">0.71</span>      <span style="color:#ae81ff">2390</span>

<span style="color:#75715e"># InceptionV3</span>
              precision    recall  f1<span style="color:#f92672">-</span>score   support

         AMD       <span style="color:#ae81ff">0.28</span>      <span style="color:#ae81ff">0.53</span>      <span style="color:#ae81ff">0.37</span>        <span style="color:#ae81ff">75</span>
       DR_DM       <span style="color:#ae81ff">0.84</span>      <span style="color:#ae81ff">0.68</span>      <span style="color:#ae81ff">0.75</span>       <span style="color:#ae81ff">620</span>
         Gla       <span style="color:#ae81ff">0.74</span>      <span style="color:#ae81ff">0.68</span>      <span style="color:#ae81ff">0.71</span>       <span style="color:#ae81ff">459</span>
          MH       <span style="color:#ae81ff">0.29</span>      <span style="color:#ae81ff">0.16</span>      <span style="color:#ae81ff">0.20</span>        <span style="color:#ae81ff">32</span>
      Normal       <span style="color:#ae81ff">0.69</span>      <span style="color:#ae81ff">0.81</span>      <span style="color:#ae81ff">0.74</span>       <span style="color:#ae81ff">871</span>
          RD       <span style="color:#ae81ff">0.91</span>      <span style="color:#ae81ff">0.80</span>      <span style="color:#ae81ff">0.85</span>       <span style="color:#ae81ff">176</span>
          RP       <span style="color:#ae81ff">0.83</span>      <span style="color:#ae81ff">0.76</span>      <span style="color:#ae81ff">0.79</span>        <span style="color:#ae81ff">50</span>
         RVO       <span style="color:#ae81ff">0.64</span>      <span style="color:#ae81ff">0.52</span>      <span style="color:#ae81ff">0.57</span>       <span style="color:#ae81ff">107</span>

    accuracy                           <span style="color:#ae81ff">0.72</span>      <span style="color:#ae81ff">2390</span>
   macro avg       <span style="color:#ae81ff">0.65</span>      <span style="color:#ae81ff">0.62</span>      <span style="color:#ae81ff">0.62</span>      <span style="color:#ae81ff">2390</span>
weighted avg       <span style="color:#ae81ff">0.74</span>      <span style="color:#ae81ff">0.72</span>      <span style="color:#ae81ff">0.72</span>      <span style="color:#ae81ff">2390</span>

<span style="color:#75715e"># DenseNet121</span>
              precision    recall  f1<span style="color:#f92672">-</span>score   support

         AMD       <span style="color:#ae81ff">0.25</span>      <span style="color:#ae81ff">0.60</span>      <span style="color:#ae81ff">0.36</span>        <span style="color:#ae81ff">75</span>
       DR_DM       <span style="color:#ae81ff">0.94</span>      <span style="color:#ae81ff">0.66</span>      <span style="color:#ae81ff">0.78</span>       <span style="color:#ae81ff">620</span>
         Gla       <span style="color:#ae81ff">0.82</span>      <span style="color:#ae81ff">0.58</span>      <span style="color:#ae81ff">0.68</span>       <span style="color:#ae81ff">459</span>
          MH       <span style="color:#ae81ff">0.45</span>      <span style="color:#ae81ff">0.16</span>      <span style="color:#ae81ff">0.23</span>        <span style="color:#ae81ff">32</span>
      Normal       <span style="color:#ae81ff">0.65</span>      <span style="color:#ae81ff">0.87</span>      <span style="color:#ae81ff">0.74</span>       <span style="color:#ae81ff">871</span>
          RD       <span style="color:#ae81ff">0.94</span>      <span style="color:#ae81ff">0.82</span>      <span style="color:#ae81ff">0.88</span>       <span style="color:#ae81ff">176</span>
          RP       <span style="color:#ae81ff">0.98</span>      <span style="color:#ae81ff">0.86</span>      <span style="color:#ae81ff">0.91</span>        <span style="color:#ae81ff">50</span>
         RVO       <span style="color:#ae81ff">0.91</span>      <span style="color:#ae81ff">0.64</span>      <span style="color:#ae81ff">0.75</span>       <span style="color:#ae81ff">107</span>

    accuracy                           <span style="color:#ae81ff">0.73</span>      <span style="color:#ae81ff">2390</span>
   macro avg       <span style="color:#ae81ff">0.74</span>      <span style="color:#ae81ff">0.65</span>      <span style="color:#ae81ff">0.67</span>      <span style="color:#ae81ff">2390</span>
weighted avg       <span style="color:#ae81ff">0.78</span>      <span style="color:#ae81ff">0.73</span>      <span style="color:#ae81ff">0.73</span>      <span style="color:#ae81ff">2390</span>

<span style="color:#75715e"># NASNetMobile</span>
              precision    recall  f1<span style="color:#f92672">-</span>score   support

         AMD       <span style="color:#ae81ff">0.25</span>      <span style="color:#ae81ff">0.52</span>      <span style="color:#ae81ff">0.34</span>        <span style="color:#ae81ff">75</span>
       DR_DM       <span style="color:#ae81ff">0.84</span>      <span style="color:#ae81ff">0.66</span>      <span style="color:#ae81ff">0.74</span>       <span style="color:#ae81ff">620</span>
         Gla       <span style="color:#ae81ff">0.59</span>      <span style="color:#ae81ff">0.81</span>      <span style="color:#ae81ff">0.69</span>       <span style="color:#ae81ff">459</span>
          MH       <span style="color:#ae81ff">0.16</span>      <span style="color:#ae81ff">0.22</span>      <span style="color:#ae81ff">0.18</span>        <span style="color:#ae81ff">32</span>
      Normal       <span style="color:#ae81ff">0.72</span>      <span style="color:#ae81ff">0.70</span>      <span style="color:#ae81ff">0.71</span>       <span style="color:#ae81ff">871</span>
          RD       <span style="color:#ae81ff">0.94</span>      <span style="color:#ae81ff">0.76</span>      <span style="color:#ae81ff">0.84</span>       <span style="color:#ae81ff">176</span>
          RP       <span style="color:#ae81ff">0.94</span>      <span style="color:#ae81ff">0.60</span>      <span style="color:#ae81ff">0.73</span>        <span style="color:#ae81ff">50</span>
         RVO       <span style="color:#ae81ff">0.75</span>      <span style="color:#ae81ff">0.43</span>      <span style="color:#ae81ff">0.55</span>       <span style="color:#ae81ff">107</span>

    accuracy                           <span style="color:#ae81ff">0.69</span>      <span style="color:#ae81ff">2390</span>
   macro avg       <span style="color:#ae81ff">0.65</span>      <span style="color:#ae81ff">0.59</span>      <span style="color:#ae81ff">0.60</span>      <span style="color:#ae81ff">2390</span>
weighted avg       <span style="color:#ae81ff">0.73</span>      <span style="color:#ae81ff">0.69</span>      <span style="color:#ae81ff">0.70</span>      <span style="color:#ae81ff">2390</span>

<span style="color:#75715e"># EfficientNet-B0</span>
              precision    recall  f1<span style="color:#f92672">-</span>score   support

         AMD       <span style="color:#ae81ff">0.32</span>      <span style="color:#ae81ff">0.44</span>      <span style="color:#ae81ff">0.37</span>        <span style="color:#ae81ff">75</span>
       DR_DM       <span style="color:#ae81ff">0.94</span>      <span style="color:#ae81ff">0.60</span>      <span style="color:#ae81ff">0.73</span>       <span style="color:#ae81ff">620</span>
         Gla       <span style="color:#ae81ff">0.79</span>      <span style="color:#ae81ff">0.57</span>      <span style="color:#ae81ff">0.66</span>       <span style="color:#ae81ff">459</span>
          MH       <span style="color:#ae81ff">0.21</span>      <span style="color:#ae81ff">0.38</span>      <span style="color:#ae81ff">0.27</span>        <span style="color:#ae81ff">32</span>
      Normal       <span style="color:#ae81ff">0.63</span>      <span style="color:#ae81ff">0.88</span>      <span style="color:#ae81ff">0.73</span>       <span style="color:#ae81ff">871</span>
          RD       <span style="color:#ae81ff">0.94</span>      <span style="color:#ae81ff">0.85</span>      <span style="color:#ae81ff">0.89</span>       <span style="color:#ae81ff">176</span>
          RP       <span style="color:#ae81ff">0.80</span>      <span style="color:#ae81ff">0.80</span>      <span style="color:#ae81ff">0.80</span>        <span style="color:#ae81ff">50</span>
         RVO       <span style="color:#ae81ff">0.83</span>      <span style="color:#ae81ff">0.56</span>      <span style="color:#ae81ff">0.67</span>       <span style="color:#ae81ff">107</span>

    accuracy                           <span style="color:#ae81ff">0.71</span>      <span style="color:#ae81ff">2390</span>
   macro avg       <span style="color:#ae81ff">0.68</span>      <span style="color:#ae81ff">0.63</span>      <span style="color:#ae81ff">0.64</span>      <span style="color:#ae81ff">2390</span>
weighted avg       <span style="color:#ae81ff">0.76</span>      <span style="color:#ae81ff">0.71</span>      <span style="color:#ae81ff">0.71</span>      <span style="color:#ae81ff">2390</span>
</code></pre></div></div></details>
<p>また、ImageNet<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>での結果と今回の結果を以下の表にまとめました。ImageNet<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>での結果はGitHubの<a href="https://github.com/keras-team/keras-applications/blob/master/README.md">keras-team/keras-applications</a>から取得しました。ImageNet<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>ではVGG16以降のネットワークはたしかにVGG16より高精度な結果になっていますが、今回の広角眼底データセット<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>ではVGG16より優れていたネットワークはDenseNet121のみとなりました。さらに、テストデータは2390件なので、95%で有意差検定すると±1～2%くらいブレが生じることになるので、DenseNet121が統計的にVGG16より優れているとは言えない結果となりました。</p>
<table>
<thead>
<tr>
<th align="center">ネットワーク</th>
<th align="center">ImageNet Top-1 Acc [%]</th>
<th align="center">広角眼底データセット F1-score</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center">VGG16</td>
<td align="center">71.268</td>
<td align="center">0.65</td>
</tr>
<tr>
<td align="center">ResNet50</td>
<td align="center">74.928</td>
<td align="center">0.63</td>
</tr>
<tr>
<td align="center">InceptionV3</td>
<td align="center">77.898</td>
<td align="center">0.62</td>
</tr>
<tr>
<td align="center">DenseNet121</td>
<td align="center">74.972</td>
<td align="center">0.67</td>
</tr>
<tr>
<td align="center">NASNetMobile</td>
<td align="center">74.366</td>
<td align="center">0.60</td>
</tr>
<tr>
<td align="center">EfficientNet-B0</td>
<td align="center">77.190</td>
<td align="center">0.64</td>
</tr>
</tbody>
</table>
<h1 id="7-まとめ">7. まとめ</h1>
<p>本記事では広角眼底データセット<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>を用いて本当にVGG16以降のネットワークは進歩しているのかを調べました。また、入門向けとして、実装済みネットワーク＆ImageNet<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>の学習済み重みをFine-turningする方法を説明しました。実験の結果、VGG16より優れていたのはDenseNet121のみでした。しかし、有意差はなく、統計的に優れているとは言えませんでした。
今回は広角眼底データセットで行いましたが他のデータセットで行うとまた違う結果になると思いますし、画像拡張処理はほとんどしていないので、そのあたりも最新手法にすれば結果が変わってくる可能性もあります。ただ、私は普段、主に眼科領域の医用画像を扱っていますが、だいたいDenseNetが良い結果を出すので、経験通りの結果かなと思います。(なぜDenseNetが良い傾向があるのかわからないので誰か教えてほしいです。)
ちなみに<a href="https://qiita.com/burokoron/items/8c011c219b7545c50355">前回(広角眼底画像データセットで画像分類)</a>の単純な10層CNNのF値は0.59だったので、ネットワークをDenseNet121にすることで0.08向上して0.67にすることができました。今後もいろいろな最新手法を適用して精度向上をめざしたいと思います。</p>
<section class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1" role="doc-endnote">
<p><a href="https://tsukazaki-ai.github.io/optos_dataset/">Tsukazaki Optos Public Project</a> <a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2" role="doc-endnote">
<p><a href="https://ieeexplore.ieee.org/document/5206848">ImageNet: A large-scale hierarchical image database</a> <a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3" role="doc-endnote">
<p><a href="https://arxiv.org/abs/1409.1556">Very Deep Convolutional Networks for Large-Scale Image Recognition</a> <a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:4" role="doc-endnote">
<p><a href="https://arxiv.org/abs/1608.06993">Densely Connected Convolutional Networks</a> <a href="#fnref:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:5" role="doc-endnote">
<p><a href="https://arxiv.org/abs/1512.03385">Deep Residual Learning for Image Recognition</a> <a href="#fnref:5" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:6" role="doc-endnote">
<p><a href="https://arxiv.org/abs/1512.00567">Rethinking the Inception Architecture for Computer Vision</a> <a href="#fnref:6" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:7" role="doc-endnote">
<p><a href="https://arxiv.org/abs/1707.07012">Learning Transferable Architectures for Scalable Image Recognition</a>[^8]:<a href="https://arxiv.org/abs/1905.11946">EfficientNet:RethinkingModelScalingforConvolutionalNeuralNetworks</a> <a href="#fnref:7" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</section>

    </article>
  </div>

  
  
  
  <script>
  window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
  ga('create', 'UA-169005401-1', 'auto');
  ga('send', 'pageview');
  </script>
  <script async src='https://www.google-analytics.com/analytics.js'></script>
  

  

  
<link rel="stylesheet" type="text/css" href="//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.0/cookieconsent.min.css" />
<script src="//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.0/cookieconsent.min.js"></script>
<script>
window.addEventListener("load", function(){
window.cookieconsent.initialise({
  "palette": {
    "popup": {
      "background": "#216942",
      "text": "#b2d192"
    },
    "button": {
      "background": "#afed71"
    }
  }
})});
</script>

</body>

</html>
