<!doctype html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  
  <meta name="generator" content="Hugo 0.72.0" />

  
  <meta name="description" content="Some description">
  

  
  <link rel="apple-touch-icon" sizes="180x180" href="https://memotut.com/apple-touch-icon.png">

  
  <link rel="icon" type="image/png" sizes="32x32" href="https://memotut.com/favicon-32x32.png">

  
  <link rel="icon" type="image/png" sizes="16x16" href="https://memotut.com/favicon-16x16.png">

  
  <link rel="manifest" href="https://memotut.com/site.webmanifest">

  
  <link rel="mask-icon" href="https://memotut.com/safari-pinned-tab.svg" color="#5bbad5">

  <meta name="msapplication-TileColor" content="#da532c">

  <meta name="theme-color" content="#ffffff">

  
  <link rel="stylesheet" href="https://memotut.com/css/bootstrap.min.css" />

  
  <title>Why is cross entropy used for the objective function of classification problems? | Memo Tut</title>
  

  <style>
body {
  min-width: 300px;
}

.custom-navbar {
  margin-bottom: 1em;
  height: 60px;
}

.custom-navbar a {
  display: inline-block; 
  padding: 18px 0;
  margin-right: 1em; 
  font-weight: bold; 
}

.custom-navbar a:hover,
.custom-navbar a:focus {
  text-decoration: none; 
}

@media print {
  .custom-navbar {
    display: none;
  }
}

article {
  padding-bottom: 1em;
}

img {
  max-width: 100%;
}


body {
  background-color: #fff;
}



body {
  color: #212529;
}



a {
  color: #007bff;
}



a:hover,
a:focus {
  color: #0056b3;
}



.custom-navbar {
  background-color: #212529;
}



.custom-navbar a {
  color: rgba(255, 255, 255, 0.75);
}



.custom-navbar a:hover,
.custom-navbar a:focus {
  color: rgba(255, 255, 255, 1);
}



.container {
  max-width: 800px;
}



pre {
  display: block;
  padding: 9.5px;
  word-break: break-all;
  word-wrap: break-word;
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 4px;
}

pre code {
  padding: 0;
  font-size: inherit;
  color: inherit; 
  white-space: pre-wrap;
  background-color: transparent;
  border: none;
  border-radius: 0;
}

code {
  padding: 2px 4px;
  color: inherit; 
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 4px;
  font-size: .9em;
}



blockquote,
.blockquote {
  padding: 10px 20px;
  margin: 0 0 20px;
  font-size: 1em;
  border-left: 5px solid #6c757d;
}

</style>
</head>

<body>
  <nav class="custom-navbar">
  <div class="container">
    
    <a href="/">Posts</a>
    
    <a href="/tags/">Tags</a>
    
    <a href="/about/">About</a>
    
    <a href="/index.xml">RSS</a>
    
  </div>
</nav>
  
  <div class="container">
    <article>
      <h1>Why is cross entropy used for the objective function of classification problems?</h1>
<p>
  <small class="text-secondary">
  
  
  Jan 1, 2020
  </small>
  

<small><code><a href="https://memotut.com/tags/python">Python</a></code></small>


<small><code><a href="https://memotut.com/tags/machine-learning"> machine learning</a></code></small>


<small><code><a href="https://memotut.com/tags/chainer"> Chainer</a></code></small>


<small><code><a href="https://memotut.com/tags/cross-entropy"> cross entropy</a></code></small>


<small><code><a href="https://memotut.com/tags/classification-problem"> classification problem</a></code></small>

</p>
<pre><code># What is a classification problem
</code></pre>
<p>The classification problem is to classify data into several categories and is one of the typical methods of machine learning.</p>
<p><img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/163591/afe802f3-ac13-f030-b257-5db6ffe60241.png" alt="image.png"></p>
<p>Let&rsquo;s take a purchasing site as an example.
Predict whether a user will buy or not buy a new product based on the user&rsquo;s purchasing information.
Classification into two categories (classes) is called binary classification.</p>
<p>Classification predictions with more than two classes are called multiclass classifications.
Judgment of objects in images (image recognition) is also one of the multi-class classification problems.
It is also a classification problem to judge that it is a cat based on the image of the cat.</p>
<p>#Cross entropy</p>
<p>Two probability distributions</p>
<pre><code class="language-math" data-lang="math">P(x): Correct data distribution \\
Q(x): Prediction model distribution
</code></pre><p>For, the cross entropy is defined by</p>
<pre><code class="language-math" data-lang="math">L =-\sum_{x} P(x) \log{Q(x)}
</code></pre><p>The more similar the two probability distributions are, the smaller the cross entropy.
This property is often used as an objective function in machine learning (especially classification problems).</p>
<p>In this article, I will consider mathematically why cross-entropy is often adopted as the objective function.</p>
<p>For more information about cross entropy, see the following:
<a href="http://cookie-box.hatenablog.com/entry/2017/05/07/121607">http://cookie-box.hatenablog.com/entry/2017/05/07/121607</a></p>
<h1 id="binomial-distribution-bernoulli-distribution">Binomial distribution (Bernoulli distribution)</h1>
<p>In considering the classification problem, let&rsquo;s take the binomial distribution, which is the simplest probability distribution, as an example.</p>
<pre><code class="language-math" data-lang="math">P(x_1) = p \;\;\; P(x_2) = 1-p \\
Q(x_1) = q \;\;\; Q(x_2) = 1-q \\
</code></pre><p><img src="https://2.bp.blogspot.com/-w3-rg9hwaac/Us_MQyshtNI/AAAAAAAAdDs/Nr6VgQvdO10/s400/kujibiki_box.png" alt="hako"></p>
<p>There is a red and white ball in the box, and the probability of pulling red is</p>
<pre><code class="language-math" data-lang="math">P (red) = p
</code></pre><p>The probability of catching white</p>
<pre><code class="language-math" data-lang="math">P (white) = 1-p
</code></pre><p>It&rsquo;s easy to understand.
If there are 10 balls in total, 2 for red and 8 for white</p>
<pre><code class="language-math" data-lang="math">P (red) = 0.2 \quad P (white) = 0.8
</code></pre><p>about it.</p>
<p>Now, the objective function at this time is</p>
<pre><code class="language-math" data-lang="math">\begin{align}
L &amp;=-\sum_{x} P(x) \log{Q(x)} \\
  &amp;=-p \log{q}-(1-p) \log{(1-q)}
\end{align}
</code></pre><p>You can expand it.</p>
<p>Consider the following simple neural network. Imagine a scenario where you want to finally find the probability distribution $q$. To take the example above, you can predict the result by building a probability distribution that draws red balls using some input data as a model, even if you do not know the contents of the box at all.</p>
<p><img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/163591/71482abf-1593-694b-5158-876f27c7f2e3.png" alt="image.png"></p>
<pre><code class="language-math" data-lang="math">y = \sum_{i} x_i w_i \\
\\
q(y) = \frac{1}{1+e^{-y}}: sigmoid function
</code></pre><p>Here, $x_i$ is input, $w_i$ is weighted, $y$ is an intermediate value, and $q$ is output.
The most representative sigmoid function was adopted as the activation function.</p>
<h1 id="neural-network-training">Neural network training</h1>
<p>Train the neural network by finding the values of the parameters that minimize the value of the objective function. It uses one of the optimization algorithms, the gradient descent.</p>
<pre><code class="language-math" data-lang="math">w \leftarrow w-\eta \frac{\partial L}{\partial w}
</code></pre><p>The gradient descent method is a method of repeatedly calculating <strong>learning rate x objective function gradient</strong> to obtain the weight that takes the minimum value of the objective function.</p>
<p><img src="https://tutorials.chainer.org/en/_images/13_13.png" alt="img"></p>
<p>For a detailed explanation of this area, the following Chainer Tutorial is very easy to understand.
<a href="https://tutorials.chainer.org/en/13_Basics_of_Neural_Networks.html">https://tutorials.chainer.org/en/13_Basics_of_Neural_Networks.html</a></p>
<p>Let&rsquo;s differentiate the objective function at once.</p>
<pre><code class="language-math" data-lang="math">\frac{\partial L }{\partial w_i} = \frac{\partial y}{\partial w_i} \frac{\partial q }{\partial y}\frac{\partial L }{\partial q}
</code></pre><p>The first derivative is</p>
<pre><code class="language-math" data-lang="math">\frac{\partial y}{\partial w_i} = \frac{\partial}{\partial w_i} \sum_i x_i w_i = x_i
</code></pre><p>The second derivative is</p>
<pre><code class="language-math" data-lang="math">\begin{align}
\frac{\partial q }{\partial y} &amp;= \frac{\partial}{\partial y} \frac{1}{1+e^{-y}} \\
&amp;= \frac{\partial u}{\partial y}\frac{\partial}{\partial u} \frac{1}{u} \\
&amp;= -e^{-y} (-u^{-2}) \\
&amp;= \frac{e^{-y}}{1+e^{-y}}\frac{1}{1+e^{-y}} \\
&amp;= \bigl( \frac{1+e^{-y}}{1+e^{-y}}-\frac{1}{1+e^{-y}} \bigr) \frac{1 }{1+e^{-y}} \\
&amp;= \bigl( 1-q(y) \bigr) q(y)
\end{align}
</code></pre><p>The third derivative is</p>
<pre><code class="language-math" data-lang="math">\begin{align}
\frac{\partial L}{\partial q} &amp;= \frac{\partial}{\partial q} \{-p \log{q}-(1-p) \log{(1-q)} \ } \\
&amp;=-\frac{p}{q} + \frac{1-p}{1-q}

\end{align}
</code></pre><p>Can be calculated as</p>
<pre><code class="language-math" data-lang="math">\frac{\partial L }{\partial w_i} = x_i (1-q) q \bigl(-\frac{p}{q} + \frac{1-p}{1-q} \bigr) = x_i (qp)
</code></pre><p>That is</p>
<pre><code class="language-math" data-lang="math">p = q
</code></pre><p>The objective function has a minimum value when.
In other words, this means that the distribution of correct answer data and the distribution of the prediction model are exactly the same.</p>
<p>Well I&rsquo;m just saying that.
The box actually contains 8 reds and 2 whites
It means that we predict that there is a probability of 80% for red and 20% for white.</p>
<p>#Apply to classification problem</p>
<p>Now, let&rsquo;s increase the variation a little as a classification problem. For example, in a classification problem, categories can be represented by 0, 1.</p>
<pre><code>Apples: [1, 0, 0]
Gorilla: [0, 1, 0]
Rappa: [0, 0, 1]
</code></pre><p>If it generalizes a little more</p>
<p>The correct answer of the class to which x belongs is</p>
<pre><code class="language-math" data-lang="math">t=[t_1, t_2 …t_K]^T
</code></pre><p>Given as a vector. However, suppose that this vector is such that only one of $t_k ; (k=1,2,…,K)$ is 1, and the others are 0. This is called a 1-hot vector.</p>
<p>Now that we can define the classification problem, the objective function is</p>
<pre><code class="language-math" data-lang="math">L =-\sum_x P(x) \log{ Q(x)} =-\log{ Q(x)}
</code></pre><p>$Q(x)$ represents the probability that the training data will be the same as the training data.
Let&rsquo;s plot it.</p>
<p><img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/163591/ff5a042c-f425-3203-7a39-19695ed434e4.png" alt="Figure_1.png"></p>
<p>In the gradient descent method, the weight $w_i$ that minimizes the objective function is obtained by repeatedly calculating the <strong>learning rate × the gradient of the objective function</strong>. If the <strong>learning rate</strong> is extremely high, or the <strong>gradient of the objective function</strong> is large, the learning efficiency will be good. It also reduces the number of calculation steps.</p>
<p>You can see that for $0 &lt;Q(x) &lt;1$, the objective function $L$ decreases sharply near $Q(x) = 0$.
From this, if the training data and the learning result are too different, it can be interpreted that the reduction amount per step is large.
In the classification problem, choosing cross entropy as the objective function gives better computational efficiency.</p>
<p>It&rsquo;s easy to forget if you actually use a library such as Chainer or Pytorch.
It is also good to look back so that you do not forget the basic theory. I learned a lot.</p>
<p>reference
<a href="https://mathwords.net/kousaentropy">https://mathwords.net/kousaentropy</a>
<a href="https://water2litter.net/rum/post/ai_loss_function/">https://water2litter.net/rum/post/ai_loss_function/</a>
<a href="http://yaju3d.hatenablog.jp/entry/2018/11/30/225841">http://yaju3d.hatenablog.jp/entry/2018/11/30/225841</a>
<a href="https://avinton.com/academy/classification-regression/">https://avinton.com/academy/classification-regression/</a></p>

    </article>
  </div>

  
  
  
  <script>
  window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
  ga('create', 'UA-169005401-1', 'auto');
  ga('send', 'pageview');
  </script>
  <script async src='https://www.google-analytics.com/analytics.js'></script>
  

  

  
<link rel="stylesheet" type="text/css" href="//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.0/cookieconsent.min.css" />
<script src="//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.0/cookieconsent.min.js"></script>
<script>
window.addEventListener("load", function(){
window.cookieconsent.initialise({
  "palette": {
    "popup": {
      "background": "#216942",
      "text": "#b2d192"
    },
    "button": {
      "background": "#afed71"
    }
  }
})});
</script>

</body>

</html>
