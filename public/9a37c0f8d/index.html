<!doctype html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  
  <meta name="generator" content="Hugo 0.72.0" />

  
  <meta name="description" content="Some description">
  

  
  <link rel="apple-touch-icon" sizes="180x180" href="https://memotut.com/apple-touch-icon.png">

  
  <link rel="icon" type="image/png" sizes="32x32" href="https://memotut.com/favicon-32x32.png">

  
  <link rel="icon" type="image/png" sizes="16x16" href="https://memotut.com/favicon-16x16.png">

  
  <link rel="manifest" href="https://memotut.com/site.webmanifest">

  
  <link rel="mask-icon" href="https://memotut.com/safari-pinned-tab.svg" color="#5bbad5">

  <meta name="msapplication-TileColor" content="#da532c">

  <meta name="theme-color" content="#ffffff">

  
  <link rel="stylesheet" href="https://memotut.com/css/bootstrap.min.css" />

  
  <title>[Python] Learning method with original data of CenterNet(Objects as Points) | Memo Tut</title>
  

  <style>
body {
  min-width: 300px;
}

.custom-navbar {
  margin-bottom: 1em;
  height: 60px;
}

.custom-navbar a {
  display: inline-block; 
  padding: 18px 0;
  margin-right: 1em; 
  font-weight: bold; 
}

.custom-navbar a:hover,
.custom-navbar a:focus {
  text-decoration: none; 
}

@media print {
  .custom-navbar {
    display: none;
  }
}

article {
  padding-bottom: 1em;
}

img {
  max-width: 100%;
}


body {
  background-color: #fff;
}



body {
  color: #212529;
}



a {
  color: #007bff;
}



a:hover,
a:focus {
  color: #0056b3;
}



.custom-navbar {
  background-color: #212529;
}



.custom-navbar a {
  color: rgba(255, 255, 255, 0.75);
}



.custom-navbar a:hover,
.custom-navbar a:focus {
  color: rgba(255, 255, 255, 1);
}



.container {
  max-width: 800px;
}



pre {
  display: block;
  padding: 9.5px;
  word-break: break-all;
  word-wrap: break-word;
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 4px;
}

pre code {
  padding: 0;
  font-size: inherit;
  color: inherit; 
  white-space: pre-wrap;
  background-color: transparent;
  border: none;
  border-radius: 0;
}

code {
  padding: 2px 4px;
  color: inherit; 
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 4px;
  font-size: .9em;
}



blockquote,
.blockquote {
  padding: 10px 20px;
  margin: 0 0 20px;
  font-size: 1em;
  border-left: 5px solid #6c757d;
}

</style>
</head>

<body>
  <nav class="custom-navbar">
  <div class="container">
    
    <a href="/">Posts</a>
    
    <a href="/tags/">Tags</a>
    
    <a href="/about/">About</a>
    
    <a href="/index.xml">RSS</a>
    
  </div>
</nav>
  
  <div class="container">
    <article>
      <h1>[Python] Learning method with original data of CenterNet(Objects as Points)</h1>
<p>
  <small class="text-secondary">
  
  
  Dec 31, 2019
  </small>
  

<small><code><a href="https://memotut.com/tags/python">Python</a></code></small>


<small><code><a href="https://memotut.com/tags/deeplearning"> DeepLearning</a></code></small>


<small><code><a href="https://memotut.com/tags/object-detection"> object detection</a></code></small>


<small><code><a href="https://memotut.com/tags/pytorch"> PyTorch</a></code></small>


<small><code><a href="https://memotut.com/tags/objectdetection"> ObjectDetection</a></code></small>

</p>
<pre><code>What is #CenterNet
</code></pre>
<p>CenterNet is the object detection method proposed in the paper <a href="https://arxiv.org/abs/1904.07850">Objects as Points</a>.</p>
<p>After detecting the center of the object as a feature point, the size of the width and height is predicted, so it seems that there are advantages such as lighter calculation than conventional methods.</p>
<p><a href="https://qiita.com/sho101/items/0f9ef0999f1124e3b633">I tried to detect the positions of tennis players, balls, and courts using CenterNet</a>, I want to do the same thing using CenterNet ..</p>
<p>Although it is CenterNet, the source code is published on <a href="https://github.com/xingyizhou/CenterNet">Github</a>, and you can infer using a trained model by following the Readme. However, there was little explanation or information about learning the original data, so I had a little difficulty in proceeding with the work. I wrote a simple article because I thought it was for sharing work notes. I&rsquo;m glad if you can use it as a reference.</p>
<p>#environment
Ubuntu 18.04
PyTorch 0.4.1</p>
<p>#Preparing the dataset
I want to detect the front side player and the back side player in the tennis match image,
・The player on the front side is &ldquo;Player Front&rdquo;
・&ldquo;Player Back&rdquo; for the player at the back
I created annotation data with the label.</p>
<p><img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/45007/0ddc54eb-1988-b25c-2004-cf5a60423644.png" alt="15129_0.png"></p>
<p>I created it using a tool called <a href="https://github.com/tzutalin/labelImg">labelImg</a>, but this is output as an xml file in Pascal VOC format.
CenterNet annotation data can only read json files in COCO format, so it is necessary to convert xml files to json files. If you are in the same situation, please refer to my article <a href="https://qiita.com/otakoma/items/e391e5e6924945b8a852">Convert Pascal VOC format xml file to COCO format json file</a> If.</p>
<p>The category ID is
・PlayerFront:1
・PlayerBack:2
Was allocated as.</p>
<p>Annotation data creates the following two files, learning data and test data. Please note that if the file names are different, an error &ldquo;file not found&rdquo; will be output during learning.
・Pascal_trainval0712.json ：COCO format data set that stores information of training data</p>
<ul>
<li>Pascal_test2007.json: COCO format data set that stores test data information</li>
</ul>
<p>Two annotation data files are stored so that the directory structure is ↓.
CenterNet/data/voc/annotations/
|&ndash;pascal_trainval0712.json
|&ndash;pascal_test2007.json</p>
<p>Then place the image data in the images directory.
CenterNet/data/voc/images/
|&ndash;**.jpg</p>
<h1 id="source-code-modification">Source code modification</h1>
<p>Rewrite self.class_name in line 30 of pascal.py according to the assigned category ID.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-/src/lib/datasets/dataset/pascal.py" data-lang="/src/lib/datasets/dataset/pascal.py">    <span style="color:#75715e"># self.class_name = [&#39;__background__&#39;, &#34;playerup&#34;, &#34;playerdown&#34;, &#34;bird&#34;, &#34;boat&#34;,</span>
    <span style="color:#75715e"># &#34;bottle&#34;, &#34;bus&#34;, &#34;car&#34;, &#34;cat&#34;, &#34;chair&#34;, &#34;cow&#34;, &#34;diningtable&#34;, &#34;dog&#34;,</span>
    <span style="color:#75715e"># &#34;horse&#34;, &#34;motorbike&#34;, &#34;person&#34;, &#34;pottedplant&#34;, &#34;sheep&#34;, &#34;sofa&#34;,</span>
    <span style="color:#75715e"># &#34;train&#34;, &#34;tvmonitor&#34;]</span>
    self<span style="color:#f92672">.</span>class_name<span style="color:#f92672">=</span>[<span style="color:#e6db74">&#39;__background__&#39;</span>, <span style="color:#e6db74">&#34;playerFront&#34;</span>, <span style="color:#e6db74">&#34;playerBack&#34;</span>]
</code></pre></div><p>#Learn with prepared dataset</p>
<pre><code class="language-terminal" data-lang="terminal">python main.py ctdet --exp_id pascal_dla_384 --dataset pascal --num_epochs 500 --lr_step 50,100,200,300,400
</code></pre><p>About arguments
ctdet Learning with object detection (CenterNet)</p>
<ul>
<li>-exp_id Specify the network such as pascal_dla_384,pascal_dla_512,pascal_resdcn18_384,pascal_resdcn18_512. See <a href="https://github.com/xingyizhou/CenterNet/blob/master/readme/MODEL_ZOO.md#pascal-voc">MODEL ZOO Pascal VOC</a> for the recommended number of GPUs for each model.</li>
<li>-dataset pascal pascal Learn with VOC method (20 classes)</li>
<li>-num_epocks ** Learn with epochs **</li>
<li>-lr_step 50,100,200,300,400 <del>Save the model at the time of epoch number 50,100,200,300,400</del> The learning rate becomes 1/10 at the specified epoch timing</li>
</ul>
<p>If the number of training data is small, there is a high possibility that a model with insufficient training will be created even if the above method is used for training.
In this case, it is advisable to train with the fine tuning that additionally trains using the trained model.</p>
<ul>
<li>-load_model Specify the trained model with the file name as an argument.
The trained model can be downloaded at <a href="https://github.com/xingyizhou/CenterNet/blob/master/readme/MODEL_ZOO.md">MODEL_ZOO.md</a>.</li>
</ul>
<p>For fine tuning, I referred to Github issues, <a href="https://github.com/xingyizhou/CenterNet/issues/307">Transfer learning on very small dataset #307</a>.</p>
<pre><code class="language-terminal" data-lang="terminal">python main.py ctdet --exp_id pascal_dla_384 --dataset pascal --num_epochs 500 --lr_step 50,100,200,300,400 --load_model ../models/ctdet_pascal_dla_384.pth
</code></pre><p>Log files and learning models are stored in /exp/ctdet/pascal_dla_384/.</p>
<p>#inference
Perform inference using the learned model. Modify the code in CenterNet/src/lib/utils/debugger.py before performing inference.
At line 439, there is a declaration of pascal_class_name, so change it to &ldquo;PlayerFront&rdquo; and &ldquo;PlayerBack&rdquo;.</p>
<p>Then, inference is executed with the following command.</p>
<pre><code class="language-terminal" data-lang="terminal">python demo.py ctdet --demo ../data/voc/images/**.jpg --dataset pascal --load_model ../exp/ctdet/pascal_dla_384/model_last.pth --debug 2
</code></pre><ul>
<li>-debug 2 Not only the detection result image but also the heat map image can be confirmed.</li>
</ul>

    </article>
  </div>

  
  
  
  <script>
  window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
  ga('create', 'UA-169005401-1', 'auto');
  ga('send', 'pageview');
  </script>
  <script async src='https://www.google-analytics.com/analytics.js'></script>
  

  

  
<link rel="stylesheet" type="text/css" href="//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.0/cookieconsent.min.css" />
<script src="//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.0/cookieconsent.min.js"></script>
<script>
window.addEventListener("load", function(){
window.cookieconsent.initialise({
  "palette": {
    "popup": {
      "background": "#216942",
      "text": "#b2d192"
    },
    "button": {
      "background": "#afed71"
    }
  }
})});
</script>

</body>

</html>
