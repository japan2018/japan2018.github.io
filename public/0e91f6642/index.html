<!doctype html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  
  <meta name="generator" content="Hugo 0.72.0" />

  
  <meta name="description" content="Some description">
  

  
  <link rel="apple-touch-icon" sizes="180x180" href="https://memotut.com/apple-touch-icon.png">

  
  <link rel="icon" type="image/png" sizes="32x32" href="https://memotut.com/favicon-32x32.png">

  
  <link rel="icon" type="image/png" sizes="16x16" href="https://memotut.com/favicon-16x16.png">

  
  <link rel="manifest" href="https://memotut.com/site.webmanifest">

  
  <link rel="mask-icon" href="https://memotut.com/safari-pinned-tab.svg" color="#5bbad5">

  <meta name="msapplication-TileColor" content="#da532c">

  <meta name="theme-color" content="#ffffff">

  
  <link rel="stylesheet" href="https://memotut.com/css/bootstrap.min.css" />

  
  <title>[Python] Machine learning completely understood by junior high school students [Updated daily] | Memo Tut</title>
  

  <style>
body {
  min-width: 300px;
}

.custom-navbar {
  margin-bottom: 1em;
  height: 60px;
}

.custom-navbar a {
  display: inline-block; 
  padding: 18px 0;
  margin-right: 1em; 
  font-weight: bold; 
}

.custom-navbar a:hover,
.custom-navbar a:focus {
  text-decoration: none; 
}

@media print {
  .custom-navbar {
    display: none;
  }
}

article {
  padding-bottom: 1em;
}

img {
  max-width: 100%;
}


body {
  background-color: #fff;
}



body {
  color: #212529;
}



a {
  color: #007bff;
}



a:hover,
a:focus {
  color: #0056b3;
}



.custom-navbar {
  background-color: #212529;
}



.custom-navbar a {
  color: rgba(255, 255, 255, 0.75);
}



.custom-navbar a:hover,
.custom-navbar a:focus {
  color: rgba(255, 255, 255, 1);
}



.container {
  max-width: 800px;
}



pre {
  display: block;
  padding: 9.5px;
  word-break: break-all;
  word-wrap: break-word;
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 4px;
}

pre code {
  padding: 0;
  font-size: inherit;
  color: inherit; 
  white-space: pre-wrap;
  background-color: transparent;
  border: none;
  border-radius: 0;
}

code {
  padding: 2px 4px;
  color: inherit; 
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 4px;
  font-size: .9em;
}



blockquote,
.blockquote {
  padding: 10px 20px;
  margin: 0 0 20px;
  font-size: 1em;
  border-left: 5px solid #6c757d;
}

</style>
</head>

<body>
  <nav class="custom-navbar">
  <div class="container">
    
    <a href="/">Posts</a>
    
    <a href="/tags/">Tags</a>
    
    <a href="/about/">About</a>
    
    <a href="/index.xml">RSS</a>
    
  </div>
</nav>
  
  <div class="container">
    <article>
      <h1>[Python] Machine learning completely understood by junior high school students [Updated daily]</h1>
<p>
  <small class="text-secondary">
  
  
  Jan 12, 2020
  </small>
  

<small><code><a href="https://memotut.com/tags/python">Python</a></code></small>


<small><code><a href="https://memotut.com/tags/machine-learning"> machine learning</a></code></small>

</p>
<pre><code>## 0 Foreword
</code></pre>
<p>My name is jp_Pythonia and I am in the 1st year of middle school. Nice to meet you. Since this article is a buzz and there is a lot of information, I have made it a divided version.</p>
<p><a href="https://qiita.com/jp_Pythonia/items/4b57a356c0250b164624">Part 1</a>
<a href="https://qiita.com/jp_Pythonia/items/484adbfd2a13885578e4">Part 2</a></p>
<h2 id="1-what-is-machine-learning">1 What is machine learning?</h2>
<p>What is machine learning? Tom Mitchell defines:</p>
<blockquote>
<p>Based on a performance criterion P for a task T, a computer program</p>
<p>If the performance for T is measured with reference P and improves with experience E, then</p>
<p>It can be said that it was learned from experience E.</p>
</blockquote>
<p>There are two main types of machine learning. In the next two sections, we will discuss two of them, <strong>Supervised Learning</strong> and <strong>Unsupervised Learning</strong>.</p>
<h2 id="2-what-is-supervised-learning">2 What is supervised learning?</h2>
<p>Supervised learning is the <strong>most common</strong> machine learning problem.</p>
<p>The main features are</p>
<ul>
<li>Given data set has correct answer</li>
<li>Can be divided into classification problem and regression problem</li>
</ul>
<p>There are two points.</p>
<h4 id="2-1-1-there-is-a-correct-answer-in-the-given-data-set">2-1-1 There is a correct answer in the given data set</h4>
<p>→ In supervised learning, there is a “correct answer” in all given data sets.</p>
<p>e.g.)</p>
<p>We have developed an algorithm that predicts the weather and judges whether it will rain or tomorrow and outputs it.</p>
<p>All of the weather data we learned included whether it was rainy or sunny that day.</p>
<p>In the case of the example above, we call it <em>supervised learning</em>. **</p>
<p>This is because the label “rain” or “clear”, which is the “correct answer” to the algorithm, was attached to the data from the beginning.</p>
<h4 id="2-1-2-can-be-divided-into-classification-problem-and-regression-problem">2-1-2 Can be divided into classification problem and regression problem</h4>
<p>There are two main categories of supervised learning.</p>
<p>They are &ldquo;classification problem&rdquo; and &ldquo;regression problem&rdquo;.</p>
<h3 id="2-2-what-is-a-classification-problem">2-2 What is a classification problem?</h3>
<p>The algorithm that predicts the weather that I showed you earlier. This applies to classification problems. This is because <strong>rainy or sunny</strong> is &ldquo;classified&rdquo;**.</p>
<p>By the way, rain or sunny data is called &ldquo;discrete value&rdquo;.</p>
<h3 id="2-3-what-is-a-regression-problem">2-3 What is a regression problem?</h3>
<p>Conversely, a regression problem is a type of problem that deals with &ldquo;continuous values.&rdquo;</p>
<p>e.g.)</p>
<p>We have developed an algorithm to predict tomorrow&rsquo;s temperature.</p>
<p>The learned data is a combination of weather conditions and temperature.</p>
<p>In the case of the above example, the temperature of the output data is a &ldquo;continuous value&rdquo;, so it is called a regression problem.</p>
<h2 id="3-unsupervised-learning">3 unsupervised learning</h2>
<blockquote>
<p>Unsupervised learning Unsupervised learning allows you to tackle problems with little or no understanding of what your results will look like. You can derive the structure from data where the effects of variables are not always known. You can derive this structure by clustering the data based on the relationships between the variables in the data. In unsupervised learning, there is no feedback based on prediction results.</p>
<p>Quoted from <a href="https://www.coursera.org/learn/machine-learning/supplement/1O0Bk/unsupervised-learning">https://www.coursera.org/learn/machine-learning/supplement/1O0Bk/unsupervised-learning</a></p>
</blockquote>
<p>As mentioned above, unsupervised learning has two main characteristics.</p>
<ul>
<li>Unlike supervised learning, there is no data that is &ldquo;correct&rdquo;</li>
<li>Use clustering algorithm</li>
</ul>
<h3 id="3-1-what-is-a-clustering-algorithm">3-1 What is a clustering algorithm?</h3>
<p>What is a clustering algorithm?</p>
<ul>
<li>All data labels are the same</li>
<li>There is no data label in the first place</li>
<li>Not labeled</li>
</ul>
<p>An algorithm that is useful when dealing with data.
The clustering algorithm finds laws, genres, etc. in given data, creates clusters by itself, and classifies them.</p>
<h2 id="4-linear-regression-model-representation">4 Linear regression (model representation)</h2>
<h4 id="4-1-oops-before-that">4-1 Oops&hellip; before that</h4>
<p>We have prepared a data set. Here it is.</p>
<table>
<thead>
<tr>
<th align="center">Case</th>
<th align="center">Land area (x)</th>
<th align="center">Land price (y)</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center">1</td>
<td align="center">2104</td>
<td align="center">460</td>
</tr>
<tr>
<td align="center">2</td>
<td align="center">1416</td>
<td align="center">232</td>
</tr>
<tr>
<td align="center">3</td>
<td align="center">1534</td>
<td align="center">315</td>
</tr>
<tr>
<td align="center">4</td>
<td align="center">852</td>
<td align="center">178</td>
</tr>
<tr>
<td align="center">・・・</td>
<td align="center">・・・</td>
<td align="center">・・・</td>
</tr>
<tr>
<td align="center">8</td>
<td align="center">680</td>
<td align="center">143</td>
</tr>
</tbody>
</table>
<p>I will define <strong>characters</strong> that I will use from now on.</p>
<ul>
<li><strong>m</strong> ・・・Number of cases. 8 in the above dataset</li>
<li><strong>x</strong> ・・・The area of land. x<sub>i</sub> represents the area of the i-th land. x<sub>2</sub> = 1416</li>
<li><strong>y</strong> ・・・ Land price. y<sub>i</sub> represents the price of the i-th land. y<sub>4</sub> = 178</li>
<li><strong>h</strong>・・・Abbreviation that means hypothesis.</li>
</ul>
<h3 id="4-2-decide-how-to-express-the-hypothesis">4-2 Decide how to express the hypothesis</h3>
<p>Create an expression to express hypothesis h. Here it is.</p>
<p>h<sub>θ</sub>(x) = θ<sub>0</sub> + θ<sub>1</sub>x</p>
<p>By the way, this seems to be difficult at first glance, but you can see that it is very similar to the equation of linear function Y = B + AX that junior high school students do. The linear function is the graph below</p>
<p><img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/448030/45dc9f6d-1316-7971-a948-6a43d86e4c9a.png" alt="Image of linear function"></p>
<ul>
<li>b → θ<sub>0</sub></li>
<li>a → θ<sub>1</sub></li>
</ul>
<p>You can see that it is only.</p>
<p>** And h<sub>θ</sub>(x) shows a straight line. **</p>
<p>**It&rsquo;s the easiest place to get stuck, so I want you to memorize only here. **</p>
<h2 id="5-cost-function-objective-function">5 cost function (objective function)</h2>
<h4 id="5-1-in-the-first-place-purpose">5-1 In the first place: Purpose</h4>
<p>Following the model representation above, we will define the objective function, but in the first place these objectives are to draw optimized lines for a given (plotted) dataset. For example, here is a figure. (Data has been plotted)</p>
<p><img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/448030/d588823a-ad8e-d30a-c08b-79f6f1dec8ec.png" alt="4d4dc8da6d0042fd77b72a773be7b8d7.png"></p>
<p>At this time, the purpose is to find the red line and minimize the square of the distance to the blue point.</p>
<h3 id="5-2-parameter-selection-method">5-2 Parameter selection method</h3>
<p>In order to achieve the purpose introduced in 5-1 we need to use a formula that works for minimization. However, since I do not understand it just by looking at the formula, I will give an explanation that leads to an intuitive understanding.</p>
<p>To begin with, parameters are names such as θ<sub>0</sub> and θ<sub>1</sub>. The direction of the red line in the above graph will change depending on what number is applied to this. It would be okay if I did the primary function when I was in junior high school.</p>
<p>e.g.)</p>
<p>When θ<sub>0</sub> = 1.5 and θ<sub>1</sub> = 0:</p>
<p>![Screenshot 2020-01-25 10.05.14.png](<a href="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/448030/e725ff3f-faa6-fcc0-8084-(cafa2980f628.png)">https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/448030/e725ff3f-faa6-fcc0-8084-(cafa2980f628.png)</a></p>
<p>Will be.</p>
<p>When θ<sub>0</sub> = 1 and θ<sub>1</sub> = 0.5:</p>
<p>![Screenshot 2020-01-25 10.11.46.png](<a href="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/448030/f8b6f3e3-6ae7-c929-4165-(9787add56adb.png)">https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/448030/f8b6f3e3-6ae7-c929-4165-(9787add56adb.png)</a></p>
<p>Will be.</p>
<h3 id="5-3-expression-for-selecting-parameters">5-3 Expression for selecting parameters</h3>
<p>After all, what you want to do is to calculate the value of θ<sub>0</sub> (y axis) based on the given value of θ<sub>1</sub>x (x axis). You just have to put it out.</p>
<p>Let&rsquo;s make a formula.</p>
<h5 id="5-3-1-step-1">5-3-1 Step 1</h5>
<p>(h<sub>θ</sub>(x)-y)<sup>2</sup></p>
<p>I will explain the above formula.</p>
<p><img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/448030/d588823a-ad8e-d30a-c08b-79f6f1dec8ec.png" alt="4d4dc8da6d0042fd77b72a773be7b8d7.png"></p>
<p>The formula of Step 1 is obtained by subtracting the value of y from the red straight line h<sub>θ</sub>(x) in the graph above and squaring it.</p>
<h5 id="5-3-2-step-2">5-3-2 Step 2</h5>
<p>Remember 4-1? You can specify the i-th value of x with X<sub>i</sub>.</p>
<p>In the formula of Step 1, no sample is specified and it is very abstract, so I will add it so that all samples are acquired. This is it</p>
<p>mΣi=1 (h<sub>θ</sub>(x<sup>( i )</sup>)-y<sup>( i )</sup>)<sup>2</sup></p>
<p>mΣi=1 means the sum flow from i=1 (case number 1) to m (the number of all cases).</p>
<p>In addition, (h<sub>θ</sub>(x)-y)<sup>2</sup> → (h<sub>θ</sub>(x<sup>( i )</sup>) -It was replaced by y<sup>( i )</sup>)<sup>2</sup>, which means subtract the ith y from the ith x.</p>
<h5 id="5-3-3-step-3">5-3-3 Step 3</h5>
<p>Finally, to simplify the calculation, take 1/2m.</p>
<p>1/2m m Σi=1 (h<sub>θ</sub>(x<sup>( i )</sup>)-y<sup>( i )</sup>)<sup>2</sup></p>
<p>However, 1/2 and 1/m have no special meaning because they are only <strong>simply because the result does not change</strong>.</p>
<h3 id="5-4-what-is-a-cost-function-after-all">5-4 What is a cost function after all?</h3>
<p>As I wrote in 5-1 I decided the number of θ<sub>0</sub> and θ<sub>1</sub>, and one of the minimized functions that fits the plotted data exactly is the cost function. Then, I have introduced the formula for selecting parameters.</p>
<p>Now, after all, the cost function is <strong>J( θ<sub>0</sub>, θ<sub>1</sub>)</strong>. So, when showing the cost function,**J( θ<sub>0</sub>, θ<sub>1</sub>) = 1/2m m Σi=1 (h<sub>θ</sub>(x<sup>( i )&lt; /sup&gt;)-y<sup>( i )</sup>)<sup>2</sup>. **</p>
<p>Therefore, from now on, the cost function will be represented by J.</p>
<h2 id="6-cost-function-objective-function-2">6 Cost function (objective function) #2</h2>
<h4 id="6-1-simplification">6-1 Simplification</h4>
<p>So far, we have used h<sub>θ</sub>( x) = θ<sub>0</sub> + θ<sub>1</sub>x, but for simplicity,</p>
<p>h<sub>θ</sub>( x) = θ<sub>1</sub>x</p>
<p>Is expressed as</p>
<p>It&rsquo;s like saying θ<sub>0</sub> = 0.</p>
<p>Since there is only one parameter, the cost function J expression is</p>
<p>J( θ<sub>1</sub> )= 1/2m mΣ i=1 (h<sub>θ</sub>(x<sup>( i )</sup>)-y<sup>( i )</sup>)<sup>2</sup></p>
<p>Can be expressed as</p>
<h3 id="6-2-deepen-understanding">6-2 Deepen understanding</h3>
<p>Simplifying as shown in 6-1, since B = θ<sub>0</sub> = 0 in the graph of linear function Y = AX + B, the objective function (cost function) J will pass through the origin. ..</p>
<p>By the way, the hypothesis h<sub>θ</sub>(x) is a function corresponding to x. This is because if θ<sub>1</sub> has already been determined, it will be an expression corresponding to x.</p>
<p>On the other hand, the cost function J is the function corresponding to θ<sub>1</sub>. You can understand it by looking at 6-1.</p>
<h3 id="6-3-connection-between-hypothesis-and-cost-function">6-3 Connection between hypothesis and cost function</h3>
<p>First, suppose you have three data, at positions (1,1), (2,2), and (3,3), respectively.</p>
<p>Since the hypothesis h<sub>θ</sub>(x) has θ<sub>0</sub> = 0 depending on the value of θ<sub>1</sub>, the hypothetical straight line is different. I will.</p>
<p>![Screenshot 2020-01-25 13.13.22.png](<a href="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/448030/89354be4-fa5a-ba0f-4d52-(2aa6180bca5c.png)">https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/448030/89354be4-fa5a-ba0f-4d52-(2aa6180bca5c.png)</a></p>
<p>Now, based on the above hypothesis, calculate the numerical value to be plotted in the objective function J.</p>
<p>First, the purple line. The green circle is the location of the data, so the difference is 0.5, 1.0, 1.5, in ascending order.</p>
<p>Now, let&rsquo;s apply these three to the formula of the cost function J.</p>
<p>J( θ<sub>1</sub> )= 1/2m mΣ i=1 (h<sub>θ</sub>(x<sup>( i )</sup>)-y<sup>( i )</sup>)<sup>2</sup></p>
<p>Calculating this,</p>
<p>J( θ<sub>1</sub>) = 1/6 (3.5) = 0.58&hellip;</p>
<p>Similarly, for magenta (pink) colored lines,</p>
<p>J(θ<sub>1</sub>) = 0</p>
<p>In the red line,</p>
<p>J(θ<sub>1</sub>) = 2.33&hellip;</p>
<p>Then plot the three values we just got.</p>
<p>Here are the results.</p>
<p>![Screenshot 2020-01-25 13.42.00.png](<a href="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/448030/32ae5929-bd9a-3571-156d-(eab4e984cbd3.png)">https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/448030/32ae5929-bd9a-3571-156d-(eab4e984cbd3.png)</a></p>
<p>The line color and the circle color correspond. The graph above shows the objective function J.</p>
<h2 id="7-steepest-descent-method-gradient-descent-method">7 steepest descent method (gradient descent method)</h2>
<p>Objective function J defined in 5-4. In Section 7, we will explain an algorithm called <strong>steepest descent method</strong> that minimizes J.</p>
<h3 id="7-1-image">7-1 image</h3>
<p>The steepest descent method has such an image. ![Screenshot 2020-01-25 15.24.52.png](<a href="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/448030/8d4f647e-aa35-056a-ab16-(965eaeb03156.png)">https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/448030/8d4f647e-aa35-056a-ab16-(965eaeb03156.png)</a></p>
<p>Quoted from <a href="https://linky-juku.com/simple-regression-analysis3/">https://linky-juku.com/simple-regression-analysis3/</a></p>
<p>![Screenshot 2020-01-25 15.26.49.png](<a href="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/448030/5870dd4f-50f0-995f-75ad-(15f49f62c56b.png)">https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/448030/5870dd4f-50f0-995f-75ad-(15f49f62c56b.png)</a></p>
<p>Quoted from <a href="https://linky-juku.com/simple-regression-analysis3/">https://linky-juku.com/simple-regression-analysis3/</a></p>
<p>However, as a weak point, there is a possibility that something called <strong>local solution</strong> (not the optimal solution, not the desired answer) can be the answer, as in the graph above.</p>
<h3 id="7-2-mathematical-explanation">7-2 Mathematical explanation</h3>
<p>This is the formula to use.</p>
<p>![Screenshot 2020-01-25 15.29.42.png](<a href="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/448030/f7b2cd95-6e21-51c8-4cb1-(2f8708c71b70.png)">https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/448030/f7b2cd95-6e21-51c8-4cb1-(2f8708c71b70.png)</a></p>
<h5 id="step-1">Step 1</h5>
<p>The := part means to update by assigning the right side to the left side.</p>
<h5 id="step-2">Step 2</h5>
<p>α is the learning rate. The steepest descent method reaches a minimum value in several steps as shown in the figure below. The larger α, the larger the step.</p>
<p>![Screenshot 2020-01-25 15.37.17.png](<a href="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/448030/e6417403-cacf-1303-935a-(993126532f6b.png)">https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/448030/e6417403-cacf-1303-935a-(993126532f6b.png)</a></p>
<h5 id="step-3">Step 3</h5>
<p>Do you remember. Cost function. In fact, J(θ<sub>0</sub>, θ<sub>1</sub>) is a cost function. As you know.</p>
<p>So you can convert it like this:</p>
<p><img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/448030/627d074d-1718-b8be-d9f0-c04aa5d9b01a.png" alt="Screenshot 2020-01-25 16.22.06.png">J(θ<sub>0</sub>,θ<sub>1</sub>)=<img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/448030/627d074d-1718-b8be-d9f0-c04aa5d9b01a.png" alt="Screenshot2020-01-2516.22.06.png">1/2mmΣi=1(h<sub>θ</sub>(x<sup>(i)</sup>)-y<sup>(i)</sup>)<sup>2</sup>=<img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/448030/627d074d-1718-b8be-d9f0-c04aa5d9b01a.png" alt="Screenshot2020-01-2516.22.06.png">1/2mmΣi=1(θ<sub>0</sub>+θ<sub>1</sub>x<sup>(i)</sup>-y<sup>(i)</sup>) <sup>2</sup></p>
<p>Then, consider the pattern of θ<sub>0</sub> and θ<sub>1</sub>.</p>
<p>θ<sub>0</sub> (j = 0): ∂/∂θ<sub>0</sub> J(θ<sub>0</sub>, θ<sub>1</sub>) = 1/m m Σi=1 (h<sub>θ</sub>(x<sup>( i )</sup>)-y<sup>( i )</sup>)<sup>2</sup></p>
<p>θ<sub>1</sub> (j = 0): ∂/∂θ<sub>1</sub> J(θ<sub>0</sub>, θ<sub>1</sub>) = 1/m m Σi=1 (h<sub>θ</sub>(x<sup>( i )</sup>)-y<sup>( i )</sup>)<sup>2</sup></p>
<p>OK.</p>
<h3 id="7-3-important-points-in-steepest-descent-method-gradient-descent-method">7-3 Important points in steepest descent method (gradient descent method)</h3>
<p>Actually, I think that you have read the formula of the steepest descent method, but there are very important elements.</p>
<p>That is, <strong>update θ<sub>0</sub> and θ<sub>1</sub> at the same time</strong>.</p>
<p>Temp0 := θ<sub>0</sub> -<img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/448030/eaab3438-8dd9-131e-77eb-7b1293c2fc37.png" alt="Screenshot 2020-01-25 15.44.43.png"></p>
<p>Temp 1 := θ<sub>1</sub> -<img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/448030/eaab3438-8dd9-131e-77eb-7b1293c2fc37.png" alt="Screenshot 2020-01-25 15.44.43.png"></p>
<p>θ<sub>0</sub> := Temp0</p>
<p>θ<sub>1</sub> := Temp1</p>
<p>In the above formula, once<img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/448030/eaab3438-8dd9-131e-77eb-7b1293c2fc37.png" alt="Screenshot 2020-01-25 15.44.43.png"> is assigned to the variables Temp0,1 and then to θ<sub>0</sub> and θ<sub>1</sub>.</p>
<p>Then don&rsquo;t do this</p>
<p>![Screenshot 2020-01-25 15.56.33.png](<a href="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/448030/d61dbee7-393a-4f60-d954-(01a814794481.png)">https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/448030/d61dbee7-393a-4f60-d954-(01a814794481.png)</a></p>
<p>What if I did something like this?</p>
<p>Actually, there is a big problem. At the time of the formula on the second line, θ<sub>0</sub> has been replaced by the variable Temp0, so on the right side of the formula on the third line, a new value of θ<sub>0&lt;/ The point is that we have to use sub&gt; in the expression.</p>
<p>Please be aware that <strong>simultaneous updates</strong> are mandatory regardless of whether the issue actually occurs.</p>
<h3 id="7-4-solve-the-steepest-descent-method-by-linear-regression">7-4 Solve the steepest descent method by linear regression</h3>
<p>As a premise, the steepest descent method is part of the cost function.![Screenshot 2020-01-25 17.00.20.png](<a href="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/448030/bfcf1aab-86d5-7ae6-70fb-(90c4423493d4.png)">https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/448030/bfcf1aab-86d5-7ae6-70fb-(90c4423493d4.png)</a>
![Screenshot 2020-01-25 17.00.31.png](<a href="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/448030/2838e2c0-a3d7-2bce-4b76-(9d4336f7697f.png)">https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/448030/2838e2c0-a3d7-2bce-4b76-(9d4336f7697f.png)</a></p>
<p>(<a href="https://www.coursera.org/learn/machine-learning/lecture/kCvQc/gradient-descent-for-linear-regression">https://www.coursera.org/learn/machine-learning/lecture/kCvQc/gradient-descent-for-linear-regression</a>)</p>
<p>Well, could you have seen the two images above?</p>
<p>By the way, the image on the right is<img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/448030/1da1ec0c-0a05-eefa-b571-2f1e04c9a53b.png" alt="Screenshot 2020-01-25 14.51.52.png"> This is a two-dimensional view of the cost function drawn with contour lines.</p>
<p>In the figure above, if you move the blue line, the red cross mark will increase. As you know, the left is the linear regression and the right is the cost function. By changing the direction of the line of the linear regression hypothesis, the result of the cost function also changes. This is what I wanted to convey in 7-4.</p>
<p>For example, it can be used in the steepest descent method so as not to be connected to the local optimum solution.</p>
<h2 id="8-matrix-and-vector">8 matrix and vector</h2>
<h3 id="8-1-what-is-a-line">8-1 What is a line?</h3>
<pre><code class="language-math" data-lang="math">\begin{bmatrix}
a &amp; b \\
c &amp; d \\
e &amp; f
\end{bmatrix}

</code></pre><p>For example, the one above is the matrix. 3 x 2.</p>
<h3 id="8-2-what-is-a-vector">8-2 What is a vector?</h3>
<p>A vector has only one column horizontally,</p>
<pre><code class="language-math" data-lang="math">y =

\begin{bmatrix}

a \\
c \\
e
\end{bmatrix}

</code></pre><p>As above, it represents n×1 rows.</p>
<h3 id="8-3-rules">8-3 rules</h3>
<p>There are some rules.</p>
<p>For example,</p>
<p>y <sub>i</sub> = i <sup>th</sup> element (a in the above example)</p>
<p>**However, you have to be careful whether the number assigned to the index starts from 0 or 1. **</p>
<p>Basically, a vector with 1-based index is used.</p>
<p>Basically most people use capital letters when referring to indexes. It is often the raw data stored that uses lowercase letters</p>
<h3 id="8-4-matrix-addition-and-subtraction">8-4 Matrix addition and subtraction</h3>
<p>The rules for adding and subtracting matrices are</p>
<p><strong>Only matrices of the same dimension</strong></p>
<p>It is not possible to add and subtract.</p>
<pre><code class="language-math" data-lang="math">\begin{bmatrix}
Ten \\
twenty five \\
3 &amp; 4
\end{bmatrix}
+
\begin{bmatrix}
4 &amp; 7 \\
2 &amp; 9 \\
0 &amp; 8
\end{bmatrix}
=
\begin{bmatrix}
5 &amp; 7 \\
4 &amp; 14 \\
3 &amp; 12
\end{bmatrix}
</code></pre><p>As you can see from the above, there are 2 elements to add and 3 x 2 to the answer. And the numbers whose vertical and horizontal locations are the same are added together.</p>
<h3 id="8-5-scalar-multiplication">8-5 Scalar multiplication</h3>
<p>Scalars are exaggerated, but they mean real numbers. Therefore,</p>
<pre><code class="language-math" data-lang="math">3 ×
\begin{bmatrix}
twenty five \\
3 &amp; 4
\end{bmatrix}
=
\begin{bmatrix}
6 &amp; 15 \\
9 &amp; 12
\end{bmatrix}
</code></pre><p>It&rsquo;s OK with a feeling.</p>
<h3 id="8-6-multiply-matrix-and-vector">8-6 Multiply matrix and vector</h3>
<p>I feel like this.</p>
<pre><code class="language-math" data-lang="math">\begin{bmatrix}
twenty five \\
3 &amp; 4
\end{bmatrix}

\begin{bmatrix}
6 \\
9
\end{bmatrix}
=
\begin{bmatrix}
57 \\
54
\end{bmatrix}
</code></pre><p>The matrix names are A, B, and C from the left.</p>
<p>Step 1: The left one step above A is applied to the upper step of B, and the right one step above A is applied to the lower step B. (Like 2x6 and 5x9.)</p>
<p>Step 2: Repeat 3x6 and 4x9 as above.</p>
<p>By the way, A was 2x2 this time. B is 2 × 1 and C is 2 × 1. Actually, this isn&rsquo;t the case. <strong>There are solid rules</strong></p>
<p>A → m × n (2 × 2)</p>
<p>B → n × 1 (2 × 1)</p>
<p>C → m × 1 (2 × 1)</p>
<p>Please be careful as you will use this rule hereafter.</p>
<h3 id="8-7-multiply-two-matrices">8-7 Multiply two matrices</h3>
<p>**Actually, there is a linear regression algorithm that does not require an iterative algorithm, and this section is the one that leads to it. **
This time,</p>
<pre><code class="language-math" data-lang="math">\begin{bmatrix}
2 &amp; 5 &amp; 2 \\
3 &amp; 4 &amp; 7
\end{bmatrix}
×
\begin{bmatrix}
6 &amp; 8\\
9 &amp; 3\\
twenty three
\end{bmatrix}
=
</code></pre><p>Is the way. First, divide B into two vectors. As below</p>
<pre><code class="language-math" data-lang="math">\begin{bmatrix}
2 &amp; 5 &amp; 2 \\
3 &amp; 4 &amp; 7
\end{bmatrix}
×
\begin{bmatrix}
6\\
9 \\
2
\end{bmatrix}
=

</code></pre><pre><code class="language-math" data-lang="math">\begin{bmatrix}
2 &amp; 5 &amp; 2 \\
3 &amp; 4 &amp; 7
\end{bmatrix}
×
\begin{bmatrix}
 8\\
 3 \\
 3
\end{bmatrix}
=

</code></pre><p>I calculated each.</p>
<pre><code class="language-math" data-lang="math">\begin{bmatrix}
2 &amp; 5 &amp; 2\\
3 &amp; 4 &amp; 7
\end{bmatrix}
×
\begin{bmatrix}
6\\
9 \\
2
\end{bmatrix}
=
\begin{bmatrix}
61\\
68
\end{bmatrix}

</code></pre><pre><code class="language-math" data-lang="math">\begin{bmatrix}
2 &amp; 5 &amp; 2\\
3 &amp; 4 &amp; 7
\end{bmatrix}
×
\begin{bmatrix}
 8\\
 3 \\
3
\end{bmatrix}
=
\begin{bmatrix}
37\\
57
\end{bmatrix}
</code></pre><p>And put together the C divided into two</p>
<pre><code class="language-math" data-lang="math">\begin{bmatrix}
61&amp; 37 \\
68 &amp; 57
\end{bmatrix}
</code></pre><p>that&rsquo;s all.</p>
<h3 id="8-8-dont-make-a-mistake-in-the-order-of-hanging">8-8 Don&rsquo;t make a mistake in the order of hanging</h3>
<p>Of course, you shouldn&rsquo;t use A × B as B × A. For some reason, see 8-6.</p>
<p>I hate to write only things that are too obvious, so I just write that you should consider yourself whether the order is wrong.</p>
<h3 id="8-9-inverse-matrix-inverse">8-9 inverse matrix (inverse)</h3>
<p>The number 1 represents the unit, the identity, in the real number space. Because even if you multiply by 1, it is equal to the number itself. Each real number has a reciprocal. For example, the reciprocal of 3 is 1/3.</p>
<p>So you can see that in real space, all numbers have no reciprocal. For example, 0.</p>
<p>Then, it is also called the inverse of matrix, inverse or inverse matrix, but what is it all about? Take that to the end of this article.</p>
<p>Let A be an m-by-m matrix. And suppose there is an inverse matrix. Then, let&rsquo;s call the inverse matrix A<sup>-1</sup>.</p>
<p>Of the matrices, only the m × m matrix has the inverse matrix, which is also called the square matrix.</p>
<p>For example, suppose you have one square matrix.</p>
<pre><code class="language-math" data-lang="math">```math
\begin{bmatrix}
3&amp; 4 \\
2 &amp; 16
\end{bmatrix}
</code></pre><pre><code>
Also, I happened to know the inverse matrix.

```math
```math
\begin{bmatrix}
0.4&amp; -0.1 \\
- 0.05 &amp; 0.075
\end{bmatrix}
</code></pre><pre><code>
And when you multiply this matrix,

```math
```math
=
\begin{bmatrix}
Ten \\
0 &amp; 1
\end{bmatrix}
</code></pre><pre><code>
= I&lt;sub&gt;2×2&lt;/sub&gt;. (I is the symbol for the identity matrix)

Of course it is possible to do the inverse matrix by hand, but nobody currently does that. So how do you calculate it?

**Use software called Octave**

Rest assured that we will explain how to use Octave in Part 2.

Now, let's summarize the terms.

A matrix that does not have an inverse matrix is called a singular matrix or a degenerate matrix.

**The real really last thing is to tell you how to calculate the transpose**.

```math
```math
\begin{bmatrix}
1 &amp; 2 &amp; 0 \\
3 &amp; 4 &amp; 9
\end{bmatrix}
</code></pre><p>→</p>
<pre><code class="language-math" data-lang="math">\begin{bmatrix}
13  \\
twenty five \\
0 &amp; 9
\end{bmatrix}
</code></pre><pre><code>
A person who thought that was all that. That's right. only this. All you have to do is bring the top row to the left and the bottom row to the right. that's all. Thank you for your hard work.

## 9 Thank you

Thank you for reading this far.

~~ I will post a link when the sequel is available, so please read it. ~~
[Part 2](https://qiita.com/jp_Pythonia/private/484adbfd2a13885578e4) I wrote.

## To Week 2: Welcome

Welcome. Go to &quot;Machine learning Part 2 that even middle school students fully understand&quot;. If you haven't read Part 1 yet, you don't need to read Part 1 first. (Until section 1)

## 1 Install Octave

### 1-1 Windows

I'm sorry, but my environment is not a Mac, so I will give a brief explanation and links.

First, install it using the link [here](http://wiki.octave.org/Octave_for_Microsoft_Windows).

 We will also introduce [here](https://www.power.mech.eng.osaka-cu.ac.jp/~takiyama/start-oct-cntrl.html) as a reference site.

### 1-2 Mac OS

**The operation is confirmed on Mac OS high sierra. **

You may refer to [here](https://wiki.octave.org/Octave_for_macOS).

##### Step 1Install Homebrew

</code></pre><p>/usr/bin/ruby -e &ldquo;$(curl -fsSL <a href="https://raw.githubusercontent.com/Homebrew/install/master/install)%22">https://raw.githubusercontent.com/Homebrew/install/master/install)&quot;</a></p>
<pre><code>
Put it in the terminal. (It's a line)

##### Step 2

Homebrew Cask is also included.

</code></pre><p>brew tap caskroom/cask</p>
<pre><code>
By the way, if you get an error, put it as ```brew cask```. If you see ```See also &quot;man brew-cask&quot;```, it is already included.

##### Step 3

Install XQuartz

Install using Homebrew Cask.

</code></pre><p>brew cask install xquartz</p>
<pre><code>
</code></pre><p>brew install octave</p>
<pre><code>
Wait patiently

##### Step 5

Start Octave

</code></pre><p>Octave</p>
<pre><code>
This should boot.

</code></pre><p>Octave:1&gt;</p>
<pre><code>
What is OK

Type ```exit``` and it will exit and run as a normal console.

** I've quoted the steps 1-5 so far from here: **

https://blog.amedama.jp/entry/2017/04/28/232547

### 1-3 Linux

sorry. I have no operating environment, but (Zorin OS) I'm so tired that I'm not doing it. Please check it out and see it.

## 2 multivariate linear regression

### 2-1 Assumption

Wow...many kanji...

Section 2 describes a more powerful linear regression that works on multiple variables.

First of all, in the previous linear regression, y was predicted based on the information of x, which was called a single feature (single variable) with only one x. However, when predicting the price of a house, for example, the price of tomorrow, it is difficult to predict based on just one piece of information.

Therefore, we will use multiple information using multivariable linear regression.

And I will present the database as a sample. Here it is.

Until now, there was only one x, but we are assigning numbers 1-4.

Case No.( i) | Size (x&lt;sub&gt;1&lt;/sub&gt;) | Number of Bedrooms (x&lt;sub&gt;2&lt;/sub&gt;) | Number of Floors (x&lt;sub&gt;3&lt;/sub&gt;) | Year of construction (x&lt;sub&gt;4&lt;/sub&gt;) | Price (y) 10,000 yen |
| :------------: | :-------------------: | :----------- ------------------: | :-----------------: | :--------- ----------: | :---------: |
| 1 | 2104 | 5 | 1 | 45 | 4600 |
2 | 1416 | 3 | 2 | 40 | 2320 |
3 | 1534 | 3 | 2 | 30 | 3150 |
| 4 | 852 | 2 | 1 | 36 | 1780 |
| ・・・ | ・・・ | ・・・ | ・・・ | ・・・ | ・・・ |

Then, we will define the preconditions.

n is 4. Indicates the number of variables x.

x&lt;sup&gt;( i )&lt;/sup&gt; ・・・Represents the i-th variable x as a vector. If x&lt;sup&gt;( 2 )&lt;/sup&gt;,

```math
\begin{bmatrix}
1416 \\
3 \\
2 \\
40
\end{bmatrix}
</code></pre><p>Will be. It does not mean x squared. And the vector above is n-dimensional (here 4-dimensional, 4 × 1).</p>
<p>x<sub>j</sub> <sup>( i )</sup> ・・・ Actually, j and (i) are arranged vertically. For example, for x<sub>3</sub><sup>( 2 )</sup>, the answer is 2, which corresponds to the third variable in the vector above.</p>
<h3 id="2-2-form-of-linear-regression">2-2 form of linear regression</h3>
<p>Until now, it has been like this.</p>
<p>h<sub>θ</sub>(x) = θ<sub>0</sub> + θ<sub>1</sub>x</p>
<p>However, the form of the expression changes because x is no longer one. Here it is.</p>
<p>h<sub>θ</sub>(x) = θ<sub>0</sub> + θ<sub>1</sub>x<sub>1</sub> + θ<sub>2</sub >x<sub>2</sub> + θ<sub>3</sub>x<sub>3</sub> + θ<sub>4</sub>x<sub>4</sub></p>
<p>However, there is a big problem because the above formula grows as n increases.</p>
<p>That is, writing mathematical expressions with markdown is surprisingly tiring. So let&rsquo;s simplify the formula.</p>
<p>What we do is define the index 0 = 1 of x.</p>
<p>What that means is that for each case i, there is a vector of functions, x<sup>( i )</sup>. Then, regarding x<sub>j</sub> <sup>( i )</sup> introduced in 2-1, x<sub>0</sub><sup>( i )</sup> = 1 is defined. At this time, it means that x<sub>0</sub> = 1 in the i-th case is defined, but x<sub>0</sub> originally does not exist. So in other words, in the vector of x<sup>( i )</sup>, the 0th feature is arbitrarily defined as 1. So x<sup>( i )</sup> becomes an n+1 dimensional vector. OK?</p>
<p>**The above is difficult. Please write it on a piece of paper and read it again. By the time you read about three times, you should be able to understand it just like I am. **</p>
<p>Then, consider the parameter as a vector. ‥</p>
<pre><code class="language-math" data-lang="math">θ =
\begin{bmatrix}
θ&lt;sub&gt;0&lt;/sub&gt; \\
θ&lt;sub&gt;1&lt;/sub&gt; \\
θ&lt;sub&gt;2&lt;/sub&gt; \\
・\\
・\\
・\\
θ&lt;sub&gt;n&lt;/sub&gt; \\
\end{bmatrix}
</code></pre><p>And now you can transform the shape of the formula.</p>
<p>h<sub>θ</sub>(x) = θ<sub>0</sub> + θ<sub>1</sub>x<sub>1</sub> + θ<sub>2</sub >x<sub>2</sub> + θ<sub>3</sub>x<sub>3</sub> + θ<sub>4</sub>x<sub>4</sub></p>
<p>= h<sub>θ</sub>(x) = θ<sub>0</sub>x<sub>1</sub> + θ<sub>1</sub>x<sub>1</sub > + θ<sub>2</sub>x<sub>2</sub> + θ<sub>3</sub>x<sub>3</sub> + θ<sub>4</sub>x <sub>4</sub></p>
<p>By the way, transpose θ. In short, change the shape as a vector.</p>
<pre><code class="language-math" data-lang="math">θ =
\begin{bmatrix}
θ0 \\
θ1 \\
θ2 \\
・\\
・\\
・\\
θn \\
\end{bmatrix}
=
\begin{bmatrix}
θ0 &amp; θ1 &amp; θ2 &amp;・
&amp; ・
&amp; ・
&amp; θn
\end{bmatrix}
</code></pre><p>What was 1 x n+1 dimensions is now n+1 x 1 dimensions. This is transposition. Represented by the wrinkled letter T. And if you multiply this by the vector of x,</p>
<pre><code class="language-math" data-lang="math">\begin{bmatrix}
θ0 &amp; θ1 &amp; θ2 &amp;・
&amp; ・
&amp; ・
&amp; θn
\end{bmatrix}
×
\begin{bmatrix}
xo \\
x1 \\
x2 \\
・\\
・\\
xn
\end{bmatrix}
=
</code></pre><p>= h<sub>θ</sub>(x) = θ<sub>0</sub>x<sub>1</sub> + θ<sub>1</sub>x<sub>1</sub > + θ<sub>2</sub>x<sub>2</sub> + θ<sub>3</sub>x<sub>3</sub> + θ<sub>4</sub>x <sub>4</sub> (formula above)</p>
<p>= θ <sup>T</sup><sub>x</sub></p>
<p>It is a linear regression of the above type variables. The most difficult thing I&rsquo;ve dealt with since Part 1&hellip;</p>
<h2 id="3-steepest-descent-method-with-multiple-variables">3 steepest descent method with multiple variables</h2>
<p>In the previous section, I used to think about linear regression when there are multiple factors. In this section we will consider how to fit the parameters to the hypothesis. ‌</p>
<p>Based on section 2, we will transform the formula.</p>
<p>J(θ<sub>0</sub>, θ<sub>1</sub>, &hellip;, θ<sub>n</sub>) = 1/2m mΣi=1 (h<sub>θ&lt; /sub&gt;(x<sup>(i)</sup>)-y<sup>(i)</sup>)<sup>2</sup></p>
<p>→ J(θ)= 1/2m mΣi=1 (h<sub>θ</sub>(x<sup>(i)</sup>)-y<sup>(i)</sup>)<sup >2</sup></p>
<p>The reason for the above formula is</p>
<pre><code class="language-math" data-lang="math">θ =
\begin{bmatrix}
θ0 \\
θ1 \\
θ2 \\
・\\
・\\
・\\
θn \\
\end{bmatrix}
</code></pre><p>That&rsquo;s why.</p>
<p>next,</p>
<p>θ<sub>j</sub> := θ<sub>j</sub>-α ∂/∂ θ<sub>j</sub> J(θ<sub>0</sub>, θ<sub> 1</sub>, &hellip;, θ<sub>n</sub>)</p>
<p>→ θ<sub>j</sub> := θ<sub>j</sub>-α ∂/∂θ<sub>j</sub> J(θ)‌</p>
<p>OK?</p>
<p>And I will write the steepest descent method.</p>
<p>If group A n=1:</p>
<p><strong>pattern 1</strong>θ<sub>0</sub> := θ<sub>0</sub>-α 1/m m Σi=1(h<sub>θ</sub>(x<sup>(i)</sup> )-y<sup>(i)</sup>)</p>
<p><strong>Pattern 2</strong></p>
<p>θ<sub>1</sub> := θ1-α 1/m m Σi=1(h<sub>θ</sub>(x<sup>(i)</sup>)-y<sup>(i )</sup>)x<sup>(i)</sup></p>
<p>If greater than group B n=1:</p>
<p><strong>pattern 1</strong></p>
<p>θ<sub>0</sub> := θ<sub>0</sub>-α 1/m m Σi=1(h<sub>θ</sub>(x<sup>(i)</sup> )-y<sup>(i)</sup>)x<sub>0</sub><sup>(i)</sup></p>
<p><strong>Pattern 2</strong>‌</p>
<p>θ<sub>1</sub> := θ<sub>1</sub>-α 1/m m Σi=1(h<sub>θ</sub>(x<sup>(i)</sup> )-y<sup>(i)</sup>)x1<sup>(i)</sup></p>
<p>First of all, notice that the A-1 and B-1 are the same. Because the rule x<sub>0</sub><sup>(i)</sup> = 1 does not make any difference.</p>
<p>A-2 and B-2 are the same.</p>
<p>Because x<sup>(i)</sup> = x<sub>1</sub><sup>(i)</sup>.</p>
<p>Well, it doesn&rsquo;t matter if you don&rsquo;t know the formulas so far, so please review carefully and carefully.</p>
<h2 id="4-future-scaling">4 future scaling</h2>
<p>In this section 4, we present some practical ideas on how to use steepest descent methods successfully.</p>
<p>Introducing Future Scaling.</p>
<p>For example, suppose x<sub>1</sub> is the size of your house and is 0-2000m<sup>2</sup>.</p>
<p>x<sub>2</sub> is the number of bedrooms, between 1-5.</p>
<p>At this time, there are two patterns in the steepest descent graph.</p>
<p><img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/448030/8c637500-918a-734d-3a68-f4ac6c07a121.png" alt="Screenshot 2020-01-28 17.38.08.png"></p>
<p>When,</p>
<p>![Screenshot 2020-01-28 17.38.18.png](<a href="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/448030/cf7ca2a6-b970-3100-7ce0-(ab13a81f86d2.png)">https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/448030/cf7ca2a6-b970-3100-7ce0-(ab13a81f86d2.png)</a></p>
<p>is. (Both quoted from <a href="https://www.coursera.org/learn/machine-learning/lecture/xx3Da/gradient-descent-in-practice-i-feature-scaling">https://www.coursera.org/learn/machine-learning/lecture/xx3Da/gradient-descent-in-practice-i-feature-scaling</a>)</p>
<p>In the first place, what is meant by this circle is the bowl shape of the cost function shown in 7-4 of <a href="https://qiita.com/jp_Pythonia/items/4b57a356c0250b164624">Part 1</a> It is a two-dimensional one that is used. So the center is the answer. It is one of the steepest descent methods.</p>
<p>Now, the red line represents the steps to get to the answer. No matter how you look at it, it is the latter of the graph that is closer to a circle that can efficiently arrive at the answer first.</p>
<p>So how do you write a graph that is close to a circle? The answer is simple. In short, the values of θ<sub>0</sub> and θ<sub>1</sub> (that is, variables x1 and x2) need to be closer.</p>
<p>For example, in this example, x1 is 1/5 and x2 is 1/2000. Actually, the result is the latter. The former uses the data as it is.</p>
<p>And the above action is called <strong>Forturing Scaling</strong>.</p>
<p>Then, what kind of rule is there to bring the values closer together? First of all, as a premise, if the range of data itself is good, there is no need to scale it. <strong>How do you decide whether to scale or not</strong></p>
<p>As a valid example, I would like to introduce Professor Andrew Ng of Stanford University.</p>
<blockquote>
<p>-3 to 3 ・・・ OK</p>
<p>-1/3 to 1/3 ・・・ OK</p>
<p>-0.001 to 0.001 ・・・ NG (my father&rsquo;s gag)</p>
<p>-100 to 100 ・・・ NG</p>
</blockquote>
<p>In short, it&rsquo;s &ldquo;rough&rdquo;.</p>
<p>Earlier, I divided the data by the maximum value. However, as is often used in other means,</p>
<p>There is something called <strong>Average Normalization</strong>.</p>
<p>This takes x<sub>i</sub> and tries to bring the mean, including the other variables x, closer to zero.</p>
<p>However, as you know, x<sub>0</sub> is always 1, so please note that it cannot be used for x<sub>0</sub>.</p>
<p>**What do you do?</p>
<p>x1 = size-mean/max of x1 (here x1 = size-1000/2000)</p>
<p>x2 = number of bedrooms-average/maximum (where x2 = number of bedrooms-2/5)</p>
<p>If you do this, it will be close to 0. that&rsquo;s strange.</p>
<p>**Feature scaling is not exact **Basically appropriate</p>
<h2 id="5-how-to-select-the-feature-function-x-to-use">5 How to select the feature (function x) to use</h2>
<p>e.g.)</p>
<p>For example, suppose you have two features for a house.</p>
<p>x1 is the length of the frontage (width facing the road)</p>
<p>x2 is the depth length</p>
<p>Of course, it is natural to use x1 and x2 as they are, but assuming that the size of the land determines the price of the house, the third feature is:</p>
<p>x is x1 x x2</p>
<p>It may be said that</p>
<p>x1 and x2 just happened to exist first. By the way, I think it seems to be used often in machine learning competitions such as kaggle. (Mystery)</p>
<p>Introducing what is called <strong>polynomial regression</strong> in defining features.</p>
<p>There are various variations in the linear regression equation. For example, here.</p>
<p>h<sub>θ</sub> + θ<sub>1</sub>x<sub>1</sub> + θ<sub>2</sub>x<sub>2</sub> + θ&lt; sub&gt;3</sub>x<sub>3</sub></p>
<p>And this is</p>
<p>= θ<sub>0</sub> + θ<sub>1</sub>(size) + θ<sub>2</sub>(size)<sup>2</sup> + θ<sub>3 It can also be expressed as </sub>(size)<sup>3</sup>.</p>
<p>x<sub>1</sub> + (size)</p>
<p>x<sub>2</sub> + (size)<sup>2</sup></p>
<p>If x<sub>3</sub> + (size)<sup>3</sup>.</p>
<p>This is a cubic function. Well, this is just an example, but for example, a graph plotting the data showing the relationship between housing area and price</p>
<p><img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/448030/59daf995-fd83-2342-3461-f48de880c825.png" alt="Screenshot 2020-01-28 18.57.06.png"></p>
<p>In cases like this, cubic functions are good. Why do you think?</p>
<p>This is because if it is a linear function, it becomes a straight line, and if it is a quadratic function, it becomes an arc and draws a low value at the end. It&rsquo;s hard to imagine that the price of a house will drop if there are too many sites.</p>
<p>And <strong>remember, feature scaling</strong>. I&rsquo;ll use it from now on.</p>
<p>If you use <strong>size: 1-1000</strong>,</p>
<p>x1 = 1-1000</p>
<p>x2 = 1-1,000,000</p>
<p>X3 = 1-1,000,000,000.</p>
<p>As expected, this is the beginning of feature scaling.</p>
<p>However, please do the calculation yourself. My homework for school today isn&rsquo;t over (laughs)</p>
<p>By the way, I wrote that the quadratic function cannot be used because it returns to the original level and becomes low, but it can be used by doing this.</p>
<p>![Screenshot 2020-01-28 19.07.55.png](<a href="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/448030/47c00821-c021-18ec-0ae0-(7195f3214f48.png)">https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/448030/47c00821-c021-18ec-0ae0-(7195f3214f48.png)</a></p>
<p>Using the above formula, we can avoid the graph going down. Somehow my homework from my teacher (laughs).</p>
<p><strong>How do you choose a future after all?</strong></p>
<p>Read Part 3, 4, &hellip; and the whole series. Then you should be able to understand.</p>
<p>(I can&rsquo;t say anything while I&rsquo;m writing Part 3 yet&hellip;)</p>
<p>By the way, please be assured that eventually the algorithm will be automatically selected **. (I wear them)</p>
<h2 id="6-normal-equation">6 normal equation</h2>
<p>Normal equations provide a better way to find the parameter θ in some linear regression problems.</p>
<p>Specifically, the algorithm used for linear regression up to this point was the steepest descent method.</p>
<p>The steepest descent method was an algorithm that converges many steps and multiple iterations by reaching an optimal solution.</p>
<p>In contrast, the <strong>normal equation</strong> is the strongest **algorithm in that it can reach the optimal solution at once without having to run the iterative algorithm in order to provide an analytical solution to θ. (I will notice a big defect later)</p>
<p>First, let&rsquo;s get an intuitive understanding.</p>
<p>As a premise, let us assume that θ is real. By the way, I don&rsquo;t think it is a vector. Please note.</p>
<p>An expression like this</p>
<p>J(θ) = αθ<sup>2</sup> + bθ + c</p>
<p>In other words, consider the cost function J.</p>
<p>But before that, do you remember that the θ we are dealing with is an n+1-dimensional vector?</p>
<p>The formula looks like this.</p>
<p>J(θ<sub>0</sub>,θ<sub>1</sub>,&hellip;,θ<sub>m</sub>) = 1/2m mΣi=1 (h<sub>θ&lt; /sub&gt;(x<sup>( i )</sup>)-y <sup>( i )</sup>)<sup>2</sup></p>
<p>The problem is how to minimize the cost function J, but if you take the partial derivative of J and solve variously for each parameter θ<sub>j</sub>, the answer of the cost function J will be Can be derived, but in this section we will approach a more conceptual part.</p>
<p>By the way, let&rsquo;s say you have a dataset like this.</p>
<p>![Screenshot 2020-01-28 21.39.07.png](<a href="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/448030/3f69eb44-bcc7-a5d4-f7d7-(2bb7bb783bcf.png)">https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/448030/3f69eb44-bcc7-a5d4-f7d7-(2bb7bb783bcf.png)</a></p>
<p>Then, create a column of x<sub>0</sub> to create a matrix of x.![Screenshot 2020-01-28 21.42.12.png](<a href="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/448030/c6e035d4-5536-47de-58c9-(d02c7adf80a1.png)">https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/448030/c6e035d4-5536-47de-58c9-(d02c7adf80a1.png)</a></p>
<p>Same for y.</p>
<p>![Screenshot 2020-01-28 21.43.40.png](<a href="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/448030/765010c7-2fa9-bc3e-9dcd-(4173bf00eb87.png)">https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/448030/765010c7-2fa9-bc3e-9dcd-(4173bf00eb87.png)</a></p>
<p>The x above is m × n+1. y is an m-dimensional vector. By the way, m is the number of cases in the dataset, and n is the number of features (variable x).</p>
<p><strong>So, what is a normal equation</strong></p>
<p>θ = (X<sup>T</sup>X)<sup>-1</sup>X<sup>T</sup><sub>y</sub></p>
<p>Remember, the wrinkled letter T is</p>
<h2 id="heading"></h2>
<h2 id="8-thank-you">8 Thank you</h2>
<p>In this second article, I was accidentally deleted data twice and made me seriously think about working on a cloud basis. Okay, which one are you?</p>
<p>Type A seems to be difficult anyway, so I came here by scrolling</p>
<p>I read it all, wondering what is Type B</p>
<p>I think 90% of the time is Type A, but&hellip;</p>
<p>Nice to meet you!</p>
<p>**Part 3 will be available Friday night. **</p>
<h1 id="week-3">Week 3</h1>
<p>Published on 16th at 21:00</p>
<h4 id="classification-problem">Classification problem</h4>
<h4 id="hypothesis-expression">Hypothesis expression</h4>
<h4 id="decision-boundary">Decision boundary</h4>
<h4 id="cost-function">cost function</h4>
<h4 id="cost-function-and-steepest-descent-method-simple">Cost function and steepest descent method (simple)</h4>
<h4 id="advanced-optimization">Advanced optimization</h4>
<h4 id="multiclass-classification-1-vs-all">Multiclass classification: 1 vs. all</h4>
<h4 id="overfit-problem">Overfit problem</h4>
<h4 id="cost-function-1">cost function</h4>
<h4 id="regularized-linear-regression">Regularized linear regression</h4>
<h4 id="regularized-logistic-regression">Regularized logistic regression</h4>
<h1 id="week-4">Week 4</h1>
<p>Published on 16th at 21:00</p>
<h4 id="nonlinear-hypothesis">Nonlinear hypothesis</h4>
<h4 id="neuron-and-brain">Neuron and brain</h4>
<h4 id="model-expression-1">Model expression (1)</h4>
<h4 id="model-representation-2">Model representation (2)</h4>
<h4 id="samples-and-intuition-1">Samples and intuition (1)</h4>
<h4 id="samples-and-intuition-2">Samples and intuition (2)</h4>
<h4 id="multi-class-classification">Multi-class classification</h4>
<h1 id="week-5">Week 5</h1>
<p>Published at 21:00 on the 17th</p>
<h4 id="cost-function-2">cost function</h4>
<h4 id="backpropagation-algorithm">Backpropagation algorithm</h4>
<h4 id="intuitive-backpropagation">Intuitive backpropagation</h4>
<h4 id="implementation-note-parameter-expansion">Implementation note: Parameter expansion</h4>
<h4 id="gradient-check">Gradient check</h4>
<h4 id="random-initialization">Random initialization</h4>
<h4 id="put-it-together">put it together</h4>
<h1 id="week-6">Week 6</h1>
<p>Published at 21:00 on the 19th</p>
<h4 id="decision-to-try-next">Decision to try next</h4>
<h4 id="how-to-evaluate-the-hypothesis">How to evaluate the hypothesis?</h4>
<h4 id="model-selection-and-train-kneading--verification--test-set">Model selection and train (kneading) / verification / test set</h4>
<h4 id="bias-and-variance-diagnostics">Bias and variance diagnostics</h4>
<h4 id="regularization-and-biasdispersion">Regularization and bias/dispersion</h4>
<h4 id="learning-curve">Learning curve</h4>
<h4 id="reconsider-what-to-do-next">Reconsider what to do next</h4>
<h4 id="prioritizing-work-content">Prioritizing work content</h4>
<h4 id="error-analysis">Error analysis</h4>
<h4 id="distorted-class-error-indicator">Distorted class error indicator</h4>
<h4 id="trade-off-between-system-and-recall">Trade-off between system and recall</h4>
<h4 id="machine-learning-data">Machine learning data</h4>
<h1 id="week-7">Week 7</h1>
<p>Published at 21:00 on the 20th</p>
<h4 id="purpose-of-optimization">Purpose of optimization</h4>
<h4 id="kernel-1">Kernel (1)</h4>
<h4 id="kernel-2">Kernel (2)</h4>
<h4 id="using-svm">Using SVM</h4>
<h1 id="week-8">Week 8</h1>
<p>Published at 21:00 on the 22nd</p>
<h4 id="unsupervised-learning-introduction">Unsupervised Learning (Introduction)</h4>
<h4 id="k-means-algorithm">K-Means algorithm</h4>
<h4 id="purpose-of-optimization-1">Purpose of optimization</h4>
<h4 id="random-initialization-1">Random initialization</h4>
<h4 id="select-number-of-clusters">Select number of clusters</h4>
<h4 id="data-compression">Data compression</h4>
<h4 id="visualization">Visualization</h4>
<h4 id="formulation-of-principal-component-analysis-problem">Formulation of principal component analysis problem</h4>
<h4 id="principal-component-analysis-algorithm">Principal component analysis algorithm</h4>
<h4 id="reconstruction-from-compressed-representation">Reconstruction from compressed representation</h4>
<h4 id="selection-of-the-number-of-principal-components">Selection of the number of principal components</h4>
<h4 id="advice-on-pca-application">Advice on PCA application</h4>
<h1 id="week-9">Week 9</h1>
<p>Published at 21:00 on the 23rd</p>
<h4 id="lets-detect-anomalies">Let&rsquo;s detect anomalies</h4>
<h4 id="gaussian-distribution">Gaussian distribution</h4>
<h4 id="algorithm">Algorithm</h4>
<h4 id="development-and-evaluation-of-anomaly-detection-system">Development and evaluation of anomaly detection system</h4>
<h4 id="anomaly-detection-and-supervised-learning">Anomaly detection and supervised learning</h4>
<h4 id="select-the-function-to-use">Select the function to use</h4>
<h4 id="multivariate-gaussian-distribution">Multivariate Gaussian distribution</h4>
<h4 id="anomaly-detection-using-multivariate-gaussian-distribution">Anomaly detection using multivariate Gaussian distribution</h4>
<h4 id="problem-formulation">Problem formulation</h4>
<h4 id="content-based-recommendations">Content-based recommendations</h4>
<h4 id="collaborative-filtering">Collaborative filtering</h4>
<h4 id="collaborative-filtering-algorithm">Collaborative filtering algorithm</h4>
<h4 id="vectorization-low-rank-matrix-decomposition">Vectorization: low rank matrix decomposition</h4>
<h4 id="implementation-details-average-normalization">Implementation details: average normalization</h4>
<h1 id="week-10">Week 10</h1>
<p>Published at 21:00 on the 24th</p>
<h4 id="learning-with-large-datasets">Learning with large datasets</h4>
<h4 id="stochastic-steepest-descent-method">Stochastic steepest descent method</h4>
<h4 id="mini-batch-steepest-descent-method">Mini-batch steepest descent method</h4>
<h4 id="stochastic-steepest-descent-method-convergence">Stochastic steepest descent method convergence</h4>
<h4 id="online-learning">Online learning</h4>
<h4 id="map-reduce-and-data-parallel-processing">Map Reduce and data parallel processing</h4>
<h1 id="week-11">Week 11</h1>
<p>Published at 21:00 on 26th</p>
<h4 id="problem-description-and-pipeline">Problem description and pipeline</h4>
<h4 id="sliding-window">sliding window</h4>
<h4 id="get-large-amounts-of-data-and-artificial-data">Get large amounts of data and artificial data</h4>
<h4 id="ceiling-analysis-which-part-of-the-pipeline-to-work-on-next">Ceiling analysis: which part of the pipeline to work on next</h4>
<p>#Postscript
how was it. Most of the contents are now, but we will increase the content.
The reason why I do my best is because the tutorial for beginners of my computer department is making games using C#.
From now on, Python x machine learning will be good.
If you like, please like.</p>

    </article>
  </div>

  
  
  
  <script>
  window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
  ga('create', 'UA-169005401-1', 'auto');
  ga('send', 'pageview');
  </script>
  <script async src='https://www.google-analytics.com/analytics.js'></script>
  

  

  
<link rel="stylesheet" type="text/css" href="//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.0/cookieconsent.min.css" />
<script src="//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.0/cookieconsent.min.js"></script>
<script>
window.addEventListener("load", function(){
window.cookieconsent.initialise({
  "palette": {
    "popup": {
      "background": "#216942",
      "text": "#b2d192"
    },
    "button": {
      "background": "#afed71"
    }
  }
})});
</script>

</body>

</html>
