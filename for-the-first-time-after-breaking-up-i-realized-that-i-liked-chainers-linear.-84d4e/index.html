<!doctype html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  
  <meta name="generator" content="Hugo 0.72.0" />

  
  <meta name="description" content="Some description">
  

  
  <link rel="apple-touch-icon" sizes="180x180" href="https://memotut.com/apple-touch-icon.png">

  
  <link rel="icon" type="image/png" sizes="32x32" href="https://memotut.com/favicon-32x32.png">

  
  <link rel="icon" type="image/png" sizes="16x16" href="https://memotut.com/favicon-16x16.png">

  
  <link rel="manifest" href="https://memotut.com/site.webmanifest">

  
  <link rel="mask-icon" href="https://memotut.com/safari-pinned-tab.svg" color="#5bbad5">

  <meta name="msapplication-TileColor" content="#da532c">

  <meta name="theme-color" content="#ffffff">

  
  <link rel="stylesheet" href="https://memotut.com/css/bootstrap.min.css" />

  
  <title>For the first time after breaking up, I realized that I liked Chainer&#39;s Linear. | Memo Tut</title>
  

  <style>
body {
  min-width: 300px;
}

.custom-navbar {
  margin-bottom: 1em;
  height: 60px;
}

.custom-navbar a {
  display: inline-block; 
  padding: 18px 0;
  margin-right: 1em; 
  font-weight: bold; 
}

.custom-navbar a:hover,
.custom-navbar a:focus {
  text-decoration: none; 
}

@media print {
  .custom-navbar {
    display: none;
  }
}

article {
  padding-bottom: 1em;
}

img {
  max-width: 100%;
}


body {
  background-color: #fff;
}



body {
  color: #212529;
}



a {
  color: #007bff;
}



a:hover,
a:focus {
  color: #0056b3;
}



.custom-navbar {
  background-color: #212529;
}



.custom-navbar a {
  color: rgba(255, 255, 255, 0.75);
}



.custom-navbar a:hover,
.custom-navbar a:focus {
  color: rgba(255, 255, 255, 1);
}



.container {
  max-width: 800px;
}



pre {
  display: block;
  padding: 9.5px;
  word-break: break-all;
  word-wrap: break-word;
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 4px;
}

pre code {
  padding: 0;
  font-size: inherit;
  color: inherit; 
  white-space: pre-wrap;
  background-color: transparent;
  border: none;
  border-radius: 0;
}

code {
  padding: 2px 4px;
  color: inherit; 
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 4px;
  font-size: .9em;
}



blockquote,
.blockquote {
  padding: 10px 20px;
  margin: 0 0 20px;
  font-size: 1em;
  border-left: 5px solid #6c757d;
}

</style>
</head>

<body>
  <nav class="custom-navbar">
  <div class="container">
    
    <a href="/">Posts</a>
    
    <a href="/tags/">Tags</a>
    
    <a href="/about/">About</a>
    
    <a href="/index.xml">RSS</a>
    
  </div>
</nav>
  
  <div class="container">
    <article>
      <h1>For the first time after breaking up, I realized that I liked Chainer&rsquo;s Linear.</h1>
<p>
  <small class="text-secondary">
  
  
  Dec 20, 2019
  </small>
  

<small><code><a href="https://memotut.com/tags/python">Python</a></code></small>


<small><code><a href="https://memotut.com/tags/deeplearning"> DeepLearning</a></code></small>


<small><code><a href="https://memotut.com/tags/chainer"> Chainer</a></code></small>


<small><code><a href="https://memotut.com/tags/pytorch"> PyTorch</a></code></small>

</p>
<pre><code>2020/05/12 update
</code></pre>
<p><a href="https://github.com/pfnet/pytorch-pfn-extras">https://github.com/pfnet/pytorch-pfn-extras</a>
It became possible by importing this.</p>
<p>===</p>
<p>I was notified when the major update of Chainer was announced, and I wondered if it was time to come, but I decided to touch PyTorch as soon as possible for the future, and I made the model I had made with Chainer until now PyTorch I decided to implement it again.</p>
<p>And immediately, I hit an error&hellip;</p>
<p><img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/22648/fcd6f6fe-362a-909e-3861-a8dca0a999a4.png" alt="image.png"></p>
<p>The relevant part is the following code.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">self<span style="color:#f92672">.</span>fc1 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(None, <span style="color:#ae81ff">120</span>)
</code></pre></div><p>Yes, you can&rsquo;t set None! ! ! ! !
Yes, you need to explicitly specify the input size! ! !</p>
<p>It&rsquo;s&hellip; dull&hellip;
After the convolutional layer, you have to do the calculation yourself or debug once&hellip;
If you hard-code it, you can&rsquo;t change the image size.</p>
<p>I wondered if this was something I was confusing, so I looked at the source.
<a href="https://pytorch.org/docs/stable/_modules/torch/nn/modules/linear.html#Linear">https://pytorch.org/docs/stable/_modules/torch/nn/modules/linear.html#Linear</a></p>
<p>Below is an excerpt from the source.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">    <span style="color:#66d9ef">def</span> __init__(self, in_features, out_features, bias<span style="color:#f92672">=</span>True):
        super(Linear, self)<span style="color:#f92672">.</span>__init__()
        self<span style="color:#f92672">.</span>in_features <span style="color:#f92672">=</span> in_features
        self<span style="color:#f92672">.</span>out_features <span style="color:#f92672">=</span> out_features
        self<span style="color:#f92672">.</span>weight <span style="color:#f92672">=</span> Parameter(torch<span style="color:#f92672">.</span>Tensor(out_features, in_features))
        <span style="color:#66d9ef">if</span> bias:
            self<span style="color:#f92672">.</span>bias <span style="color:#f92672">=</span> Parameter(torch<span style="color:#f92672">.</span>Tensor(out_features))
        <span style="color:#66d9ef">else</span>:
            self<span style="color:#f92672">.</span>register_parameter(<span style="color:#e6db74">&#39;bias&#39;</span>, None)
        self<span style="color:#f92672">.</span>reset_parameters()

    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">reset_parameters</span>(self):
        init<span style="color:#f92672">.</span>kaiming_uniform_(self<span style="color:#f92672">.</span>weight, a<span style="color:#f92672">=</span>math<span style="color:#f92672">.</span>sqrt(<span style="color:#ae81ff">5</span>))
        <span style="color:#66d9ef">if</span> self<span style="color:#f92672">.</span>bias <span style="color:#f92672">is</span> <span style="color:#f92672">not</span> None:
            fan_in, _ <span style="color:#f92672">=</span> init<span style="color:#f92672">.</span>_calculate_fan_in_and_fan_out(self<span style="color:#f92672">.</span>weight)
            bound <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span> <span style="color:#f92672">/</span> math<span style="color:#f92672">.</span>sqrt(fan_in)
            init<span style="color:#f92672">.</span>uniform_(self<span style="color:#f92672">.</span>bias, <span style="color:#f92672">-</span>bound, bound)

    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, input):
        <span style="color:#66d9ef">return</span> F<span style="color:#f92672">.</span>linear(input, self<span style="color:#f92672">.</span>weight, self<span style="color:#f92672">.</span>bias)
</code></pre></div><p>As you can see, self.weight is decided at the time of initialization.
At the time of forward propagation, self.weight was a pre-requisite. Kuu.</p>
<p>By the way, Chainer is as follows
<a href="https://github.com/chainer/chainer/blob/master/chainer/links/connection/linear.py">https://github.com/chainer/chainer/blob/master/chainer/links/connection/linear.py</a></p>
<p>Below is an excerpt from the source.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">
    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(
            self,
            x: variable<span style="color:#f92672">.</span>Variable,
            n_batch_axes: int <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span>
    ) <span style="color:#f92672">-&gt;</span> variable<span style="color:#f92672">.</span>Variable:
        <span style="color:#e6db74">&#34;&#34;&#34;Applies the linear layer.
</span><span style="color:#e6db74">        Args:
</span><span style="color:#e6db74">            x (~chainer.Variable): Batch of input vectors.
</span><span style="color:#e6db74">            n_batch_axes (int): The number of batch axes. The default is 1. The
</span><span style="color:#e6db74">                input variable is reshaped into
</span><span style="color:#e6db74">                (:math:`{</span><span style="color:#ae81ff">\\</span><span style="color:#e6db74">rm n</span><span style="color:#ae81ff">\\</span><span style="color:#e6db74">_batch</span><span style="color:#ae81ff">\\</span><span style="color:#e6db74">_axes} + 1`)-dimensional tensor.
</span><span style="color:#e6db74">                This should be greater than 0.
</span><span style="color:#e6db74">        Returns:
</span><span style="color:#e6db74">            ~chainer.Variable: Output of the linear layer.
</span><span style="color:#e6db74">        &#34;&#34;&#34;</span>
        <span style="color:#66d9ef">if</span> self<span style="color:#f92672">.</span>W<span style="color:#f92672">.</span>array <span style="color:#f92672">is</span> None:
            in_size <span style="color:#f92672">=</span> utils<span style="color:#f92672">.</span>size_of_shape(x<span style="color:#f92672">.</span>shape[n_batch_axes:])
            self<span style="color:#f92672">.</span>_initialize_params(in_size)
        <span style="color:#66d9ef">return</span> linear<span style="color:#f92672">.</span>linear(x, self<span style="color:#f92672">.</span>W, self<span style="color:#f92672">.</span>b, n_batch_axes<span style="color:#f92672">=</span>n_batch_axes)
</code></pre></div><p>If self.W.array is not initialized at the time of forward propagation, it will be initialized again. What kind of kindness, world, this is Define By Run! (The comments are very polite.)</p>
<p>This was how I realized that I liked Chainer&rsquo;s Linear for the first time after breaking up with Chainer. .. ..</p>
<p>By the way, there was a person who pointed out this inconvenience in the issue.
<a href="https://github.com/pytorch/pytorch/issues/30880">https://github.com/pytorch/pytorch/issues/30880</a>
He seems to have respect for MXNet, but I would like to tell you the same thing.
I want to watch over it quietly.</p>
<h2 id="bonus">bonus</h2>
<p>I tried to initialize nn.Linear by forward propagation as a measure of bitterness.
The following is a partial modification of the source described in the pytorch tutorial.
Specifically, self.fc1 is initialized by forward propagation.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">
<span style="color:#66d9ef">class</span> <span style="color:#a6e22e">Net</span>(nn<span style="color:#f92672">.</span>Module):

    <span style="color:#66d9ef">def</span> __init__(self):
        super(Net, self)<span style="color:#f92672">.</span>__init__()
        <span style="color:#75715e"># 1 input image channel, 6 output channels, 3x3 square convolution</span>
        <span style="color:#75715e"># kernel</span>
        self<span style="color:#f92672">.</span>conv1 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Conv2d(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">6</span>, <span style="color:#ae81ff">3</span>)
        self<span style="color:#f92672">.</span>conv2 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Conv2d(<span style="color:#ae81ff">6</span>, <span style="color:#ae81ff">16</span>, <span style="color:#ae81ff">3</span>)
        <span style="color:#75715e"># an affine operation: y = Wx + b</span>
        self<span style="color:#f92672">.</span>fc1 <span style="color:#f92672">=</span> None
        self<span style="color:#f92672">.</span>fc2 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(<span style="color:#ae81ff">120</span>, <span style="color:#ae81ff">84</span>)
        self<span style="color:#f92672">.</span>fc3 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(<span style="color:#ae81ff">84</span>, <span style="color:#ae81ff">10</span>)

    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x):
        <span style="color:#75715e"># Max pooling over a (2, 2) window</span>
        x <span style="color:#f92672">=</span> F<span style="color:#f92672">.</span>max_pool2d(F<span style="color:#f92672">.</span>relu(self<span style="color:#f92672">.</span>conv1(x)), (<span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">2</span>))
        <span style="color:#75715e"># If the size is a square you can only specify a single number</span>
        x <span style="color:#f92672">=</span> F<span style="color:#f92672">.</span>max_pool2d(F<span style="color:#f92672">.</span>relu(self<span style="color:#f92672">.</span>conv2(x)), <span style="color:#ae81ff">2</span>)
        <span style="color:#66d9ef">if</span> self<span style="color:#f92672">.</span>fc1 <span style="color:#f92672">is</span> None:
            self<span style="color:#f92672">.</span>fc1 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(self<span style="color:#f92672">.</span>num_flat_features(x), <span style="color:#ae81ff">120</span>)

        x <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>view(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, self<span style="color:#f92672">.</span>num_flat_features(x))
        x <span style="color:#f92672">=</span> F<span style="color:#f92672">.</span>relu(self<span style="color:#f92672">.</span>fc1(x))
        x <span style="color:#f92672">=</span> F<span style="color:#f92672">.</span>relu(self<span style="color:#f92672">.</span>fc2(x))
        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>fc3(x)
        <span style="color:#66d9ef">return</span> x

    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">num_flat_features</span>(self, x):
        size <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>size()[<span style="color:#ae81ff">1</span>:] <span style="color:#75715e"># all dimensions except the batch dimension</span>
        num_features <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span>
        <span style="color:#66d9ef">for</span> s <span style="color:#f92672">in</span> size:
            num_features <span style="color:#f92672">*=</span> s
        <span style="color:#66d9ef">return</span> num_features

</code></pre></div><p>Well, it feels like something.</p>
<h2 id="bonus-2">Bonus 2</h2>
<p>By the way, when connecting pytorch to forward propagation from convolution,
It seems that Adaptive Average Pooling is diverse.</p>
<p>Specific example below
<a href="https://github.com/pytorch/vision/blob/d2c763e14efe57e4bf3ebf916ec243ce8ce3315c/torchvision/models/vgg.py#L29">https://github.com/pytorch/vision/blob/d2c763e14efe57e4bf3ebf916ec243ce8ce3315c/torchvision/models/vgg.py#L29</a></p>
<p>This should have been a nice stride or filter size adjustment if you specify the output size. However, I don&rsquo;t know if this layer is to make input size less sensitive or to make it more accurate.</p>

    </article>
  </div>

  
  
  
  <script>
  window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
  ga('create', 'UA-169005401-1', 'auto');
  ga('send', 'pageview');
  </script>
  <script async src='https://www.google-analytics.com/analytics.js'></script>
  

  

  
<link rel="stylesheet" type="text/css" href="//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.0/cookieconsent.min.css" />
<script src="//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.0/cookieconsent.min.js"></script>
<script>
window.addEventListener("load", function(){
window.cookieconsent.initialise({
  "palette": {
    "popup": {
      "background": "#216942",
      "text": "#b2d192"
    },
    "button": {
      "background": "#afed71"
    }
  }
})});
</script>

</body>

</html>
