<!doctype html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  
  <meta name="generator" content="Hugo 0.72.0" />

  
  <meta name="description" content="Some description">
  

  
  <link rel="apple-touch-icon" sizes="180x180" href="https://memotut.com/apple-touch-icon.png">

  
  <link rel="icon" type="image/png" sizes="32x32" href="https://memotut.com/favicon-32x32.png">

  
  <link rel="icon" type="image/png" sizes="16x16" href="https://memotut.com/favicon-16x16.png">

  
  <link rel="manifest" href="https://memotut.com/site.webmanifest">

  
  <link rel="mask-icon" href="https://memotut.com/safari-pinned-tab.svg" color="#5bbad5">

  <meta name="msapplication-TileColor" content="#da532c">

  <meta name="theme-color" content="#ffffff">

  
  <link rel="stylesheet" href="https://memotut.com/css/bootstrap.min.css" />

  
  <title>An amateur stumbled on Deep Learning made from scratch Memo: Chapter 4 | Memo Tut</title>
  

  <style>
body {
  min-width: 300px;
}

.custom-navbar {
  margin-bottom: 1em;
  height: 60px;
}

.custom-navbar a {
  display: inline-block; 
  padding: 18px 0;
  margin-right: 1em; 
  font-weight: bold; 
}

.custom-navbar a:hover,
.custom-navbar a:focus {
  text-decoration: none; 
}

@media print {
  .custom-navbar {
    display: none;
  }
}

article {
  padding-bottom: 1em;
}

img {
  max-width: 100%;
}


body {
  background-color: #fff;
}



body {
  color: #212529;
}



a {
  color: #007bff;
}



a:hover,
a:focus {
  color: #0056b3;
}



.custom-navbar {
  background-color: #212529;
}



.custom-navbar a {
  color: rgba(255, 255, 255, 0.75);
}



.custom-navbar a:hover,
.custom-navbar a:focus {
  color: rgba(255, 255, 255, 1);
}



.container {
  max-width: 800px;
}



pre {
  display: block;
  padding: 9.5px;
  word-break: break-all;
  word-wrap: break-word;
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 4px;
}

pre code {
  padding: 0;
  font-size: inherit;
  color: inherit; 
  white-space: pre-wrap;
  background-color: transparent;
  border: none;
  border-radius: 0;
}

code {
  padding: 2px 4px;
  color: inherit; 
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 4px;
  font-size: .9em;
}



blockquote,
.blockquote {
  padding: 10px 20px;
  margin: 0 0 20px;
  font-size: 1em;
  border-left: 5px solid #6c757d;
}

</style>
</head>

<body>
  <nav class="custom-navbar">
  <div class="container">
    
    <a href="/">Posts</a>
    
    <a href="/tags/">Tags</a>
    
    <a href="/about/">About</a>
    
    <a href="/index.xml">RSS</a>
    
  </div>
</nav>
  
  <div class="container">
    <article>
      <h1>An amateur stumbled on Deep Learning made from scratch Memo: Chapter 4</h1>
<p>
  <small class="text-secondary">
  
  
  Jan 3, 2020
  </small>
  

<small><code><a href="https://memotut.com/tags/python">Python</a></code></small>


<small><code><a href="https://memotut.com/tags/machine-learning"> machine learning</a></code></small>


<small><code><a href="https://memotut.com/tags/deeplearning"> DeepLearning</a></code></small>

</p>
<pre><code>#Introduction
</code></pre>
<p>I suddenly started studying <a href="https://www.oreilly.co.jp/books/9784873117584/">[Deep Learning from scratch-the theory and implementation of deep learning learned with Python]</a> It is a memo of the trip.</p>
<p>The execution environment is macOS Mojave + Anaconda 2019.10, and the Python version is 3.7.4. For details, please refer to <a href="https://qiita.com/segavvy/items/1945aa1a0f91a1473555">Chapter 1 of this memo</a>.</p>
<p>(To other chapters of this memo: <a href="https://qiita.com/segavvy/items/1945aa1a0f91a1473555">Chapter 1</a>/<a href="https://qiita.com/segavvy/items/d8e9e70437e35083a459">Chapter2</a>/<a href="https://qiita.com/segavvy/items/6d79d0c3b4367869f4ea">Chapter3</a>/Chapter4/<a href="https://qiita.com/segavvy/items/8707e4e65aa7fa357d8a">Chapter5</a>/<a href="https://qiita.com/segavvy/items/ca4ac4c9ee1a126bff41">Chapter6</a>/<a href="https://qiita.com/segavvy/items/8541c6ae1868d9b2b805">Chapter7</a>/<a href="https://qiita.com/segavvy/items/3eb6ea0ea2ea68af68af68af2af6868">Chapter8</a>)/<a href="https://qiita.com/segavvy/items/4e8c36cac9c6f3543ffd">Summary</a>))</p>
<p>#4 Learning Neural Networks</p>
<p>This chapter describes learning neural networks.</p>
<h1 id="41-learning-from-data">4.1 Learning from data</h1>
<p>Usually, a person derives regularity, thinks about an algorithm, writes it in a program, and causes a computer to execute it. Machine learning, neural networks, and deep learning let the computer do the work itself of thinking about this algorithm.</p>
<p>In this book, for the data to be processed, what requires the extraction of feature quantities (vectorization, etc.) that humans have considered beforehand is called &ldquo;machine learning&rdquo;, and the &ldquo;machine learning&rdquo; is left to the extraction of feature quantities. The one that allows raw data to be passed as is is defined as &ldquo;neural network (deep learning)&rdquo;. This definition feels a little rough, but I&rsquo;m not very interested in the proper use of words, so don&rsquo;t worry and proceed.</p>
<p>Training data, test data, overfitting, etc. are explained, but there was no particular stumbling block.</p>
<h1 id="42-loss-function">4.2 Loss function</h1>
<p>It is an explanation of square sum error and cross entropy error that are often used as loss functions, and an explanation of mini-batch learning in which a part of the training data is used for learning. There was no particular stumbling block here either. It seems to be good to use all the training data, but it is time consuming and inefficient. I think it&rsquo;s a so-called sample survey.</p>
<p>In addition, as a reason why recognition accuracy cannot be used instead of loss function, it is explained that recognition accuracy cannot be well learned because it does not react with minute changes in the result and changes discontinuously. It may not come at first at first, but I think that I will be hungry after the explanation of the next derivative.</p>
<h1 id="43-numerical-differentiation">4.3 Numerical differentiation</h1>
<p>Explanation of differentiation. I would appreciate any practical explanation of rounding errors during implementation. It seems difficult to hear the word &ldquo;differential&rdquo; or &ldquo;partial differential&rdquo;, but how does the result change if the value is changed a little? That&rsquo;s all, so I&rsquo;ll move forward even if I don&rsquo;t like high school mathematics.</p>
<p>By the way, the symbol $ \partial $ that appears in the differential is read as <a href="https://en.wikipedia.org/wiki/%E2%88%82">Wikipedia</a> says Dell, Dee, Partial Dee, Round Dee, etc. That&rsquo;s right.</p>
<p>Even so, Python can easily pass a function as an argument. When I was a programmer, I used to be C/C++ main, but I didn&rsquo;t like the function pointer notation because it was really hard to understand :sweat:</p>
<h1 id="44-slope">4.4 slope</h1>
<p>The gradient is the vector of partial derivatives of all variables. This in itself is not difficult.</p>
<p>It&rsquo;s nice to see that the numbers are rounded and displayed when outputting decimal numbers in the NumPy array.</p>
<pre><code class="language-python:" data-lang="python:">&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; a = np.array([1.00000000123, 2.99999999987])
&gt;&gt;&gt; a
array([1., 3.])
</code></pre><p>However, there are times when it is difficult to be rounded up arbitrarily, and when I looked into what the specifications were, I had a function to set the display method. In <a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.set_printoptions.html#numpy-set-printoptions"><code>numpy.set_printoptions</code></a>, the decimal display method and the number of elements are large. In this case, you can change the omission method. For example, if you specify a large number of digits after the decimal point with <code>precision</code>, it will be displayed without being rounded properly.</p>
<pre><code class="language-python:" data-lang="python:">&gt;&gt;&gt; np.set_printoptions(precision=12)
&gt;&gt;&gt; a
array([1.00000000123, 2.99999999987])
</code></pre><p>This is convenient!</p>
<h2 id="441-gradient-method">4.4.1 Gradient method</h2>
<p>The word &ldquo;gradient descent method&rdquo; appears in the text, which was translated as &ldquo;the steepest descent method&rdquo; in the teaching materials I used to study.</p>
<p>After that, the symbol \ \eta $ that shows the learning rate appears, which is read in Greek letters as Eta (I remembered how to read it when I studied before, but I forgot it completely, I googled it) :sweat:).</p>
<h2 id="442-gradient-for-neural-network">4.4.2 Gradient for neural network</h2>
<p>I use <code>numerical_gradient(f, x)</code> to find the gradient, but the function I pass to this <code>f</code> is</p>
<pre><code class="language-python:" data-lang="python:">def f(W):
    return net.loss(x, t)
</code></pre><p>And that? Is this function not using the argument <code>W</code>? I was a little confused with this, but I am trying to use the form of the <code>numerical_gradient(f, x)</code> function implemented in &ldquo;4.4 Gradient&rdquo; as is, so the argument <code>W</code> is a dummy. Certainly, the <code>simpleNet</code> class holds its own weight <code>W</code>, so it is not necessary to pass the weight <code>W</code> to the loss function <code>simpleNet.loss</code>. Since it is hard to understand if there is a dummy, I decided to implement it without arguments.</p>
<p>After that, it is necessary to modify the <code>numerical_gradient</code> so that it can be used for multidimensional arrays.</p>
<p>#4.5 Implementation of learning algorithm</p>
<p>Now we&rsquo;ll use what we&rsquo;ve learned so far to actually implement stochastic gradient descent (SGD).</p>
<p>First, it is <code>functions.py</code> which is a collection of necessary functions.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python:functions.py" data-lang="python:functions.py"><span style="color:#75715e"># coding: utf-8</span>
<span style="color:#f92672">import</span> numpy <span style="color:#f92672">as</span> np


<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">sigmoid</span>(x):
    <span style="color:#e6db74">&#34;&#34;&#34;Sigmoid function
</span><span style="color:#e6db74">    Since it overflows in the implementation of the book, refer to the following site and modify it.
</span><span style="color:#e6db74">    http://www.kamishima.net/mlmpyja/lr/sigmoid.html
</span><span style="color:#e6db74">
</span><span style="color:#e6db74">    Args:
</span><span style="color:#e6db74">        x (numpy.ndarray): input
</span><span style="color:#e6db74">    
</span><span style="color:#e6db74">    Returns:
</span><span style="color:#e6db74">        numpy.ndarray: output
</span><span style="color:#e6db74">    &#34;&#34;&#34;</span>
    Correct <span style="color:#75715e">#x to a range that does not overflow</span>
    sigmoid_range <span style="color:#f92672">=</span> <span style="color:#ae81ff">34.538776394910684</span>
    x2 <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>maximum(np<span style="color:#f92672">.</span>minimum(x, sigmoid_range), <span style="color:#f92672">-</span>sigmoid_range)

    <span style="color:#75715e">#Sigmoid function</span>
    <span style="color:#66d9ef">return</span> <span style="color:#ae81ff">1</span> <span style="color:#f92672">/</span> (<span style="color:#ae81ff">1</span> <span style="color:#f92672">+</span> np<span style="color:#f92672">.</span>exp(<span style="color:#f92672">-</span>x2))


<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">softmax</span>(x):
    <span style="color:#e6db74">&#34;&#34;&#34; softmax function
</span><span style="color:#e6db74">    
</span><span style="color:#e6db74">    Args:
</span><span style="color:#e6db74">        x (numpy.ndarray): input
</span><span style="color:#e6db74">    
</span><span style="color:#e6db74">    Returns:
</span><span style="color:#e6db74">        numpy.ndarray: output
</span><span style="color:#e6db74">    &#34;&#34;&#34;</span>
    <span style="color:#75715e"># In case of batch processing, x is a 2D array of (number of batches, 10).</span>
    <span style="color:#75715e"># In this case, you have to use broadcast to successfully calculate for each image.</span>
    <span style="color:#75715e"># Here, np.max() and np.sum() are calculated with axis=-1 so that they can be used in both one and two dimensions.</span>
    <span style="color:#75715e"># Keep dimensions with keepdims=True so that you can broadcast as is.</span>
    c <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>max(x, axis<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>, keepdims<span style="color:#f92672">=</span>True)
    exp_a <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>exp(x<span style="color:#f92672">-</span>c) <span style="color:#75715e"># Overflow protection</span>
    sum_exp_a <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>sum(exp_a, axis<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>, keepdims<span style="color:#f92672">=</span>True)
    y <span style="color:#f92672">=</span> exp_a <span style="color:#f92672">/</span> sum_exp_a
    <span style="color:#66d9ef">return</span> y


<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">numerical_gradient</span>(f, x):
    <span style="color:#e6db74">&#34;&#34;&#34; Gradient calculation
</span><span style="color:#e6db74">    
</span><span style="color:#e6db74">    Args:
</span><span style="color:#e6db74">        f (function): Loss function
</span><span style="color:#e6db74">        x (numpy.ndarray): array of weight parameters for which you want to examine the gradient
</span><span style="color:#e6db74">    
</span><span style="color:#e6db74">    Returns:
</span><span style="color:#e6db74">        numpy.ndarray: gradient
</span><span style="color:#e6db74">    &#34;&#34;&#34;</span>
    h <span style="color:#f92672">=</span> <span style="color:#ae81ff">1e-4</span>
    grad <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>zeros_like(x)

    <span style="color:#75715e"># Enumerate elements of multidimensional array with np.nditer</span>
    it <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>nditer(x, flags<span style="color:#f92672">=</span>[<span style="color:#e6db74">&#39;multi_index&#39;</span>])
    <span style="color:#66d9ef">while</span> <span style="color:#f92672">not</span> it<span style="color:#f92672">.</span>finished:

        idx <span style="color:#f92672">=</span> it<span style="color:#f92672">.</span>multi_index <span style="color:#75715e"># it.multi_index is the element number in the enumeration</span>
        tmp_val <span style="color:#f92672">=</span> x[idx] <span style="color:#75715e"># save original value</span>

        <span style="color:#75715e">#f(x + h) calculation</span>
        x[idx] <span style="color:#f92672">=</span> tmp_val <span style="color:#f92672">+</span> h
        fxh1 <span style="color:#f92672">=</span> f()

        <span style="color:#75715e">#f(x-h) calculation</span>
        x[idx] <span style="color:#f92672">=</span> tmp_val<span style="color:#f92672">-</span>h
        fxh2 <span style="color:#f92672">=</span> f()

        <span style="color:#75715e">#Calculate slope</span>
        grad[idx] <span style="color:#f92672">=</span> (fxh1<span style="color:#f92672">-</span>fxh2) <span style="color:#f92672">/</span> (<span style="color:#ae81ff">2</span> <span style="color:#f92672">*</span> h)
    
        x[idx] <span style="color:#f92672">=</span> tmp_val <span style="color:#75715e"># returns value</span>
        it<span style="color:#f92672">.</span>iternext()

    <span style="color:#66d9ef">return</span> grad


<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">cross_entropy_error</span>(y, t):
    <span style="color:#e6db74">&#34;&#34;&#34;Calculation of cross entropy error
</span><span style="color:#e6db74">    
</span><span style="color:#e6db74">    Args:
</span><span style="color:#e6db74">        y (numpy.ndarray): Neural network output
</span><span style="color:#e6db74">        t (numpy.ndarray): correct answer label
</span><span style="color:#e6db74">    
</span><span style="color:#e6db74">    Returns:
</span><span style="color:#e6db74">        float: cross entropy error
</span><span style="color:#e6db74">    &#34;&#34;&#34;</span><span style="color:#75715e">#Shape the shape if there is only one data</span>
    <span style="color:#66d9ef">if</span> y<span style="color:#f92672">.</span>ndim <span style="color:#f92672">==</span> <span style="color:#ae81ff">1</span>:
        t <span style="color:#f92672">=</span> t<span style="color:#f92672">.</span>reshape(<span style="color:#ae81ff">1</span>, t<span style="color:#f92672">.</span>size)
        y <span style="color:#f92672">=</span> y<span style="color:#f92672">.</span>reshape(<span style="color:#ae81ff">1</span>, y<span style="color:#f92672">.</span>size)

    <span style="color:#75715e"># Calculate error and normalize by batch number</span>
    batch_size <span style="color:#f92672">=</span> y<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>]
    <span style="color:#66d9ef">return</span> <span style="color:#f92672">-</span>np<span style="color:#f92672">.</span>sum(t <span style="color:#f92672">*</span> np<span style="color:#f92672">.</span>log(y <span style="color:#f92672">+</span> <span style="color:#ae81ff">1e-7</span>)) <span style="color:#f92672">/</span> batch_size


<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">sigmoid_grad</span>(x):
    <span style="color:#e6db74">&#34;&#34;&#34;Functions learned in Chapter 5. Required when using backpropagation.
</span><span style="color:#e6db74">    &#34;&#34;&#34;</span>
    <span style="color:#66d9ef">return</span> (<span style="color:#ae81ff">1.0</span><span style="color:#f92672">-</span>sigmoid(x)) <span style="color:#f92672">*</span> sigmoid(x)
</code></pre></div><p><code>softmax</code> is <a href="https://qiita.com/segavvy/items/6d79d0c3b4367869f4ea#35-%E5%87%BA%E5%8A%9B%E5%B1%A4%E3%81%AE%E8%A8%AD%E8%A8%88">Note that amateur stumbled upon from Deep Learning made from scratch: Chapter 3</a>Itriedtomakeitevenmorerefreshed.Referto<a href="https://github.com/oreilly-japan/deep-learning-from-scratch/issues/45">Codeimprovementplanforsoftmaxfunction#45</a> which was in the issue of GitHub repository of this book. ..</p>
<p>As mentioned above, <code>numerical_gradient</code> has no function argument to pass with <code>f</code>. Moreover, in order to correspond to the multi-dimensional array, it is looped with <a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.nditer.html"><code>numpy.nditer</code></a>.Inaddition,inthecodeofthebook,<code>op_flags=['readwrite']</code>isspecifiedwhenusing<code>numpy.nditer</code>,buttheindexforaccessing<code>x</code>issimplyretrievedwith<code>multi_index</code>.,Sincetheobjectsenumeratedbytheiteratorarenotupdated,<code>op_flags</code>isomitted(<code>op_flags=['readonly']</code>isset).DetailsareinEnglish,butsee<a href="https://docs.scipy.org/doc/numpy/reference/arrays.nditer.html#modifying-array-values">IteratingOverArrays#ModifyingArrayValues</a>.</p>
<p>The last function, <code>sigmoid_grad</code>, is what we will learn in Chapter 5, but since it is necessary to reduce the processing time (described later), it is implemented as in the book.</p>
<p>Next is <code>two_layer_net.py</code>, which implements a two-layer neural network.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python:two_layer_net.py" data-lang="python:two_layer_net.py"><span style="color:#75715e"># coding: utf-8</span>
<span style="color:#f92672">from</span> functions <span style="color:#f92672">import</span> sigmoid, softmax, numerical_gradient, \
    cross_entropy_error, sigmoid_grad
<span style="color:#f92672">import</span> numpy <span style="color:#f92672">as</span> np


<span style="color:#66d9ef">class</span> <span style="color:#a6e22e">TwoLayerNet</span>:

    <span style="color:#66d9ef">def</span> __init__(self, input_size, hidden_size, output_size,
                 weight_init_std<span style="color:#f92672">=</span><span style="color:#ae81ff">0.01</span>):
        <span style="color:#e6db74">&#34;&#34;&#34; Two layer neural network
</span><span style="color:#e6db74">        
</span><span style="color:#e6db74">        Args:
</span><span style="color:#e6db74">            input_size (int): Number of neurons in the input layer
</span><span style="color:#e6db74">            hidden_size (int): number of hidden layer neurons
</span><span style="color:#e6db74">            output_size (int): Number of neurons in output layer
</span><span style="color:#e6db74">            weight_init_std (float, optional): Tuning parameter for initial value of weight. The default is 0.01.
</span><span style="color:#e6db74">        &#34;&#34;&#34;</span>

        <span style="color:#75715e"># Weight initialization</span>
        self<span style="color:#f92672">.</span>params <span style="color:#f92672">=</span> {}
        self<span style="color:#f92672">.</span>params[<span style="color:#e6db74">&#39;W1&#39;</span>] <span style="color:#f92672">=</span> weight_init_std <span style="color:#f92672">*</span> \
            np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>randn(input_size, hidden_size)
        self<span style="color:#f92672">.</span>params[<span style="color:#e6db74">&#39;b1&#39;</span>] <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>zeros(hidden_size)
        self<span style="color:#f92672">.</span>params[<span style="color:#e6db74">&#39;W2&#39;</span>] <span style="color:#f92672">=</span> weight_init_std <span style="color:#f92672">*</span> \
            np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>randn(hidden_size, output_size)
        self<span style="color:#f92672">.</span>params[<span style="color:#e6db74">&#39;b2&#39;</span>] <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>zeros(output_size)

    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">predict</span>(self, x):
        <span style="color:#e6db74">&#34;&#34;&#34; Neural network inference
</span><span style="color:#e6db74">        
</span><span style="color:#e6db74">        Args:
</span><span style="color:#e6db74">            x (numpy.ndarray): Input to neural network
</span><span style="color:#e6db74">        
</span><span style="color:#e6db74">        Returns:
</span><span style="color:#e6db74">            numpy.ndarray: neural network output
</span><span style="color:#e6db74">        &#34;&#34;&#34;</span>
        <span style="color:#75715e"># Parameter extraction</span>
        W1, W2 <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>params[<span style="color:#e6db74">&#39;W1&#39;</span>], self<span style="color:#f92672">.</span>params[<span style="color:#e6db74">&#39;W2&#39;</span>]
        b1, b2 <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>params[<span style="color:#e6db74">&#39;b1&#39;</span>], self<span style="color:#f92672">.</span>params[<span style="color:#e6db74">&#39;b2&#39;</span>]

        <span style="color:#75715e"># Neural network calculation (forward)</span>
        a1 <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>dot(x, W1) <span style="color:#f92672">+</span> b1
        z1 <span style="color:#f92672">=</span> sigmoid(a1)
        
        a2 <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>dot(z1, W2) <span style="color:#f92672">+</span> b2
        y <span style="color:#f92672">=</span> softmax(a2)

        <span style="color:#66d9ef">return</span> y

    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">loss</span>(self, x, t):
        <span style="color:#e6db74">&#34;&#34;&#34; Loss function value calculation
</span><span style="color:#e6db74">        
</span><span style="color:#e6db74">        Args:
</span><span style="color:#e6db74">            x (numpy.ndarray): Input to neural network
</span><span style="color:#e6db74">            t (numpy.ndarray): correct answer label
</span><span style="color:#e6db74">
</span><span style="color:#e6db74">        Returns:
</span><span style="color:#e6db74">            float: value of loss function
</span><span style="color:#e6db74">        &#34;&#34;&#34;</span>
        <span style="color:#75715e"># Inference</span>
        y <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>predict(x)

        <span style="color:#75715e">#Calculation of cross entropy error</span>
        loss <span style="color:#f92672">=</span> cross_entropy_error(y, t)

        <span style="color:#66d9ef">return</span> loss

    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">accuracy</span>(self, x, t):
        <span style="color:#e6db74">&#34;&#34;&#34; Calculation of recognition accuracy
</span><span style="color:#e6db74">        
</span><span style="color:#e6db74">        Args:
</span><span style="color:#e6db74">            x (numpy.ndarray): Input to neural network
</span><span style="color:#e6db74">            t (numpy.ndarray): correct answer label
</span><span style="color:#e6db74">        
</span><span style="color:#e6db74">        Returns:
</span><span style="color:#e6db74">            float: recognition accuracy
</span><span style="color:#e6db74">        &#34;&#34;&#34;</span>
        y <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>predict(x)
        y <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>argmax(y, axis<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
        t <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>argmax(t, axis<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
        
        accuracy <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>sum(y <span style="color:#f92672">==</span> t) <span style="color:#f92672">/</span> x<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>]
        <span style="color:#66d9ef">return</span> accuracy

    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">numerical_gradient</span>(self, x, t):
        <span style="color:#e6db74">&#34;&#34;&#34; Gradient calculation for weight parameters
</span><span style="color:#e6db74">        
</span><span style="color:#e6db74">        Args:
</span><span style="color:#e6db74">            x (numpy.ndarray): Input to neural network
</span><span style="color:#e6db74">            t (numpy.ndarray): correct answer label
</span><span style="color:#e6db74">        
</span><span style="color:#e6db74">        Returns:
</span><span style="color:#e6db74">            dictionary: dictionary that stores the gradient
</span><span style="color:#e6db74">        &#34;&#34;&#34;</span>
        grads <span style="color:#f92672">=</span> {}
        grads[<span style="color:#e6db74">&#39;W1&#39;</span>] <span style="color:#f92672">=</span> \
            numerical_gradient(<span style="color:#66d9ef">lambda</span>: self<span style="color:#f92672">.</span>loss(x, t), self<span style="color:#f92672">.</span>params[<span style="color:#e6db74">&#39;W1&#39;</span>])
        grads[<span style="color:#e6db74">&#39;b1&#39;</span>] <span style="color:#f92672">=</span> \
            numerical_gradient(<span style="color:#66d9ef">lambda</span>: self<span style="color:#f92672">.</span>loss(x, t), self<span style="color:#f92672">.</span>params[<span style="color:#e6db74">&#39;b1&#39;</span>])
        grads[<span style="color:#e6db74">&#39;W2&#39;</span>] <span style="color:#f92672">=</span> \
            numerical_gradient(<span style="color:#66d9ef">lambda</span>: self<span style="color:#f92672">.</span>loss(x, t), self<span style="color:#f92672">.</span>params[<span style="color:#e6db74">&#39;W2&#39;</span>])
        grads[<span style="color:#e6db74">&#39;b2&#39;</span>] <span style="color:#f92672">=</span> \
            numerical_gradient(<span style="color:#66d9ef">lambda</span>: self<span style="color:#f92672">.</span>loss(x, t), self<span style="color:#f92672">.</span>params[<span style="color:#e6db74">&#39;b2&#39;</span>])

        <span style="color:#66d9ef">return</span> grads

    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">gradient</span>(self, x, t):
        <span style="color:#e6db74">&#34;&#34;&#34;Function learned in Chapter 5. Implementation of error backpropagation
</span><span style="color:#e6db74">        &#34;&#34;&#34;</span>
        W1, W2 <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>params[<span style="color:#e6db74">&#39;W1&#39;</span>], self<span style="color:#f92672">.</span>params[<span style="color:#e6db74">&#39;W2&#39;</span>]
        b1, b2 <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>params[<span style="color:#e6db74">&#39;b1&#39;</span>], self<span style="color:#f92672">.</span>params[<span style="color:#e6db74">&#39;b2&#39;</span>]
        grads <span style="color:#f92672">=</span> {}
        
        batch_num <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>]
        
        <span style="color:#75715e"># forward</span>
        a1 <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>dot(x, W1) <span style="color:#f92672">+</span> b1
        z1 <span style="color:#f92672">=</span> sigmoid(a1)
        a2 <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>dot(z1, W2) <span style="color:#f92672">+</span> b2
        y <span style="color:#f92672">=</span> softmax(a2)
        
        <span style="color:#75715e"># backward</span>
        dy <span style="color:#f92672">=</span> (y<span style="color:#f92672">-</span>t) <span style="color:#f92672">/</span> batch_num
        grads[<span style="color:#e6db74">&#39;W2&#39;</span>] <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>dot(z1<span style="color:#f92672">.</span>T, dy)
        grads[<span style="color:#e6db74">&#39;b2&#39;</span>] <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>sum(dy, axis<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)
        
        dz1 <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>dot(dy, W2<span style="color:#f92672">.</span>T)
        da1 <span style="color:#f92672">=</span> sigmoid_grad(a1) <span style="color:#f92672">*</span> dz1
        grads[<span style="color:#e6db74">&#39;W1&#39;</span>] <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>dot(x<span style="color:#f92672">.</span>T, da1)
        grads[<span style="color:#e6db74">&#39;b1&#39;</span>] <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>sum(da1, axis<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)

        <span style="color:#66d9ef">return</span> grads
</code></pre></div><p>Almost the same as the code in the book. The last <code>gradient</code> is what we will learn in Chapter 5, but since it is necessary to shorten the processing time (described later), it is implemented as in the book.</p>
<p>Finally, we will implement mini-batch learning.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python:mnist.py" data-lang="python:mnist.py"><span style="color:#75715e"># coding: utf-8</span>
<span style="color:#f92672">import</span> numpy <span style="color:#f92672">as</span> np
<span style="color:#f92672">import</span> matplotlib.pylab <span style="color:#f92672">as</span> plt
<span style="color:#f92672">import</span> os
<span style="color:#f92672">import</span> sysfrom two_layer_net import TwoLayerNet
sys<span style="color:#f92672">.</span>path<span style="color:#f92672">.</span>append(os<span style="color:#f92672">.</span>pardir) <span style="color:#75715e"># add parent directory to path</span>
<span style="color:#f92672">from</span> dataset.mnist <span style="color:#f92672">import</span> load_mnist


<span style="color:#75715e"># Load MNIST training and test data</span>
(x_train, t_train), (x_test, t_test) <span style="color:#f92672">=</span> \
    load_mnist(normalize<span style="color:#f92672">=</span>True, one_hot_label<span style="color:#f92672">=</span>True)

<span style="color:#75715e"># Hyper parameter setting</span>
iters_num <span style="color:#f92672">=</span> <span style="color:#ae81ff">10000</span> <span style="color:#75715e"># Number of updates</span>
batch_size <span style="color:#f92672">=</span> <span style="color:#ae81ff">100</span> <span style="color:#75715e"># batch size</span>
learning_rate <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.1</span> <span style="color:#75715e"># learning rate</span>

<span style="color:#75715e"># Result record list</span>
train_loss_list <span style="color:#f92672">=</span> [] <span style="color:#75715e"># Transition of loss function value</span>
train_acc_list <span style="color:#f92672">=</span> [] <span style="color:#75715e"># Recognition accuracy for training data</span>
test_acc_list <span style="color:#f92672">=</span> [] <span style="color:#75715e"># Recognition accuracy for test data</span>

train_size <span style="color:#f92672">=</span> x_train<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>] <span style="color:#75715e"># size of training data</span>
iter_per_epoch <span style="color:#f92672">=</span> max(train_size <span style="color:#f92672">/</span> batch_size, <span style="color:#ae81ff">1</span>) <span style="color:#75715e"># 1 iterations per epoch</span>

<span style="color:#75715e"># 2 layer neural work generation</span>
network <span style="color:#f92672">=</span> TwoLayerNet(input_size<span style="color:#f92672">=</span><span style="color:#ae81ff">784</span>, hidden_size<span style="color:#f92672">=</span><span style="color:#ae81ff">50</span>, output_size<span style="color:#f92672">=</span><span style="color:#ae81ff">10</span>)

<span style="color:#75715e"># Start learning</span>
<span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(iters_num):

    <span style="color:#75715e"># Mini batch generation</span>
    batch_mask <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>choice(train_size, batch_size, replace<span style="color:#f92672">=</span>False)
    x_batch <span style="color:#f92672">=</span> x_train[batch_mask]
    t_batch <span style="color:#f92672">=</span> t_train[batch_mask]

    <span style="color:#75715e">#Gradient calculation</span>
    <span style="color:#75715e"># grad = network.numerical_gradient(x_batch, t_batch) It is slow, so the error backpropagation method...</span>
    grad <span style="color:#f92672">=</span> network<span style="color:#f92672">.</span>gradient(x_batch, t_batch)

    <span style="color:#75715e"># Update weight parameter</span>
    <span style="color:#66d9ef">for</span> key <span style="color:#f92672">in</span> (<span style="color:#e6db74">&#39;W1&#39;</span>,<span style="color:#e6db74">&#39;b1&#39;</span>,<span style="color:#e6db74">&#39;W2&#39;</span>,<span style="color:#e6db74">&#39;b2&#39;</span>):
        network<span style="color:#f92672">.</span>params[key] <span style="color:#f92672">-=</span> learning_rate <span style="color:#f92672">*</span> grad[key]
    
    <span style="color:#75715e"># Loss function value calculation</span>
    loss <span style="color:#f92672">=</span> network<span style="color:#f92672">.</span>loss(x_batch, t_batch)
    train_loss_list<span style="color:#f92672">.</span>append(loss)

    <span style="color:#75715e">#1 Calculation of recognition accuracy for each epoch</span>
    <span style="color:#66d9ef">if</span> i <span style="color:#f92672">%</span>iter_per_epoch <span style="color:#f92672">==</span> <span style="color:#ae81ff">0</span>:
        train_acc <span style="color:#f92672">=</span> network<span style="color:#f92672">.</span>accuracy(x_train, t_train)
        test_acc <span style="color:#f92672">=</span> network<span style="color:#f92672">.</span>accuracy(x_test, t_test)
        train_acc_list<span style="color:#f92672">.</span>append(train_acc)
        test_acc_list<span style="color:#f92672">.</span>append(test_acc)

        <span style="color:#75715e"># Progress display</span>
        <span style="color:#66d9ef">print</span>(f<span style="color:#e6db74">&#34; [number of updates] {i: &gt;4} [value of loss function] {loss:.4f} &#34;</span>
              f<span style="color:#e6db74">&#34;[Recognition accuracy of training data]{train_acc:.4f} [Recognition accuracy of test data]{test_acc:.4f}&#34;</span>)

<span style="color:#75715e"># Draw the transition of the value of the loss function</span>
x <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>arange(len(train_loss_list))
plt<span style="color:#f92672">.</span>plot(x, train_loss_list, label<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;loss&#39;</span>)
plt<span style="color:#f92672">.</span>xlabel(<span style="color:#e6db74">&#34;iteration&#34;</span>)
plt<span style="color:#f92672">.</span>ylabel(<span style="color:#e6db74">&#34;loss&#34;</span>)
plt<span style="color:#f92672">.</span>xlim(left<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)
plt<span style="color:#f92672">.</span>ylim(bottom<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)
plt<span style="color:#f92672">.</span>show()

<span style="color:#75715e">#Draw the transition of recognition accuracy of training data and test data</span>
x2 <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>arange(len(train_acc_list))
plt<span style="color:#f92672">.</span>plot(x2, train_acc_list, label<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;train acc&#39;</span>)
plt<span style="color:#f92672">.</span>plot(x2, test_acc_list, label<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;test acc&#39;</span>, linestyle<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;--&#39;</span>)
plt<span style="color:#f92672">.</span>xlabel(<span style="color:#e6db74">&#34;epochs&#34;</span>)
plt<span style="color:#f92672">.</span>ylabel(<span style="color:#e6db74">&#34;accuracy&#34;</span>)
plt<span style="color:#f92672">.</span>xlim(left<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)
plt<span style="color:#f92672">.</span>ylim(<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1.0</span>)
plt<span style="color:#f92672">.</span>legend(loc<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;lower right&#39;</span>)
plt<span style="color:#f92672">.</span>show()
</code></pre></div><p>In the code of the book, I used [<code>numpy.random.choice</code>](<a href="https://docs.scipy.org/doc/numpy-1.15.0/reference/generated/numpy.random.choicewhichisusedforminibatchgeneration.Thereisno%60replace=False%60specificationintheargumentof(.html),">https://docs.scipy.org/doc/numpy-1.15.0/reference/generated/numpy.random.choicewhichisusedforminibatchgeneration.Thereisno`replace=False`specificationintheargumentof(.html),</a> but I tried specifying it because it seems that the same element may be taken out twice.</p>
<p>To calculate the gradient, originally it is done by numerical differentiation using <code>TwoLayerNet.numerical_gradient</code>, but the processing speed is slow and it seems that 10,000 updates are not completed even in one day ~~ It can be updated about 600 times in half a day, and it seems that it will take about 8 days to update 10,000 times. Therefore, following the advice of the book, I used <code>TwoLayerNet.gradient</code> which implements the error propagation method learned in Chapter 5.</p>
<p>Finally, the transition of the value of the loss function and the transition of the recognition accuracy of the training data and test data are displayed in a graph.</p>
<p>The following is the execution result.</p>
<pre><code>[Number of updates] 0 [Loss function value] 2.2882 [Training data recognition accuracy] 0.1044 [Test data recognition accuracy] 0.1028
[Number of updates] 600 [Value of loss function] 0.8353 [Recognition accuracy of training data] 0.7753 [Recognition accuracy of test data] 0.7818
[Number of updates] 1200 [Loss function value] 0.4573 [Training data recognition accuracy] 0.8744 [Test data recognition accuracy] 0.8778
[Number of updates] 1800 [Loss function value] 0.4273 [Training data recognition accuracy] 0.8972 [Test data recognition accuracy] 0.9010
[Number of updates] 2400 [Loss function value] 0.3654 [Training data recognition accuracy] 0.9076 [Test data recognition accuracy] 0.9098
[Number of updates] 3000 [Loss function value] 0.2816 [Training data recognition accuracy] 0.9142 [Test data recognition accuracy] 0.9146
[Number of updates] 3600 [Value of loss function] 0.3238 [Recognition accuracy of training data] 0.9195 [Recognition accuracy of test data] 0.9218
[Number of updates] 4200 [Value of loss function] 0.2017 [Recognition accuracy of training data] 0.9231 [Recognition accuracy of test data] 0.9253
[Number of updates] 4800 [Loss function value] 0.1910 [Training data recognition accuracy] 0.9266 [Test data recognition accuracy] 0.9289
[Number of updates] 5400 [Loss function value] 0.1528 [Training data recognition accuracy] 0.9306 [Test data recognition accuracy] 0.9320
[Number of updates] 6000 [Value of loss function] 0.1827 [Recognition accuracy of training data] 0.9338 [Recognition accuracy of test data] 0.9347
[Number of updates] 6600 [Loss function value] 0.1208 [Training data recognition accuracy] 0.9362 [Test data recognition accuracy] 0.9375
[Number of updates] 7200 [Loss function value] 0.1665 [Training data recognition accuracy] 0.9391 [Test data recognition accuracy] 0.9377
[Number of updates] 7800 [Value of loss function] 0.1787 [Recognition accuracy of training data] 0.9409 [Recognition accuracy of test data] 0.9413
[Number of updates] 8400 [Loss function value] 0.1564 [Training data recognition accuracy] 0.9431 [Test data recognition accuracy] 0.9429
[Number of updates] 9000 [Loss function value] 0.2361 [Recognition accuracy of training data] 0.9449 [Recognition accuracy of test data] 0.9437
[Number of updates] 9600 [Loss function value] 0.2183 [Training data recognition accuracy] 0.9456 [Test data recognition accuracy] 0.9448
</code></pre><p><img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/139624/afb5f936-30a8-8cce-f0a0-ed93b42078a7.png" alt="1.png">
<img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/139624/d4c56645-6aea-c926-3593-6ced20e8f7cc.png" alt="2.png"></p>
<p>Looking at the results, the recognition accuracy was already around 94.5%, which exceeded the recognition accuracy of the learned parameters prepared in Chapter 3.</p>
<h1 id="46-summary">4.6 Summary</h1>
<p>It may be good to read Chapter 4 as a book, but it was quite difficult to proceed while implementing it.
(I wanted a commentary on the part that makes the softmax function and the function of numerical differentiation correspond to the multidimensional array&hellip;)</p>
<p>That&rsquo;s it for this chapter. If you have any mistakes, I would appreciate it if you could point me out.
(To other chapters of this memo: <a href="https://qiita.com/segavvy/items/1945aa1a0f91a1473555">Chapter 1</a>/<a href="https://qiita.com/segavvy/items/d8e9e70437e35083a459">Chapter2</a>/<a href="https://qiita.com/segavvy/items/6d79d0c3b4367869f4ea">Chapter3</a>/Chapter4/<a href="https://qiita.com/segavvy/items/8707e4e65aa7fa357d8a">Chapter5</a>/<a href="https://qiita.com/segavvy/items/ca4ac4c9ee1a126bff41">Chapter6</a>/<a href="https://qiita.com/segavvy/items/8541c6ae1868d9b2b805">Chapter7</a>/<a href="https://qiita.com/segavvy/items/3eb6ea0ea2ea68af68af68af2af6868">Chapter8</a>)/<a href="https://qiita.com/segavvy/items/4e8c36cac9c6f3543ffd">Summary</a>))</p>

    </article>
  </div>

  
  
  
  <script>
  window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
  ga('create', 'UA-169005401-1', 'auto');
  ga('send', 'pageview');
  </script>
  <script async src='https://www.google-analytics.com/analytics.js'></script>
  

  

  
<link rel="stylesheet" type="text/css" href="//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.0/cookieconsent.min.css" />
<script src="//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.0/cookieconsent.min.js"></script>
<script>
window.addEventListener("load", function(){
window.cookieconsent.initialise({
  "palette": {
    "popup": {
      "background": "#216942",
      "text": "#b2d192"
    },
    "button": {
      "background": "#afed71"
    }
  }
})});
</script>

</body>

</html>
