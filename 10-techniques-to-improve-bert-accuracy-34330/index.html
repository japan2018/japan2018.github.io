<!doctype html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  
  <meta name="generator" content="Hugo 0.72.0" />

  
  <meta name="description" content="Some description">
  

  
  <link rel="apple-touch-icon" sizes="180x180" href="https://memotut.com/apple-touch-icon.png">

  
  <link rel="icon" type="image/png" sizes="32x32" href="https://memotut.com/favicon-32x32.png">

  
  <link rel="icon" type="image/png" sizes="16x16" href="https://memotut.com/favicon-16x16.png">

  
  <link rel="manifest" href="https://memotut.com/site.webmanifest">

  
  <link rel="mask-icon" href="https://memotut.com/safari-pinned-tab.svg" color="#5bbad5">

  <meta name="msapplication-TileColor" content="#da532c">

  <meta name="theme-color" content="#ffffff">

  
  <link rel="stylesheet" href="https://memotut.com/css/bootstrap.min.css" />

  
  <title>[Python] 10 techniques to improve BERT accuracy | Memo Tut</title>
  

  <style>
body {
  min-width: 300px;
}

.custom-navbar {
  margin-bottom: 1em;
  height: 60px;
}

.custom-navbar a {
  display: inline-block; 
  padding: 18px 0;
  margin-right: 1em; 
  font-weight: bold; 
}

.custom-navbar a:hover,
.custom-navbar a:focus {
  text-decoration: none; 
}

@media print {
  .custom-navbar {
    display: none;
  }
}

article {
  padding-bottom: 1em;
}

img {
  max-width: 100%;
}


body {
  background-color: #fff;
}



body {
  color: #212529;
}



a {
  color: #007bff;
}



a:hover,
a:focus {
  color: #0056b3;
}



.custom-navbar {
  background-color: #212529;
}



.custom-navbar a {
  color: rgba(255, 255, 255, 0.75);
}



.custom-navbar a:hover,
.custom-navbar a:focus {
  color: rgba(255, 255, 255, 1);
}



.container {
  max-width: 800px;
}



pre {
  display: block;
  padding: 9.5px;
  word-break: break-all;
  word-wrap: break-word;
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 4px;
}

pre code {
  padding: 0;
  font-size: inherit;
  color: inherit; 
  white-space: pre-wrap;
  background-color: transparent;
  border: none;
  border-radius: 0;
}

code {
  padding: 2px 4px;
  color: inherit; 
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 4px;
  font-size: .9em;
}



blockquote,
.blockquote {
  padding: 10px 20px;
  margin: 0 0 20px;
  font-size: 1em;
  border-left: 5px solid #6c757d;
}

</style>
</head>

<body>
  <nav class="custom-navbar">
  <div class="container">
    
    <a href="/">Posts</a>
    
    <a href="/tags/">Tags</a>
    
    <a href="/about/">About</a>
    
    <a href="/index.xml">RSS</a>
    
  </div>
</nav>
  
  <div class="container">
    <article>
      <h1>[Python] 10 techniques to improve BERT accuracy</h1>
<p>
  <small class="text-secondary">
  
  
  May 15, 2020
  </small>
  

<small><code><a href="https://memotut.com/tags/python">Python</a></code></small>


<small><code><a href="https://memotut.com/tags/natural-language-processing"> Natural Language Processing</a></code></small>


<small><code><a href="https://memotut.com/tags/python3"> Python3</a></code></small>


<small><code><a href="https://memotut.com/tags/pytorch"> PyTorch</a></code></small>


<small><code><a href="https://memotut.com/tags/bert"> bert</a></code></small>

</p>
<pre><code>## Introduction
</code></pre>
<p>It has become commonplace to fine-tune and use BERT for natural language processing tasks. It is expected that there will be more and more scenes where you want to improve the accuracy even a little when doing competitions such as Kaggle or projects where accuracy requirements are tight. Therefore, we will summarize the accuracy improvement method. Classification task is assumed as a task.</p>
<h2 id="adjust-number-of-characters">Adjust number of characters</h2>
<p>You can enter up to 512 words in the learned BERT. Therefore, if you use 512 words or more of text, you need to make a special effort. It is necessary to check because changing the processing method here often contributes to improvement in accuracy.</p>
<p>As an example, consider getting 6 words from the following text (the punctuation is also 1 word)
<code>I / ha / cat / de / aru /. / there is no name yet / . </code></p>
<h3 id="1-head-tail">1. Head-Tail</h3>
<p><code>I / ha / cat</code> / de / aru /. / Name / is / <code>yet / not /. </code></p>
<p>From [How to Fine-tune BERT for Text Classification] <a href="http://arxiv.org/abs/1905.05583">how_to_bert</a>. Get words from both the beginning and the end. In the above figure, we have used 3 words from the beginning and 3 words from the end. It is easy to implement and high performance, and it is a method often used in Kaggle. How many words should be taken before and after is case by case.</p>
<h3 id="2-random">2. Random</h3>
<p>I / ha / <code>cat / de / aru /. / Name / is </code>/ not yet /.
I / ha / cat / de / <code>aru /. / Name / is / not yet / </code>/.</p>
<p>Get words continuously from any place. If you change the acquisition location for each Epoch, you can expect an effect like Augmentation. However, there is no impression that the accuracy is higher than that of the Head-Tail method. How about combining with TTA (Test Time Augmentation).</p>
<h3 id="3-sliding-window">3. Sliding Window</h3>
<p><code>I / ha / cat / de / aru /. </code>/name/ is / not yet /.
I / ha / cat / <code>de / aru /. / Name / is / not yet /. I / ha / cat / de / aru /. </code>/name/ is / not yet /. `</p>
<p>This method is often used in Google&rsquo;s Natural Question Dataset, such as <a href="https://arxiv.org/pdf/1901.08634.pdf">A BERT Baseline for the Natural Questions</a>. The above figure shows the case of shifting by 3 words. Its strength is that it can completely cover data. The disadvantage is that the learning data becomes too large if the data has a large number of words. It is used when it is important to use all words in QA tasks, etc., but it can also contribute to improving the accuracy of classification tasks.</p>
<h2 id="additional-meta-information">Additional meta information</h2>
<p>Suppose you want to enter a question and answer and title such as:</p>
<p>Title: About Trump
Q: Where did President Trump come from?
Answer: New York.</p>
<h3 id="4-add-a-separator">4. Add a separator</h3>
<p><code>[CLS] About President Trump [NEW_SEP] Where is President Trump from? [SEP] New York. [SEP]</code></p>
<p>From <a href="https://www.kaggle.com/c/google-quest-challenge/discussion/129895">Google QUEST Q&amp;A Labeling 19th solution</a>. If there are two sentences in BERT, it is okay to separate them with the <code>[SEP]</code> tag, but it does not support more sentences. Therefore, you can express a sentence break by defining a token with an appropriate name and using it as a separator like <code>[NEW_SEP]</code>. Such tokens can be added using <code>tokenizer.add_special_tokens</code>. Also, in the case of the English version of BERT, since there are unused tokens from <code>[unused0] to [unused993]</code>, they can also be used.</p>
<h3 id="5-add-category-information">5. Add category information</h3>
<p><code>[CLS][CATEGORY_0] Where is President Trump's hometown? [SEP] New York. [SEP]</code></p>
<p><a href="https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification/discussion/103280">Jigsaw Unintended Bias in Toxicity Classification 1ST PLACE SOLUTION</a>.Let&rsquo;ssayyousolvethetaskofdeterminingiftheabovesentenceispairedwiththeappropriatequestionandanswer.Sincethequestionandanswerlogisoftencategorized,itmaybenecessarytoaddittothefeatures.Inthatcase,theaccuracycanbeimprovedbydefiningthenewtokens<code>[CATEGORY_0]to[CATEGORY_n]</code>(nisthenumberofcategories) and incorporating them in the text as described above.</p>
<p>It is also effective to perform category classification as a subtask using the vector of [[CATEGORY_0]` as a feature amount.</p>
<h2 id="model-building">Model building</h2>
<p>A typical BERT model consists of 12 layers of submodules. When fine tuning the BERT, the default implementation is to use the vector of the first <code>[CLS]</code> of the output of the final layer as the feature quantity. As for accuracy, that is often enough, but you can expect a slight improvement in accuracy by using other features.</p>
<h3 id="6-use-4-layers-from-the-last-layer">6. Use 4 layers from the last layer</h3>
<p><img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/590061/964f47f1-e21e-97d5-dc92-ce043835005a.png" alt="last_4_layers.png">
From [How to Fine-tune BERT for Text Classification] <a href="http://arxiv.org/abs/1905.05583">how_to_bert</a>. We aim to improve the precision of the finetuning task by combining the four <code>[CLS]</code> vectors from the bottom of the 12 layers. The vector is finally 768-dimensional (first-order tensor) using average pooling, max pooling, concat, etc.</p>
<h3 id="7-learnable-weighted-sum">7. Learnable Weighted Sum</h3>
<p><img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/590061/e9f448ac-930b-bbd1-75a7-2a2a2d169064.png" alt="weighted_sum.png">
From <a href="https://www.kaggle.com/c/google-quest-challenge/discussion/129840">Google QUEST Q&amp;A Labeling 1st place solution</a>. Compute the weighted sum of all the BERT Layer <code>[CLS]</code> vectors, with the learnable weights set in the model. Simply averaging all layers is a powerful technique, but it can be said to be a further development of it.</p>
<h3 id="8-add-cnn-layer">8. Add CNN layer</h3>
<p><img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/590061/b11b6fdd-40ab-d9b1-9038-f4230b02603f.png" alt="bert_with_cnn.png">
<a href="http://web.stanford.edu/class/cs224n/reports/custom/15739845.pdf">Identifying Russian Trolls on Reddit with Deep Learning and BERT Word Embeddings</a>. Entering not only the vector of <code>[CLS]</code> but also the vector of all words into CNN is one of the effective methods. Compute a one-dimensional convolution for up to 512 sequence lengths as shown. If you perform max pooling or average pooling after calculating the convolution, you can extract features with the number of dimensions of filter, so enter them in Dense. Compared to Attention, CNN can collect the features of surrounding words, so combining them can improve accuracy. It is also effective to combine it with LSTM as well as CNN.</p>
<h2 id="learning">learning</h2>
<h3 id="9-fix-the-bert-weight">9. Fix the BERT weight</h3>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">model_params <span style="color:#f92672">=</span> list(model<span style="color:#f92672">.</span>named_parameters())

Fix <span style="color:#75715e"># BERT weight</span>
params <span style="color:#f92672">=</span> [p <span style="color:#66d9ef">for</span> n, p <span style="color:#f92672">in</span> model_params <span style="color:#66d9ef">if</span> <span style="color:#f92672">not</span> <span style="color:#e6db74">&#34;bert&#34;</span> <span style="color:#f92672">in</span> n]
optimizer <span style="color:#f92672">=</span> AdamW(params, lr<span style="color:#f92672">=</span><span style="color:#ae81ff">2e-5</span>)

Release the <span style="color:#75715e"># BERT weight lock</span>
params <span style="color:#f92672">=</span> [p <span style="color:#66d9ef">for</span> n, p <span style="color:#f92672">in</span> model_params <span style="color:#66d9ef">if</span> <span style="color:#e6db74">&#34;bert&#34;</span> <span style="color:#f92672">in</span> n]
optimizer<span style="color:#f92672">.</span>add_param_group({<span style="color:#e6db74">&#39;params&#39;</span>: params })
</code></pre></div><p>From <a href="https://www.kaggle.com/c/google-quest-challenge/discussion/129895">Google QUEST Q&amp;A Labeling 19th solution</a>. Similar to image trained models, BERT may improve accuracy by fixing weights and learning only task-dependent layers. With 19th solution, only the first 1 epoch is fixed, and all layers are learned later. In the above, the code that fixes the BERT weight and starts learning, and the code that can be used when releasing the fixing from the middle and restarting learning are described.</p>
<h3 id="10-change-the-learning-rate-for-bert-and-other-layers">10. Change the learning rate for BERT and other layers</h3>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">model_params <span style="color:#f92672">=</span> list(model<span style="color:#f92672">.</span>named_parameters())

bert_params <span style="color:#f92672">=</span> [p <span style="color:#66d9ef">for</span> n, p <span style="color:#f92672">in</span> model_params <span style="color:#66d9ef">if</span> <span style="color:#e6db74">&#34;bert&#34;</span> <span style="color:#f92672">in</span> n]
other_params <span style="color:#f92672">=</span> [p <span style="color:#66d9ef">for</span> n, p <span style="color:#f92672">in</span> model_params <span style="color:#66d9ef">if</span> <span style="color:#f92672">not</span> <span style="color:#e6db74">&#34;bert&#34;</span> <span style="color:#f92672">in</span> n]

params <span style="color:#f92672">=</span> [
    {<span style="color:#e6db74">&#39;params&#39;</span>: bert_params,<span style="color:#e6db74">&#39;lr&#39;</span>: params<span style="color:#f92672">.</span>lr},
    {<span style="color:#e6db74">&#39;params&#39;</span>: other_params,<span style="color:#e6db74">&#39;lr&#39;</span>: params<span style="color:#f92672">.</span>lr <span style="color:#f92672">*</span> <span style="color:#ae81ff">500</span>}
]
</code></pre></div><p>From <a href="https://www.kaggle.com/c/google-quest-challenge/discussion/129840">Google QUEST Q&amp;A Labeling 1st place solution</a>. Adopting different learning rates is as effective as the trained model of the image system. In the 1st place solution, the task-specific layer is learned at a learning rate 500 times higher than usual. The code at that time is shown above.</p>
<h2 id="in-conclusionwe-introduced-a-method-that-may-improve-accuracy-in-the-bert-classification-task-however-i-could-not-specifically-show-how-much-the-accuracy-would-improve-so-i-would-like-to-compare-with-an-appropriate-data-set-in-addition-to-the-ones-listed-above-methods-for-improving-accuracy-are-still-emerging-so-i-will-continue-to-investigate">in conclusionWe introduced a method that may improve accuracy in the BERT classification task. However, I could not specifically show how much the accuracy would improve, so I would like to compare with an appropriate data set. In addition to the ones listed above, methods for improving accuracy are still emerging, so I will continue to investigate.</h2>

    </article>
  </div>

  
  
  
  <script>
  window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
  ga('create', 'UA-169005401-1', 'auto');
  ga('send', 'pageview');
  </script>
  <script async src='https://www.google-analytics.com/analytics.js'></script>
  

  

  
<link rel="stylesheet" type="text/css" href="//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.0/cookieconsent.min.css" />
<script src="//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.0/cookieconsent.min.js"></script>
<script>
window.addEventListener("load", function(){
window.cookieconsent.initialise({
  "palette": {
    "popup": {
      "background": "#216942",
      "text": "#b2d192"
    },
    "button": {
      "background": "#afed71"
    }
  }
})});
</script>

</body>

</html>
