<!doctype html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  
  <meta name="generator" content="Hugo 0.72.0" />

  
  <meta name="description" content="Some description">
  

  
  <link rel="apple-touch-icon" sizes="180x180" href="https://memotut.com/apple-touch-icon.png">

  
  <link rel="icon" type="image/png" sizes="32x32" href="https://memotut.com/favicon-32x32.png">

  
  <link rel="icon" type="image/png" sizes="16x16" href="https://memotut.com/favicon-16x16.png">

  
  <link rel="manifest" href="https://memotut.com/site.webmanifest">

  
  <link rel="mask-icon" href="https://memotut.com/safari-pinned-tab.svg" color="#5bbad5">

  <meta name="msapplication-TileColor" content="#da532c">

  <meta name="theme-color" content="#ffffff">

  
  <link rel="stylesheet" href="https://memotut.com/css/bootstrap.min.css" />

  
  <title>I made a learning kit of word2vec/doc2vec/GloVe/fastText | Memo Tut</title>
  

  <style>
body {
  min-width: 300px;
}

.custom-navbar {
  margin-bottom: 1em;
  height: 60px;
}

.custom-navbar a {
  display: inline-block; 
  padding: 18px 0;
  margin-right: 1em; 
  font-weight: bold; 
}

.custom-navbar a:hover,
.custom-navbar a:focus {
  text-decoration: none; 
}

@media print {
  .custom-navbar {
    display: none;
  }
}

article {
  padding-bottom: 1em;
}

img {
  max-width: 100%;
}


body {
  background-color: #fff;
}



body {
  color: #212529;
}



a {
  color: #007bff;
}



a:hover,
a:focus {
  color: #0056b3;
}



.custom-navbar {
  background-color: #212529;
}



.custom-navbar a {
  color: rgba(255, 255, 255, 0.75);
}



.custom-navbar a:hover,
.custom-navbar a:focus {
  color: rgba(255, 255, 255, 1);
}



.container {
  max-width: 800px;
}



pre {
  display: block;
  padding: 9.5px;
  word-break: break-all;
  word-wrap: break-word;
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 4px;
}

pre code {
  padding: 0;
  font-size: inherit;
  color: inherit; 
  white-space: pre-wrap;
  background-color: transparent;
  border: none;
  border-radius: 0;
}

code {
  padding: 2px 4px;
  color: inherit; 
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 4px;
  font-size: .9em;
}



blockquote,
.blockquote {
  padding: 10px 20px;
  margin: 0 0 20px;
  font-size: 1em;
  border-left: 5px solid #6c757d;
}

</style>
</head>

<body>
  <nav class="custom-navbar">
  <div class="container">
    
    <a href="/">Posts</a>
    
    <a href="/tags/">Tags</a>
    
    <a href="/about/">About</a>
    
    <a href="/index.xml">RSS</a>
    
  </div>
</nav>
  
  <div class="container">
    <article>
      <h1>I made a learning kit of word2vec/doc2vec/GloVe/fastText</h1>
<p>
  <small class="text-secondary">
  
  
  Dec 21, 2019
  </small>
  

<small><code><a href="https://memotut.com/tags/python">Python</a></code></small>


<small><code><a href="https://memotut.com/tags/natural-language-processing"> Natural Language Processing</a></code></small>


<small><code><a href="https://memotut.com/tags/word2vec"> word2vec</a></code></small>


<small><code><a href="https://memotut.com/tags/doc2vec">doc2vec</a></code></small>


<small><code><a href="https://memotut.com/tags/glove">GloVe</a></code></small>

</p>
<pre><code>This article is the 21st day of [Natural Language Processing #2 Advent Calendar 2019](https://qiita.com/advent-calendar/2019/nlp2).
</code></pre>
<p>By the way, it&rsquo;s my birthday today. Please celebrate
~~ M&rsquo;s book that sets a deadline for birthday ~ ~</p>
<p>#Introduction</p>
<p>In the word embedding world, BERT has been terrifying over the past year, and even ELMo is becoming less visible.
Sometimes I still want to use legacy distributed representations like word2vec or GloVe.
In addition, you may want to learn with your data (at least for me)
So, I made a learning kit for word2vec/doc2vec/GloVe/fastText for myself, so I will publish it.</p>
<ul>
<li><a href="https://github.com/stfate/word2vec-trainer">word2vec-trainer</a></li>
<li><a href="https://github.com/stfate/doc2vec-trainer">doc2vec-trainer</a></li>
<li><a href="https://github.com/stfate/fasttext-trainer">fastText-trainer</a></li>
<li><a href="https://github.com/stfate/GloVe-trainer">GloVe-trainer</a></li>
</ul>
<p>Word2vec/doc2vec/fastText can learn models from <a href="https://radimrehurek.com/gensim/">gensim</a>andGloVe<a href="https://nlp.stanford.edu/projects/glove/">officialimplementation</a>. ï¼Ž</p>
<p>Since I wrote the usage in the README of each package, I will write about the design concept of the learning kit here.</p>
<h1 id="1-common-api-for-model-learning-functions">1. Common API for model learning functions</h1>
<p>There are various libraries/packages for word distribution expression,
The assumed format of the dataset is different for each library,
When I write a preprocessing script that formats it into a form suitable for the library,
The code gets more and more dirty.
So, common iterator for reading text data set,
I am trying to format the data format suitable for each library in the function.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">train_</span><span style="color:#f92672">*****</span>_model(
    output_model_path,
    iter_docs,
    <span style="color:#f92672">**</span>kwargs
)
</code></pre></div><p>For word2vec:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">train_word2vec_model</span>(
    output_model_path,
    iter_docs,
    size<span style="color:#f92672">=</span><span style="color:#ae81ff">300</span>,
    window<span style="color:#f92672">=</span><span style="color:#ae81ff">8</span>,
    min_count<span style="color:#f92672">=</span><span style="color:#ae81ff">5</span>,
    sg<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>,
    epoch<span style="color:#f92672">=</span><span style="color:#ae81ff">5</span>
):
    <span style="color:#e6db74">&#34;&#34;&#34;
</span><span style="color:#e6db74">    Parameters
</span><span style="color:#e6db74">    ----------
</span><span style="color:#e6db74">    output_model_path: string
</span><span style="color:#e6db74">        path of Word2Vec model
</span><span style="color:#e6db74">    iter_docs: iterator
</span><span style="color:#e6db74">        iterator of documents, which are raw texts
</span><span style="color:#e6db74">    size: int
</span><span style="color:#e6db74">        size of word vector
</span><span style="color:#e6db74">    window: int
</span><span style="color:#e6db74">        window size of word2vec
</span><span style="color:#e6db74">    min_count: int
</span><span style="color:#e6db74">        minimum word count
</span><span style="color:#e6db74">    sg: int
</span><span style="color:#e6db74">        word2vec training algorithm (1: skip-gram other:CBOW)
</span><span style="color:#e6db74">    epoch :int
</span><span style="color:#e6db74">        number of epochs
</span><span style="color:#e6db74">    &#34;&#34;&#34;</span>
</code></pre></div><p>iter_docs is an iterator of word lists for each document.</p>
<h1 id="2-allow-learning-from-any-text-dataset">2. Allow learning from any text dataset</h1>
<p>Prepare the abstract class <code>TextDatasetBase</code> that defines the API for reading the dataset.
By implementing a read class of the data set that the user wants to use by inheriting this class, any data set can be handled.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">TextDatasetBase</span>(ABC):
    <span style="color:#e6db74">&#34;&#34;&#34;
</span><span style="color:#e6db74">    a bass class for text dataset
</span><span style="color:#e6db74">    
</span><span style="color:#e6db74">    Attributes
</span><span style="color:#e6db74">    ----------
</span><span style="color:#e6db74">    &#34;&#34;&#34;</span>
    <span style="color:#a6e22e">@abstractmethod</span>
    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">iter_docs</span>(self):
        <span style="color:#e6db74">&#34;&#34;&#34;
</span><span style="color:#e6db74">        iterator of documents
</span><span style="color:#e6db74">        
</span><span style="color:#e6db74">        Parameters
</span><span style="color:#e6db74">        ----------
</span><span style="color:#e6db74">        &#34;&#34;&#34;</span>
        <span style="color:#66d9ef">yield</span> None
</code></pre></div><p>Example of dataset class for <a href="https://www.upf.edu/web/mtg/mard">MARD</a></p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">MARDDataset</span>(TextDatasetBase):
    <span style="color:#66d9ef">def</span> __init__(self, word_tokenizer):
        self<span style="color:#f92672">.</span>root_path <span style="color:#f92672">=</span> None
        self<span style="color:#f92672">.</span>word_tokenizer <span style="color:#f92672">=</span> word_tokenizer

    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">iter_docs</span>(self, dataset_path):
        <span style="color:#e6db74">&#34;&#34;&#34;
</span><span style="color:#e6db74">        get iterator of texts in one document
</span><span style="color:#e6db74">        
</span><span style="color:#e6db74">        Parameters
</span><span style="color:#e6db74">        ----------
</span><span style="color:#e6db74">        dataset_path: string
</span><span style="color:#e6db74">            path to dataset
</span><span style="color:#e6db74">        &#34;&#34;&#34;</span>
        self<span style="color:#f92672">.</span>root_path <span style="color:#f92672">=</span> Path(dataset_path)
        reviews_json_fn <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>root_path <span style="color:#f92672">/</span> <span style="color:#e6db74">&#34;mard_reviews.json&#34;</span>
        <span style="color:#66d9ef">with</span> open(reviews_json_fn, <span style="color:#e6db74">&#34;r&#34;</span>) <span style="color:#66d9ef">as</span> fi:
            <span style="color:#66d9ef">for</span> line <span style="color:#f92672">in</span> fi:
                review_dict <span style="color:#f92672">=</span> json<span style="color:#f92672">.</span>loads(line, encoding<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;utf-8&#34;</span>)
                title <span style="color:#f92672">=</span> review_dict[<span style="color:#e6db74">&#34;reviewerID&#34;</span>]
                text <span style="color:#f92672">=</span> review_dict[<span style="color:#e6db74">&#34;reviewText&#34;</span>]
                <span style="color:#66d9ef">yield</span> self<span style="color:#f92672">.</span>word_tokenizer<span style="color:#f92672">.</span>tokenize(text)
</code></pre></div><p>I think pytorch&rsquo;s <code>DataLoader</code> is about 200 million times more sophisticated than this, but I came up with something like this.
Please let me know if you have a better design.</p>
<h1 id="how-to-use">How to use</h1>
<h2 id="install">Install</h2>
<p>taking word2vec as an example</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">git clone git@github.com:stfate/word2vec-trainer.git
cd word2vec-trainer

git submodule init
git submodule update
pip install -r requirements.txt
</code></pre></div><h2 id="perform-learning">Perform learning</h2>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">python train_text_dataset.py -o $OUTPUT_PATH --dictionary-path<span style="color:#f92672">=</span>$DIC_PATH --corpus-path<span style="color:#f92672">=</span>$CORPUS_PATH --size<span style="color:#f92672">=</span><span style="color:#ae81ff">100</span> --window<span style="color:#f92672">=</span><span style="color:#ae81ff">8</span> --min-count<span style="color:#f92672">=</span><span style="color:#ae81ff">5</span>
</code></pre></div><h2 id="how-to-use-the-model">How to use the model</h2>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">model_path <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;model/word2vec.gensim.model&#34;</span>
model <span style="color:#f92672">=</span> Word2Vec<span style="color:#f92672">.</span>load(model_path)
</code></pre></div><h1 id="important-point">important point</h1>
<p>When you train on a large dataset like Wikipedia, you may run out of memory and fall. investigation in progress.</p>
<h1 id="in-conclusion">in conclusion</h1>
<p>It&rsquo;s fun to think about the library API</p>

    </article>
  </div>

  
  
  
  <script>
  window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
  ga('create', 'UA-169005401-1', 'auto');
  ga('send', 'pageview');
  </script>
  <script async src='https://www.google-analytics.com/analytics.js'></script>
  

  

  
<link rel="stylesheet" type="text/css" href="//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.0/cookieconsent.min.css" />
<script src="//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.0/cookieconsent.min.js"></script>
<script>
window.addEventListener("load", function(){
window.cookieconsent.initialise({
  "palette": {
    "popup": {
      "background": "#216942",
      "text": "#b2d192"
    },
    "button": {
      "background": "#afed71"
    }
  }
})});
</script>

</body>

</html>
