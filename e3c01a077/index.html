<!doctype html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  
  <meta name="generator" content="Hugo 0.72.0" />

  
  <meta name="description" content="Some description">
  

  
  <link rel="apple-touch-icon" sizes="180x180" href="https://memotut.com/apple-touch-icon.png">

  
  <link rel="icon" type="image/png" sizes="32x32" href="https://memotut.com/favicon-32x32.png">

  
  <link rel="icon" type="image/png" sizes="16x16" href="https://memotut.com/favicon-16x16.png">

  
  <link rel="manifest" href="https://memotut.com/site.webmanifest">

  
  <link rel="mask-icon" href="https://memotut.com/safari-pinned-tab.svg" color="#5bbad5">

  <meta name="msapplication-TileColor" content="#da532c">

  <meta name="theme-color" content="#ffffff">

  
  <link rel="stylesheet" href="https://memotut.com/css/bootstrap.min.css" />

  
  <title>[Python] The relationship between brain science and unsupervised learning. Information maximization with unsupervised learning MNIST: Google Colabratory (PyTorch) | Memo Tut</title>
  

  <style>
body {
  min-width: 300px;
}

.custom-navbar {
  margin-bottom: 1em;
  height: 60px;
}

.custom-navbar a {
  display: inline-block; 
  padding: 18px 0;
  margin-right: 1em; 
  font-weight: bold; 
}

.custom-navbar a:hover,
.custom-navbar a:focus {
  text-decoration: none; 
}

@media print {
  .custom-navbar {
    display: none;
  }
}

article {
  padding-bottom: 1em;
}

img {
  max-width: 100%;
}


body {
  background-color: #fff;
}



body {
  color: #212529;
}



a {
  color: #007bff;
}



a:hover,
a:focus {
  color: #0056b3;
}



.custom-navbar {
  background-color: #212529;
}



.custom-navbar a {
  color: rgba(255, 255, 255, 0.75);
}



.custom-navbar a:hover,
.custom-navbar a:focus {
  color: rgba(255, 255, 255, 1);
}



.container {
  max-width: 800px;
}



pre {
  display: block;
  padding: 9.5px;
  word-break: break-all;
  word-wrap: break-word;
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 4px;
}

pre code {
  padding: 0;
  font-size: inherit;
  color: inherit; 
  white-space: pre-wrap;
  background-color: transparent;
  border: none;
  border-radius: 0;
}

code {
  padding: 2px 4px;
  color: inherit; 
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 4px;
  font-size: .9em;
}



blockquote,
.blockquote {
  padding: 10px 20px;
  margin: 0 0 20px;
  font-size: 1em;
  border-left: 5px solid #6c757d;
}

</style>
</head>

<body>
  <nav class="custom-navbar">
  <div class="container">
    
    <a href="/">Posts</a>
    
    <a href="/tags/">Tags</a>
    
    <a href="/about/">About</a>
    
    <a href="/index.xml">RSS</a>
    
  </div>
</nav>
  
  <div class="container">
    <article>
      <h1>[Python] The relationship between brain science and unsupervised learning. Information maximization with unsupervised learning MNIST: Google Colabratory (PyTorch)</h1>
<p>
  <small class="text-secondary">
  
  
  May 17, 2020
  </small>
  

<small><code><a href="https://memotut.com/tags/python">Python</a></code></small>


<small><code><a href="https://memotut.com/tags/machine-learning"> machine learning</a></code></small>


<small><code><a href="https://memotut.com/tags/deeplearning"> DeepLearning</a></code></small>


<small><code><a href="https://memotut.com/tags/ai"> AI</a></code></small>


<small><code><a href="https://memotut.com/tags/pytorch"> PyTorch</a></code></small>

</p>
<pre><code>In this article, I will explain unsupervised learning and brain science, and IIC's implementation of high performance in MNIST for unsupervised learning.
</code></pre>
<p>The papers of interest in this article are:
<a href="https://arxiv.org/abs/1807.06653">Invariant Information Clustering for Unsupervised Image Classification and Segmentation</a>
is.</p>
<p>In this paper, we utilize an index called <strong>mutual information</strong> to classify handwritten digit images by unsupervised learning clustering.</p>
<p>It is called IIC (Invariant Information Clustering).</p>
<p>In this article, we will discuss brain science and unsupervised learning, IIC points, and implementation examples in MNIST.</p>
<p>The table of contents is as follows.</p>
<ol>
<li>Brain science and unsupervised learning</li>
<li>Focus on unsupervised learning in the artificial intelligence community</li>
<li>Points of Proposed Method IIC</li>
<li>Implementation of IIC with MNIST (Google Colabratory and PyTorch)</li>
</ol>
<p>The implementation code &ldquo;MNIST_IIC.ipynb&rdquo; is located here.
<a href="https://github.com/YutaroOgawa/Qiita">Implementation code repository</a></p>
<h2 id="1-brain-science-and-unsupervised-learning">1. Brain science and unsupervised learning</h2>
<p>The recent boom in AI and artificial intelligence is driven by deep learning technology.</p>
<p>However, most deep learning is supervised learning or reinforcement learning (such as Alpha Go).</p>
<p>It&rsquo;s not very common to use deep learning for unsupervised learning
(I see dimensional compression represented by AutoEncoder, but there is little clustering.)</p>
<p>Looking at the field of brain science,
Book by Professor Dotani of OIST (Okinawa Institute of Science and Technology Graduate University)
<a href="https://www.amazon.co.jp/dp/B006YKU67M/">&ldquo;Invitation to Computational Neuroscience&rdquo; Aiming to understand the learning mechanism of the brain&rdquo;</a>
Have suggested a relationship between the three learning types of artificial intelligence and the brain, as shown in the figure below.</p>
<p><img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/191401/ab7afcf2-1f6c-c2e0-2016-1e548bddd22a.jpeg" alt="l_bit201806011320437219.jpg"></p>
<p>Image citation: <a href="https://www.sbbit.jp/article/cont1/34991">Why circuit modules in the brain can be successfully connected&ndash;Professor Kenji Doya, Okinawa Institute of Technology</a></p>
<p>What sets humans apart from other organisms by intelligence is the development of the cerebral cortex (especially the frontal lobe),
**In the cerebral cortex, the importance of unsupervised learning is suggested. **</p>
<p>Winner of the Nobel Prize in Physiology or Medicine in 1981, the study of Hubel and Wiesel is well known among brain science researchers.</p>
<p>They pierce the neurons in the cat&rsquo;s brain with electrodes,
By measuring the activity of nerve cells when showing various symbols and moving sticks,
In the visual cortex of the cerebral cortex, each nerve cell has a symbol presentation position and the direction in which the rod extends (longitudinal direction, lateral direction, diagonal direction), etc.
<strong>Clarified that nerve cells are selectively activated according to the presented object</strong>.</p>
<p>In the figure below, this nerve cell is a neuron that responds strongly to the vertical bar.</p>
<p><img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/191401/96b305dd-d4a7-7d5b-4615-9f2397b1f34a.png" alt="1024px-Orientation_V1.svg.png">
<a href="https://en.wikipedia.org/wiki/Orientation_selectivity">Orientation selectivity</a></p>
<p>In other words, there are nerve cells corresponding to horizontal bars, nerve cells corresponding to vertical bars, etc., and the processing of these neurons is processed in a complex manner to recognize an object.</p>
<p>Then, in the 21st century,
**A brain part that ignites in response to a face image and a brain part that ignites in response to a specific celebrity have been discovered. **
<a href="http://www.nikkei-science.com/?p=54220">Face recognition mechanism-From Nikkei Science October 2017 issue</a></p>
<p>Because there was such a premise (the existence of neurons in the cerebral cortex that responds specifically to objects),
In the early days of deep learning, Google
** Announcement that neurons corresponding to cat&rsquo;s face were born in the middle layer of deep learning **
Was a big news.
<a href="https://googleblog.blogspot.com/2012/06/using-large-scale-brain-simulations-for.html">Using large-scale brain simulations for machine learning and A.I.</a></p>
<p>This is because deep learning has acquired the same characteristics as the brain of a living organism.</p>
<p>And there is another well-known experiment on the brain and especially unsupervised learning.</p>
<p>Blackmore and Cooper experiment. They are,
<strong>If you raise the kitten temporarily in an environment where you can only see vertical lines, the horizontal lines cannot be recognized for the time being</strong>
I made it clear.</p>
<p>The environment is like the picture below.</p>
<p><img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/191401/7917708f-4a33-17f7-93ca-bc2a99844113.gif" alt="catexperiment.gif"></p>
<p><a href="https://computervisionblog.wordpress.com/2013/06/01/cats-and-vision-is-vision-acquired-or-innate/">Cats and Vision: is vision acquired or innate?</a></p>
<p>In other words, we clarify the existence of neurons in which Hubel and Weissel selectively respond to vertical and horizontal bars,
Blackmore and Cooper have revealed that these neurons are born in the cerebral cortex during the kitten&rsquo;s development, when these landscapes are input into the eyes.</p>
<p>Brain science shows that unsupervised learning may play an important role in object recognition because it is the development of the visual cortex in kittens**.</p>
<h2 id="2-focus-on-unsupervised-learning-in-the-artificial-intelligence-community">2. Focus on unsupervised learning in the artificial intelligence community</h2>
<p>Even in the area of AI researchers, there are remarks that emphasize the importance of unsupervised learning, not the deep learning of supervised learning that has been done so far.</p>
<p>Hinton is also featured in the blog of PFN Okanohara in 2017,</p>
<blockquote>
<p>Professor Geoffrey Hinton [link] &ldquo;There are 10^14 brain synapses, but humans can only live for 10^9 seconds. This means that the number of parameters is much larger than the number of samples. We need 10^5 constraints per second (to determine weights), which leads us to the idea that we are doing a lot of unsupervised learning.&rdquo;</p>
</blockquote>
<p><a href="https://tech.preferred.jp/ja/blog/deeplearning-5years-later/">Five years after the counterattack of the neural network</a></p>
<p>He said.</p>
<p>We have actually conducted research and announced SimCLR (A Simple Framework for Contrastive Learning of Visual Representations) in February 2020.</p>
<p><a href="https://syncedreview.com/2020/02/19/geoffrey-hinton-google-brain-unsupervised-learning-algorithm-improves-sota-accuracy-on-imagenet-by-7/">Geoffrey Hinton &amp; Google Brain Unsupervised Learning Algorithm Improves SOTA Accuracy on ImageNet by 7%</a></p>
<p><img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/191401/1cc3a0f9-de1c-62bb-adf9-bbdcaabcfb9e.gif" alt="illustration-of-the-proposed-SimCLR-framework.gif"></p>
<p><a href="https://webbigdata.jp/ai/post-5469">Image citation and explanation of SimCLR: SimCLR: Improving the performance of self-teaching learning by contrast learning</a></p>
<p>In SimCLR, ** input different transformed pairs to the same image and learn that they are the same, while learning that different images are different from them by unsupervised learning, Get the extraction method first. **</p>
<p>In addition, Yang Lucan was AAAA in February 2020,</p>
<blockquote>
<p>Mr. Lekan said that the next innovation of deep learning is not supervised learning, but “unsupervised learning” that extracts features from data without correct answer tags and “self-supervised learning (Self-supervised learning) Supervised Learning)”. &ldquo;These are the same tasks that newborn babies are performing on the world,&rdquo; he explains. Babies can learn by themselves without giving the &ldquo;correct answer.&rdquo;</p>
</blockquote>
<p>He said.
<a href="https://xtech.nikkei.com/atcl/nxt/column/18/01218/022000001/">What is lacking in the current AI pointed out by three deep learning &ldquo;godfathers&rdquo;</a></p>
<p>If you continue talking about brain science and AI, you will not be able to reach the original implementation, so go here.</p>
<p>Hinton also published a paper ``Backpropagation and the brain&rsquo;&rsquo; in Nature Reviews Neuroscience in April 2020.
<a href="https://www.nature.com/articles/s41583-020-0277-3?proof=trueIn%EF%BB%BF">Backpropagation and the brain</a></p>
<p>NGRAD Law (<a href="https://syncedreview.com/2020/04/23/new-hinton-nature-paper-revisits-backpropagation-offers-insights-for-understanding-learning-in-the-cortex/">Neural Gradient Representation by Activity Differences</a>)</p>
<p><img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/191401/f31ed5cb-0d43-024c-4616-27903239f0d9.png" alt="image-78.png"></p>
<p>Is introduced.</p>
<p>Other,
●<a href="https://qiita.com/sugulu/items/046309a0e39664c3da31">Conversation about DL and general artificial intelligence between brain scientist and IT engineer</a>●<a href="https://qiita.com/sugulu/items/824b6951088ad79222da">BusinessapplicationofdeepreinforcementlearningandhowtomakeAIunderstandnaturallanguage</a></p>
<p>Is also a recommended past article on brain science and AI.</p>
<h2 id="3-points-of-proposed-method-iic">3. Points of Proposed Method IIC</h2>
<p>Then, this paper,
<a href="https://arxiv.org/abs/1807.06653">Invariant Information Clustering for Unsupervised Image Classification and Segmentation</a></p>
<p>I will explain the points of IIC (Invariant Information Clustering).</p>
<p>It should be noted that the IIC paper itself does not have a description like &ldquo;how the brain works&hellip;&rdquo;, but is a paper on pure calculation methods.</p>
<p>There are two points in IIC.</p>
<p><strong>The first point is input</strong>.
Input
・Data (this time, handwritten number image)
.Data converted appropriately
I&rsquo;ll use two.
Input each into the network and get each output.</p>
<p>IIC&rsquo;s deep learning network is similar to supervised learning.
The number of neurons in the output layer is 10 (corresponding to the numbers 0 to 9).
Multiply the output by the softmax function and output the probability value that the input image is one of 0 to 9.</p>
<p>The second of the points is the <strong>loss function</strong>.
Since it is unsupervised learning, we do not use teacher labels.
Instead,</p>
<ul>
<li>Output vector (10 elements) that appears when a handwritten numeral image is input to the neural network
When,
・An output vector (10 elements) that comes out when an image obtained by randomly converting a handwritten number image is input.
Calculate the <strong>mutual information</strong> of and train it to maximize it.</li>
</ul>
<p>Therefore, to understand IIC you need to be friends with mutual information.</p>
<p>Below is a slide explaining the mutual information.</p>
<p><img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/191401/8a7b53e8-59a0-f10d-85b3-16f51fd503a5.png" alt="Figure 1.png"></p>
<p>There are some difficult expressions in the line, but in the end, the information shared by two data is called mutual information**.</p>
<p>The key is the <strong>simultaneous distribution and marginal probability distribution</strong> in the log.</p>
<p>In the picture above, Example 1 throws two independent coins, and Example 2 throws two non-independent coins.
(That is, in Example 2, the second coin has the same result as the first coin.)</p>
<p>The picture above is an example of a coin, and it will output either the back or the front.</p>
<p>In the case of MNIST neural network, it outputs 10 from 0 to 9.</p>
<p>Think of the two output versions of MNIST&rsquo;s neural network as coin flips above.</p>
<p>If the two in Example 1 are independent, the probability of (back, back) is 0.25.
And the probability that the first sheet will be the back side after peripheralization is 0.5 (back side, back side) + (back side, front side).
Similarly, the probability that the second sheet will be the back is (front, back) + (back, back) is 0.5.
If you put these into the log calculation, the log result will be 0.</p>
<p>If you calculate not only (back, back) but also (back, front), (front, back), (front, front), all will be 0, and the sum will be 0.</p>
<p>Therefore, when two independent coins are thrown, the mutual information amount between the first result and the second result is 0.</p>
<p>Since the coin tossing trial is independent and the result of the first and the result of the second are unrelated, there is no information to carry with each other, and I think that it is convincing that it becomes 0.</p>
<p>Consider the mutual information when the second coin, which is not independent in Example 2, has the same result as the first coin.</p>
<p>The probability of (back, back) is 0.5.
And the probability that the first sheet will be the back after peripheralization is (back, back) + (back, front) is 0.5.
Similarly, the second card is 0.5.
Then, the calculation in log is (1/0.5) and log2 = 0.69.
The probability (back, back) of 0.5 is multiplied here, and it becomes about 0.35.
Since (back, front) is 0, (front, front) is about 0.35, and (front, back) is 0, the total is 0.69.</p>
<p>Therefore, in the case of coin tosses that are not independent, the mutual information will be a value greater than 0.</p>
<p>In other words, the result of flipping the first coin and the result of flipping the second coin have information.
This time, the result of the second piece is the same as that of the first piece, so I think it&rsquo;s convincing.</p>
<p>At IIC, think of this as a 10-version version of a MNIST image output 0-9 coin flip.</p>
<p>And the second coin is the MNIST image with the appropriate conversion.</p>
<p>As a suitable conversion, the image is rotated and stretched by the affine transformation, and the noise is dissipated.</p>
<p><strong>I want to train the network so that the output result of the probability that it is one of the numbers 0 to 9 is the same for both the image that has undergone this slight conversion and the original image</strong></p>
<p>That is the feeling of IIC.</p>
<p>An image that has undergone some transformation will eventually mimic an image of the same class.</p>
<p>There are two points here.</p>
<p>First, the output of the network does not correspond to the numbers 0 to 9 in order, nor does it correspond to the numbers.
** Simply separate similar images into the same class. **</p>
<p>The second point is, &ldquo;Isn&rsquo;t it all in the same class?&rdquo;, but remember the mutual information about coin flips.
The second coin in example 2 has the same result as the first coin.</p>
<p>The amount of mutual information is larger for coins with the same front and back than for coins with all the back.</p>
<p>The probability that all coins will be flipped (back, back) is 1.0.
And the probability that the first piece will be the back after peripheralization is 1.0 (back, back) + (back, front).
Similarly, the second card is 1.0.
Then, the calculation in log is (1/1) and log1 = 0.0.</p>
<p>**In order to maximize the mutual information, the mutual information is maximized when the probabilities are evenly distributed if there are two types, and even if there are 10 types. **</p>
<p>Therefore, if you have calculated the mini-batch, naturally there will be 10 classes that are disjointed.</p>
<p>The above is the point explanation of IIC.</p>
<p>Next, we will start the implementation.</p>
<h2 id="4-implementation-of-iic-in-mnist-google-colabratory-and-pytorch">4. Implementation of IIC in MNIST (Google Colabratory and PyTorch)</h2>
<p>The environment uses Google Colaboratory and the framework uses PyTorch.</p>
<p>Implement IIC for MNIST.</p>
<p>The implementation code &ldquo;MNIST_IIC.ipynb&rdquo; is located here.
<a href="https://github.com/YutaroOgawa/Qiita">Implementation code repository</a></p>
<p><a href="https://github.com/RuABraun/phone-clustering">https://github.com/RuABraun/phone-clustering</a>
It is implemented with reference, but it has been changed considerably.</p>
<p>First, fix the seed</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># Fixed random seed</span>
<span style="color:#f92672">import</span> os
<span style="color:#f92672">import</span> random
<span style="color:#f92672">import</span> numpy <span style="color:#f92672">as</span> np
<span style="color:#f92672">import</span> torch

SEED_VALUE <span style="color:#f92672">=</span> <span style="color:#ae81ff">1234</span> <span style="color:#75715e"># This can be anything</span>
os<span style="color:#f92672">.</span>environ[<span style="color:#e6db74">&#39;PYTHONHASHSEED&#39;</span>] <span style="color:#f92672">=</span> str(SEED_VALUE)
random<span style="color:#f92672">.</span>seed(SEED_VALUE)
np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>seed(SEED_VALUE)
torch<span style="color:#f92672">.</span>manual_seed(SEED_VALUE) <span style="color:#75715e"># When using PyTorch</span>
</code></pre></div><p>Then check the GPU usage.
For Google Colaboratory, select &ldquo;Runtime&rdquo; from the top menu
Select &ldquo;Change runtime type&rdquo; and switch None to GPU.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># If you can use GPU, select GPU (In case of Google Colaboratory, specify GPU from runtime)</span>
device <span style="color:#f92672">=</span><span style="color:#e6db74">&#39;cuda&#39;</span> <span style="color:#66d9ef">if</span> torch<span style="color:#f92672">.</span>cuda<span style="color:#f92672">.</span>is_available() <span style="color:#66d9ef">else</span><span style="color:#e6db74">&#39;cpu&#39;</span>
<span style="color:#66d9ef">print</span>(device)

<span style="color:#75715e"># Use GPU. Confirm that the output is cuda.</span>
</code></pre></div><p>Download the MNIST image and use it as a PyTorch data loader.
Prepare training and tests.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">Download the <span style="color:#75715e">#MNIST image and use it as a DataLoader (Train and Test)</span>
<span style="color:#f92672">from</span> torchvision <span style="color:#f92672">import</span> datasets, transforms

batch_size_train <span style="color:#f92672">=</span> <span style="color:#ae81ff">512</span>

train_loader <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>utils<span style="color:#f92672">.</span>data<span style="color:#f92672">.</span>DataLoader(
    datasets<span style="color:#f92672">.</span>MNIST(<span style="color:#e6db74">&#39;.&#39;</span>, train<span style="color:#f92672">=</span>True, download<span style="color:#f92672">=</span>True,
                   transform<span style="color:#f92672">=</span>transforms<span style="color:#f92672">.</span>Compose((
                       transforms<span style="color:#f92672">.</span>ToTensor(),
                   ])),
    batch_size<span style="color:#f92672">=</span>batch_size_train, shuffle<span style="color:#f92672">=</span>True, drop_last<span style="color:#f92672">=</span>True)
<span style="color:#75715e"># drop_last is not used when the last mini-batch is smaller than the specified size</span>


test_loader <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>utils<span style="color:#f92672">.</span>data<span style="color:#f92672">.</span>DataLoader(
    datasets<span style="color:#f92672">.</span>MNIST(<span style="color:#e6db74">&#39;.&#39;</span>, train<span style="color:#f92672">=</span>False, transform<span style="color:#f92672">=</span>transforms<span style="color:#f92672">.</span>Compose((
        transforms<span style="color:#f92672">.</span>ToTensor(),
    ])),
    batch_size<span style="color:#f92672">=</span><span style="color:#ae81ff">1024</span>, shuffle<span style="color:#f92672">=</span>False)
</code></pre></div><p>Next is the IIC deep learning model.
It is the same as normal supervised learning, but it has a heavier structure than just supervised learning in order to acquire richer expressive power in the network.</p>
<p>And the final output layer uses 10 kinds of classes that we expect to correspond to 0-9 and a technology called overclustring separately.</p>
<p>This will make it fall into more than the 10 types you would expect.</p>
<p>The final output layer will be 10 types and overclustering version, and the loss function will calculate the outputs of both and use the sum.</p>
<p>The expectation is that if subtle changes can be captured in an overclustring network, the performance of the usual 10 types of classification will also improve.</p>
<p>In IIC paper, this output layer is further multiplexed as multi-head.
This is supposed to prevent failures depending on the initial value of the output layer, but
I have omitted it because it doesn&rsquo;t make much sense to my senses and implementation is troublesome.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e">#Deep learning model</span>
<span style="color:#f92672">import</span> torch.nn <span style="color:#f92672">as</span> nn
<span style="color:#f92672">import</span> torch.nn.functional <span style="color:#f92672">as</span> F

OVER_CLUSTRING_Rate <span style="color:#f92672">=</span> <span style="color:#ae81ff">10</span> <span style="color:#75715e"># Also prepare overclsutering to classify a lot</span>


<span style="color:#66d9ef">class</span> <span style="color:#a6e22e">NetIIC</span>(nn<span style="color:#f92672">.</span>Module):
    <span style="color:#66d9ef">def</span> __init__(self):
        super(NetIIC, self)<span style="color:#f92672">.</span>__init__()

        self<span style="color:#f92672">.</span>conv1 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Conv2d(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">128</span>, <span style="color:#ae81ff">5</span>, <span style="color:#ae81ff">2</span>, bias<span style="color:#f92672">=</span>False)self<span style="color:#f92672">.</span>bn1 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>BatchNorm2d(<span style="color:#ae81ff">128</span>)
        self<span style="color:#f92672">.</span>conv2 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Conv2d(<span style="color:#ae81ff">128</span>, <span style="color:#ae81ff">128</span>, <span style="color:#ae81ff">5</span>, <span style="color:#ae81ff">1</span>, bias<span style="color:#f92672">=</span>False)
        self<span style="color:#f92672">.</span>bn2 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>BatchNorm2d(<span style="color:#ae81ff">128</span>)
        self<span style="color:#f92672">.</span>conv3 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Conv2d(<span style="color:#ae81ff">128</span>, <span style="color:#ae81ff">128</span>, <span style="color:#ae81ff">5</span>, <span style="color:#ae81ff">1</span>, bias<span style="color:#f92672">=</span>False)
        self<span style="color:#f92672">.</span>bn3 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>BatchNorm2d(<span style="color:#ae81ff">128</span>)
        self<span style="color:#f92672">.</span>conv4 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Conv2d(<span style="color:#ae81ff">128</span>, <span style="color:#ae81ff">256</span>, <span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">1</span>, bias<span style="color:#f92672">=</span>False)
        self<span style="color:#f92672">.</span>bn4 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>BatchNorm2d(<span style="color:#ae81ff">256</span>)
        
        <span style="color:#ae81ff">10</span> types of classes you want to expect to respond to <span style="color:#75715e">#0-9</span>
        self<span style="color:#f92672">.</span>fc <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(<span style="color:#ae81ff">256</span>, <span style="color:#ae81ff">10</span>)

        <span style="color:#75715e"># overclustering</span>
        <span style="color:#75715e"># Allow more minute changes in the network by capturing more clusters than expected</span>
        self<span style="color:#f92672">.</span>fc_overclustering <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(<span style="color:#ae81ff">256</span>, <span style="color:#ae81ff">10</span><span style="color:#f92672">*</span>OVER_CLUSTRING_Rate)

    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x):
        x <span style="color:#f92672">=</span> F<span style="color:#f92672">.</span>relu(self<span style="color:#f92672">.</span>bn1(self<span style="color:#f92672">.</span>conv1(x)))
        x <span style="color:#f92672">=</span> F<span style="color:#f92672">.</span>relu(self<span style="color:#f92672">.</span>bn2(self<span style="color:#f92672">.</span>conv2(x)))
        x <span style="color:#f92672">=</span> F<span style="color:#f92672">.</span>relu(self<span style="color:#f92672">.</span>bn3(self<span style="color:#f92672">.</span>conv3(x)))
        x <span style="color:#f92672">=</span> F<span style="color:#f92672">.</span>relu(self<span style="color:#f92672">.</span>bn4(self<span style="color:#f92672">.</span>conv4(x)))
        x_prefinal <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>view(x<span style="color:#f92672">.</span>size(<span style="color:#ae81ff">0</span>), <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>)
        y <span style="color:#f92672">=</span> F<span style="color:#f92672">.</span>softmax(self<span style="color:#f92672">.</span>fc(x_prefinal), dim<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)

        y_overclustering <span style="color:#f92672">=</span> F<span style="color:#f92672">.</span>softmax(self<span style="color:#f92672">.</span>fc_overclustering(
            x_prefinal), dim<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>) <span style="color:#75715e"># overclustering</span>

        <span style="color:#66d9ef">return</span> y, y_overclustering
</code></pre></div><p>Define an initialization function for model weights.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> torch.nn.init <span style="color:#f92672">as</span> init


<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">weight_init</span>(m):
    <span style="color:#e6db74">&#34;&#34;&#34;Weight initialization&#34;&#34;&#34;</span>
    <span style="color:#66d9ef">if</span> isinstance(m, nn<span style="color:#f92672">.</span>Conv2d):
        init<span style="color:#f92672">.</span>xavier_normal_(m<span style="color:#f92672">.</span>weight<span style="color:#f92672">.</span>data)
        <span style="color:#66d9ef">if</span> m<span style="color:#f92672">.</span>bias <span style="color:#f92672">is</span> <span style="color:#f92672">not</span> None:
            init<span style="color:#f92672">.</span>normal_(m<span style="color:#f92672">.</span>bias<span style="color:#f92672">.</span>data)
    <span style="color:#66d9ef">elif</span> isinstance(m, nn<span style="color:#f92672">.</span>BatchNorm2d):
        init<span style="color:#f92672">.</span>normal_(m<span style="color:#f92672">.</span>weight<span style="color:#f92672">.</span>data, mean<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, std<span style="color:#f92672">=</span><span style="color:#ae81ff">0.02</span>)
        init<span style="color:#f92672">.</span>constant_(m<span style="color:#f92672">.</span>bias<span style="color:#f92672">.</span>data, <span style="color:#ae81ff">0</span>)
    <span style="color:#66d9ef">elif</span> isinstance(m, nn<span style="color:#f92672">.</span>Linear):
        <span style="color:#75715e"># Xavier</span>
        <span style="color:#75715e">#init.xavier_normal_(m.weight.data)</span>

        <span style="color:#75715e"># He</span>
        init<span style="color:#f92672">.</span>kaiming_normal_(m<span style="color:#f92672">.</span>weight<span style="color:#f92672">.</span>data)
        
        <span style="color:#66d9ef">if</span> m<span style="color:#f92672">.</span>bias <span style="color:#f92672">is</span> <span style="color:#f92672">not</span> None:
            init<span style="color:#f92672">.</span>normal_(m<span style="color:#f92672">.</span>bias<span style="color:#f92672">.</span>data)
</code></pre></div><p>Next, we define a transformation to create an image that pairs with the input image.
Rotate and stretch with an affine transformation, then add noise to each pixel.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e">#Define a function that adds noise to the data</span>
<span style="color:#f92672">import</span> torchvision <span style="color:#f92672">as</span> tv
<span style="color:#f92672">import</span> torchvision.transforms.functional <span style="color:#f92672">as</span> TF


<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">perturb_imagedata</span>(x):
    y <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>clone()
    batch_size <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>size(<span style="color:#ae81ff">0</span>)

    <span style="color:#75715e"># Perform random affine transformation</span>
    trans <span style="color:#f92672">=</span> tv<span style="color:#f92672">.</span>transforms<span style="color:#f92672">.</span>RandomAffine(<span style="color:#ae81ff">15</span>, (<span style="color:#ae81ff">0.2</span>, <span style="color:#ae81ff">0.2</span>,), (<span style="color:#ae81ff">0.2</span>, <span style="color:#ae81ff">0.75</span>,))
    <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(batch_size):
        y[i, <span style="color:#ae81ff">0</span>] <span style="color:#f92672">=</span> TF<span style="color:#f92672">.</span>to_tensor(trans(TF<span style="color:#f92672">.</span>to_pil_image(y[i, <span style="color:#ae81ff">0</span>])))

    <span style="color:#75715e"># Add noise</span>
    noise <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>randn(batch_size, <span style="color:#ae81ff">1</span>, x<span style="color:#f92672">.</span>size(<span style="color:#ae81ff">2</span>), x<span style="color:#f92672">.</span>size(<span style="color:#ae81ff">3</span>))
    div <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>randint(<span style="color:#ae81ff">20</span>, <span style="color:#ae81ff">30</span>, (batch_size,),
                        dtype<span style="color:#f92672">=</span>torch<span style="color:#f92672">.</span>float32)<span style="color:#f92672">.</span>view(batch_size, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>)
    y <span style="color:#f92672">+=</span> noise <span style="color:#f92672">/</span> div

    <span style="color:#66d9ef">return</span> y
</code></pre></div><p>And the calculation of mutual information, which is the key to IIC.
What we are doing is the calculation of mutual information as explained in 3.</p>
<p>I want to maximize the amount of mutual information, but in order to make it a loss, multiply it by minus,
Replaced with the minimization problem.</p>
<p>Also,
<a href="https://qiita.com/Amanokawa/items/0aa24bc396dd88fb7d2a">Simple method to obtain MNIST correct rate of 97% or more by unsupervised learning (without transfer learning)</a>
A coefficient term has been added to the calculation of mutual information introduced in, to make it easier for classes to vary.</p>
<p>The above article carefully introduced the implementation in TensorFlow2, and is highly recommended for TensorFlow.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># IIS defines loss function</span>
<span style="color:#75715e"># Reference: https://github.com/RuABraun/phone-clustering/blob/master/mnist_basic.py</span>
<span style="color:#f92672">import</span> sys


<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">compute_joint</span>(x_out, x_tf_out):

    <span style="color:#75715e"># x_out and x_tf_out are torch.Size([512, 10]). Torch.Size([2048, 10, 10]) is calculated by multiplying these two to obtain the joint distribution.</span>
    <span style="color:#75715e"># torch.Size([512, 10, 1]) * torch.Size([512, 1, 10])</span>
    p_i_j <span style="color:#f92672">=</span> x_out<span style="color:#f92672">.</span>unsqueeze(<span style="color:#ae81ff">2</span>) <span style="color:#f92672">*</span> x_tf_out<span style="color:#f92672">.</span>unsqueeze(<span style="color:#ae81ff">1</span>)
    <span style="color:#75715e"># p_i_j is torch.Size([512, 10, 10])</span>

    <span style="color:#75715e"># Add all mini batches ⇒ torch.Size([10, 10])</span>
    p_i_j <span style="color:#f92672">=</span> p_i_j<span style="color:#f92672">.</span>sum(dim<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)

    <span style="color:#75715e"># Add with transpose matrix and divide (symmetrization) ⇒ torch.Size([10, 10])</span>
    p_i_j <span style="color:#f92672">=</span> (p_i_j <span style="color:#f92672">+</span> p_i_j<span style="color:#f92672">.</span>t()) <span style="color:#f92672">/</span> <span style="color:#ae81ff">2.</span>

    <span style="color:#75715e"># Normalization ⇒ torch.Size([10, 10])</span>
    p_i_j <span style="color:#f92672">=</span> p_i_j <span style="color:#f92672">/</span> p_i_j<span style="color:#f92672">.</span>sum()

    <span style="color:#66d9ef">return</span> p_i_j
    <span style="color:#75715e"># After all, p_i_j shows the probability distribution table of all 100 patterns of all mini-batch for 100 patterns of 10 kinds of judgment output of normal image and 10 kinds of judgment of converted image</span>


<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">IID_loss</span>(x_out, x_tf_out, EPS<span style="color:#f92672">=</span>sys<span style="color:#f92672">.</span>float_info<span style="color:#f92672">.</span>epsilon):
    <span style="color:#75715e"># torch.Size([512, 10]), the last 10 is a classification number, so 100 when overclustering</span>
    bs, k <span style="color:#f92672">=</span> x_out<span style="color:#f92672">.</span>size()
    p_i_j <span style="color:#f92672">=</span> compute_joint(x_out, x_tf_out) <span style="color:#75715e"># torch.Size([10, 10])</span>

    <span style="color:#75715e"># From the distribution table of joint probabilities, sum the 10 patterns of the converted image and marginize them to create a distribution table of marginal probabilities of only the original image.</span>
    p_i <span style="color:#f92672">=</span> p_i_j<span style="color:#f92672">.</span>sum(dim<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)<span style="color:#f92672">.</span>view(k, <span style="color:#ae81ff">1</span>)<span style="color:#f92672">.</span>expand(k, k)
    <span style="color:#75715e"># From the joint probability distribution table, sum 10 patterns of the original image and marginize them to create a marginal probability distribution table of only the transformed image</span>
    p_j <span style="color:#f92672">=</span> p_i_j<span style="color:#f92672">.</span>sum(dim<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)<span style="color:#f92672">.</span>view(<span style="color:#ae81ff">1</span>, k)<span style="color:#f92672">.</span>expand(k, k)

    If you put a value close to <span style="color:#75715e"># 0 in log, it will diverge, so avoid it</span>
    <span style="color:#75715e">#p_i_j[(p_i_j &lt;EPS).data] = EPS</span>
    <span style="color:#75715e">#p_j[(p_j &lt;EPS).data] = EPS</span>
    <span style="color:#75715e">#p_i[(p_i &lt;EPS).data] = EPS</span>
    <span style="color:#75715e"># Reference GitHub implementation (↑) gives an error if PyTorch version 1.3 or above</span>
    <span style="color:#75715e"># https://discuss.pytorch.org/t/pytorch-1-3-showing-an-error-perhaps-for-loss-computed-from-paired-outputs/68790/3</span>

    If you put a value close to <span style="color:#75715e"># 0 in log, it will diverge, so avoid it</span>
    p_i_j <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>where(p_i_j <span style="color:#f92672">&lt;</span>EPS, torch<span style="color:#f92672">.</span>tensor(
        [EPS], device<span style="color:#f92672">=</span>p_i_j<span style="color:#f92672">.</span>device), p_i_j)
    p_j <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>where(p_j <span style="color:#f92672">&lt;</span>EPS, torch<span style="color:#f92672">.</span>tensor([EPS], device<span style="color:#f92672">=</span>p_j<span style="color:#f92672">.</span>device), p_j)
    p_i <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>where(p_i <span style="color:#f92672">&lt;</span>EPS, torch<span style="color:#f92672">.</span>tensor([EPS], device<span style="color:#f92672">=</span>p_i<span style="color:#f92672">.</span>device), p_i)

    <span style="color:#75715e"># Mutual information is calculated from the joint probability and marginal probability of the original image and the converted image</span>
    <span style="color:#75715e"># However, multiply by minus to make it a minimization</span>
    <span style="color:#e6db74">&#34;&#34;&#34;
</span><span style="color:#e6db74">    Want to maximize mutual information
</span><span style="color:#e6db74">    ⇒ In the end, I want x_out and x_tf_out to have more information
</span><span style="color:#e6db74">    ⇒ In short, I want x_out and x_tf_out to be together
</span><span style="color:#e6db74">
</span><span style="color:#e6db74">    p_i_j is a joint probability distribution of x_out and x_tf_out, mini-batch as much as possible, various patterns of 10×10, evenly uniform
</span><span style="color:#e6db74">    
</span><span style="color:#e6db74">    The first half of the term, torch.log(p_i_j), has a large value (close to 0) when p_ij is close to 1.
</span><span style="color:#e6db74">    If any one is 1 and 0 does not vary, log0 will have a small value (negative large value)
</span><span style="color:#e6db74">    So the first half of the term is
</span><span style="color:#e6db74">
</span><span style="color:#e6db74">    The latter half is a term that calculates which of the original image or the transformed image is 10 different by marginalizing each.
</span><span style="color:#e6db74">    Subtracting the marginalized 10×10 pattern, if the first half term becomes smaller,
</span><span style="color:#e6db74">    x_out and x_tf_out did not share much information.
</span><span style="color:#e6db74">    &#34;&#34;&#34;</span>
    <span style="color:#75715e"># https://qiita.com/Amanokawa/items/0aa24bc396dd88fb7d2a</span>
    Add weight alpha <span style="color:#66d9ef">with</span> reference to <span style="color:#75715e">## Penalties due to variations in the joint probability distribution table are reduced = The joint probability distribution tends to vary</span>
    alpha <span style="color:#f92672">=</span> <span style="color:#ae81ff">2.0</span> <span style="color:#75715e"># papers and normal mutual information calculations have an alpha of 1</span>

    loss <span style="color:#f92672">=</span> <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span><span style="color:#f92672">*</span>(p_i_j <span style="color:#f92672">*</span> (torch<span style="color:#f92672">.</span>log(p_i_j)<span style="color:#f92672">-</span>alpha <span style="color:#f92672">*</span>
                        torch<span style="color:#f92672">.</span>log(p_j)<span style="color:#f92672">-</span>alpha<span style="color:#f92672">*</span>torch<span style="color:#f92672">.</span>log(p_i)))<span style="color:#f92672">.</span>sum()

    <span style="color:#66d9ef">return</span> loss
</code></pre></div><p>Conduct training.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># Conduct training</span>
total_epoch <span style="color:#f92672">=</span> <span style="color:#ae81ff">20</span>


<span style="color:#75715e"># model</span>
model <span style="color:#f92672">=</span> NetIIC()
model<span style="color:#f92672">.</span>apply(weight_init)
model<span style="color:#f92672">.</span>to(device)

<span style="color:#75715e"># Set optimization function</span>
optimizer <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>optim<span style="color:#f92672">.</span>Adam(model<span style="color:#f92672">.</span>parameters(), lr<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-3</span>)


<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">train</span>(total_epoch, model, train_loader, optimizer, device):

    model<span style="color:#f92672">.</span>train()
    scheduler <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>optim<span style="color:#f92672">.</span>lr_scheduler<span style="color:#f92672">.</span>CosineAnnealingWarmRestarts(
        optimizer, T_0<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>, T_mult<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>)

    <span style="color:#66d9ef">for</span> epoch <span style="color:#f92672">in</span> range(total_epoch):
        <span style="color:#66d9ef">for</span> batch_idx, (data, target) <span style="color:#f92672">in</span> enumerate(train_loader):

            <span style="color:#75715e"># Change in learning rate</span>
            scheduler<span style="color:#f92672">.</span>step()

            <span style="color:#75715e"># Make delicately converted data. Make pairs for SIMULTANEOUS_NUM</span>
            data_perturb <span style="color:#f92672">=</span> perturb_imagedata(data) <span style="color:#75715e"># give noise</span>

            <span style="color:#75715e">#Send if you can send to GPU</span>
            data <span style="color:#f92672">=</span> data<span style="color:#f92672">.</span>to(device)
            data_perturb <span style="color:#f92672">=</span> data_perturb<span style="color:#f92672">.</span>to(device)

            <span style="color:#75715e"># Optimization function initialization</span>
            optimizer<span style="color:#f92672">.</span>zero_grad()

            <span style="color:#75715e"># Neural network output</span>
            output, output_overclustering <span style="color:#f92672">=</span> model(data)
            output_perturb, output_perturb_overclustering <span style="color:#f92672">=</span> model(data_perturb)

            <span style="color:#75715e"># Loss calculation</span>
            loss1 <span style="color:#f92672">=</span> IID_loss(output, output_perturb)
            loss2 <span style="color:#f92672">=</span> IID_loss(output_overclustering,
                             output_perturb_overclustering)
            loss <span style="color:#f92672">=</span> loss1 <span style="color:#f92672">+</span> loss2

            <span style="color:#75715e">#Updated to reduce losses</span>
            loss<span style="color:#f92672">.</span>backward()
            optimizer<span style="color:#f92672">.</span>step()

            <span style="color:#75715e"># Log output</span>
            <span style="color:#66d9ef">if</span> batch_idx <span style="color:#f92672">%</span><span style="color:#ae81ff">10</span> <span style="color:#f92672">==</span> <span style="color:#ae81ff">0</span>:
                <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#39;Train Epoch {}:iter{}-</span><span style="color:#ae81ff">\t</span><span style="color:#e6db74">Loss1: {:.6f}- </span><span style="color:#ae81ff">\t</span><span style="color:#e6db74">Loss2: {:.6f}- </span><span style="color:#ae81ff">\t</span><span style="color:#e6db74">Loss_total: {:.6f}&#39;</span><span style="color:#f92672">.</span>format(
                    epoch, batch_idx, loss1<span style="color:#f92672">.</span>item(), loss2<span style="color:#f92672">.</span>item(), loss1<span style="color:#f92672">.</span>item()<span style="color:#f92672">+</span>loss2<span style="color:#f92672">.</span>item()))

    <span style="color:#66d9ef">return</span> model, optimizer


model_trained, optimizer <span style="color:#f92672">=</span> train(
    total_epoch, model, train_loader, optimizer, device)
</code></pre></div><p>At the time of training, the CosineAnnealingWarmRestarts of the scheduler is used to change the learning rate.
This scheduler raises and lowers the learning rate as shown below.</p>
<p><img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/191401/35665e4c-a555-def1-22d8-c9a09c7489c8.jpeg" alt="sgdr.jpg">
Figure: Quote
<a href="https://www.kaggle.com/c/imet-2019-fgvc6/discussion/94783">https://www.kaggle.com/c/imet-2019-fgvc6/discussion/94783</a></p>
<p>When the learning rate decreases and suddenly increases, it is possible to escape from the local solution, and it becomes easier to approach the parameter of the global minimum solution.</p>
<p>Finally, infer the test data with the trained model.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># Check the results of the model classification cluster</span>


<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">test</span>(model, device, train_loader):
    model<span style="color:#f92672">.</span>eval()

    <span style="color:#75715e"># A list to store the results</span>
    out_targs <span style="color:#f92672">=</span> []
    ref_targs <span style="color:#f92672">=</span> []
    cnt <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>

    <span style="color:#66d9ef">with</span> torch<span style="color:#f92672">.</span>no_grad():
        <span style="color:#66d9ef">for</span> data, target <span style="color:#f92672">in</span> test_loader:
            cnt <span style="color:#f92672">+=</span> <span style="color:#ae81ff">1</span>
            data <span style="color:#f92672">=</span> data<span style="color:#f92672">.</span>to(device)
            target <span style="color:#f92672">=</span> target<span style="color:#f92672">.</span>to(device)
            outputs, outputs_overclustering <span style="color:#f92672">=</span> model(data)

            <span style="color:#75715e"># Add classification result to list</span>
            out_targs<span style="color:#f92672">.</span>append(outputs<span style="color:#f92672">.</span>argmax(dim<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)<span style="color:#f92672">.</span>cpu())
            ref_targs<span style="color:#f92672">.</span>append(target<span style="color:#f92672">.</span>cpu())

    <span style="color:#75715e"># List together</span>
    out_targs <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>cat(out_targs)
    ref_targs <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>cat(ref_targs)

    <span style="color:#66d9ef">return</span> out_targs<span style="color:#f92672">.</span>numpy(), ref_targs<span style="color:#f92672">.</span>numpy()


out_targs, ref_targs <span style="color:#f92672">=</span> test(model_trained, device, train_loader)
</code></pre></div><p>Finally, obtain the frequency table of output results.
The vertical axis shows the actual labels from 0 to 9, and the horizontal axis shows the determined class.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> numpy <span style="color:#f92672">as</span> np
<span style="color:#f92672">import</span> scipy.stats <span style="color:#f92672">as</span> stats

<span style="color:#75715e"># Make a confusion matrix</span>
matrix <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>zeros((<span style="color:#ae81ff">10</span>, <span style="color:#ae81ff">10</span>))

<span style="color:#75715e"># Create a frequency table for the classes identified by numbers 0 to 9 vertically and horizontally</span>
<span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(len(out_targs)):
    row <span style="color:#f92672">=</span> ref_targs[i]
    col <span style="color:#f92672">=</span> out_targs[i]
    matrix[row][col] <span style="color:#f92672">+=</span> <span style="color:#ae81ff">1</span>

np<span style="color:#f92672">.</span>set_printoptions(suppress<span style="color:#f92672">=</span>True)
<span style="color:#66d9ef">print</span>(matrix)
</code></pre></div><p>The output result is</p>
<pre><code>[[ 1. 978. 1. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 4. 1110. 2. 0. 2. 0. 13. 3.]
 [0. 5. 4. 0. 0. 0. 0. 0. 1023. 0.]
 [0. 0. 2. 0. 0. 4. 962. 0. 39. 3.]
 [1.0.0.0.0.9.960.0.0.19.1.1]
 [1. 1. 0. 0. 0. 866. 17. 0. 3. 4.]
 [940. 7. 0. 0.0. 0. 3. 0. 0. 4. 4.]
 [0. 0. 921.1. 0. 0. 1. 92. 13. 0.]
 [0. 6. 0. 0. 0. 2. 4. 2. 2. 958.]
 [0. 4. 14. 0. 2. 7. 27. 949. 2. 4.]]
</code></pre><p>For example, the number 0 is the first 978 in the class.
If the number is 9, the 949th item is in the 7th place.</p>
<p>Applying the same number in each estimated class to each correct label,</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e">#All data</span>
total_num <span style="color:#f92672">=</span> matrix<span style="color:#f92672">.</span>sum()<span style="color:#f92672">.</span>sum()
<span style="color:#66d9ef">print</span>(total_num)

<span style="color:#75715e"># Each number is nicely divided into each class.</span>
<span style="color:#75715e"># For example, the number 0 gathered 978 in the first class. If the number is 9, the 949th gathered in the 7th.</span>
<span style="color:#75715e"># Therefore, if you add the largest ones, the number of correct answers will be</span>
correct_num_list <span style="color:#f92672">=</span> matrix<span style="color:#f92672">.</span>max(axis<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)
<span style="color:#66d9ef">print</span>(correct_num_list)
<span style="color:#66d9ef">print</span>(correct_num_list<span style="color:#f92672">.</span>sum())

<span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;correctness rate:&#34;</span>, correct_num_list<span style="color:#f92672">.</span>sum()<span style="color:#f92672">/</span>total_num<span style="color:#f92672">*</span><span style="color:#ae81ff">100</span>)
</code></pre></div><p>And the output is</p>
<pre><code>10000.0
[940. 978. 921. 1110. 960. 866. 962. 949. 1023. 958.]
9667.0
Correct answer rate: 96.67
</code></pre><p>is. The accuracy rate was 97%.</p>
<p>The implementation code &ldquo;MNIST_IIC.ipynb&rdquo; is located here.
<a href="https://github.com/YutaroOgawa/Qiita">Implementation code repository</a></p>
<h2 id="at-the-end">at the end</h2>
<p>Introduced IIC (Invariant Information Clustering), which is unsupervised learning that maximizes mutual information.</p>
<p>The idea that IIC considers similar inputs to be the same from the same image is very similar to the contrast learning of SimCLR (A Simple Framework for Contrastive Learning of Visual Representations) in the latest paper by Hinton et al.</p>
<p>In IIC itself, mutual information only appears in the error function, and there is a big gap between IIC papers and the brain of living things.</p>
<p>However, mutual information can also be expressed by Kalbach-Leibler information, and can be extended (I feel) to Karl Friston&rsquo;s free-energy principle.<a href="https://www.jstage.jst.go.jp/article/jnns/25/3/25_71/_pdf/-char/en">● Explanation of the principle of free energy: Perception, behavior, and reasoning of others&rsquo; thoughts</a></p>
<p>IIC was a very interesting paper and method.
In the future, I would like to see more progress in unsupervised learning and progress in brain and neuroscience.</p>
<p>I conducted IIC with image data this time, but next time I will write an article about what happens when IIC is performed with text data processed by BERT (I will write here an organism that has a visual cortex called object recognition. This is an interesting result because the difference between the physical brain mechanism and the natural language characteristic of human beings becomes noticeable.)</p>
<p>Thank you for reading this.</p>
<p>[Disclaimer] This article is the opinion/dissemination of the author and is not the official opinion of the company to which the author belongs.</p>

    </article>
  </div>

  
  
  
  <script>
  window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
  ga('create', 'UA-169005401-1', 'auto');
  ga('send', 'pageview');
  </script>
  <script async src='https://www.google-analytics.com/analytics.js'></script>
  

  

  
<link rel="stylesheet" type="text/css" href="//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.0/cookieconsent.min.css" />
<script src="//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.0/cookieconsent.min.js"></script>
<script>
window.addEventListener("load", function(){
window.cookieconsent.initialise({
  "palette": {
    "popup": {
      "background": "#216942",
      "text": "#b2d192"
    },
    "button": {
      "background": "#afed71"
    }
  }
})});
</script>

</body>

</html>
