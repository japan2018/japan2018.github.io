<!doctype html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  
  <meta name="generator" content="Hugo 0.72.0" />

  
  <meta name="description" content="Some description">
  

  
  <link rel="apple-touch-icon" sizes="180x180" href="https://memotut.com/apple-touch-icon.png">

  
  <link rel="icon" type="image/png" sizes="32x32" href="https://memotut.com/favicon-32x32.png">

  
  <link rel="icon" type="image/png" sizes="16x16" href="https://memotut.com/favicon-16x16.png">

  
  <link rel="manifest" href="https://memotut.com/site.webmanifest">

  
  <link rel="mask-icon" href="https://memotut.com/safari-pinned-tab.svg" color="#5bbad5">

  <meta name="msapplication-TileColor" content="#da532c">

  <meta name="theme-color" content="#ffffff">

  
  <link rel="stylesheet" href="https://memotut.com/css/bootstrap.min.css" />

  
  <title>Few shot NODOGURO turning, I tried to automatically count Nodoguro | Memo Tut</title>
  

  <style>
body {
  min-width: 300px;
}

.custom-navbar {
  margin-bottom: 1em;
  height: 60px;
}

.custom-navbar a {
  display: inline-block; 
  padding: 18px 0;
  margin-right: 1em; 
  font-weight: bold; 
}

.custom-navbar a:hover,
.custom-navbar a:focus {
  text-decoration: none; 
}

@media print {
  .custom-navbar {
    display: none;
  }
}

article {
  padding-bottom: 1em;
}

img {
  max-width: 100%;
}


body {
  background-color: #fff;
}



body {
  color: #212529;
}



a {
  color: #007bff;
}



a:hover,
a:focus {
  color: #0056b3;
}



.custom-navbar {
  background-color: #212529;
}



.custom-navbar a {
  color: rgba(255, 255, 255, 0.75);
}



.custom-navbar a:hover,
.custom-navbar a:focus {
  color: rgba(255, 255, 255, 1);
}



.container {
  max-width: 800px;
}



pre {
  display: block;
  padding: 9.5px;
  word-break: break-all;
  word-wrap: break-word;
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 4px;
}

pre code {
  padding: 0;
  font-size: inherit;
  color: inherit; 
  white-space: pre-wrap;
  background-color: transparent;
  border: none;
  border-radius: 0;
}

code {
  padding: 2px 4px;
  color: inherit; 
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 4px;
  font-size: .9em;
}



blockquote,
.blockquote {
  padding: 10px 20px;
  margin: 0 0 20px;
  font-size: 1em;
  border-left: 5px solid #6c757d;
}

</style>
</head>

<body>
  <nav class="custom-navbar">
  <div class="container">
    
    <a href="/">Posts</a>
    
    <a href="/tags/">Tags</a>
    
    <a href="/about/">About</a>
    
    <a href="/index.xml">RSS</a>
    
  </div>
</nav>
  
  <div class="container">
    <article>
      <h1>Few shot NODOGURO turning, I tried to automatically count Nodoguro</h1>
<p>
  <small class="text-secondary">
  
  
  Dec 5, 2019
  </small>
  

<small><code><a href="https://memotut.com/tags/python">Python</a></code></small>


<small><code><a href="https://memotut.com/tags/deeplearning"> DeepLearning</a></code></small>


<small><code><a href="https://memotut.com/tags/image-recognition"> image recognition</a></code></small>


<small><code><a href="https://memotut.com/tags/chainer"> Chainer</a></code></small>

</p>
<pre><code>This article is from the 5th day of [Furukawa Laboratory Advent_calendar](https://qiita.com/advent-calendar/2019/flab).
</code></pre>
<p>#Introduction
It is said that various frameworks such as PyTorch, Chainer, Keras, and TensorFlow have appeared, making it easy for anyone to use Deep Learning.
For those of you who are actually using Deep Learning, it may seem easy just to move it. However, it is more difficult for people who do not use Python much than Deep Learning.
I think moving Deep Learning is like riding a bicycle.
People who can ride a bicycle once say that it is easy to ride a bicycle, or that they can ride other bicycles in the same way, but from people who have never or who cannot ride a bicycle. I feel like, &ldquo;What are you saying?&rdquo;</p>
<p>In addition, when using Deep Learning, I think that the skill required depends on how much you want to do with the feeling as shown in the figure below, which is one of the reasons for raising the hurdle to use Deep.
<img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/301489/ea879436-a59b-83a7-d754-274d3022617c.png" alt="Slide 1.png"></p>
<p>In this article, I will explain how I actually did the 2nd Step to help ride a bicycle called Deep Learning.</p>
<h1 id="for-the-time-being-try-object-recognition-with-deep">For the time being, try object recognition with Deep</h1>
<h2 id="preparation">Preparation</h2>
<p>This time I use Chainer.
For that, let&rsquo;s add Chainer.</p>
<pre><code>$ pip install chainer
$ pip install chainercv
</code></pre><h2 id="execute">execute</h2>
<p>Move it as follows.</p>
<pre><code class="language-Python:" data-lang="Python:">import matplotlib.pyplot as plt
import numpy as np

from PIL import Image
from chainercv.visualizations import vis_bbox
from chainercv.datasets import voc_bbox_label_names
from chainercv.links import FasterRCNNVGG16

# Label to use (this time the default one)
label_names = voc_bbox_label_names

# Load the data Use &quot;..fish/test.jpeg&quot; as your favorite image file
test_data = Image.open('./fish/test.jpg')
test_data = np.asarray(test_data).transpose(2, 0, 1).astype(np.float32)

# Build the model, for the time being, the model uses the trained voc07
model_frcnn = FasterRCNNVGG16(n_fg_class=len(voc_bbox_label_names), pretrained_model='voc07')

# Forecast
bboxes, labels, scores = model_frcnn.predict([test_data])
predict_result = [test_data, bboxes[0], labels[0], scores[0]]

#Draw results
res = predict_result
fig = plt.figure(figsize=(6, 6))
ax = fig.subplots(1, 1)
line = 0.0
vis_bbox(res[0], res[1][res[3]&gt;line], res[2][res[3]&gt;line], res[3][res[3]&gt;line], label_names=label_names, ax=ax)
plt.show()
</code></pre><h2 id="result">result</h2>
<p>I was able to recognize it well!
<img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/301489/1c3ee884-c626-d4c5-f584-6b20e35a02e7.png" alt="image.png"></p>
<p>Next, I tried to put a nodoguro image.
<img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/301489/239235ea-74a6-c025-1312-f6f1f7c6f3b8.png" alt="image.png"></p>
<p>Of course, if you keep the default, there is no label of Nodoro and it will not work.
Therefore, I do Fine-turning to make it a specialized discriminator.
The detailed explanation of fine-turning will be skipped, but the point is that it is a feeling that additional learning is applied to the already trained model.</p>
<h1 id="data-preparation">Data preparation</h1>
<p>In the first place, learning data is necessary for additional learning, so let&rsquo;s create learning data.
The one called <a href="https://github.com/tzutalin/labelImg">labelImg</a> is recommended.
How to put it and how to use it are written in the README on the github site in ↑, so I will explain only the simple and easy flow for the time being.
First, add the one you need to run labelImg.</p>
<pre><code>$ brew install qt # Install qt-5.x.x by Homebrew
$ brew install libxml2
$ pip3 install pyqt5 lxml # Install qt and lxml by pip
$ make qt5py3
</code></pre><p>I will do it.
I don&rsquo;t think there is anything to watch out for, but I think it&rsquo;s about operating in the cloned directory. I get an error saying <code>No such file or directory</code></p>
<pre><code>$python3 labelImg.py
</code></pre><p>When you execute <code>labelImg.py</code>, the following screen appears.</p>
<p><img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/301489/cf7acbc9-d076-49b3-d555-842c60324ea4.png" alt="image.png"></p>
<p>Open the image with open and enter &ldquo;nodoguro&rdquo; in the right label
You can select the range by pressing the <code>w</code> key, so select the Nodoro.
<img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/301489/a6c9827b-ff3d-3b39-b30b-99d600368847.png" alt="image.png"></p>
<p>Then you can attach a label like this.
<img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/301489/2d25dc61-b49d-cd28-3002-2d56e89e8c01.png" alt="image.png"></p>
<p>You can also attach two like this.
<img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/301489/1c89193c-4240-2363-ec09-911b55318155.png" alt="image.png"></p>
<p>Finally, click the save button to create an xml file. This file contains information about where the labels and borders are located.
Please name your Farui like <code>image_1.jpg</code> and <code>image_2.jpg</code>.
After that, create a file called <code>classes.txt</code> in which the label names are written in bullet points.</p>
<pre><code class="language-classes.txt:" data-lang="classes.txt:">nodoguro
iwashi
cat
</code></pre><p>The learning data creation is now complete!
The points to be aware of are &ldquo;to make the image sizes uniform&rdquo; and &ldquo;to make the label two or more types&rdquo;.
When there was only one type of label, it did not work well during learning.</p>
<h1 id="nodoguro-turning">NODOGURO turning</h1>
<p>Now that we have the learning data, let&rsquo;s actually train it. Imagenet was used for the trained model.
This time, we will study additional 7 images.</p>
<p>The directory structure is as follows.</p>
<pre><code>sample/
├ fish/
│ ├ res_images/
│ │ ├ images.npy
│ │ ├ bounding_box_data.npy
│ │ └ object_ids.npy
│ ├ classes.txt
│ ├ image_1.jpg
│ ├ image_1.xml
│ ├ ...
│ ├ image_7.xml
│ └ test.jpg
├ out/
├ learn.py
├ predict.py
└ xml2numpyarray.py
</code></pre><h2 id="data-shaping">Data shaping</h2>
<p>Since it was convenient to have it in the form of numpyarray before learning this time, I converted it using the following code.
If you get an import error, please use pip.</p>
<pre><code class="language-xml2numpyarray.py:" data-lang="xml2numpyarray.py:">import matplotlib.pyplot as plt
import numpy as np
import glob
import os
import cv2
from PIL import Image
import xmltodict

# Global Variables

classes_file ='fish/classes.txt'
data_dir ='fish'

classes = list()
with open(classes_file) as fd:
    for one_line in fd.readlines():
        cl = one_line.split('\n')[0]
        classes.append(cl)
print(classes)

def getBBoxData(anno_file, classes, data_dir):
    with open(anno_file) as fd:
        pars = xmltodict.parse(fd.read())
    ann_data = pars['annotation']

    print(ann_data['filename'])
    # read image
    img = Image.open(os.path.join(data_dir, ann_data['filename']))
    img_arr = np.asarray(img).transpose(2, 0, 1).astype(np.float32)
    bbox_list = list()
    obj_names = list()
    for obj in ann_data['object']:bbox_list.append([obj['bndbox']['ymin'], obj['bndbox']['xmin'], obj['bndbox']['ymax'], obj['bndbox']['xmax']])
        obj_names.append(obj['name'])
    bboxs = np.array(bbox_list, dtype=np.float32)
    obj_names = np.array(obj_names)
    obj_ids = np.array(list(map(lambda x:classes.index(x), obj_names)), dtype=np.int32)
    return {'img':img, 'img_arr':img_arr, 'bboxs':bboxs, 'obj_names':obj_names, 'obj_ids':obj_ids}

def getBBoxDataSet(data_dir, classes):
    anno_files = glob.glob(os.path.join(data_dir, '*.xml'))
    img_list = list()
    bboxs = list()
    obj_ids = list()
    # imgs = np.zeros([4, 3, 189, 267])
    # num = 0
    for ann_file in anno_files:
        ret = getBBoxData(anno_file=ann_file, classes=classes, data_dir=data_dir)
        print(ret['img_arr'].shape)
        img_list.append(ret['img_arr'])
        # imgs[num] = ret['img_arr']
        bboxs.append(ret['bboxs'])
        obj_ids.append(ret['obj_ids'])

    imgs = np.array(img_list)
    return (imgs, bboxs, obj_ids)

imgs, bboxs, obj_ids = getBBoxDataSet(data_dir=data_dir, classes=classes)

np.save(os.path.join(data_dir, 'images.npy'), imgs)
np.save(os.path.join(data_dir, 'bounding_box_data.npy'), bboxs)
np.save(os.path.join(data_dir, 'object_ids.npy'), obj_ids)
</code></pre><h2 id="学習">学習</h2>
<p>以下のコードで実行</p>
<pre><code class="language-learn.py:" data-lang="learn.py:">import os
import numpy as np
import chainer
import random
from chainercv.chainer_experimental.datasets.sliceable import TupleDataset
from chainercv.links import FasterRCNNVGG16
from chainercv.links.model.faster_rcnn import FasterRCNNTrainChain
from chainer.datasets import TransformDataset
from chainercv import transforms
from chainer import training
from chainer.training import extensions

HOME = './'

data_dir = os.path.join(HOME, './fish/res_images')
file_img_set = os.path.join(data_dir, 'images.npy')
file_bbox_set = os.path.join(data_dir, 'bounding_box_data.npy')
file_object_ids = os.path.join(data_dir, 'object_ids.npy')
file_classes = os.path.join(data_dir, 'classes.txt')

# データセットの読み込み
imgs = np.load(file_img_set)
bboxs = np.load(file_bbox_set, allow_pickle=True)
objectIDs = np.load(file_object_ids, allow_pickle=True)

# ラベル情報の読み込み
classes = list()
with open(file_classes) as fd:
    for one_line in fd.readlines():
        cl = one_line.split('\n')[0]
        classes.append(cl)

dataset = TupleDataset(('img', imgs), ('bbox', bboxs), ('label', objectIDs))

N = len(dataset)
N_train = (int)(N*0.9)
N_test = N - N_train
print('total:{}, train:{}, test:{}'.format(N, N_train, N_test))

# ネットワーク構築
faster_rcnn = FasterRCNNVGG16(n_fg_class=len(classes), pretrained_model='imagenet')
faster_rcnn.use_preset('evaluate')
model = FasterRCNNTrainChain(faster_rcnn)

# GPUの設定(今回は使用しない)
gpu_id = -1
# chainer.cuda.get_device_from_id(gpu_id).use()
# model.to_gpu()

# 何の手法で最適化するか設定
optimizer = chainer.optimizers.MomentumSGD(lr=0.001, momentum=0.9)
optimizer.setup(model)
optimizer.add_hook(chainer.optimizer_hooks.WeightDecay(rate=0.0005))


# データの用意
class Transform(object):

    def __init__(self, faster_rcnn):
        self.faster_rcnn = faster_rcnn

    def __call__(self, in_data):
        img, bbox, label = in_data
        _, H, W = img.shape
        img = self.faster_rcnn.prepare(img)
        _, o_H, o_W = img.shape
        scale = o_H / H
        bbox = transforms.resize_bbox(bbox, (H, W), (o_H, o_W))

        # horizontally flip
        img, params = transforms.random_flip(
            img, x_random=True, return_param=True)
        bbox = transforms.flip_bbox(
            bbox, (o_H, o_W), x_flip=params['x_flip'])

        return img, bbox, label, scale

idxs = list(np.arange(N))
random.shuffle(idxs)
train_idxs = idxs[:N_train]
test_idxs = idxs[N_train:]

# 学習するためのいろいろな設定
train_data = TransformDataset(dataset[train_idxs], Transform(faster_rcnn))
train_iter = chainer.iterators.SerialIterator(train_data, batch_size=1)
test_iter = chainer.iterators.SerialIterator(dataset[test_idxs], batch_size=1, repeat=False, shuffle=False)

updater = chainer.training.updaters.StandardUpdater(train_iter, optimizer, device=gpu_id)

n_epoch = 20
out_dir = './out'
trainer = training.Trainer(updater, (n_epoch, 'epoch'), out=out_dir)

step_size = 100
trainer.extend(extensions.snapshot_object(model.faster_rcnn, 'snapshot_model.npz'), trigger=(n_epoch, 'epoch'))
trainer.extend(extensions.ExponentialShift('lr', 0.1), trigger=(step_size, 'iteration'))

log_interval = 1, 'epoch'
plot_interval = 1, 'epoch'
print_interval = 1, 'epoch'

trainer.extend(chainer.training.extensions.observe_lr(), trigger=log_interval)
trainer.extend(extensions.LogReport(trigger=log_interval))
trainer.extend(extensions.PrintReport(['iteration', 'epoch', 'elapsed_time', 'lr', 'main/loss', 'main/roi_loc_loss', 'main/roi_cls_loss', 'main/rpn_loc_loss', 'main/rpn_cls_loss', 'validation/main/map', ]), trigger=print_interval)
trainer.extend(extensions.PlotReport(['main/loss'], file_name='loss.png', trigger=plot_interval), trigger=plot_interval)
trainer.extend(extensions.dump_graph('main/loss'))

# 学習trainer.run()
</code></pre><p>As parameters to set here
・Gpu place (I do not use gpu this time)</p>
<pre><code class="language-Python:" data-lang="Python:"># chainer.cuda.get_device_from_id(gpu_id).use()
# model.to_gpu()
</code></pre><p>· At the optimizer</p>
<pre><code class="language-Python:" data-lang="Python:">optimizer = chainer.optimizers.MomentumSGD(lr=0.001, momentum=0.9)
optimizer.setup(model)
optimizer.add_hook(chainer.optimizer_hooks.WeightDecay(rate=0.0005))
</code></pre><p>・Place of study</p>
<pre><code class="language-Python:" data-lang="Python:">n_epoch = 20
step_size = 100
</code></pre><p>Will be.
There are many other things such as <code>batch_size</code> and how many test data to make (<code>N_train = (int)(N*0.9)</code> <code>N_test = N-N_train</code>), but for the time being, the above 3 It&rsquo;s about.</p>
<p>By the way, the learned network is saved in the file as <code>out/snapshot_model.npz</code>.</p>
<h2 id="forecast">Forecast</h2>
<p>I actually tried to recognize Nodoro.
I tried to recognize only those with a score of 0.9 or higher.</p>
<pre><code class="language-Python:" data-lang="Python:">import matplotlib.pyplot as plt
import numpy as np
from PIL import Image
from chainercv.visualizations import vis_bbox
from chainercv.links import FasterRCNNVGG16

#Read label
classes = list()
with open('./fish/classes.txt') as fd:
    for one_line in fd.readlines():
        cl = one_line.split('\n')[0]
        classes.append(cl)

#Load test data
test_data = Image.open('./fish/test.jpg')
test_data = np.asarray(test_data).transpose(2, 0, 1).astype(np.float32)

# Load learned model
pretrain_model ='out/snapshot_model.npz'

# Network construction
model_frcnn = FasterRCNNVGG16(n_fg_class=len(classes), pretrained_model=pretrain_model)

# Forecast
bboxes, labels, scores = model_frcnn.predict([test_data])
predict_result = [test_data, bboxes[0], labels[0], scores[0]]

# Set threshold so that people with a score below 0.9 are not recognized
line = 0.9

#Draw
res = predict_result
fig = plt.figure(figsize=(6, 6))
ax = fig.subplots(1, 1)
vis_bbox(res[0], res[1][res[3]&gt;line], res[2][res[3]&gt;line], res[3][res[3]&gt;line], label_names=classes, ax=ax)
plt.show()
</code></pre><p>Here are the results.</p>
<p><img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/301489/32d35e53-90df-a966-0298-ae4d8a2f5754.png" alt="image.png"></p>
<p>I was able to recognize it properly!
You can also print the number recognized by <code>print(np.sum(labels[0] == 0))</code>.</p>
<h1 id="at-the-end">at the end</h1>
<p>This time, I tried fine-turning with Ayah to detect throat.
It was pretty easy to finish. Next, it&rsquo;s relatively easy to implement because all you have to do is change the image of your throat.
However, in order to actually achieve highly accurate detection and counting, it is difficult to revise from the original network structure and problem settings such as what to do with overlapping parts and how to rotate.
It is difficult to go to the research level and product level, but I think that you could understand that it is relatively easy to go to the point of &ldquo;playing with Deep for the time being&rdquo; through this implementation. think.</p>
<h1 id="reference-site">Reference site</h1>
<p>Most of the time I did this by referring to this site.
<a href="http://chocolate-ball.hatenablog.com/entry/2018/05/23/012449">http://chocolate-ball.hatenablog.com/entry/2018/05/23/012449</a></p>

    </article>
  </div>

  
  
  
  <script>
  window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
  ga('create', 'UA-169005401-1', 'auto');
  ga('send', 'pageview');
  </script>
  <script async src='https://www.google-analytics.com/analytics.js'></script>
  

  

  
<link rel="stylesheet" type="text/css" href="//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.0/cookieconsent.min.css" />
<script src="//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.0/cookieconsent.min.js"></script>
<script>
window.addEventListener("load", function(){
window.cookieconsent.initialise({
  "palette": {
    "popup": {
      "background": "#216942",
      "text": "#b2d192"
    },
    "button": {
      "background": "#afed71"
    }
  }
})});
</script>

</body>

</html>
