<!doctype html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  
  <meta name="generator" content="Hugo 0.72.0" />

  
  <meta name="description" content="Some description">
  

  
  <link rel="apple-touch-icon" sizes="180x180" href="https://memotut.com/apple-touch-icon.png">

  
  <link rel="icon" type="image/png" sizes="32x32" href="https://memotut.com/favicon-32x32.png">

  
  <link rel="icon" type="image/png" sizes="16x16" href="https://memotut.com/favicon-16x16.png">

  
  <link rel="manifest" href="https://memotut.com/site.webmanifest">

  
  <link rel="mask-icon" href="https://memotut.com/safari-pinned-tab.svg" color="#5bbad5">

  <meta name="msapplication-TileColor" content="#da532c">

  <meta name="theme-color" content="#ffffff">

  
  <link rel="stylesheet" href="https://memotut.com/css/bootstrap.min.css" />

  
  <title>[Python] Machine learning on a gaming PC and comparing the differences in CPU/GPU performance with Colabo [Windows machine learning environment construction procedure definitive version. TF2.0 compatible] | Memo Tut</title>
  

  <style>
body {
  min-width: 300px;
}

.custom-navbar {
  margin-bottom: 1em;
  height: 60px;
}

.custom-navbar a {
  display: inline-block; 
  padding: 18px 0;
  margin-right: 1em; 
  font-weight: bold; 
}

.custom-navbar a:hover,
.custom-navbar a:focus {
  text-decoration: none; 
}

@media print {
  .custom-navbar {
    display: none;
  }
}

article {
  padding-bottom: 1em;
}

img {
  max-width: 100%;
}


body {
  background-color: #fff;
}



body {
  color: #212529;
}



a {
  color: #007bff;
}



a:hover,
a:focus {
  color: #0056b3;
}



.custom-navbar {
  background-color: #212529;
}



.custom-navbar a {
  color: rgba(255, 255, 255, 0.75);
}



.custom-navbar a:hover,
.custom-navbar a:focus {
  color: rgba(255, 255, 255, 1);
}



.container {
  max-width: 800px;
}



pre {
  display: block;
  padding: 9.5px;
  word-break: break-all;
  word-wrap: break-word;
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 4px;
}

pre code {
  padding: 0;
  font-size: inherit;
  color: inherit; 
  white-space: pre-wrap;
  background-color: transparent;
  border: none;
  border-radius: 0;
}

code {
  padding: 2px 4px;
  color: inherit; 
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 4px;
  font-size: .9em;
}



blockquote,
.blockquote {
  padding: 10px 20px;
  margin: 0 0 20px;
  font-size: 1em;
  border-left: 5px solid #6c757d;
}

</style>
</head>

<body>
  <nav class="custom-navbar">
  <div class="container">
    
    <a href="/">Posts</a>
    
    <a href="/tags/">Tags</a>
    
    <a href="/about/">About</a>
    
    <a href="/index.xml">RSS</a>
    
  </div>
</nav>
  
  <div class="container">
    <article>
      <h1>[Python] Machine learning on a gaming PC and comparing the differences in CPU/GPU performance with Colabo [Windows machine learning environment construction procedure definitive version. TF2.0 compatible]</h1>
<p>
  <small class="text-secondary">
  
  
  Mar 28, 2020
  </small>
  

<small><code><a href="https://memotut.com/tags/python">Python</a></code></small>


<small><code><a href="https://memotut.com/tags/gpu"> GPU</a></code></small>


<small><code><a href="https://memotut.com/tags/machine-learning"> machine learning</a></code></small>


<small><code><a href="https://memotut.com/tags/anaconda"> Anaconda</a></code></small>


<small><code><a href="https://memotut.com/tags/colaboratory"> colaboratory</a></code></small>

</p>
<pre><code># In a nutshell
</code></pre>
<p>Purchase a cheap gaming PC (Windows),
Enable to switch CPU/GPU in virtual environment of Anaconda,
Tensorflow-Running the GPU (v2.0) code,
With 4 patterns including CPU/GPU of Colaboratory,
I compared the performance.
I will summarize the performance comparison results and the Windows version environment construction procedure. Article.</p>
<p>*I often hear that GPU is effective for machine learning</p>
<ul>
<li>Gaming PCs have GPUs and are cheap these days</li>
</ul>
<p>⇒ ** Let&rsquo;s use GPU of gaming PC for machine learning! **</p>
<p>So, I implemented a GPU environment on Windows,
There were many places to get hooked and it was tough.</p>
<p>** Say the gaming PC you want &ldquo;because it&rsquo;s for machine learning!&rdquo; **
**The definitive article that is an excuse to buy. **</p>
<p>~~I will tell you that the difference in GPU performance is the decisive difference in learning time&hellip; ~~</p>
<h1 id="5-options-for-machine-learning-gpu-environment-and-imho">5 options for machine learning GPU environment and IMHO</h1>
<h2 id="ubuntu-on-your-own-pc-with-gpu">ubuntu on your own PC with GPU</h2>
<ul>
<li>Recommended for machine learning, high performance</li>
<li>It tends to be expensive (about 300,000 yen when all are combined)</li>
<li>~~ Greater expense if purchased separately from your usual Windows or Mac ~~</li>
<li>As it is a royal road, it is easy to move.</li>
<li>
<ul>
<li>The pattern to replace from the OS of the gaming PC is also this</li>
</ul>
</li>
</ul>
<h2 id="google-colaboratory">Google Colaboratory</h2>
<ul>
<li>Widely applicable to beginners to advanced users</li>
<li>High-performance GPU can be used for free</li>
<li>No need for setup. Convenient connection with Drive</li>
<li>I&rsquo;m worried about resetting every fixed time</li>
<li>Can be used on Windows/Mac/Chromebook from anywhere!</li>
</ul>
<h2 id="nvidia-jetson-nano">NVIDIA Jetson Nano</h2>
<ul>
<li>Intermediate and above. Or even a beginner&rsquo;s trial is interesting</li>
<li>Low price (about 20,000 yen), but performance is also reasonable</li>
<li>Be careful that the upper limit of learning is different with the upper limit of GPU memory</li>
<li>Only the main body is cheap, but it is troublesome to arrange peripheral devices from 0</li>
<li>Select this if you want to use it as an IoT device</li>
<li>(I think it is more for Predict than for learning)</li>
</ul>
<h2 id="gpu-version-cloud-instance-or-service-usage">GPU version cloud instance or service usage</h2>
<ul>
<li>It&rsquo;s pay-as-you-go, so it&rsquo;s different for the beginner</li>
<li>Be careful of cloud bankruptcy when using it as an execution environment</li>
<li>When used as a simple development environment, it tends to be locked on.
*Varies by service</li>
</ul>
<h2 id="gaming-notebook-pc">Gaming (notebook) PC</h2>
<ul>
<li>Existence like a dark horse. How do you normally see Windows?</li>
<li>The price of the chassis is sold as a set, so it is cheap if compared with the same performance</li>
<li>About 100,000 to 300,000 yen?</li>
<li>If you use it for purposes other than machine learning, select this</li>
<li>~~ Or, if you use it for games, there is only 100% this ~~</li>
</ul>
<h1 id="why-do-you-do-it-with-a-gaming-laptop-">Why do you do it with a gaming laptop? ?</h1>
<p>~~ Personally, it is a notebook type rather than a desktop, another purpose machine ~ ~
~~ The answer comes out that it will be diverted to machine learning ~ ~
Playful that I want to try it when I buy a PC.
Especially expensive machine learning machines are not required because machine learning is not so important.</p>
<p>Purchase the lowest priced gaming notebook PC (about 100,000 yen),
When I try it for machine learning,
What kind of usability will it have? I tried after studying.</p>
<p>Colaboratory is great, but depending on the application
It was also considered that a local environment that could last for 12 hours would be useful.</p>
<h1 id="purpose-of-this-article">Purpose of this article</h1>
<p>I felt like there were many pioneers,
It was unexpectedly difficult, so I will summarize it.</p>
<p>Machine learning ⇒ GPU
GPU ⇒ Gaming PC
Many people have the image.</p>
<p>Recently, gaming PCs have become cheaper,
I bought it for machine learning, but I can not divert it,
I wonder if there are many people who think that.
Or, since machine learning is important in the future, when buying a PC,
Is it better to have a GPU? Some people may be wondering.
In fact, notebook PCs with a little GPU are also increasing.</p>
<p>However, the impression that there is very little information that links these three as in this paper.</p>
<p>Of course, advanced machine learning people often talk on ubuntu,
On the other hand, such as trying to convert the gaming PC
The undelivered beginners, including myself,
Building an environment on Windows is likely to be addictive and difficult.</p>
<p>Also, the environment of machine learning is changing drastically,
The information of Tensorflow2 system is still much less than that of 1 system.</p>
<p>So, like this article
Windows x GPU x Tensorflow2.0 x Anaconda
I thought it would be good to summarize the environment construction procedure of.
Also, including performance comparison with Colaboratory,
There are many information and codes that can be used in other environments.</p>
<p>It was more difficult than I had expected, so I hope it helps someone who is also addicted.
Article as information exchange for beginners.</p>
<h1 id="there-are-serious-miscalculations-before-starting">There are serious miscalculations before starting</h1>
<p>Building a machine learning environment
The reputation that Docker is easy on ubuntu,
&ldquo;WSL2&rdquo; makes it easy to run Docker on Windows,
I heard that, maybe it&rsquo;s quite easy to make? I was misunderstanding.</p>
<p>In fact, <strong>WSL2 does not support GPU devices as of March 2020</strong>.
Therefore, the GPU usage environment needs to be built on Windows.</p>
<p>However, the Python environment built from WSL2 on Windows
Since it can be called and executed, it can be simulated on Windows
It can be used like ubuntu or Docker environment.
*This article does not describe WSL2.</p>
<p>This time it was difficult to build on Windows after all.</p>
<p>If you are going to buy a machine,
Keeping the GPU made by NVIDIA
Note that it is almost mandatory in terms of information volume.
NVIDIA&rsquo;s main GPU performance for laptops is
The following feelings in order from the good one.
~~ First of all, it is difficult for beginners to understand this ~~</p>
<ul>
<li>GeForce RTX 2080</li>
<li>GeForce RTX 2070</li>
<li>GeForce GTX 1070</li>
<li>GeForce GTX 1660 Ti</li>
<li>GeForce RTX 2060</li>
<li>GeForce GTX 1080</li>
<li>GeForce GTX 1060</li>
<li>GeForce GTX 1650</li>
<li>GeForce GTX 1050 Ti</li>
<li>GeForce MX 350</li>
<li>GeForce GTX 1050</li>
<li>GeForce MX 250</li>
</ul>
<p>Reference: <a href="https://pcfreebook.com/article/459993300.html">https://pcfreebook.com/article/459993300.html</a></p>
<p>#Version information of the environment constructed this time</p>
<ul>
<li>Windows10 Home</li>
<li>GPU = GTX1650</li>
<li>Anaconda Python = 3.7</li>
<li>tensorflow-gpu = 2.0.0</li>
<li>CUDA = 10.0</li>
<li>cuDNN = v7.6.5(cudnn-10.0-windows10-x64-v7.6.5.32)</li>
</ul>
<p>#Overview of the procedure for building an environment</p>
<ul>
<li>How to determine the version of each library</li>
<li>Introduction of Anaconda / How to set up virtual environment and Jupyter</li>
<li>Build Python virtual environment (CPU version)</li>
<li>Flow sample code of Tensorflow 2.0 on CPU version</li>
<li>Construction of Python virtual environment (GPU version)</li>
<li>CUDA, cuDNN, driver installation</li>
<li>Flow sample code of Tensorflow 2.0 in GPU version</li>
<li>(The story that an error occurred and it was difficult)</li>
<li>Performance comparison including CPU/GPU version of Colaboratory</li>
</ul>
<p>#How to determine the version (most important)</p>
<p>tensorflow-gpu, CUDA, cuDNN,
The three versions need to match perfectly.
If you fail to do this, you will suffer from a storm of unexplained errors such as</p>
<pre><code class="language-py:" data-lang="py:">UnknownError: Failed to get convolution algorithm.This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.
</code></pre><p>Check the official Tensorflow website.
<a href="https://www.tensorflow.org/install/source_windows">https://www.tensorflow.org/install/source_windows</a></p>
<p>At the time of 2020/03,
Although Tensorflow 2.1.0 is the latest,
In the tested build configuration (Windows version),
Since 2.0.0 was the latest, we adopted 2.0.0.
Since the Tensorflow code has many differences between the 1st and 2nd series,
Depending on the code you want to execute, you should choose 1 series and 2 series.</p>
<p>However, I believed in the following description and was in pain.</p>
<pre><code class="language-sh:" data-lang="sh:">Version Python version Compiler build tool cuDNN CUDA
tensorflow_gpu-2.0.0 3.5-3.7 MSVC 2017 Bazel 0.26.1 7.4 10
</code></pre><p>The correct answer is that the version of cuDNN is 7.6 for CUDA 10.0.</p>
<p>For the 7.4 system, the above-mentioned meaningless error occurs.
And this error has multiple possible causes,
Since it often occurs in cases other than version differences,
The difficulty of identifying the cause is high.</p>
<p>Reference: <a href="https://github.com/tensorflow/tensorflow/issues/24496">https://github.com/tensorflow/tensorflow/issues/24496</a>
*Furthermore, change the code of the Tensorflow official website
Even if you move it as it is, it is in the above issues
/&gt; A lot of devils need to be added.</p>
<h1 id="install-anaconda-python-37-version">Install Anaconda Python 3.7 version</h1>
<p><a href="https://www.anaconda.com/distribution/#download-section">https://www.anaconda.com/distribution/#download-section</a></p>
<p>Basically, proceed with the installation with the default settings.</p>
<h1 id="using-condas-python-virtual-environment">Using Conda&rsquo;s Python virtual environment</h1>
<p>I want to use the CPU version/GPU version easily,
Not only for applications such as wanting to try another version,
Including the ease of retrying when building the environment,
It is good to proceed with the construction using a Python virtual environment.</p>
<p>Operate as follows on Anaconda Comand Prompt.</p>
<pre><code class="language-sh:AnacondaComandPrompt" data-lang="sh:AnacondaComandPrompt"># Create a virtual environment MyEnvName with the following command.
conda create -n MyEnvName python=3.7
# Activate the virtual environment with the following command.
conda activate MyEnvName
# ⇒ &quot;(base)&quot; on the left side of the command line
# It can be confirmed that it changes to “(MyEnvName)”.

# You can check the current virtual environment list with the following command.
conda env list
# You can delete the target virtual environment with the following command.
conda remove -n MyEnvName --all
# You can also see it from the menu on the left side of Anaconda Navigator.
</code></pre><p>For commands other than the above, see the following for details.
<a href="https://qiita.com/naz_/items/84634fbd134fbcd25296">https://qiita.com/naz_/items/84634fbd134fbcd25296</a></p>
<h1 id="how-to-connect-to-the-target-virtual-environment-from-jupyter">How to connect to the target virtual environment from Jupyter</h1>
<p>Finally, after communicating on Anaconda Comand Prompt,
It is better to configure Jupyter to use the virtual environment.Therefore, this procedure should be performed last.
(However, even if you execute it together with the creation of a virtual environment,
(Because there are many, write it directly under the virtual environment creation)</p>
<p>Use the command below to make the virtual environment visible from Jupyter side.</p>
<pre><code class="language-sh:AnacondaComandPrompt" data-lang="sh:AnacondaComandPrompt">#ipython(Jupyter) is in the base, so
# Return to base once with the following command.
conda activate base
# Set the virtual environment name with --name and the display name with --display-name.
ipython kernel install --user --name=MyEnvName --display-name=MyEnvName
# ⇒ Jupyter connection settings are created in the folder below. (Show hidden folders)
# C:\Users\[username]\AppData\Roaming\jupyter\kernels
</code></pre><p>At first glance you can connect to the virtual environment from the Jupyter side,
Actually, the following setting file is incorrect, so it needs to be corrected.
Change the startup path of python.exe to the virtual environment side of Anaconda.</p>
<p>◇ Target file:
≪<code>C:\Users\[user name]\AppData\Roaming\jupyter\kernels\MyEnvName\kernel.json</code>
Before correction:
<code>&quot;C:\\Users\\[user name]\\anaconda3\\python.exe&quot;,</code>
◇ After correction:
<code>&quot;C:\\Users\\[user name]\\Anaconda3\\envs\\MyEnvName\\python.exe&quot;, </code></p>
<p>Furthermore, of the call argument described in <code>kernel.json</code>,
It is necessary to add <code>ipykernel_launcher</code> to the virtual environment side.
Run the following in Anaconda Comand Prompt.</p>
<pre><code class="language-sh:AnacondaComandPrompt" data-lang="sh:AnacondaComandPrompt">conda activate MyEnvName
pip install ipykernel
</code></pre><p>Reference: <a href="https://qiita.com/howahowa/items/480607a06264426f24ed">https://qiita.com/howahowa/items/480607a06264426f24ed</a></p>
<p>If you start Anaconda Navigator ⇒ Jupyter Notebook in this state,
When creating a &ldquo;New&rdquo; or new notebook,
In addition to &ldquo;Python3&rdquo;, &ldquo;MyEnvName&rdquo; is increasing as an option.</p>
<p>Or for ipynb already created in Python3,
If you select Kernel ⇒ Change kernel,
You can change the kernel to MyEnvName.</p>
<p>If you select kernel from Jupyter,
Anaconda Comand Prompt with virtual environment selection execution
If the result is different,
Run the script below to see what Python you are using
Check the path of the library etc.</p>
<pre><code class="language-py:" data-lang="py:">import sys
print(sys.prefix)
print(sys.path)
</code></pre><p>Also, in Jupyter, just closing the browser
The process still remains behind the scenes,
Check the running processes from the &ldquo;running&rdquo; tab,
Shutdown everything once,
When Jupyter itself is also terminated and restarted,
In many cases, the settings are reflected properly and the operation becomes correct.</p>
<p>Usually, GPU seems to use the device as &ldquo;occupied type&rdquo;,
So as not to leave other GPU execution process
It&rsquo;s better to shut down as appropriate.</p>
<p>#Install Tensorflow (CPU version)</p>
<p>First, I used only the CPU
Build a virtual environment for Tensorflow execution and communicate.</p>
<pre><code class="language-sh:AnacondaComandPrompt" data-lang="sh:AnacondaComandPrompt">conda create -n cpuenv python=3.7
conda activate cpuenv
pip install tensorflow==2.0.0
</code></pre><p>Remark: If you do not mix pip and conda,
▽ Anaconda can manage all packages with pip.
(I don&rsquo;t think there is a big difference between the two)
Note that the Tensorflow official website is pip compliant.
After introducing Tensorflow, if you see the difference in conda list and pip list,
Conda is a tensorflow system dependent library
I feel like I&rsquo;m trying my best to install it,
The trouble does not change.
In this paper, the basics are implemented on the pip side, but if you implement conda somewhere,
It is better to unify everything with conda.</p>
<p>#Tensorflow 2.0 advanced tutorial code execution (CPU version)</p>
<p>Prepare a one-shot execution code that can be completed and executed with only one file.
Official Tensofrflow &ldquo;Introduction to TensorFlow 2.0 for Experts&rdquo;
Connect the codes of, insert the code for time measurement,
Execute it in a single file as follows.</p>
<p>Reference: <a href="https://www.tensorflow.org/tutorials/quickstart/advanced">https://www.tensorflow.org/tutorials/quickstart/advanced</a></p>
<p>If you have already set up Jupyter Notebook,
Paste the following code directly into one cell and execute, but OK</p>
<pre><code class="language-sh:AnacondaComandPrompt" data-lang="sh:AnacondaComandPrompt">python tensorflow-tutorial-ex.py
#&gt; ～～ Omitting results in the middle～
#&gt; The execution time of this CPU version is as follows.
# &gt;Execution time: 143.45523118972778 [sec]
</code></pre><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py:tensorflow-tutorial-ex.py" data-lang="py:tensorflow-tutorial-ex.py"><span style="color:#75715e">#https://www.tensorflow.org/tutorials/quickstart/advanced</span>
<span style="color:#75715e">#Tensofrflow Official &#34;Introduction to TensorFlow 2.0 for Experts&#34;</span>

<span style="color:#f92672">from</span> __future__ <span style="color:#f92672">import</span> absolute_import, division, print_function, unicode_literals
<span style="color:#f92672">import</span> tensorflow <span style="color:#f92672">as</span> tf
<span style="color:#f92672">from</span> tensorflow.keras.layers <span style="color:#f92672">import</span> Dense, Flatten, Conv2D
<span style="color:#f92672">from</span> tensorflow.keras <span style="color:#f92672">import</span> Model

<span style="color:#75715e">## UnknownError: Failed to get convolution algorithm.This is probably because cuDNN failed to initialize</span>
<span style="color:#75715e"># gpu_devices = tf.config.experimental.list_physical_devices(&#39;GPU&#39;)</span>
<span style="color:#75715e"># for device in gpu_devices: tf.config.experimental.set_memory_growth(device, True)</span>

Load <span style="color:#f92672">and</span> prepare the <span style="color:#75715e">#MNIST dataset.</span>
mnist <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>datasets<span style="color:#f92672">.</span>mnist

<span style="color:#75715e">#Add to measure the execution time after downloading.</span>
<span style="color:#f92672">import</span> time
start_time <span style="color:#f92672">=</span> time<span style="color:#f92672">.</span>time()

(x_train, y_train), (x_test, y_test) <span style="color:#f92672">=</span> mnist<span style="color:#f92672">.</span>load_data()
x_train, x_test <span style="color:#f92672">=</span> x_train <span style="color:#f92672">/</span> <span style="color:#ae81ff">255.0</span>, x_test <span style="color:#f92672">/</span> <span style="color:#ae81ff">255.0</span>

<span style="color:#75715e"># Add a channels dimension</span>
x_train <span style="color:#f92672">=</span> x_train[<span style="color:#f92672">...</span>, tf<span style="color:#f92672">.</span>newaxis]
x_test <span style="color:#f92672">=</span> x_test[<span style="color:#f92672">...</span>, tf<span style="color:#f92672">.</span>newaxis]

<span style="color:#75715e">#Use tf.data to shuffle and batch the dataset.</span>
train_ds <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>data<span style="color:#f92672">.</span>Dataset<span style="color:#f92672">.</span>from_tensor_slices(
    (x_train, y_train))<span style="color:#f92672">.</span>shuffle(<span style="color:#ae81ff">10000</span>)<span style="color:#f92672">.</span>batch(<span style="color:#ae81ff">32</span>)
test_ds <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>data<span style="color:#f92672">.</span>Dataset<span style="color:#f92672">.</span>from_tensor_slices((x_test, y_test))<span style="color:#f92672">.</span>batch(<span style="color:#ae81ff">32</span>)

Create a model using the <span style="color:#75715e">#Keras model subclassing API and create tf.keras.</span>
<span style="color:#66d9ef">class</span> <span style="color:#a6e22e">MyModel</span>(Model):
  <span style="color:#66d9ef">def</span> __init__(self):
    super(MyModel, self)<span style="color:#f92672">.</span>__init__()
    self<span style="color:#f92672">.</span>conv1 <span style="color:#f92672">=</span> Conv2D(<span style="color:#ae81ff">32</span>, <span style="color:#ae81ff">3</span>, activation<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;relu&#39;</span>)
    self<span style="color:#f92672">.</span>flatten <span style="color:#f92672">=</span> Flatten()
    self<span style="color:#f92672">.</span>d1 <span style="color:#f92672">=</span> Dense(<span style="color:#ae81ff">128</span>, activation<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;relu&#39;</span>)
    self<span style="color:#f92672">.</span>d2 <span style="color:#f92672">=</span> Dense(<span style="color:#ae81ff">10</span>, activation<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;softmax&#39;</span>)

  <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">call</span>(self, x):
    x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>conv1(x)
    x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>flatten(x)
    x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>d1(x)
    <span style="color:#66d9ef">return</span> self<span style="color:#f92672">.</span>d2(x)

<span style="color:#75715e">#Create model instance</span>
model <span style="color:#f92672">=</span> MyModel()

<span style="color:#75715e"># Choose an optimizer and loss function for training.</span>
loss_object <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>losses<span style="color:#f92672">.</span>SparseCategoricalCrossentropy()

optimizer <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>optimizers<span style="color:#f92672">.</span>Adam()

<span style="color:#75715e"># Select metrics to measure model loss and accuracy. These metrics aggregate the values for each epoch and output the final result.</span>

train_loss <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>metrics<span style="color:#f92672">.</span>Mean(name<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;train_loss&#39;</span>)
train_accuracy <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>metrics<span style="color:#f92672">.</span>SparseCategoricalAccuracy(name<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;train_accuracy&#39;</span>)

test_loss <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>metrics<span style="color:#f92672">.</span>Mean(name<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;test_loss&#39;</span>)
test_accuracy <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>metrics<span style="color:#f92672">.</span>SparseCategoricalAccuracy(name<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;test_accuracy&#39;</span>)

Use <span style="color:#75715e"># tf.GradientTape to define the function that trains the model.</span>
<span style="color:#a6e22e">@tf.function</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">train_step</span>(images, labels):
  <span style="color:#66d9ef">with</span> tf<span style="color:#f92672">.</span>GradientTape() <span style="color:#66d9ef">as</span> tape:
    predictions <span style="color:#f92672">=</span> model(images)
    loss <span style="color:#f92672">=</span> loss_object(labels, predictions)
  gradients <span style="color:#f92672">=</span> tape<span style="color:#f92672">.</span>gradient(loss, model<span style="color:#f92672">.</span>trainable_variables)
  optimizer<span style="color:#f92672">.</span>apply_gradients(zip(gradients, model<span style="color:#f92672">.</span>trainable_variables))

  train_loss(loss)
  train_accuracy(labels, predictions)

<span style="color:#75715e"># Define a function to test the model.</span>
<span style="color:#a6e22e">@tf.functiondef</span> test_step(images, labels):
  predictions <span style="color:#f92672">=</span> model(images)
  t_loss <span style="color:#f92672">=</span> loss_object(labels, predictions)

  test_loss(t_loss)
  test_accuracy(labels, predictions)

EPOCHS <span style="color:#f92672">=</span> <span style="color:#ae81ff">5</span>

<span style="color:#66d9ef">for</span> epoch <span style="color:#f92672">in</span> range (EPOCHS):
  <span style="color:#66d9ef">for</span> images, labels <span style="color:#f92672">in</span> train_ds:
    train_step(images, labels)

  <span style="color:#66d9ef">for</span> test_images, test_labels <span style="color:#f92672">in</span> test_ds:
    test_step(test_images, test_labels)

  template <span style="color:#f92672">=</span><span style="color:#e6db74">&#39;Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}&#39;</span>
  <span style="color:#66d9ef">print</span> (template<span style="color:#f92672">.</span>format(epoch<span style="color:#f92672">+</span><span style="color:#ae81ff">1</span>,
                         train_loss<span style="color:#f92672">.</span>result(),
                         train_accuracy<span style="color:#f92672">.</span>result()<span style="color:#f92672">*</span><span style="color:#ae81ff">100</span>,
                         test_loss<span style="color:#f92672">.</span>result(),
                         test_accuracy<span style="color:#f92672">.</span>result()<span style="color:#f92672">*</span><span style="color:#ae81ff">100</span>))
  
  <span style="color:#75715e">#Reset metrics for next epoch</span>
  train_loss<span style="color:#f92672">.</span>reset_states()
  train_accuracy<span style="color:#f92672">.</span>reset_states()
  test_loss<span style="color:#f92672">.</span>reset_states()
  test_accuracy<span style="color:#f92672">.</span>reset_states()

<span style="color:#75715e"># Output the measurement result</span>
tat_time <span style="color:#f92672">=</span> time<span style="color:#f92672">.</span>time()<span style="color:#f92672">-</span>start_time
<span style="color:#66d9ef">print</span> (<span style="color:#e6db74">&#34;Execution time: {0}&#34;</span><span style="color:#f92672">.</span>format(tat_time) <span style="color:#f92672">+</span> <span style="color:#e6db74">&#34;[seconds]&#34;</span>)
</code></pre></div><p>#Install Tensorflow (GPU version)</p>
<p>Other than adding &ldquo;-gpu&rdquo; to the library name to install,
All steps are the same as the CPU version.</p>
<pre><code class="language-sh:AnacondaComandPrompt" data-lang="sh:AnacondaComandPrompt">conda create -n gpuenv python=3.7
conda activate gpuenv
pip install tensorflow-gpu==2.0.0
</code></pre><p>However, if you execute the above code in this state,
For example, the following error occurs.</p>
<pre><code class="language-sh:" data-lang="sh:">W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library'cudart64_100.dll'; dlerror: cudart64_100.dll not found
~~~ Omitted~~~
tensorflow.python.framework.errors_impl.InternalError: cudaGetDevice() failed.Status: cudaGetErrorString symbol not found.
</code></pre><p>When such a dll not found error occurs,
The command from Anaconda Prompt is as follows.
You can check if the path is valid.
Since I haven&rsquo;t entered anything yet this time, first install CUDA and cuDNN.</p>
<pre><code class="language-sh:AnacondaComandPrompt" data-lang="sh:AnacondaComandPrompt">where cudart64_100.dll
#&gt; Info: The file with the given pattern was not found.
</code></pre><p>Continue to install CUDA, cuDNN, Nvidia Driver
Let&rsquo;s install and then go back to code execution.</p>
<p>While there is a lot of installation procedure information for old and new mixed / cobblestone mixed,
The following sites are the most helpful sites.
<a href="https://www.kkaneko.jp/tools/win/tensorflow2.html">https://www.kkaneko.jp/tools/win/tensorflow2.html</a>
If an error occurs in the environment construction system, check once.</p>
<h1 id="download-and-install-cuda">Download and install CUDA</h1>
<p>CUDA is developed by NVIDIA,
Development environment for parallel computing by GPU.</p>
<p>Since the latest version is not the same as the latest version,
From the archive sites below
Find the one that matches your version (this time 10.0)
<a href="https://developer.nvidia.com/cuda-toolkit-archive">https://developer.nvidia.com/cuda-toolkit-archive</a></p>
<p>Since it is a file according to the GPU model number of my machine,
Check the GPU device name in advance.
The following files were used in this build.
<code>cuda_10.0.130_411.31_win10.exe</code></p>
<p>At the time of installation, the installation option
Select &ldquo;Custom (Details)&rdquo; instead of &ldquo;High Speed (Recommended)&rdquo;,
From &ldquo;Select Driver Components&rdquo;,
CUDA-Visual Studio Integration
I&rsquo;m a little happy if I uncheck.
(There is no related Visual Studio,
(You don&rsquo;t have to be warned)</p>
<p>There are many installation procedures that require Visual Studio,
Since it is heavy and installation is troublesome,
If you do not recompile the module,
Visual Studio related is not a must.
At worst, you only need to install buildtool.</p>
<h1 id="download-and-install-cudnn">Download and install cuDNN</h1>
<p>cuDNN stands for CUDA Deep Neural Network library.
A library for fast neural network calculations.</p>
<p><a href="https://developer.nvidia.com/cudnn">https://developer.nvidia.com/cudnn</a>
Free and simple user registration is required to download.
After user registration, open the above page again,
Go to cuDNN Download.</p>
<p>The following files were used in this environment
<code>cudnn-10.0-windows10-x64-v7.6.5.32.zip</code></p>
<p>[Important] CUDA version (10.0) and cudnn version (7.6)
Because the file differs depending on the multiplication pattern, check it carefully.
Because the combination for which operation confirmation has been taken differs depending on the version of Tensorflow,
It is necessary to select the one that corresponds to the Tensorflow version (2.0.0) to be used.
Https://www.tensorflow.org/install/source_windows
Although it can be confirmed on the official website above,
It seems that it was necessary to change from 7.4 to 7.6 this time.</p>
<p>When you unzip the downloaded file,
The following is stored under the cuda folder.</p>
<ul>
<li><code>bin/cudnn64_7.dll</code></li>
<li><code>include/cudnn.h</code></li>
<li><code>lib/x64/cudnn.lib</code></li>
<li><code>NVIDIA_SLA_cuDNN_Support.txt</code></li>
</ul>
<p>Copy it to the following folder with the file structure as it is.
<code>C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.0</code></p>
<p>If you don&rsquo;t know which version you are using later,
Open include/cudnn.h with a text editor and write about line 57.</p>
<p>After installation, restart Anaconda Prompot once,
Execute the following command and check the version of cuda again.</p>
<pre><code class="language-sh:AnacondaComandPrompt" data-lang="sh:AnacondaComandPrompt">(gpuenv) C:\Users\[username]&gt;nvcc -V
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2018 NVIDIA Corporation
Built on Sat_Aug_25_21:08:04_Central_Daylight_Time_2018
Cuda compilation tools, release 10.0, V10.0.130
</code></pre><p>The dll that was not able to be acquired previously is also recognized.</p>
<pre><code class="language-sh:AnacondaComandPrompt" data-lang="sh:AnacondaComandPrompt">(gpuenv) C:\Users\[username]&gt;where cudart64_100.dll
C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.0\bin\cudart64_100.dll
</code></pre><p>If not recognized,
Refer to &ldquo;path&rdquo; of system environment variable,
Check if the following path is set.
<code>C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.0\bin</code></p>
<h1 id="nvidia-driver-update-if-needed">Nvidia Driver update (if needed)</h1>
<p>In principle, Nvidia Driver does not matter if a new version is included.
Which version of driver is required for CUDA
There is a list on the official page below.
<a href="https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html">https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html</a></p>
<p>Generally, I think that the latest version of the driver at the time of PC purchase is included, so
It seems that there is a high possibility that this may be omitted.</p>
<p>To check the version,
Open the Windows application NVIDIA Control Panel,
Help ⇒ system information ⇒ components
⇒ Check NVCUDA.DLL.</p>
<p>Product name: NVIDIA CUDA 10.0.132 driver
File version: 25.21.14.1971
41971 on the right side is the driver version.</p>
<p>Alternatively, you can also check with the nvidia-smi command.</p>
<pre><code class="language-sh:AnacondaComandPrompt" data-lang="sh:AnacondaComandPrompt">(gpuenv) C:\Users\[username]&gt;nvidia-smi
+------------------------------------------------- ----------------------------+
NVIDIA-SMI 419.71 Driver Version: 419.71 CUDA Version: 10.0 |
|-------------------------------+----------------- -----+----------------------+
GPU Name TCC/WDDM | Bus-Id Disp.A | Volatile Uncorr. ECC |
| Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. |
|===============================+================== =====+======================|
| 0 GeForce GTX 1650 WDDM | 00000000:01:00.0 Off | N/A |N/A 41C P8 1W / N/A | 134MiB / 4096MiB | 0% Default |
+-------------------------------+----------------- -----+----------------------+

+------------------------------------------------- ----------------------------+
| Processes: GPU Memory |
| GPU PID Type Process name Usage |
|================================================== ============================|
| No running processes found |
+------------------------------------------------- ----------------------------+
</code></pre><p>By hitting the above nvidia-smi command while using the GPU,
Since you can check the running process and memory usage,
It is recommended to hit it appropriately during the following steps.</p>
<h1 id="check-communication-with-gpu-version-tensorflow">Check communication with GPU version Tensorflow</h1>
<p>In Anaconda Prompt, activate the GPU version virtual environment,
Execute commands in order as follows.</p>
<pre><code class="language-sh:AnacondaComandPrompt" data-lang="sh:AnacondaComandPrompt">conda activate gpuenv
python -c &quot;import tensorflow as tf; print(tf.__version__)&quot;
python -c &quot;from tensorflow.python.client import device_lib; print(device_lib.list_local_devices())&quot;
</code></pre><p>As a result, with tensorflow version 2.0.0,
Success if the following GPU device display is displayed.
In the CPU version, only the device_type: &ldquo;CPU&rdquo; is displayed.
If the GPU is recognized, the model number is displayed.</p>
<pre><code class="language-sh:" data-lang="sh:">- Omitted in the middle-
Created TensorFlow device (/device:GPU:0 with 2913 MB memory) -&gt; physical GPU (device: 0, name: GeForce GTX 1650, pci bus id: 0000:01:00.0, compute capability: 7.5)
[name: &quot;/device:CPU:0&quot;
device_type: &quot;CPU&quot;
memory_limit: 268435456
locality {
}
incarnation: 4688799603900704883
, name: &quot;/device:GPU:0&quot;
device_type: &quot;GPU&quot;
memory_limit: 3055235892
locality {
  bus_id: 1
  links {
  }
}
incarnation: 15201502304566343823
physical_device_desc: &quot;device: 0, name: GeForce GTX 1650, pci bus id: 0000:01:00.0, compute capability: 7.5&quot;
]
</code></pre><p>Also, even if it fails, in the above execution log,
I can confirm which dll is failing to call.</p>
<p><del>Thank you so much for building the environment so far</del>
I can see a lot of procedure information saying ~~, ~~
~~ I think it&rsquo;s not good. ~~
~~Unknown Ergo below, and unless you compare the performance ~~
~~ It&rsquo;s hard to tell if the GPU is actually applied. ~~
~~ Also, not only the procedure to proceed smoothly, ~~
~~ Please write the error that occurred and the solution and confirmation method. ~~</p>
<p>#Tutorial code execution (GPU version) (*UnknownError supported)</p>
<p>Immediately, run the CPU version of the communication code in this environment.
<code>python tensorflow-tutorial-ex.py</code></p>
<p>Even if communication with the GPU version is implemented, the following error occurs.</p>
<pre><code class="language-sh:" data-lang="sh:">～～Omitted on the way～～
 W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Unknown: Failed to get convolution algorithm.This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.
         [[{{node my_model/conv2d/Conv2D}}]]
Traceback (most recent call last):
  File &quot;tensorflow-tutorial-ex.py&quot;, line 85, in &lt;module&gt;
    train_step(images, labels)
～～Omitted on the way～～
 line 67, in quick_execute
    six.raise_from(core._status_to_exception(e.code, message), None)
  File &quot;&lt;string&gt;&quot;, line 3, in raise_from
tensorflow.python.framework.errors_impl.UnknownError: Failed to get convolution algorithm.This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.
～～Omitted on the way～～
Function call stack:
train_step
</code></pre><p>This UnknownError is messy,
Similar errors can occur due to multiple factors.
See below for details:
<a href="https://github.com/tensorflow/tensorflow/issues/24496">https://github.com/tensorflow/tensorflow/issues/24496</a></p>
<p>Roughly speaking, it seems that there are the following patterns.</p>
<ul>
<li>Tensofrow, CUDA, cuDNN version mismatch</li>
<li>Incorrect installation procedure, lack of DLL, etc.</li>
<li>When there is a process using GPU in another thread,
Shortage of GPU memory due to second execution (first time succeeds)</li>
<li>Problem in how to specify how to allocate GPU memory (* Only possible to correct the code)</li>
</ul>
<blockquote>
<p>Aside:
This error correction took the longest time.
I also checked the installation procedure, modified it several times and tried again,
Eventually double trouble of version inconsistency &amp; code problem.
Both are based on information from the official Tensorflow website
As it was a part that was implemented, the delay in finding out.
Don&rsquo;t worry too much about the Windows version.
Furthermore, if it is executed from Jupyter even after correction, it is likely to occur again.
&ldquo;When the GPU-using process remains in another thread&rdquo; applies.
A simple running⇒ shutdown process kill
I wonder if there is anything left, at worst I have to restart the PC.
For this reason, we recommend that you do it outside of Jupyter until communication is completed.</p>
</blockquote>
<p>In order to respond only by correcting the code,
Add the following code under the import statement.
(For tensorflow 2 series. Check the reference URL for 1 series)
*Since the code above has already been commented out,
OK if you uncomment</p>
<p>Code added below the ```py:import statement</p>
<h1 id="unknownerror-failed-to-get-convolution-algorithmthis-is-probably-because-cudnn-failed-to-initialize">UnknownError: Failed to get convolution algorithm.This is probably because cuDNN failed to initialize</h1>
<p>gpu_devices = tf.config.experimental.list_physical_devices(&lsquo;GPU&rsquo;)
for device in gpu_devices: tf.config.experimental.set_memory_growth(device, True)</p>
<pre><code>
When executing the sample code related to Keras,
Since we use Tensorflow in the backend,
If you execute similar code at the beginning,
A hindrance that can prevent UnknownError.

If you are using keras, the code for countermeasure of UnknownError is
May be code like below (unconfirmed)

```py:keras version additional code
# Allowing GPU memory growth
#config = tf.ConfigProto() #V1 code
config = tf.compat.v1.ConfigProto()
config.gpu_options.allow_growth = True
#tf.keras.backend.set_session(tf.Session(config=config)) #V1 code
tf.compat.v1.keras.backend.set_session(tf.compat.v1.Session(config=config));
</code></pre><p>As a result of the implementation, this time it ended at the following time.
Execution time: 29.763328075408936 [seconds]</p>
<p>CPU version is about 143 seconds,
Since the GPU version on the GTX1650 is about 30 seconds,
Seems to be much faster.</p>
<p>However, instead of simply getting faster at this rate,
Note that it also depends on the contents of the code to be executed.</p>
<p>For example, using the code for the beginner&rsquo;s quickstart on the official website,
<a href="https://www.tensorflow.org/tutorials/quickstart/beginner">https://www.tensorflow.org/tutorials/quickstart/beginner</a></p>
<p>CPU version: about 14 seconds
GPU version: about 18 seconds
It will be reversed. (There is also a rough handling of initialization time)</p>
<p>The code actually executed is as follows.</p>
<pre><code class="language-py:" data-lang="py:">#https://www.tensorflow.org/tutorials/quickstart/beginner
from __future__ import absolute_import, division, print_function, unicode_literals
import tensorflow as tf

mnist = tf.keras.datasets.mnist#Add to measure the execution time after downloading.
import time
start_time = time.time()

(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0

model = tf.keras.models.Sequential((
  tf.keras.layers.Flatten(input_shape=(28, 28)),
  tf.keras.layers.Dense(128, activation='relu'),
  tf.keras.layers.Dropout(0.2),
  tf.keras.layers.Dense(10, activation='softmax')
])

model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

model.fit(x_train, y_train, epochs=5)

model.evaluate(x_test, y_test, verbose=2)

# Output the measurement result
tat_time = time.time()-start_time
print (&quot;Execution time: {0}&quot;.format(tat_time) + &quot;[seconds]&quot;)
</code></pre><p>In this code, GPU version is also CPU version
It works with exactly the same code, so it is recommended for easy communication.</p>
<p>Then, add another code that can be used for comparison/communication.
A short code for the fashion_mnist classification problem.
As with the previous samples, copy and paste on Jupyter and Colab cells,
I think that it is easy to use because it can be executed as it is as a .py file.</p>
<pre><code class="language-py:fashion_mnist" data-lang="py:fashion_mnist">#Reference: https://github.com/tensorflow/tensorflow/issues/34888
import tensorflow as tf
#UnknownError avoidance
gpu_devices = tf.config.experimental.list_physical_devices('GPU')
for device in gpu_devices: tf.config.experimental.set_memory_growth(device, True)

import numpy as np
print(&quot;Num GPUs Available: &quot;, len(tf.config.experimental.list_physical_devices('GPU'))))
from tensorflow import keras
fashion_mnist = keras.datasets.fashion_mnist
#Add to measure the execution time after downloading.
import time
start_time = time.time()

(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()
train_images = train_images / 255.0
test_images = test_images / 255.0
train_images = np.expand_dims(train_images, axis=3)
test_images = np.expand_dims(test_images, axis=3)

model = tf.keras.Sequential()
model.add(tf.keras.layers.Conv2D(filters=64, kernel_size=2, padding='same', activation='relu', input_shape=(28,28,1)))
model.add(tf.keras.layers.MaxPooling2D(pool_size=2))
model.add(tf.keras.layers.Dropout(0.3))
model.add(tf.keras.layers.Flatten())
model.add(tf.keras.layers.Dense(256, activation='relu'))
model.add(tf.keras.layers.Dropout(0.5))
model.add(tf.keras.layers.Dense(10, activation='softmax'))
model.summary()
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

model.fit(train_images, train_labels, batch_size=64, epochs=10, validation_data=(test_images, test_labels))

# Output the measurement result
tat_time = time.time()-start_time
print (&quot;Execution time: {0}&quot;.format(tat_time) + &quot;[seconds]&quot;)
</code></pre><p>#Comparison result when executed with #Colaboratory</p>
<p>Now when you try to use GPU on Windows like this
It requires a procedure that is quite troublesome and addictive.</p>
<p>If ubuntu, NVIDIA Docker is the shortest,
Unfortunately in WSL1,2 of Windows
At the time of writing this article, I heard that GPU images cannot be used yet,
&ldquo;Commonly packaged gaming PC (Windows)&rdquo;
When trying to use for machine learning with GPU,
This procedure still seems to be working for a while.</p>
<p>*Docker available in WSL ≠ NVIDIA Docker available,
Was a big mistake.</p>
<p>Colaboratory has this GPU-configured environment,
It&rsquo;s available free of charge.</p>
<p>**I feel the greatness of Colaboratory again. **</p>
<p>Incidentally, for reference only,
I tried a simple comparative measurement of execution time.</p>
<h2 id="performance-comparison-result-list"><strong>Performance comparison result list</strong></h2>
<table>
<thead>
<tr>
<th align="left">Environment-CPU/GPU information</th>
<th align="right">Beginner Tutorial</th>
<th align="right">Advanced Tutorial</th>
<th align="right">fashion_mnist</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">CPU(Local) i7-9750H 2.60GHz</td>
<td align="right">About 14 seconds</td>
<td align="right">About 143 seconds</td>
<td align="right">About 289 seconds</td>
</tr>
<tr>
<td align="left">GPU(Local) GTX1650</td>
<td align="right">About 19 seconds</td>
<td align="right">About 29 seconds</td>
<td align="right">About 77 seconds</td>
</tr>
<tr>
<td align="left">CPU(Colab) Xeon(R) 2.20GHz</td>
<td align="right">About 23 seconds</td>
<td align="right">About 291 seconds</td>
<td align="right">About 760 seconds</td>
</tr>
<tr>
<td align="left">GPU(Colab) Tesla P100</td>
<td align="right">About 24 seconds</td>
<td align="right">About 18 seconds</td>
<td align="right">About 46 seconds</td>
</tr>
</tbody>
</table>
<p>Any code used for evaluation is included in this article,
Data set download is also included, so
If you already have an environment, try running it at hand
It may be more fun.</p>
<p>In Colaboratory, the version of tensorflow is
The default is &ldquo;1.15.0&rdquo;,
First, set &ldquo;2.1.0&rdquo; with the following dedicated command
Changed and running.
It is a little different from the local side, but it is the same 2 system and it moved as it is.
However, from March 27, 2020, the default will be 2 series, so
This command is unnecessary after that. (*Conversely, it will be used when you want to make it 1 system)</p>
<pre><code class="language-py:" data-lang="py:">%tensorflow_version 2.x
import tensorflow as tf
print(tf.__version__)
</code></pre><p>In Colaboratory, every time you connect to the runtime,
Since there is unevenness in which specification machine hits,
Write down the information about the closed machine this time.
CPU(Colab) ⇒ Intel(R) Xeon(R) CPU @ 2.20GHz × 2
GPU(Colab) ⇒ Tesla P100-PCIE-16GB (best guy?)
Therefore, the number of seconds of execution time is just a reference value.</p>
<p>Detailed machine spec information can be confirmed with the following command.</p>
<p>`Check py:Colaboratory specs (the lower two are for GPU)
!cat /proc/cpuinfo
!cat /proc/driver/nvidia/gpus/0000:00:04.0/information
!nvidia-smi</p>
<pre><code>
The information on my gaming laptop is as follows.
CPU = Intel(R)Core(TM)i7-9750H CPU 2.60GHz
GPU = GeForce GTX 1650
The lowest entry-level gaming notebook PC as of 2020.
I bought it for about 100,000 yen. It is not the lowest performance because it is better than GTX1050 or MX250.

&gt; Aside:
&gt; Nowadays, even if it's not a game-specific rainbow-colored guy,
&gt; While loading a small GPU such as MX250 or equipped with a GPU,
&gt; A model that is quite thin and is suitable for business like ordinary notebook PCs
&gt; The number is increasing, and even for ordinary notebook PCs in the future,
&gt; There may be more models with a little GPU.
&gt; Even so, I could not think of it until a while ago,
&gt; I feel that gaming laptops are cheaper and thinner.

As a result of performance comparison, after all (unless considering the problem of 12 hours / 90 minutes)
The convenience and performance of Colaboratory (GPU) were outstanding.
Even if you want to use it for a long time, in the near future,
It would be even more convenient if the paid version [Colab Pro](https://colab.research.google.com/signup) could be used in Japan.

&gt; Aside:
&gt; Colab Pro pays about $9.99/month,
&gt; It seems to be usable with faster GPU/longer use/more memory.
&gt; As of March 2020, it is only open to residents of the United States,
&gt; It seems that you cannot register with a Japanese credit card.

#Comments/Conclusion

Colaboratory's GPU is good,
How much is that?
Also, how does the difference in execution time between CPU and GPU appear,
I was able to understand it as a real experience.

** After all, Colaboratory is the strongest feeling for using GPU. **

How to use GPU in Windows environment (gaming notebook PC),
I was able to put together the troubleshooting.
How to switch the virtual environment of Anaconda,
Tensorflow 2.0 one-shot timing execution code may also be useful.

Why don't Mac users buy a **suspicious machine that glows in seven colors**?

**Buy a gaming laptop for machine learning**
**Play a lot of games after studying! ! **

that's all.</code></pre>
    </article>
  </div>

  
  
  
  <script>
  window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
  ga('create', 'UA-169005401-1', 'auto');
  ga('send', 'pageview');
  </script>
  <script async src='https://www.google-analytics.com/analytics.js'></script>
  

  

  
<link rel="stylesheet" type="text/css" href="//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.0/cookieconsent.min.css" />
<script src="//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.0/cookieconsent.min.js"></script>
<script>
window.addEventListener("load", function(){
window.cookieconsent.initialise({
  "palette": {
    "popup": {
      "background": "#216942",
      "text": "#b2d192"
    },
    "button": {
      "background": "#afed71"
    }
  }
})});
</script>

</body>

</html>
