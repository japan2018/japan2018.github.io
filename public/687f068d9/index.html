<!doctype html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  
  <meta name="generator" content="Hugo 0.72.0" />

  
  <meta name="description" content="Some description">
  

  
  <link rel="apple-touch-icon" sizes="180x180" href="https://memotut.com/apple-touch-icon.png">

  
  <link rel="icon" type="image/png" sizes="32x32" href="https://memotut.com/favicon-32x32.png">

  
  <link rel="icon" type="image/png" sizes="16x16" href="https://memotut.com/favicon-16x16.png">

  
  <link rel="manifest" href="https://memotut.com/site.webmanifest">

  
  <link rel="mask-icon" href="https://memotut.com/safari-pinned-tab.svg" color="#5bbad5">

  <meta name="msapplication-TileColor" content="#da532c">

  <meta name="theme-color" content="#ffffff">

  
  <link rel="stylesheet" href="https://memotut.com/css/bootstrap.min.css" />

  
  <title>[Python] [ML-Aents] I tried machine learning using TensorFlow of Unity and Python (v0.11β compatible) | Memo Tut</title>
  

  <style>
body {
  min-width: 300px;
}

.custom-navbar {
  margin-bottom: 1em;
  height: 60px;
}

.custom-navbar a {
  display: inline-block; 
  padding: 18px 0;
  margin-right: 1em; 
  font-weight: bold; 
}

.custom-navbar a:hover,
.custom-navbar a:focus {
  text-decoration: none; 
}

@media print {
  .custom-navbar {
    display: none;
  }
}

article {
  padding-bottom: 1em;
}

img {
  max-width: 100%;
}


body {
  background-color: #fff;
}



body {
  color: #212529;
}



a {
  color: #007bff;
}



a:hover,
a:focus {
  color: #0056b3;
}



.custom-navbar {
  background-color: #212529;
}



.custom-navbar a {
  color: rgba(255, 255, 255, 0.75);
}



.custom-navbar a:hover,
.custom-navbar a:focus {
  color: rgba(255, 255, 255, 1);
}



.container {
  max-width: 800px;
}



pre {
  display: block;
  padding: 9.5px;
  word-break: break-all;
  word-wrap: break-word;
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 4px;
}

pre code {
  padding: 0;
  font-size: inherit;
  color: inherit; 
  white-space: pre-wrap;
  background-color: transparent;
  border: none;
  border-radius: 0;
}

code {
  padding: 2px 4px;
  color: inherit; 
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 4px;
  font-size: .9em;
}



blockquote,
.blockquote {
  padding: 10px 20px;
  margin: 0 0 20px;
  font-size: 1em;
  border-left: 5px solid #6c757d;
}

</style>
</head>

<body>
  <nav class="custom-navbar">
  <div class="container">
    
    <a href="/">Posts</a>
    
    <a href="/tags/">Tags</a>
    
    <a href="/about/">About</a>
    
    <a href="/index.xml">RSS</a>
    
  </div>
</nav>
  
  <div class="container">
    <article>
      <h1>[Python] [ML-Aents] I tried machine learning using TensorFlow of Unity and Python (v0.11β compatible)</h1>
<p>
  <small class="text-secondary">
  
  
  Nov 13, 2019
  </small>
  

<small><code><a href="https://memotut.com/tags/python">Python</a></code></small>


<small><code><a href="https://memotut.com/tags/unity"> Unity</a></code></small>


<small><code><a href="https://memotut.com/tags/machine-learning"> machine learning</a></code></small>


<small><code><a href="https://memotut.com/tags/tensorflow"> TensorFlow</a></code></small>


<small><code><a href="https://memotut.com/tags/ml-agents"> ML-Agents</a></code></small>

</p>
<pre><code>## Introduction
</code></pre>
<p>I couldn&rsquo;t find any Japanese article about v0.11.0, so please take a note.</p>
<p>This article is for beginners.
Unity beginners mimic <a href="https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Learning-Environment-Create-New.md">one of ML-Agents official tutorials</a> do it
It is one of the machine learning, __ which I tried __ Reinforcement Learning.</p>
<p><img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/217271/c9bdcfba-7fd5-af00-caf1-259d9ccb219e.gif" alt="Rollerball.gif">
I will make something like this. :arrow_up:__</p>
<p>For those who know how to operate Unity easily, but who have not done machine learning yet.
Rather than focusing on theory, I introduce you to Zakuri so that you can experience while moving your hands.</p>
<p><b>*This article is as of November 13, 2019. </b>
ML-Agents are constantly being upgraded, so always check the latest information.
~~ <a href="https://www.amazon.co.jp/Unity%E3%81%A7%E3%81%AF%E3%81%98%E3%82%81%E3%82%8B%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92%E3%83%BB%E5%BC%B7%E5%8C%96%E5%AD%A6%E7%BF%92-Unity-ML-Agents%E5%AE%9F%E8%B7%B5%E3%82%B2%E3%83%BC%E3%83%A0%E3%83%97%E3%83%AD%E3%82%B0%E3%83%A9%E3%83%9F%E3%83%B3%E3%82%B0-%E5%B8%83%E7%95%99%E5%B7%9D-%E8%8B%B1%E4%B8%80/dp/4862464181">Book published last year</a> was useless~~
(By the way, this year ⇒ January 2019: *v0.6* ➡ April: *v0.8* ➡ October: *v0.10* ➡ As of November: *v0.11*)</p>
<h1 id="roughly-outline">Roughly outline</h1>
<p>There are indispensable words when doing machine learning with Unity.
There are three __ of &ldquo;Academy&rdquo;, &ldquo;Brain&rdquo;, and &ldquo;Agent&rdquo;.</p>
<p>Basically, &ldquo;Brain&rdquo; controls the action that &ldquo;Agent&rdquo; causes in the environment defined by &ldquo;Academy&rdquo; in Unity.
This time, we will perform reinforcement learning via an external TensorFlow (Python framework), and load the generated neural network model in Unity and execute it.
(Since this is a simple tutorial, I will not touch Academy a lot.)</p>
<p><img src="https://blogs.unity3d.com/wp-content/uploads/2017/09/image4.png" alt="ML-Agents"></p>
<h2 id="major-changes-from-version-0100">Major changes from version 0.10.0</h2>
<p>__If you are new to me, you can skip it. __
I&rsquo;m using v0.8x and v0.9x, but I&rsquo;m not sure if I can&rsquo;t find the Brain Parameters, but I wonder if those who are just looking at it are okay.</p>
<ul>
<li><em>BroadcastHub</em> is abolished.</li>
<li>Abolition of <em>Brain Scriptable Objects</em>. ⇒ Change to <b><em>Behavior Parameters</em></b> item</li>
<li>Major setup changes for <em>Visual Observation</em>.</li>
<li>Renewal of gRPC definition.</li>
<li>Abolition of online BC training.</li>
</ul>
<h2 id="execution-environment">Execution environment</h2>
<p>-Windows10
-Unity 2019.1.4f1
-ML-Agents Beta 0.11.0
-Python 3.6 (Anaconda)</p>
<h2 id="preparation">Preparation</h2>
<p>First, install the following.</p>
<p>-<b><a href="https://store.unity.com/en#plans-individual">Unity5</a></b>(vershouldbefineif2017.4migration)</p>
<p>-<b><a href="https://github.com/Unity-Technologies/ml-agents">ML-Agents v0.11.0</a></b>
-<b><a href="https://www.anaconda.com/distribution/#download-section">Anaconda 2019.10</a></b>(Pleaseselect3.7__for__PythonVersion)</p>
<h2 id="create-project">Create project</h2>
<ol>
<li>
<p>Launch Unity and create a project called <code>RollerBall</code>.</p>
</li>
<li>
<p>*File -&gt; Build Settings&hellip; -&gt; Player Settings&hellip; -&gt; Other Settings -&gt; Configuration *
<em><b>Scripting Runtime Version</b></em> and <em><b>Api Compatibility Level</b></em> are <em><b>.NET 4.x Equivalent</b></em> and <em><b>.NET respectively Make sure it is 4.x</b></em>.
<img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/217271/6dd98e24-7537-b246-a864-70aae8f13ee4.png" alt="YHN.png"></p>
</li>
<li>
<p>Import the ML-Agents assets into your project.
It is in the downloaded <code>ml-agents-master\UnitySDK\Assets</code>
<em>D&amp;D</em> the <code>ML-Agents</code> folder in your project.
<img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/217271/f7c4c4ff-c8a0-fb62-6bcb-529c59296219.png" alt="ooop.png"></p>
</li>
</ol>
<h2 id="stage-creation">Stage creation</h2>
<h3 id="creating-a-floor">Creating a floor</h3>
<p>-Create a plane with <em>3D Object&gt; Plane</em>.
-Name the created <em>Plane</em> as <code>Floor</code>.
-Change the <em>Transform</em> of the <code>Floor</code>
-<code>Position = (0, 0, 0)</code>
-<code>Rotation = (0, 0, 0)</code>
-<code>Scale = (1, 1, 1)</code>
I will.
-<em>Inpector</em> &gt;<em>Materials</em> <em>Element</em> to play around with and make it look like you like.
<img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/217271/5363e781-9210-28e4-abe5-d5c50065f8d6.png" alt="ppppp.png"></p>
<h3 id="create-a-box-target">Create a box (Target)</h3>
<ul>
<li>Create a cube with <em>3D Object &gt;Cube</em>.
-Name the created <em>Cube</em> as <code>Target</code>.</li>
<li>Set the <em>Transform</em> of the <code>Target</code>
-<code>Position = (3, 0.5, 3)</code>
-<code>Rotation = (0, 0, 0)</code>
-<code>Scale = (1, 1, 1)</code>
I will.</li>
<li>Like <code>Floor</code>, change the appearance to your liking.
<img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/217271/c7046fbd-01bd-473c-0b43-55cc3b90ecfa.png" alt="box.png"></li>
</ul>
<h3 id="creating-a-soccer-ball-agent">Creating a soccer ball (Agent)</h3>
<ul>
<li><em>3D Object&gt; Sphere</em> to make a sphere.
-Name the created <em>Sphere</em> a <code>RollerAgent</code>.</li>
<li>Set the <em>Transform</em> of the <code>RollerAgent</code>
-<code>Position = (0, 0.5, 0)</code>
-<code>Rotation = (0, 0, 0)</code>
-<code>Scale = (1, 1, 1)</code>
I will.</li>
<li>As before, change the look to your liking.
If you want it to look like a ball, choose the <code>CheckerSquare</code> material.
-Add <em>Rigidbody</em> from <em>Add Component</em>.
<img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/217271/f8da6cc1-4267-95d0-c27f-9b4d4c62c148.png" alt="kkkkk.png"></li>
</ul>
<h3 id="creating-an-empty-object-academy">Creating an empty object (Academy)</h3>
<ul>
<li>Create an empty <em>GameObject</em> with <em>Create Empty</em>.</li>
<li>Name the created <em>GameObject</em> as <code>Academy</code>.
<img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/217271/474b49fc-f6fe-ca15-a899-ac906dc56286.png" alt="oooiiiii.png"></li>
</ul>
<p>Next, I will describe the contents in C#.</p>
<h2 id="implement-an-academy">Implement an Academy</h2>
<ul>
<li>With <code>Academy</code> still selected in the <em>Hierarchy</em> window, use <em>Add Component -&gt; New Script</em> to create a script called <code>RollerAcademy.cs</code>.</li>
<li>Rewrite the contents of <code>RollerAcademy.cs</code> to the following. You can erase the original contents.</li>
</ul>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-c#:RollerAcademy.cs" data-lang="c#:RollerAcademy.cs"><span style="color:#66d9ef">using</span> MLAgents;
<span style="color:#66d9ef">public</span> <span style="color:#66d9ef">class</span> <span style="color:#a6e22e">RollerAcademy</span> :Academy{}
</code></pre></div><p>In this description,
Basic functions such as &ldquo;observation-decision-action&rdquo; (here omitted) are inherited from <em>Academy</em> class to <em>RollerAcademy</em> class.
So it&rsquo;s okay with two lines.</p>
<h2 id="implement-an-agent">Implement an Agent</h2>
<p>Select the <code>Roller Agent</code> in the <em>Hierarchy</em> window,
Create a script named <code>RollerAgent.cs</code> with <em>Add Component -&gt; New Script</em>.</p>
<h3 id="inheritance-the-base">Inheritance the <em>Base</em></h3>
<p>Rewrite the contents of <code>RollerAgent.cs</code> to the following.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-c#:RollerAgent.cs" data-lang="c#:RollerAgent.cs"><span style="color:#66d9ef">using</span> MLAgents;
<span style="color:#66d9ef">public</span> <span style="color:#66d9ef">class</span> <span style="color:#a6e22e">RollerAgent</span> :Agent{}
</code></pre></div><p>Like <em>Academy</em>, it reads the namespace <em>MLAgents</em> and inherits it by specifying <em>Agent</em> as the base class.</p>
<p>This is the basic procedure for incorporating ML-agents into __Unity.
Next, we will add a mechanism in which the ball by reinforcement learning attacks the box toward the box.</p>
<h3 id="initialization-and-resetting">Initialization and Resetting</h3>
<p>Rewrite the contents of <code>RollerAgent.cs</code> to the following.```c#:RollerAgent.cs
using unityEngine;
using MLAgents;</p>
<p>public class RollerAgent:Agent
{
Rigidbody rBody;
void Start(){
rBody = GetComponent<Rigidbody>();
}</p>
<pre><code>public Transform Target;
public override void AgentReset()
{
    if (this.transform.position.y &lt;0)
    {
        //Rotation acceleration and acceleration reset
        this.rBody.angularVelocity = Vector3.zero;
        this.rBody.velocity = Vector3.zero;
        // Return the agent to the initial position
        this.transform.position = new Vector3( 0, 0.5f, 0)
    }
    // Target relocation
    Target.position = new Vector3(Random.value * 8-4, 0.5f,
                                  Random.value * 8-4);
}
</code></pre>
<p>}</p>
<pre><code>
here,

 -When the `RollerAgent` reaches the box (`Target`), __relocation and initialization to the next __
 -__Return__ when `RollerAgent` falls from the floor (`Floor`)
 
Is being processed.

`Rigidbody` is a component used in physical simulation of Unity.
This time it is used to move the agent.
The values of *Position, Rotation, Scale* are recorded in `Transform`.
By defining it as `public`, you can pass the `Transform` of *Target* in *Inpector*.

### Observing the Environment

Add the following in the class of `RollerAgent.cs`.

```c#
public override void CollectObservations()
{
    // Target and agent location
    AddVectorObs(Target.position);
    AddvectorObs(This.transform.position);

    // agent speed
    AddVectorObs(rBody.velocity.x);
    AddVectorObs(rBody.velocity.z);
}
</code></pre><p>here,
__ Processing to collect observed data as feature vectors __
I am doing.</p>
<p>The 3D coordinates of <em>Target</em> and <em>Agent</em> and the total 8D vector of <em>Agent</em> velocity <em>x</em> and <em>z</em> are passed to the neural network. ~~ The expression of 8 dimensions is really cool~~</p>
<h3 id="actions-and-rewards">Actions and Rewards</h3>
<p>Add processing related to the following <code>AgentAction()</code> function to <code>RollerAgent.cs</code>.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-c#" data-lang="c#"><span style="color:#66d9ef">public</span> <span style="color:#66d9ef">float</span> speed = <span style="color:#ae81ff">10</span>
<span style="color:#66d9ef">public</span> <span style="color:#66d9ef">override</span> <span style="color:#66d9ef">void</span> AgentAction(<span style="color:#66d9ef">float</span>[] vectorAction, <span style="color:#66d9ef">string</span> textAction)
{
    <span style="color:#75715e">// action
</span><span style="color:#75715e"></span>    Vector3 controlSignal = Vector3.zero;
    controlSignal.x = vectorAction[<span style="color:#ae81ff">0</span>];
    controlSignal.z = vectorAction[<span style="color:#ae81ff">1</span>];
    rBody.AddForce(controlSignal * speed);

    
    <span style="color:#75715e">// reward
</span><span style="color:#75715e"></span>    <span style="color:#75715e">//Get the distance to the box (target) from the distance the ball (agent) moves
</span><span style="color:#75715e"></span>   <span style="color:#66d9ef">float</span> distanceToTarget = Vector3.Distance(<span style="color:#66d9ef">this</span>.transform.position,
                                             Target.position);

    <span style="color:#75715e">// When the box (target) is reached
</span><span style="color:#75715e"></span>    <span style="color:#66d9ef">if</span> (distanceToTarget &lt;<span style="color:#ae81ff">1.42f</span>)
    {
        <span style="color:#75715e">// reward and complete
</span><span style="color:#75715e"></span>        SetReward(<span style="color:#ae81ff">1.0f</span>);
        Done();
    }

    <span style="color:#75715e">// If you fall from the floor
</span><span style="color:#75715e"></span>    <span style="color:#66d9ef">if</span> (<span style="color:#66d9ef">this</span>.transform.position.y &lt;<span style="color:#ae81ff">0</span>)
    {
        Done();
    }
}
</code></pre></div><p>here,
__The &ldquo;action&rdquo; of trying to move the agent by reading two types of forces (continuous values) applied in the X and Z directions
The learning algorithm processes __ that the agent gives a reward if it can reach the box safely and picks up the reward if it falls.</p>
<p>The <code>AddForce</code> function is a function for applying physical force to an object that has a <em>Rigidbody</em> component and moving it.
The reward is reset and reset only when the distance less than the reference value for determining whether the target is reached is calculated.</p>
<p>In addition to picking up rewards, punishing statements are also effective for getting sufficient learning in more complex situations. ~~ (It was <code>-1</code> when it fell from the floor at v0, 5x, but it seems that it was judged unnecessary in the latest version) ~~</p>
<h5 id="in-summary-rolleragentscs-looks-like-this">In summary, RollerAgents.cs looks like this:</h5>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-c#:RollerAgents.cs" data-lang="c#:RollerAgents.cs"><span style="color:#66d9ef">using</span> unityEngine;
<span style="color:#66d9ef">using</span> MLAgents;

<span style="color:#66d9ef">public</span> <span style="color:#66d9ef">class</span> <span style="color:#a6e22e">RollerAgent</span>:Agent
{
    Rigidbody rBody;
    <span style="color:#66d9ef">void</span> Start(){
        rBody = GetComponent&lt;Rigidbody&gt;();
    }

    <span style="color:#66d9ef">public</span> Transform Target;
    <span style="color:#66d9ef">public</span> <span style="color:#66d9ef">override</span> <span style="color:#66d9ef">void</span> AgentReset()
    {
        <span style="color:#66d9ef">if</span> (<span style="color:#66d9ef">this</span>.transform.position.y &lt;<span style="color:#ae81ff">0</span>)
        {
            <span style="color:#75715e">//Rotation acceleration and acceleration reset
</span><span style="color:#75715e"></span>            <span style="color:#66d9ef">this</span>.rBody.angularVelocity = Vector3.zero;
            <span style="color:#66d9ef">this</span>.rBody.velocity = Vector3.zero;
            <span style="color:#75715e">// Return the agent to the initial position
</span><span style="color:#75715e"></span>            <span style="color:#66d9ef">this</span>.transform.position = <span style="color:#66d9ef">new</span> Vector3( <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0.5f</span>, <span style="color:#ae81ff">0</span>)
        }
        <span style="color:#75715e">// Target relocation
</span><span style="color:#75715e"></span>        Target.position = <span style="color:#66d9ef">new</span> Vector3(Random.<span style="color:#66d9ef">value</span> * <span style="color:#ae81ff">8</span>-<span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">0.5f</span>,
                                      Random.<span style="color:#66d9ef">value</span> * <span style="color:#ae81ff">8</span>-<span style="color:#ae81ff">4</span>);
    }

    <span style="color:#66d9ef">public</span> <span style="color:#66d9ef">override</span> <span style="color:#66d9ef">void</span> CollectObservations()
    {
        <span style="color:#75715e">// Target and agent location
</span><span style="color:#75715e"></span>        AddVectorObs(Target.position);
        AddvectorObs(This.transform.position);

        <span style="color:#75715e">// agent speed
</span><span style="color:#75715e"></span>        AddVectorObs(rBody.velocity.x);
        AddVectorObs(rBody.velocity.z);
    }

    <span style="color:#66d9ef">public</span> <span style="color:#66d9ef">override</span> <span style="color:#66d9ef">void</span> AgentAction(<span style="color:#66d9ef">float</span>[] vectorAction, <span style="color:#66d9ef">string</span> textAction)
    {
        <span style="color:#75715e">// action
</span><span style="color:#75715e"></span>        Vector3 controlSignal = Vector3.zero;
        controlSignal.x = vectorAction[<span style="color:#ae81ff">0</span>];
        controlSignal.z = vectorAction[<span style="color:#ae81ff">1</span>];
        rBody.AddForce(controlSignal * speed);

    
        <span style="color:#75715e">// reward
</span><span style="color:#75715e"></span>        <span style="color:#75715e">//Get the distance to the box (target) from the distance the ball (agent) moves
</span><span style="color:#75715e"></span>        <span style="color:#66d9ef">float</span> distanceToTarget = Vector3.Distance(<span style="color:#66d9ef">this</span>.transform.position,
                                                 Target.position);

        <span style="color:#75715e">// When the box (target) is reached
</span><span style="color:#75715e"></span>        <span style="color:#66d9ef">if</span> (distanceToTarget &lt;<span style="color:#ae81ff">1.42f</span>)
        {
            <span style="color:#75715e">// reward and complete
</span><span style="color:#75715e"></span>            SetReward(<span style="color:#ae81ff">1.0f</span>);
            Done();
        }

        <span style="color:#75715e">// If you fall from the floor
</span><span style="color:#75715e"></span>        <span style="color:#66d9ef">if</span> (<span style="color:#66d9ef">this</span>.transform.position.y &lt;<span style="color:#ae81ff">0</span>)
        {
            Done();
        }
    }
}
</code></pre></div><h2 id="finish-on-unity-editor">Finish on Unity editor</h2>
<p>-In the <em>Hierarchy</em> window, select <code>RollerAgent</code> and change the two items in <code>RollerAgent(Script)</code>.
<code>Decision Interval = 10</code>
<code>Target = Target(Transform)</code>
<img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/217271/3788c9c1-5603-c443-639f-cde54104b4a3.png" alt="sct.png"></p>
<p>-Add <em>Add Component&gt; Behavior Parameters</em> and change the settings as follows.</p>
<p><code>Behavior Name = RollerBallBrain</code>
<code>Vector Observation Space Size = 8</code>
<code>Vector Action Space Type = Continuous</code>
<code>Vector Action Space Size = 2</code></p>
<p><img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/217271/d00f8d5d-1b9f-5346-d64f-9e27a5f687e4.png" alt="Be.png">Also,accordingto<a href="https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Learning-Environment-Create-New.md">officialdocs</a>, if you use the default parameters as they are, 30 It takes 10 million steps to learn and seems to take time.
This time the learning is not so complicated, so let&rsquo;s rewrite some of the parameters and reduce the number of trials to less than 20,000 steps.</p>
<p>-Open <code>trainer_config.yaml</code> in <em>ml-agents-master-0.11 &gt;config &gt;</em> with an editor (VS code or Notepad) and rewrite the values of the following items.</p>
<p><img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/217271/e79f09f1-ba8b-ac25-3e1f-9509b005200f.png" alt="aaaa.png"></p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#66d9ef">batch_size</span>: <span style="color:#ae81ff">10</span>
<span style="color:#66d9ef">buffer_size</span>: <span style="color:#ae81ff">100</span>
</code></pre></div><p>Now you are ready to train.</p>
<h2 id="manual-test">Manual test</h2>
<p>It&rsquo;s only a few minutes until now.
Before reinforcement learning, let&rsquo;s confirm by hand whether the environment created so far works properly.
Add the following method to the class of <code>RollerAgent.cs</code>.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-c#" data-lang="c#"><span style="color:#66d9ef">public</span> <span style="color:#66d9ef">override</span> <span style="color:#66d9ef">float</span>[] Heuristic()
{
    <span style="color:#66d9ef">var</span> action = <span style="color:#66d9ef">new</span> <span style="color:#66d9ef">float</span>[<span style="color:#ae81ff">2</span>];
    action[<span style="color:#ae81ff">0</span>] = Input.GetAxis(<span style="color:#e6db74">&#34;Horizontal&#34;</span>);
    action[<span style="color:#ae81ff">1</span>] = Input.GetAxis(<span style="color:#e6db74">&#34;Vertical&#34;</span>);
    <span style="color:#66d9ef">return</span> action;
}

</code></pre></div><p>Horizontal (horizontal) input axis with <code>Hotizontal</code>,
Accept vertical (vertical) input axis with <code>Vertical</code>.</p>
<p>Now you can operate with &ldquo;W&rdquo; &ldquo;A&rdquo; &ldquo;S&rdquo; &ldquo;D&rdquo; or arrow keys.</p>
<p>Finally, in the <em>Inspector</em> of the <code>RollerAgent</code>,
Check the <em>Use Heuristic</em> check box under <em>Behavior Parameters</em>.</p>
<p><img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/217271/08a8c612-ad33-930a-0cbb-067e3c11afcb.png" alt="he.png"></p>
<p>Press Play to run it.
If you can confirm that it works with key input, you are successful.</p>
<h2 id="train-with-tensorflow">Train with TensorFlow</h2>
<p>Now it&rsquo;s time to start learning.</p>
<h3 id="environment-constructionlibrary-installation">Environment construction/library installation</h3>
<p>First, launch Anaconda Prompt.
It will come out immediately when you search from the start menu (Win key).
<img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/217271/faa056b1-f10d-615f-998f-d44821c95125.png" alt="an.png"></p>
<pre><code>conda create -n ml-agents python=3.6
</code></pre><p>Enter to build the virtual environment. <sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>
<img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/217271/1739c589-73b7-6fff-9d40-dd4d07897ea0.png" alt="on.png"></p>
<pre><code>Proceed([y]/n)?
</code></pre><p>Enter <code>y</code> when asked if you want to install. continue,</p>
<pre><code>activate ml-agents
</code></pre><p>Enter to move to the virtual environment. <sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>
Make sure you prefix the command line with <code>(ml-agents)</code>.
<img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/217271/fb45672f-f888-85e7-550c-9828d8589569.png" alt="aaaa.png"></p>
<pre><code>cd &lt;ml-agent folder&gt;
</code></pre><p>Go to. <sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup></p>
<pre><code>pip install mlagents
</code></pre><p>Will install the library that ML-Agents uses independently. (It will take a few minutes)
This installation will have dependencies such as TensorFlow/Jupyter.</p>
<p>After a while,
It is OK if a screen like this appears.
<img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/217271/62e5c9cf-ba9f-983e-3585-7135de3a02b7.png" alt="wewe.png"></p>
<pre><code>cd &lt;ml-agents folder&gt;\ml-agents-envs
</code></pre><p>Go to.</p>
<pre><code>pip install -e.
</code></pre><p>Enter to install the package.
<img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/217271/060daa62-421e-5afb-8562-961d5493ff16.png" alt="konnna.png">
It is OK if this screen is displayed.
And</p>
<pre><code>cd &lt;ml-agents folder&gt;\ml-agents
</code></pre><p>Go to.</p>
<pre><code>pip install -e.
</code></pre><p>Enter to install the package.
<img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/217271/6c0d804e-81da-9fe6-b3f2-85fd6c4707d6.png" alt="www.png"></p>
<p>This completes the preparation on the Python side.</p>
<p>__:collision: [Note]: TensorFlowSharp plugin is not used in v0.6.x or later. __ If you were proceeding with reference to old books, we recommend that you create a new virtual environment.</p>
<blockquote>
<p>Until ML-Agents ver0.5.0, TensorFlowSharp was used to communicate with Python, but please do not use it in the latest version. If you use it, the following error will occur.</p>
</blockquote>
<blockquote>
<p>No model was present for the Brain 3DBallLearning.
UnityEngine.Debug:LogError(Object)
MLAgents.LearningBrain:DecideAction() (at Assets/ML-Agents/Scripts/LearningBrain.cs:191)
MLAgents.Brain:BrainDecideAction() (at Assets/ML-Agents/Scripts/Brain.cs:80)
MLAgents.Academy:EnvironmentStep() (at Assets/ML-Agents/Scripts/Academy.cs:601)
MLAgents.Academy:FixedUpdate() (at Assets/ML-Agents/Scripts/Academy.cs:627)</p>
</blockquote>
<blockquote>
<p><a href="https://www.fast-system.jp/unity-ml-agents-version-0-9-0-howto/">(Citation source)</a></p>
</blockquote>
<hr>
<h3 id="reinforcement-learning">Reinforcement Learning</h3>
<p>Now, let&rsquo;s finally start learning. It&rsquo;s almost a dream AI experience. let&rsquo;s do our best.</p>
<pre><code>cd &lt;ml-agents&gt; folder
</code></pre><p>Enter to move to the downloaded folder hierarchy.</p>
<pre><code>mlagents-learn config/trainer_config.yaml --run-id=firstRun --train
</code></pre><p>Run. <sup id="fnref:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup>
<img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/217271/b83e3998-d01c-d3d3-ffb6-754428a5f6ce.png" alt="aaaaw.png">
At the bottom of the command line,
__INFO:mlagents.envs:Start training by pressing the Play button in the Unity Editor.
(Go back to the Unity editor and press the Play button to start training.) __
Confirm that is displayed.</p>
<p>Let&rsquo;s go back to the Unity screen and uncheck <em>Use Heuristic</em> in __<em>Behavior Parameters</em> and press the __ and :arrow_forward: buttons.</p>
<p>When the ball started chasing the box, learning started normally.</p>
<p>__ If you don&rsquo;t press the Play button for a while, a timeout error will occur, so please execute the same command again. __</p>
<p>The log will be output to the console log every 1000 steps.
If you want to interrupt in the middle, you can interrupt with Ctrl+C. (By daring to finish early, &ldquo;weak AI&rdquo; can be made)
<img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/217271/c45f8742-acfa-9d91-c033-408674821109.png" alt="Oh! ! ! ! .png"></p>
<p>__Step is the number of trials (learning), __
__ The average value of the rewards earned by Mean Reward, __
__Std of Reward is the standard deviation __ (value that represents the dispersion of data)
Represents.</p>
<p>After learning, a <code>RollerBallBrain.nn</code> file is created under <code>&lt;ml-agents folder&gt;\models\&lt;id name~&gt;</code>.</p>
<p><img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/217271/5b667d39-c067-8d01-f073-0dd80872454d.png" alt="hyhy.png"></p>
<h1 id="reflect-learning">Reflect learning</h1>
<p>Now it&rsquo;s time to try out the generated neural network model.</p>
<p>Copy the <code>RollerBallBrain.nn</code> file from above to the <em>Assets</em> folder in Unity&rsquo;s Project.
(The location can be anywhere in the project)
<img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/217271/7fd0ca95-b559-72eb-460e-4bba8b962187.png" alt="wwqqqq.png"></p>
<p>Then, click the :radio_button: button at the right end of the <em>Model</em> item in the <em>Inspector</em> of the <code>RollerAgent</code> and select the imported <code>.nn</code> file. (*At this time, please be careful not to confuse if there is a <code>.nn</code> extension file with the same name.)Also, if <em>Use Heuristic</em> of <em>Behavior Parameters</em> is checked, it will not work properly.
__ Be sure to uncheck it when you&rsquo;re done testing. __
<img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/217271/195798ed-a538-0a9e-694c-648cd090e6da.png" alt="aaaqqqq.png"></p>
<p>Now, let&rsquo;s press :arrow_forward: Play.</p>
<p>0 If the ball starts to chase safely, you are successful. __
<img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/217271/4d380e12-c9a9-e5b2-cc6b-f1f9eb4e95c5.gif" alt="30fps.gif"></p>
<h1 id="bonus-observe-the-transition-graph-on-tensorboard">(Bonus) Observe the transition graph on TensorBoard</h1>
<p>In Anaconda Prompt, do the following:</p>
<pre><code>tensorboard --logdir=summaries --port=6006
</code></pre><p>If you open <a href="http://localhost:6006/">localhost:6006</a> in your browser, you can see a graph of __ learning progress.
<img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/217271/883056a1-8900-5e5f-2346-83f6e72f7293.png" alt="Hoho, TensorBoard? (Nichia&hellip;.png"></p>
<p>#Summary</p>
<p>-If you can read more rugged C# __, you can fine tune the algorithm yourself __
-In reinforcement learning, the cleverness of AI can be classified into weak, medium, strong etc. by the number of learning __
-Ver is frequently updated, and __ information is likely to deteriorate __
-~~ After all, learning is much faster than humans. The power of science is amazing! ! ~~</p>
<p>Even beginners can use assets and it becomes a convenient world where you can imitate simple machine learning in one day.
How was it when you actually touched it?
I would be happy if you could get interested in machine learning.</p>
<p>If you find any expressions or typographical errors that interest you, we would appreciate it if you could point it out.
Also, if you find this article helpful, I like it! I will be encouraged to give you __.</p>
<p>Thank you for all the above.</p>
<p>##reference
The following are articles from ancestors who have been very helpful in learning. I would like to take this opportunity to state <strong>Acknowledgment</strong>.</p>
<p><a href="https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Learning-Environment-Create-New.md">Unity-Technologies Official Document (GitHub)</a>
<a href="https://github.com/Unity-Technologies/ml-agents/blob/0.11.0/docs/Migrating.md">ml-agents Migration Guide (GitHub)</a>
<a href="https://www.fast-system.jp/unity-ml-agents-version-0-9-0-howto/">Unity: ML-Agents How to use in September 2019 (ver0.9.0/0.9.1/0.9.2)</a>
<a href="https://qiita.com/VeyronSakai/items/bae72c43f4680a388431">[Unity] I tried a reinforcement learning tutorial (ML-Agents v0.8.1)</a>
[Create a new learning environment with Unity&rsquo;s ML-Agents (0.6.0a version)](<a href="http://am1tanaka.hatenablog.com/entry/2019/01/18/212915#%E5%AD%A6%E7%BF%92%E5%8A%B9%E6%9E%9C%E3%82%92%E9%AB%98%E3%82%81%E3%82%8B%E3%81%8A%E3%81(%BE%E3%81%91)">http://am1tanaka.hatenablog.com/entry/2019/01/18/212915#%E5%AD%A6%E7%BF%92%E5%8A%B9%E6%9E%9C%E3%82%92%E9%AB%98%E3%82%81%E3%82%8B%E3%81%8A%E3%81(%BE%E3%81%91)</a></p>
<section class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1" role="doc-endnote">
<p>*&rdquo; ml-agents&rdquo; * can be changed to any name. <a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2" role="doc-endnote">
<p>Activate with the virtual environment name set by yourself <a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3" role="doc-endnote">
<p>Directory where <em>ml-agents-master</em> is downloaded in <a href="#Preparation">Preparation</a> <a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:4" role="doc-endnote">
<p>*&rdquo; firstRun&rdquo; * can be changed to any name. <a href="#fnref:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</section>

    </article>
  </div>

  
  
  
  <script>
  window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
  ga('create', 'UA-169005401-1', 'auto');
  ga('send', 'pageview');
  </script>
  <script async src='https://www.google-analytics.com/analytics.js'></script>
  

  

  
<link rel="stylesheet" type="text/css" href="//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.0/cookieconsent.min.css" />
<script src="//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.0/cookieconsent.min.js"></script>
<script>
window.addEventListener("load", function(){
window.cookieconsent.initialise({
  "palette": {
    "popup": {
      "background": "#216942",
      "text": "#b2d192"
    },
    "button": {
      "background": "#afed71"
    }
  }
})});
</script>

</body>

</html>
