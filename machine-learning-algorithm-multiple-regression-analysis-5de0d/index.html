<!doctype html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  
  <meta name="generator" content="Hugo 0.72.0" />

  
  <meta name="description" content="Some description">
  

  
  <link rel="apple-touch-icon" sizes="180x180" href="https://memotut.com/apple-touch-icon.png">

  
  <link rel="icon" type="image/png" sizes="32x32" href="https://memotut.com/favicon-32x32.png">

  
  <link rel="icon" type="image/png" sizes="16x16" href="https://memotut.com/favicon-16x16.png">

  
  <link rel="manifest" href="https://memotut.com/site.webmanifest">

  
  <link rel="mask-icon" href="https://memotut.com/safari-pinned-tab.svg" color="#5bbad5">

  <meta name="msapplication-TileColor" content="#da532c">

  <meta name="theme-color" content="#ffffff">

  
  <link rel="stylesheet" href="https://memotut.com/css/bootstrap.min.css" />

  
  <title>Machine learning algorithm (multiple regression analysis) | Memo Tut</title>
  

  <style>
body {
  min-width: 300px;
}

.custom-navbar {
  margin-bottom: 1em;
  height: 60px;
}

.custom-navbar a {
  display: inline-block; 
  padding: 18px 0;
  margin-right: 1em; 
  font-weight: bold; 
}

.custom-navbar a:hover,
.custom-navbar a:focus {
  text-decoration: none; 
}

@media print {
  .custom-navbar {
    display: none;
  }
}

article {
  padding-bottom: 1em;
}

img {
  max-width: 100%;
}


body {
  background-color: #fff;
}



body {
  color: #212529;
}



a {
  color: #007bff;
}



a:hover,
a:focus {
  color: #0056b3;
}



.custom-navbar {
  background-color: #212529;
}



.custom-navbar a {
  color: rgba(255, 255, 255, 0.75);
}



.custom-navbar a:hover,
.custom-navbar a:focus {
  color: rgba(255, 255, 255, 1);
}



.container {
  max-width: 800px;
}



pre {
  display: block;
  padding: 9.5px;
  word-break: break-all;
  word-wrap: break-word;
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 4px;
}

pre code {
  padding: 0;
  font-size: inherit;
  color: inherit; 
  white-space: pre-wrap;
  background-color: transparent;
  border: none;
  border-radius: 0;
}

code {
  padding: 2px 4px;
  color: inherit; 
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 4px;
  font-size: .9em;
}



blockquote,
.blockquote {
  padding: 10px 20px;
  margin: 0 0 20px;
  font-size: 1em;
  border-left: 5px solid #6c757d;
}

</style>
</head>

<body>
  <nav class="custom-navbar">
  <div class="container">
    
    <a href="/">Posts</a>
    
    <a href="/tags/">Tags</a>
    
    <a href="/about/">About</a>
    
    <a href="/index.xml">RSS</a>
    
  </div>
</nav>
  
  <div class="container">
    <article>
      <h1>Machine learning algorithm (multiple regression analysis)</h1>
<p>
  <small class="text-secondary">
  
  
  Feb 14, 2020
  </small>
  

<small><code><a href="https://memotut.com/tags/python">Python</a></code></small>


<small><code><a href="https://memotut.com/tags/machine-learning"> machine learning</a></code></small>


<small><code><a href="https://memotut.com/tags/data-science"> data science</a></code></small>

</p>
<pre><code>#Introduction
</code></pre>
<p>Step-by-step about the theory, the implementation in python, and the analysis using scikit-learn of the algorithm taken up in &ldquo;<a href="https://qiita.com/hiro88hyo/items/c00dcf8f083ba3d76af8">Classification of machine learning</a>&rdquo; Learn with. I wrote it for personal learning, so I would like you to take a look at any mistakes.</p>
<p>This time, I would like to develop a simple linear regression analysis and try &ldquo;multiple regression analysis&rdquo;. I referred to the next page.</p>
<ul>
<li><a href="https://qiita.com/karaage0703/items/f38d18afc1569fcc0418">From Python to Machine Learning &ldquo;Multiple Regression Analysis&rdquo;</a></li>
<li><a href="https://qiita.com/g-k/items/71310ac3a42a7ce25b5d">Understanding Multiple Regression Analysis (Theory)
Machine learning</a></li>
</ul>
<h1 id="basic">Basic</h1>
<p>In the single regression analysis, $A$ and $B$ were obtained to draw an approximate straight line $$y = Ax + B $$ for $N$ $(x, y)$ on the plane. Specifically, the sum of the squares of the differences between the straight line and the $$i$th point $$\sum_{i=1}^{N} (y_i-(Ax+B))^2$$ should be the smallest. Asked for $A$ and $B$.
In multiple regression, the coefficient is calculated when the number of variables (explanatory variables) that was one in single regression is increased.</p>
<p>In other words, if the formula of the straight line is set as $$y=w_0x_0+w_1x_1+\cdots+w_nx_n$$ (assuming $x_0=1$), then $(w_0, w_1, \cdots, w_n)$ should be obtained. Become.</p>
<p>#How to solve multiple regression
From this point, it will be almost the same as the article I referred to, but I will try to write it as easy as possible.</p>
<p>If the straight line formula is in the form of a matrix,</p>
<pre><code class="language-math" data-lang="math">y = \begin{bmatrix} w_0 \\ w_1 \\ w_2 \\ \vdots \\ w_n \end{bmatrix} \begin{bmatrix} x_0, x_1, x_2, \cdots, x_n\end{bmatrix}
</code></pre><p>Becomes ($x_0=1$).
The sum of squares of the difference from the measured value $\hat{y}$ is $$ \sum_{i=1}^{n}(y-\hat{y})^2$$. To go. Note that $(w_0, w_1, \cdots, w_n)$ is $\boldsymbol{w}$, and all explanatory variables are the matrix $\boldsymbol{X}$.</p>
<pre><code class="language-math" data-lang="math">\sum_{i=1}^{n}(y-\hat{y})^2 \\
= (\boldsymbol{y}-\hat{\boldsymbol{y}})^{T}(\boldsymbol{y}-\hat{\boldsymbol{y}}) \\
= (\boldsymbol{y}-\boldsymbol{Xw})^{T}(\boldsymbol{y}-\boldsymbol{Xw}) \\
= (\boldsymbol{y}^{T}-(\boldsymbol{Xw})^{T})(\boldsymbol{y}-\boldsymbol{Xw}) \\
= (\boldsymbol{y}^{T}-\boldsymbol{w}^{T}\boldsymbol{X}^{T})(\boldsymbol{y}-\boldsymbol{Xw}) \\
= \boldsymbol{y}^{T}\boldsymbol{y}-\boldsymbol{y}^{T}\boldsymbol{X}\boldsymbol{w}-\boldsymbol{w}^{T}\boldsymbol{X} ^{T}\boldsymbol{y}-\boldsymbol{w}^{T}\boldsymbol{X}^{T}\boldsymbol{X}\boldsymbol{w} \\
= \boldsymbol{y}^{T}\boldsymbol{y}-2\boldsymbol{y}^{T}\boldsymbol{X}\boldsymbol{w}-\boldsymbol{w}^{T}\boldsymbol{X }^{T}\boldsymbol{X}\boldsymbol{w}
</code></pre><p>Those that are unrelated to $\boldsymbol{w}$ are constants, so $\boldsymbol{X}^{T}\boldsymbol{X}=A$ and $-2\boldsymbol{y}^{T}\boldsymbol{ respectively. If you set X}=B$, $\boldsymbol{y}^{T}\boldsymbol{y}=C$, the least sum of squares $L$ is $$L = CB\boldsymbol{w}-\boldsymbol{ w}^{T}A\bold symbol{w}$$.
Since $L$ is a quadratic function of $\boldsymbol{w}$, $L$ is the minimum, $\boldsymbol{w}$ is a partial derivative of $L$ with $\boldsymbol{w}$ We just need to find $\boldsymbol{w}$ where the expression is 0.</p>
<pre><code class="language-math" data-lang="math">\begin{split}\begin{aligned}
\frac{\partial}{\partial {\boldsymbol{w}}} L
&amp;= \frac{\partial}{\boldsymbol{w}} (C + B\boldsymbol{w} + \boldsymbol{w}^T{A}\boldsymbol{w}) \\
&amp;=\frac{\partial}{\partial {\boldsymbol{w}}} (C) + \frac{\partial}{\partial {\boldsymbol{w}}} ({B}{\boldsymbol{w} }) + \frac{\partial}{\partial {\boldsymbol{w}}} ({\boldsymbol{w}}^{T}{A}{\boldsymbol{w}}) \\
&amp;={B} + {w}^{T}({A} + {A}^{T})
\end{aligned}\end{split}
</code></pre><p>I want to set this to 0,</p>
<pre><code class="language-math" data-lang="math">\boldsymbol{w}^T(A+A^T)=-B \\
\boldsymbol{w}^T(\boldsymbol{X}^{T}\boldsymbol{X}+(\boldsymbol{X}^{T}\boldsymbol{X})^T)=2\boldsymbol{y}^ {T}\boldsymbol{X} \\
\boldsymbol{w}^T\boldsymbol{X}^{T}\boldsymbol{X}=\boldsymbol{y}^T\boldsymbol{X} \\
\boldsymbol{X}^{T}\boldsymbol{X}\boldsymbol{w} = \boldsymbol{X}^T\boldsymbol{y} \\
</code></pre><p>This form is a form of simultaneous linear equations, and if $\boldsymbol{X}^{T}\boldsymbol{X}$ is not regular, the simultaneous equations cannot be solved. It is not regular if some $x$ has a strong correlation, that is, one data column can explain another. This state is called <strong>multicollinearity</strong>, commonly known as multicollinearity.</p>
<p>Assuming it is regular,</p>
<pre><code class="language-math" data-lang="math">(\boldsymbol{X}^{T}\boldsymbol{X})^{-1}\boldsymbol{X}^{T}\boldsymbol{X}\boldsymbol{w}=(\boldsymbol{X}^{T }\boldsymbol{X})^{-1}\boldsymbol{X}^T\boldsymbol{y} \\
\boldsymbol{w}=(\boldsymbol{X}^{T}\boldsymbol{X})^{-1}\boldsymbol{X}^T\boldsymbol{y}
</code></pre><p>Now we have $\boldsymbol{w}$.</p>
<h1 id="ptyhon-to-implement-in-a-straightforward-manner">ptyhon to implement in a straightforward manner</h1>
<p>The data uses diabetes data (diabetes) of scikit-learn. Let&rsquo;s investigate how the target (progress after 1 year) and BMI, S5 (ltg: lamotriogin) data are related.</p>
<h2 id="first-look-at-the-data">First look at the data</h2>
<p>First, let&rsquo;s plot the data. Since there are two explanatory variables and the target, it becomes three-dimensional data. If it exceeds 3 dimensions, the graph will not be applied.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> numpy <span style="color:#f92672">as</span> np
<span style="color:#f92672">import</span> pandas <span style="color:#f92672">as</span> pd
<span style="color:#f92672">import</span> matplotlib.pyplot <span style="color:#f92672">as</span> plt
<span style="color:#f92672">from</span> sklearn <span style="color:#f92672">import</span> datasets
<span style="color:#f92672">from</span> mpl_toolkits.mplot3d <span style="color:#f92672">import</span> Axes3D
<span style="color:#f92672">import</span> seaborn <span style="color:#f92672">as</span> sns

<span style="color:#f92672">%</span>matplotlib inline

diabetes <span style="color:#f92672">=</span> datasets<span style="color:#f92672">.</span>load_diabetes()

df <span style="color:#f92672">=</span> pd<span style="color:#f92672">.</span>DataFrame(diabetes<span style="color:#f92672">.</span>data, columns<span style="color:#f92672">=</span>diabetes<span style="color:#f92672">.</span>feature_names)

fig<span style="color:#f92672">=</span>plt<span style="color:#f92672">.</span>figure()
ax<span style="color:#f92672">=</span>Axes3D(fig)

x1 <span style="color:#f92672">=</span> df[<span style="color:#e6db74">&#39;bmi&#39;</span>]
x2 <span style="color:#f92672">=</span> df[<span style="color:#e6db74">&#39;s5&#39;</span>]
y <span style="color:#f92672">=</span> diabetes<span style="color:#f92672">.</span>target

ax<span style="color:#f92672">.</span>scatter3D(x1, x2, y)
ax<span style="color:#f92672">.</span>set_xlabel(<span style="color:#e6db74">&#34;x1&#34;</span>)
ax<span style="color:#f92672">.</span>set_ylabel(<span style="color:#e6db74">&#34;x2&#34;</span>)
ax<span style="color:#f92672">.</span>set_zlabel(<span style="color:#e6db74">&#34;y&#34;</span>)

plt<span style="color:#f92672">.</span>show()
</code></pre></div><p>The result is the following graph, which looks like a slope.</p>
<img width="424" alt="regression_multi_1.png" src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/18497/61be441c-792c-8bb4-6a6b-ff2f31fb2442.png">
<h2 id="try-to-calculate">Try to calculate</h2>
<p>The formula to find $ \boldsymbol{w} $ was as follows. It seems to be a normal equation.
$$ \boldsymbol{w}=(\boldsymbol{X}^{T}\boldsymbol{X})^{-1}\boldsymbol{X}^T\boldsymbol{y} $$ Try this as code I will.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">X <span style="color:#f92672">=</span> pd<span style="color:#f92672">.</span>concat([pd<span style="color:#f92672">.</span>Series(np<span style="color:#f92672">.</span>ones(len(df[<span style="color:#e6db74">&#39;bmi&#39;</span>]))), df<span style="color:#f92672">.</span>loc[:,[<span style="color:#e6db74">&#39;bmi&#39;</span>,<span style="color:#e6db74">&#39;s5&#39;</span>]]], axis<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, ignore_index <span style="color:#f92672">=</span>True)<span style="color:#f92672">.</span>values
y <span style="color:#f92672">=</span> diabetes<span style="color:#f92672">.</span>target

w <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>linalg<span style="color:#f92672">.</span>inv(X<span style="color:#f92672">.</span>T <span style="color:#960050;background-color:#1e0010">@</span> X) <span style="color:#960050;background-color:#1e0010">@</span> X<span style="color:#f92672">.</span>T <span style="color:#960050;background-color:#1e0010">@</span> y

Results: [<span style="color:#ae81ff">152.13348416</span> <span style="color:#ae81ff">675.06977443</span> <span style="color:#ae81ff">614.95050478</span>]
</code></pre></div><p>Matrix calculation in python is intuitive and nice. By the way, if there is only one explanatory variable, the result will be the same as that of simple regression. Of course, it is generalized with n explanatory variables.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">X <span style="color:#f92672">=</span> pd<span style="color:#f92672">.</span>concat([pd<span style="color:#f92672">.</span>Series(np<span style="color:#f92672">.</span>ones(len(df[<span style="color:#e6db74">&#39;bmi&#39;</span>]))), df<span style="color:#f92672">.</span>loc[:,[<span style="color:#e6db74">&#39;bmi&#39;</span>]]], axis<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, ignore_index<span style="color:#f92672">=</span>True)<span style="color:#f92672">.</span> values
y <span style="color:#f92672">=</span> diabetes<span style="color:#f92672">.</span>target

w <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>linalg<span style="color:#f92672">.</span>inv(X<span style="color:#f92672">.</span>T <span style="color:#960050;background-color:#1e0010">@</span> X) <span style="color:#960050;background-color:#1e0010">@</span> X<span style="color:#f92672">.</span>T <span style="color:#960050;background-color:#1e0010">@</span> y
<span style="color:#66d9ef">print</span>(w)

[<span style="color:#ae81ff">152.13348416</span> <span style="color:#ae81ff">949.43526038</span>]
<span style="color:#e6db74">``</span><span style="color:#960050;background-color:#1e0010">`</span>Let<span style="color:#e6db74">&#39;s draw a graph based on the calculated values when there are two explanatory variables.</span>

<span style="color:#e6db74">``</span><span style="color:#960050;background-color:#1e0010">`</span>python
fig<span style="color:#f92672">=</span>plt<span style="color:#f92672">.</span>figure()
ax<span style="color:#f92672">=</span>Axes3D(fig)

mesh_x1 <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>arange(x1<span style="color:#f92672">.</span>min(), x1<span style="color:#f92672">.</span>max(), (x1<span style="color:#f92672">.</span>max()<span style="color:#f92672">-</span>x1<span style="color:#f92672">.</span>min())<span style="color:#f92672">/</span><span style="color:#ae81ff">20</span>)
mesh_x2 <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>arange(x2<span style="color:#f92672">.</span>min(), x2<span style="color:#f92672">.</span>max(), (x2<span style="color:#f92672">.</span>max()<span style="color:#f92672">-</span>x2<span style="color:#f92672">.</span>min())<span style="color:#f92672">/</span><span style="color:#ae81ff">20</span>)
mesh_x1, mesh_x2 <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>meshgrid(mesh_x1, mesh_x2)

x1 <span style="color:#f92672">=</span> df[<span style="color:#e6db74">&#39;bmi&#39;</span>]<span style="color:#f92672">.</span>values
x2 <span style="color:#f92672">=</span> df[<span style="color:#e6db74">&#39;s5&#39;</span>]<span style="color:#f92672">.</span>values
y <span style="color:#f92672">=</span> diabetes<span style="color:#f92672">.</span>target
ax<span style="color:#f92672">.</span>scatter3D(x1, x2, y)
ax<span style="color:#f92672">.</span>set_xlabel(<span style="color:#e6db74">&#34;x1&#34;</span>)
ax<span style="color:#f92672">.</span>set_ylabel(<span style="color:#e6db74">&#34;x2&#34;</span>)
ax<span style="color:#f92672">.</span>set_zlabel(<span style="color:#e6db74">&#34;y&#34;</span>)

mesh_y <span style="color:#f92672">=</span> w[<span style="color:#ae81ff">1</span>] <span style="color:#f92672">*</span> mesh_x1 <span style="color:#f92672">+</span> w[<span style="color:#ae81ff">2</span>] <span style="color:#f92672">*</span> mesh_x2 <span style="color:#f92672">+</span> w[<span style="color:#ae81ff">0</span>]
ax<span style="color:#f92672">.</span>plot_wireframe(mesh_x1, mesh_x2, mesh_y, color<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;red&#39;</span>)

plt<span style="color:#f92672">.</span>show()
</code></pre></div><p>The result is shown below. It looks like it&rsquo;s done properly
<img width="413" alt="regression_multi_2.png" src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/18497/af651cb1-e523-8026-0ca3-e82735c43534.png"></p>
<h3 id="evaluation">Evaluation</h3>
<p>Evaluate the degree of coincidence of the planes with the coefficient of determination.
The coefficient of determination $R^2$ needs to be calculated as &ldquo;total change&rdquo; and &ldquo;regression change&rdquo;.</p>
<ul>
<li>Total fluctuation: The difference between the measured value and the overall average value</li>
<li>Regression variability: Difference between predicted value and overall mean</li>
</ul>
<p>The coefficient of determination calculates how much the explanatory variable explains the objective variable, that is, &ldquo;how much the regression variation is relative to the total variation&rdquo;. Using the sum of squares (variance) of total variation and regression variation,</p>
<pre><code class="language-math" data-lang="math">R^2=\frac{\sum_{i=0}^{N}(\hat{y}_i-\bar{y})^2}{\sum_{i=0}^{N}(y_i- \bar{y})^2}
</code></pre><p>Since the total fluctuation is the sum of the regression fluctuation and the total difference fluctuation (predicted value and measured value)</p>
<pre><code class="language-math" data-lang="math">R^2=1-\frac{\sum_{i=0}^{N}(y_i-\hat{y}_i)^2}{\sum_{i=0}^{N}(y_i-\bar {y})^2}
</code></pre><p>Write this in python and calculate the coefficient of determination.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">u <span style="color:#f92672">=</span> ((y<span style="color:#f92672">-</span>(X <span style="color:#960050;background-color:#1e0010">@</span> w))<span style="color:#f92672">**</span><span style="color:#ae81ff">2</span>)<span style="color:#f92672">.</span>sum()
v <span style="color:#f92672">=</span> ((y<span style="color:#f92672">-</span>y<span style="color:#f92672">.</span>mean())<span style="color:#f92672">**</span><span style="color:#ae81ff">2</span>)<span style="color:#f92672">.</span>sum()

R2 <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span><span style="color:#f92672">-</span>u<span style="color:#f92672">/</span>v
<span style="color:#66d9ef">print</span>(R2)

<span style="color:#ae81ff">0.4594852440167805</span>
</code></pre></div><p>It was the result.</p>
<h2 id="normalization-standardization">Normalization, standardization</h2>
<p>By the way, in this example, the values of &ldquo;BMI&rdquo; and &ldquo;ltg&rdquo; were used, but if the number of variables increases, for example, the number of $10^5$ order and the data of $10^{-5}$ order can be mixed. There is also a nature. If this happens, the calculation may not work. Aligning the data while keeping the original data is called normalization.</p>
<h3 id="min-max-scaling">Min-Max scaling</h3>
<p>Min-Max scaling converts the minimum value to -1 and the maximum value to 1. That is, calculate $$ x_{i_{new}}=\frac{x_i-x_{min}}{x_{max}-x_{min}} $$.</p>
<h3 id="standardization">standardization</h3>
<p>Normalization transforms so that the mean is 0 and the variance is 1. That is, calculate $$ x_{i_{new}}=\frac{x_i-\bar{x}}{\sigma} $$.</p>
<p>Details are written on the following pages.</p>
<ul>
<li>Why is Feature Scaling needed? ](<a href="https://qiita.com/ttskng/items/2a33c1ca925e4501e609">https://qiita.com/ttskng/items/2a33c1ca925e4501e609</a>)</li>
<li><a href="https://fisproject.jp/2016/06/data-standardization-using-python/">[NumPy / SciPy] Data standardization with Python</a></li>
</ul>
<h2 id="try-standardizing-with-python">Try standardizing with python</h2>
<p>I tried to calculate it with python, but it seems that the scikit-learn&rsquo;s diabetes data has already been normalized, so it made no sense.</p>
<h2 id="try-to-calculate-with-scikit-learn">Try to calculate with scikit-learn</h2>
<p>For multiple regression, use LinearRegression of scikit-learn and just fit the training data.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">from</span> sklearn <span style="color:#f92672">import</span> linear_model

clf <span style="color:#f92672">=</span> linear_model<span style="color:#f92672">.</span>LinearRegression()
clf<span style="color:#f92672">.</span>fit(df[[<span style="color:#e6db74">&#39;bmi&#39;</span>,<span style="color:#e6db74">&#39;s5&#39;</span>]], diabetes<span style="color:#f92672">.</span>target)

<span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;coef: &#34;</span>, clf<span style="color:#f92672">.</span>coef_)
<span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;intercept: &#34;</span>, clf<span style="color:#f92672">.</span>intercept_)
<span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;score: &#34;</span>, clf<span style="color:#f92672">.</span>score(df[[<span style="color:#e6db74">&#39;bmi&#39;</span>,<span style="color:#e6db74">&#39;s5&#39;</span>]], diabetes<span style="color:#f92672">.</span>target))

coef: [<span style="color:#ae81ff">675.06977443</span> <span style="color:#ae81ff">614.95050478</span>]
intercept: <span style="color:#ae81ff">152.1334841628967</span>
score: <span style="color:#ae81ff">0.45948524401678054</span>
</code></pre></div><p>Only this. The results are the same as the results without scikit-learn.</p>
<p>#Summary
Developed from simple regression to multiple regression. Normal equation
$$ \boldsymbol{w}=(\boldsymbol{X}^{T}\boldsymbol{X})^{-1}\boldsymbol{X}^T\boldsymbol{y} $$ We were able to extend to multiple explanatory variables. Since multiple explanatory variables must be scaled, they must be calculated with a standardization technique.</p>
<p>Now you understand linear approximation.</p>

    </article>
  </div>

  
  
  
  <script>
  window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
  ga('create', 'UA-169005401-1', 'auto');
  ga('send', 'pageview');
  </script>
  <script async src='https://www.google-analytics.com/analytics.js'></script>
  

  

  
<link rel="stylesheet" type="text/css" href="//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.0/cookieconsent.min.css" />
<script src="//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.0/cookieconsent.min.js"></script>
<script>
window.addEventListener("load", function(){
window.cookieconsent.initialise({
  "palette": {
    "popup": {
      "background": "#216942",
      "text": "#b2d192"
    },
    "button": {
      "background": "#afed71"
    }
  }
})});
</script>

</body>

</html>
