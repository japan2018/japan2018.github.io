<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title> NLP on Some Title</title>
    <link>https://japan2018.github.io/tags/nlp/</link>
    <description>Recent content in  NLP on Some Title</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 23 May 2020 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://japan2018.github.io/tags/nlp/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Natural Language : ChatBot Part1 - Twitter API Corpus</title>
      <link>https://japan2018.github.io/natural-language-chatbot-part1-twitter-api-corpus-fc09c/</link>
      <pubDate>Sat, 23 May 2020 00:00:00 +0000</pubDate>
      
      <guid>https://japan2018.github.io/natural-language-chatbot-part1-twitter-api-corpus-fc09c/</guid>
      <description>#Target  I have summarized a chatbot using the Microsoft Cognitive Toolkit (CNTK).
Part 1 prepares you for training your chatbot with the Microsoft Cognitive Toolkit.
We will introduce them in the following order.
 Register Twitter Developer Collecting conversation datasets using Twitter API Pre-processing text data and creating Sentence Piece model Create a file to be read by the built-in reader provided by CNTK  #Introduction Assuming you have a Twitter account and Linux is available.</description>
    </item>
    
    <item>
      <title>[Chapter 4] Introduction to Python with 100 knocks</title>
      <link>https://japan2018.github.io/chapter-4-introduction-to-python-with-100-knocks-fc4ad/</link>
      <pubDate>Sun, 03 May 2020 00:00:00 +0000</pubDate>
      
      <guid>https://japan2018.github.io/chapter-4-introduction-to-python-with-100-knocks-fc4ad/</guid>
      <description>This article is a sequel to my book [Introduction to Python with 100 knocks](https://qiita.com/hi-asano/items/8e303425052781d95f09).[100knocksChapter4](https://nlp100.github.io/ja/ch04.html) will be explained.  First, install the morphological analyzer MeCab, download neko.txt, and perform morphological analysis to check the contents.
$ mecab &amp;lt;neko.txt&amp;gt; neko.txt.mecab It is &amp;ldquo;I am a cat&amp;rdquo; from Aozora Bunko.
MeCab&amp;rsquo;s default dictionary system is similar to school grammar, but adjective verbs are nouns + auxiliary verbs, and Sa verbs are nouns + verbs.</description>
    </item>
    
    <item>
      <title>Use KNP as a Universal Dependency parser in spaCy</title>
      <link>https://japan2018.github.io/use-knp-as-a-universal-dependency-parser-in-spacy-ff7ea/</link>
      <pubDate>Thu, 16 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>https://japan2018.github.io/use-knp-as-a-universal-dependency-parser-in-spacy-ff7ea/</guid>
      <description>The other day, we released spaCy plugin [Camphr version 0.5.20](https://github.com/PKSHATechnology-Research/camphr/releases/tag/0.5.20).  The feature of this time is [universal dependency] from the analysis result of KNPimplementedbyProfessorKoichiYasuokaofKyotoUniversity.Itisafunctiontooutputthelabelof(https://universaldependencies.org/). In this article, I will briefly introduce how to use it.
Install  Install knponthesystemorKNPdockerimage $ pip install camphr[juman]  #Analysis
import camphr import spacy nlp = camphr.load(&amp;#34;knp&amp;#34;) doc = nlp (&amp;#34;Taro went to Mt. Fuji while eating an apple and a mandarin orange.&amp;#34;) spacy.displacy.render(doc) &amp;ldquo;Apple&amp;rdquo; and &amp;ldquo;Mikan&amp;rdquo; are connected by conj(conjunct), which is a wonderful analysis result.</description>
    </item>
    
    <item>
      <title>100 language processing knocks-38 (using pandas): histogram</title>
      <link>https://japan2018.github.io/100-language-processing-knocks-38-using-pandas-histogram-f9e48/</link>
      <pubDate>Mon, 02 Mar 2020 00:00:00 +0000</pubDate>
      
      <guid>https://japan2018.github.io/100-language-processing-knocks-38-using-pandas-histogram-f9e48/</guid>
      <description>[100 language processing knocks 2015](http://www.cl.ecei.tohoku.ac.jp/nlp100/)[&amp;quot;Chapter4:Morphologicalanalysis&amp;quot;](http://www.cl.ecei.tohokuItisa[38th&amp;quot;histogram&amp;quot;](http://www.cl.ecei.tohoku.ac.jp/nlp100/#sec38)recordof.ac.jp/nlp100/#ch4).  It&amp;rsquo;s easy as long as you&amp;rsquo;ve overcome the tofu from the previous knock. If you don&amp;rsquo;t put out a label, you don&amp;rsquo;t have to deal with &amp;ldquo;tofu&amp;rdquo;.
Reference link    Links Remarks     038.Histogram.ipynb Answer Program GitHub Link   [Amateur language processing 100 knocks: 38 Copy and paste source of many source parts   MeCab Official MeCab page to check first    #Environment</description>
    </item>
    
    <item>
      <title>[Machine learning] Use WordNet to mechanically extract similar words</title>
      <link>https://japan2018.github.io/machine-learning-use-wordnet-to-mechanically-extract-similar-words-f82e9/</link>
      <pubDate>Sun, 23 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>https://japan2018.github.io/machine-learning-use-wordnet-to-mechanically-extract-similar-words-f82e9/</guid>
      <description>#Outline of natural language processing  Before touching on WordNet, let me briefly touch on natural language processing. In order for a machine to understand sentences in natural language processing, the following morphological analysis, syntactic analysis, semantic analysis, and contextual analysis are required in order.
Morphological analysis It is the work of dividing from natural language text data without annotations of information into columns of morphemes (the smallest unit that has meaning in the language) based on information such as the grammar of the target language and the part of speech of words called a dictionary.</description>
    </item>
    
    <item>
      <title>Youtube live It analyzes the sentiment of the comment and judges the emo part from Youtube LIve.</title>
      <link>https://japan2018.github.io/youtube-live-it-analyzes-the-sentiment-of-the-comment-and-judges-the-emo-part-from-youtube-live.-fa198/</link>
      <pubDate>Fri, 14 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>https://japan2018.github.io/youtube-live-it-analyzes-the-sentiment-of-the-comment-and-judges-the-emo-part-from-youtube-live.-fa198/</guid>
      <description># TL;DR  It is a suggestion and experiment that it is possible to extract &amp;ldquo;emo&amp;rdquo; and &amp;ldquo;highlight&amp;rdquo; with better accuracy than just the number of comments per hour by performing sentiment analysis of comments on Youtube live. ..
In fact, we were able to get the result that the accuracy is likely to improve compared to just looking at the number of comments.
#Introduction
Last time, I borrowed COTOHA API and tried to think freely, not practical Article.</description>
    </item>
    
    <item>
      <title>Language processing 100 knocks-88: 10 words with high similarity</title>
      <link>https://japan2018.github.io/language-processing-100-knocks-88-10-words-with-high-similarity-fa498/</link>
      <pubDate>Thu, 16 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>https://japan2018.github.io/language-processing-100-knocks-88-10-words-with-high-similarity-fa498/</guid>
      <description>[Language Processing 100 Knock 2015](http://www.cl.ecei.tohoku.ac.jp/nlp100/) 88th record &amp;quot;10 words with high similarity&amp;quot; is recorded.  Extract similar words from all words. This is also the process I want to do from my mailbox and minutes. Technically it is almost the same as the previous content.
Reference link    Links Remarks     088. 10 words with high similarity.ipynb Answer program GitHub link   Amateur language processing 100 knocks: 88 I have always taken care of 100 language processing knocks    #Environment</description>
    </item>
    
    <item>
      <title>[Model construction] Using the data set of Reuters communication, create a model (MLP) that classifies news into topics with keras (TensorFlow 2 system)</title>
      <link>https://japan2018.github.io/model-construction-using-the-data-set-of-reuters-communication-create-a-model-mlp-that-classifies-news-into-topics-with-keras-tensorflow-2-system-ff9e0/</link>
      <pubDate>Sat, 04 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>https://japan2018.github.io/model-construction-using-the-data-set-of-reuters-communication-create-a-model-mlp-that-classifies-news-into-topics-with-keras-tensorflow-2-system-ff9e0/</guid>
      <description>## Last pre-processing!  This article is a sequel to [Pre-processing]. [Pre-processing] Creating a model (MLP) for classifying news into topics using Reuters communication data set (TensorFlow 2 series)
Please refer to the pre-processing section for the operating environment.
Model learning Create a model using the preprocessed news article text x_train and the news label y_train.
This time, we will use a two-layer MLP (Multilayer Perceptron) as a simple model.</description>
    </item>
    
    <item>
      <title>Morphological analysis tool installation (MeCab,Juman&#43;&#43;,Janome,GiNZA)</title>
      <link>https://japan2018.github.io/morphological-analysis-tool-installation-mecabjuman-janomeginza-fdd58/</link>
      <pubDate>Mon, 09 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://japan2018.github.io/morphological-analysis-tool-installation-mecabjuman-janomeginza-fdd58/</guid>
      <description>A morphological analysis tool is indispensable for Japanese text processing, but it was troublesome to check the installation method of the tool each time, so the following 4 installations are summarized.   MeCab Juman++ Janome GiNZA  Installation environment: Ubuntu 18.04
MeCab Quoted from official site
 MeCab is an open source morphological analysis engine developed through the Joint Research Unit Project of Kyoto University Graduate School of Informatics-Nippon Telegraph and Telephone Corporation, Communication Science Laboratories.</description>
    </item>
    
  </channel>
</rss>