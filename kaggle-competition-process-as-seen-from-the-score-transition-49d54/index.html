<!doctype html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  
  <meta name="generator" content="Hugo 0.72.0" />

  
  <meta name="description" content="Some description">
  

  
  <link rel="apple-touch-icon" sizes="180x180" href="https://memotut.com/apple-touch-icon.png">

  
  <link rel="icon" type="image/png" sizes="32x32" href="https://memotut.com/favicon-32x32.png">

  
  <link rel="icon" type="image/png" sizes="16x16" href="https://memotut.com/favicon-16x16.png">

  
  <link rel="manifest" href="https://memotut.com/site.webmanifest">

  
  <link rel="mask-icon" href="https://memotut.com/safari-pinned-tab.svg" color="#5bbad5">

  <meta name="msapplication-TileColor" content="#da532c">

  <meta name="theme-color" content="#ffffff">

  
  <link rel="stylesheet" href="https://memotut.com/css/bootstrap.min.css" />

  
  <title>Kaggle competition process as seen from the score transition | Memo Tut</title>
  

  <style>
body {
  min-width: 300px;
}

.custom-navbar {
  margin-bottom: 1em;
  height: 60px;
}

.custom-navbar a {
  display: inline-block; 
  padding: 18px 0;
  margin-right: 1em; 
  font-weight: bold; 
}

.custom-navbar a:hover,
.custom-navbar a:focus {
  text-decoration: none; 
}

@media print {
  .custom-navbar {
    display: none;
  }
}

article {
  padding-bottom: 1em;
}

img {
  max-width: 100%;
}


body {
  background-color: #fff;
}



body {
  color: #212529;
}



a {
  color: #007bff;
}



a:hover,
a:focus {
  color: #0056b3;
}



.custom-navbar {
  background-color: #212529;
}



.custom-navbar a {
  color: rgba(255, 255, 255, 0.75);
}



.custom-navbar a:hover,
.custom-navbar a:focus {
  color: rgba(255, 255, 255, 1);
}



.container {
  max-width: 800px;
}



pre {
  display: block;
  padding: 9.5px;
  word-break: break-all;
  word-wrap: break-word;
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 4px;
}

pre code {
  padding: 0;
  font-size: inherit;
  color: inherit; 
  white-space: pre-wrap;
  background-color: transparent;
  border: none;
  border-radius: 0;
}

code {
  padding: 2px 4px;
  color: inherit; 
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 4px;
  font-size: .9em;
}



blockquote,
.blockquote {
  padding: 10px 20px;
  margin: 0 0 20px;
  font-size: 1em;
  border-left: 5px solid #6c757d;
}

</style>
</head>

<body>
  <nav class="custom-navbar">
  <div class="container">
    
    <a href="/">Posts</a>
    
    <a href="/tags/">Tags</a>
    
    <a href="/about/">About</a>
    
    <a href="/index.xml">RSS</a>
    
  </div>
</nav>
  
  <div class="container">
    <article>
      <h1>Kaggle competition process as seen from the score transition</h1>
<p>
  <small class="text-secondary">
  
  
  Dec 6, 2019
  </small>
  

<small><code><a href="https://memotut.com/tags/python">Python</a></code></small>


<small><code><a href="https://memotut.com/tags/machine-learning"> machine learning</a></code></small>


<small><code><a href="https://memotut.com/tags/kaggle"> Kaggle</a></code></small>

</p>
<pre><code># Kaggle competition process from the perspective of score transition
</code></pre>
<p>This article is on the 6th day of Advent Calendar of <a href="https://qiita.com/advent-calendar/2019/xtech-businessai-kaggle">Road to AI Dojo &ldquo;Kaggle&rdquo; by Nikkei xTECH Business AI ① Advent Calendar 2019</a> This is an article.</p>
<p>This article is for <strong>Kaggle newbies</strong> who don&rsquo;t know how to get started with Kaggle. While watching the score transition of the competition, I will write an article about what the kagglers are doing at a certain time.
As a level, <a href="https://qiita.com/upura/items/3c10ff6fed4e7c3d70f0">If you register with Kaggle, the next thing to do is to do this! After learning the basics of machine learning and Kaggle around 10 Kernel ~</a> that goes beyond Titanic, try to challenge the competition that is actually being held It is intended for people who are thinking.</p>
<p>*The content of this article is based on my own experiences and experiences, and not all people do.</p>
<h2 id="score-transition">Score transition</h2>
<p>First, use <a href="https://github.com/Kaggle/kaggle-api">kaggle api</a> to extract participant score transitions from the leaderboard.</p>
<pre><code>kaggle competitions leaderboard competition name --download
</code></pre><p>With the above command, the Submission Date and Public Score when each participant updates the score can be downloaded as a csv file.</p>
<p>It is my score transition of <a href="https://www.kaggle.com/c/nfl-big-data-bowl-2020">NFL competition</a> that ended the other day. Unfortunately I didn&rsquo;t notice anything, my process in this competition is divided into four periods.
<img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/250105/eeb3cd2b-a2fd-191a-afbb-8723e2b11284.png" alt="nfl_my_score.png"></p>
<p>This is the score transition of the top 5 public teams. Imagine this too, and if you divide the process, it will look like this.
<img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/250105/5ff05146-5e79-cc41-19c3-c93d9b863720.png" alt="nfl_top5_score.png"></p>
<p>Some teams are constantly improving their scores.</p>
<h3 id="baseline-construction-period">Baseline construction period</h3>
<p><img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/250105/e7672b31-13dc-7c79-2708-0e58eacb091f.png" alt="nfl_my_score_1.png"></p>
<p>It is time to create a plain model that does data comprehension and light EDA, and does not create features or other features. Also build appropriate cross validation here (if possible).
Many people don&rsquo;t submit here, but for comparison, I *<em>will</em> always submit. One of the findings is how much there is a difference between the plain model and the higher model.
In the case of participation in the middle of the race, the kernel may be substituted as the baseline.</p>
<h3 id="no-matter-what-you-do-the-golden-age">No matter what you do, the golden age</h3>
<p><img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/250105/4f3f6b43-176b-b46e-f2a4-534615c91c29.png" alt="nfl_my_score_2.png"></p>
<p>In the case of a table competition, start creating features from here. Here, we will give priority to features that are relatively easy to come up with and think that this will increase. The first parameter tuning is also done here (by the way, I am a warm manual tuning person).
From here, the table and the image will be described separately.</p>
<h4 id="table">table</h4>
<ul>
<li>Feature creation that anyone can think of based on domain knowledge (generally posted in kernel)</li>
<li>Parameter tuning (1st time)</li>
<li>Aggregate features</li>
<li>frequency encoding</li>
<li>target encoding</li>
<li>clipping</li>
<li>binning</li>
<li>Time series shift, diff, averaging</li>
</ul>
<h4 id="image">image</h4>
<ul>
<li>Training on a relatively light network (I often use resnet34)</li>
<li>Adjustment of learning rate and batch size</li>
<li>Try some scheduling</li>
<li>Augmentation that seems to work according to the image (I will not try here if the label is likely to change visually after conversion)</li>
<li>Device of pre-processing (resize, noise removal, background processing, etc.)</li>
<li>Threshold optimization (segmentation)</li>
</ul>
<h3 id="i-dont-know-anything">I don&rsquo;t know anything</h3>
<p><img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/250105/58f8e678-9e80-2905-b67c-c5dfd5402719.png" alt="nfl_my_score_3.png">
The time when anything goes wrong.
It&rsquo;s the time when you don&rsquo;t know anything, such as the feature amount you think is effective is not effective, cv rises but LB does not rise, cv does not rise but LB rises. When the feature quantity can be made to some extent, it tends to overfit by creating the feature quantity that has already been taken into consideration (I feel).</p>
<h4 id="table-1">table</h4>
<ul>
<li>Exhaustively crawl the kernel and discussion to search for hints.</li>
<li>Try to make a large amount of interaction features.</li>
<li>Feature selection</li>
<li>Try rebuilding the model from scratch</li>
<li>Think with a feeling of decision tree</li>
<li>Find magic feature</li>
<li>Fishing for past competition solutions</li>
</ul>
<h4 id="image-1">image</h4>
<p>It may take too much time to learn images once, and I feel that I have noticed something before the unknown period arrives and I am in the final adjustment period.</p>
<ul>
<li>Exhaustively crawl the kernel and discussion to search for hints.</li>
<li>Try out Augmentation of mixing system such as mixup, cutmix and the ones that appeared in the latest paper</li>
<li>Try Augmentation which seems not to work intuitively for the time being</li>
<li>Fiddle around the network (← It usually does not mean that it is an image)</li>
<li>Pseudo Labeling (In many cases, it is difficult to timing because the original accuracy must be high to some extent)</li>
<li>Use Grad-CAM etc. to confirm what the NN uses as the basis</li>
<li>Try changing loss a little</li>
<li>Try another variation of image resizing</li>
<li>Think about NN</li>
</ul>
<p>This time is hard because it is pulled out on the LeaderBoard.
In <a href="https://www.kaggle.com/c/freesound-audio-tagging-2019/leaderboard">Freesound Audio Tagging 2019</a>, which won the gold medal, scrutiny of the puclic kernel was a breakthrough.</p>
<h3 id="when-you-notice-something">When you notice something</h3>
<p><img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/250105/ae84173c-2583-fb10-0201-7441885b1f37.png" alt="nfl_top5_score_1.png">
(The upper arrow is a delusion)</p>
<p>Unfortunately I didn&rsquo;t have this time in the NFL competition, but when I look at the leaderboard, there are quite a few people who jump up suddenly.
I think there are various reasons, but when reading solutions etc., I think that the same thing can be said, <strong>I often see data</strong>.</p>
<ul>
<li>Discover Leak</li>
<li>Leak-based feature creation and optimization</li>
<li>Creation of features based on deep insight</li>
<li>Normalization to absorb the difference in the distribution of train/test, etc.</li>
<li>Understand the feeling of a decision tree</li>
<li>I understand the feelings of NN</li>
<li>find magic feature</li>
</ul>
<p>It is generally difficult to express &ldquo;creating features based on deep insight&rdquo; because it is a competition, but I think this article will be very helpful.
(Reference: [Difference between ordinary data scientist and world-class data scientist]](<a href="https://newswitch.jp/p/19990">https://newswitch.jp/p/19990</a>))</p>
<h3 id="ensemble--final-adjustment-period">Ensemble &amp; final adjustment period</h3>
<p><img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/250105/6e0113d7-eba4-6c43-4763-03b9836aa8f6.png" alt="nfl_my_score_4.png">
In this NFL competition, I started running the ensemble and final adjustment early because there was no way to hit it, but it is an impression that many people usually do it about 1 week ago.</p>
<p>Basically, <strong>I know that the score will go up, but I think that there are many things to do at the end for things that increase the amount of calculation</strong>.
Since the score of the ensemble surely increases, I will do my best until it reaches the limit unless I am caught in the time limit such as a kernel competition. The second time of parameter tuning is also here. In the case of tables, we usually add a lot of features, so we should make another adjustment here. It also reduces the learning rate. If you&rsquo;re teaming up to create different models, the ensembles will probably have a big effect.</p>
<h4 id="table-2">table</h4>
<ul>
<li>Parameter tuning (2nd time)</li>
<li>Lower the GBDT learning rate</li>
<li>Ensemble (seed averaging, mix with xgboost/catboost, mix with models with different features, mix with team member sub)</li>
<li>Stacking</li>
</ul>
<h4 id="image-2">image</h4>
<p>It&rsquo;s often faster to start training for an ensemble, as images take a long time to learn once.</p>
<ul>
<li>Migrate to a heavier network (ResNet-101, Densenet-121~, inceptionv3, ResNeXt-50-32x4d~, Wide ResNet-50-2~)</li>
<li>Ensemble with various variations of network</li>
<li>tta(test time augmentation)</li>
<li>snapshot ensemble</li>
<li>Extend epoch (in case of lack of learning)</li>
<li>Fine adjustment of parameters</li>
</ul>
<p>This completes the process from Kaggle competition start to final submission.
Of course, I don&rsquo;t think everyone is doing this process, and the order in which they work is different depending on the challenge of the competition, but I feel that the process will converge to some extent if you experience multiple competitions. I also want to know the process of stronger people.</p>
<p>Finally, it is a bonus.</p>
<h2 id="best-practices-for-anyone-starting-kaggle">Best practices for anyone starting Kaggle</h2>
<p>In my opinion, if you&rsquo;re new to Kaggle, I recommend this route.① <a href="https://qiita.com/upura/items/3c10ff6fed4e7c3d70f0">If you register with Kaggle, the next thing to do is to do this! Introduction to Titanic beyond 10 Kernel ～</a>
This is scheduled to be published by Kodansha as a Kaggle introductory book in March 2020 (<a href="https://upura.hatenablog.com/entry/2019/12/04/220200">https://upura.hatenablog.com/entry/2019/12/04/220200</a>)
<img width="600" alt="20191203164651.png" src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/250105/ce33099b-0df5-0318-4f98-93b04caf2119.png"></p>
<p>② Copy the kernel with many votes in the past competition/current competition
A great kernel is a wealth of knowledge. Especially for beginners, there is a tendency to get a lot of votes, so let&rsquo;s choose one that has many votes and seems to explain carefully from the beginning. If you have a score, you can learn the flow of submission. I feel like I started with the Home Credit competition <a href="https://www.kaggle.com/willkoehrsen/start-here-a-gentle-introduction">Start Here: A Gentle Introduction</a>.
<img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/250105/3f09ae46-d001-091c-d2be-0a3e1145b698.png" alt="homecredit_kernel.png"></p>
<p>③ <a href="https://www.amazon.co.jp/dp/B07YTDBC3Z/ref=dp-kindle-redirect?_encoding=UTF8&amp;btkr=1">Technology of data analysis to win with Kaggle</a>
Needless to say, this is an iron plate book. It&rsquo;s not a beginner&rsquo;s book at all, so it&rsquo;s better to go through the above process.
Since the code is also listed, if it is a table competition, it will be more powerful if you participate in the current competition with this one hand.</p>
<p><img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/250105/35ec95b9-0e5f-382c-6bfa-3b548a7796ce.jpeg" alt="418YjfYRlhL.jpg"></p>
<h2 id="at-the-end">At the end</h2>
<p>I think it&rsquo;s a great opportunity to get started with Kaggle, as the information that has been scattered around and tacitly known in Kaggler has been collected in the book.
I hope this article helps anyone who wants to get started with Kaggle.</p>

    </article>
  </div>

  
  
  
  <script>
  window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
  ga('create', 'UA-169005401-1', 'auto');
  ga('send', 'pageview');
  </script>
  <script async src='https://www.google-analytics.com/analytics.js'></script>
  

  

  
<link rel="stylesheet" type="text/css" href="//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.0/cookieconsent.min.css" />
<script src="//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.0/cookieconsent.min.js"></script>
<script>
window.addEventListener("load", function(){
window.cookieconsent.initialise({
  "palette": {
    "popup": {
      "background": "#216942",
      "text": "#b2d192"
    },
    "button": {
      "background": "#afed71"
    }
  }
})});
</script>

</body>

</html>
