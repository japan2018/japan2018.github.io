<!doctype html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  
  <meta name="generator" content="Hugo 0.72.0" />

  
  <meta name="description" content="Some description">
  

  
  <link rel="apple-touch-icon" sizes="180x180" href="https://memotut.com/apple-touch-icon.png">

  
  <link rel="icon" type="image/png" sizes="32x32" href="https://memotut.com/favicon-32x32.png">

  
  <link rel="icon" type="image/png" sizes="16x16" href="https://memotut.com/favicon-16x16.png">

  
  <link rel="manifest" href="https://memotut.com/site.webmanifest">

  
  <link rel="mask-icon" href="https://memotut.com/safari-pinned-tab.svg" color="#5bbad5">

  <meta name="msapplication-TileColor" content="#da532c">

  <meta name="theme-color" content="#ffffff">

  
  <link rel="stylesheet" href="https://memotut.com/css/bootstrap.min.css" />

  
  <title>[Python] [Implementation explanation] Japanese BERT x unsupervised learning (information maximization clustering) to classify livedoor news: Google Colaboratory (PyTorch) | Memo Tut</title>
  

  <style>
body {
  min-width: 300px;
}

.custom-navbar {
  margin-bottom: 1em;
  height: 60px;
}

.custom-navbar a {
  display: inline-block; 
  padding: 18px 0;
  margin-right: 1em; 
  font-weight: bold; 
}

.custom-navbar a:hover,
.custom-navbar a:focus {
  text-decoration: none; 
}

@media print {
  .custom-navbar {
    display: none;
  }
}

article {
  padding-bottom: 1em;
}

img {
  max-width: 100%;
}


body {
  background-color: #fff;
}



body {
  color: #212529;
}



a {
  color: #007bff;
}



a:hover,
a:focus {
  color: #0056b3;
}



.custom-navbar {
  background-color: #212529;
}



.custom-navbar a {
  color: rgba(255, 255, 255, 0.75);
}



.custom-navbar a:hover,
.custom-navbar a:focus {
  color: rgba(255, 255, 255, 1);
}



.container {
  max-width: 800px;
}



pre {
  display: block;
  padding: 9.5px;
  word-break: break-all;
  word-wrap: break-word;
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 4px;
}

pre code {
  padding: 0;
  font-size: inherit;
  color: inherit; 
  white-space: pre-wrap;
  background-color: transparent;
  border: none;
  border-radius: 0;
}

code {
  padding: 2px 4px;
  color: inherit; 
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 4px;
  font-size: .9em;
}



blockquote,
.blockquote {
  padding: 10px 20px;
  margin: 0 0 20px;
  font-size: 1em;
  border-left: 5px solid #6c757d;
}

</style>
</head>

<body>
  <nav class="custom-navbar">
  <div class="container">
    
    <a href="/">Posts</a>
    
    <a href="/tags/">Tags</a>
    
    <a href="/about/">About</a>
    
    <a href="/index.xml">RSS</a>
    
  </div>
</nav>
  
  <div class="container">
    <article>
      <h1>[Python] [Implementation explanation] Japanese BERT x unsupervised learning (information maximization clustering) to classify livedoor news: Google Colaboratory (PyTorch)</h1>
<p>
  <small class="text-secondary">
  
  
  May 21, 2020
  </small>
  

<small><code><a href="https://memotut.com/tags/python">Python</a></code></small>


<small><code><a href="https://memotut.com/tags/machine-learning"> machine learning</a></code></small>


<small><code><a href="https://memotut.com/tags/deeplearning"> DeepLearning</a></code></small>


<small><code><a href="https://memotut.com/tags/ai"> AI</a></code></small>


<small><code><a href="https://memotut.com/tags/pytorch"> PyTorch</a></code></small>

</p>
<pre><code>In this article, we will implement and explain how clustering is performed by using the Japanese version BERT in Google Colaboratory and performing unsupervised learning information maximization clustering for livedoor news.
</code></pre>
<ul>
<li>Using the Japanese version of BERT with Google Colaboratory, the content until vectorization of sentences,</li>
<li>Information maximization clustering for unsupervised learning in MNIST</li>
</ul>
<p>About this, I have explained it in the series articles so far, so please see here first.</p>
<p>** Serialization list **
<a href="https://qiita.com/sugulu/items/e522a38b812b8edb8a54">[1] [Implementation Description] How to use Japanese version BERT with Google Colaboratory (PyTorch)</a>
<a href="https://qiita.com/sugulu/items/697bd03499c1de9cf082">[2] [Implementation Description] Japanese version BERT with livedoor news classification: Google Colaboratory (PyTorch)</a>
<a href="https://qiita.com/sugulu/items/e3c01a0776a5df3ad552">[3] [Implementation explanation] Brain science and unsupervised learning. Classify MNIST by information maximization clustering</a>
<a href="https://qiita.com/sugulu/items/6a887e612edda9e35c0b">[4] *This article [Implementation explanation] Japanese BERT x Unsupervised learning (information maximization clustering) classifies livedoor news</a></p>
<hr>
<p>Last time, I wrote a paper on the handwritten number image of MNIST
<a href="https://arxiv.org/abs/1807.06653">Invariant Information Clustering for Unsupervised Image Classification and Segmentation</a></p>
<p>Clustering of unsupervised learning using mutual information
<strong>IIC (Invariant Information Clustering)</strong>
Was carried out.</p>
<p>This time, we will show what happens when each news article of livedoor news is vectorized with Japanese BERT and it is subjected to unsupervised learning (clustering) with IIC.</p>
<p>In the case of MNIST, the clustering result is divided according to the number label of supervised learning, and it seems to be useful for supervised learning, but what about text data?</p>
<p>I will try this.</p>
<p>The flow of this implementation is</p>
<ol>
<li>Download livedoor news and convert it to PyTorch&rsquo;s DataLoader</li>
<li>Vectorize livedoor news articles with BERT</li>
<li>Prepare IIC deep learning model</li>
<li>Train the IIC network</li>
<li>Infer test data</li>
</ol>
<p>Will be.</p>
<p>The implementation code of this post is placed in the following GitHub repository.</p>
<p><a href="https://github.com/YutaroOgawa/BERT_Japanese_Google_Colaboratory">GitHub: How to use Japanese version of BERT on Google Colaboratory: Implementation code</a>
This is 4_BERT_livedoor_news_IIC_on_Google_Colaboratory.ipynb.</p>
<h2 id="1-download-livedoor-news-and-convert-it-to-pytorchs-dataloader">1. Download livedoor news and convert it to PyTorch&rsquo;s DataLoader</h2>
<p>Up to this point,
<a href="https://">[2] [Implementation Description] Japanese version of BERT for livedoor news classification: Google Colaboratory (PyTorch)</a></p>
<p>Since it is as it is, it is omitted in this article.</p>
<p>Finally, create the training, validation, and test DataLoaders by doing the following:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">Create a <span style="color:#75715e">#DataLoader (simply called an iterater in the context of torchtext)</span>
batch_size <span style="color:#f92672">=</span> <span style="color:#ae81ff">16</span> <span style="color:#75715e"># BERT uses around 16, 32</span>

dl_train <span style="color:#f92672">=</span> torchtext<span style="color:#f92672">.</span>data<span style="color:#f92672">.</span>Iterator(
    dataset_train, batch_size<span style="color:#f92672">=</span>batch_size, train<span style="color:#f92672">=</span>True)

dl_eval <span style="color:#f92672">=</span> torchtext<span style="color:#f92672">.</span>data<span style="color:#f92672">.</span>Iterator(
    dataset_eval, batch_size<span style="color:#f92672">=</span>batch_size, train<span style="color:#f92672">=</span>False, sort<span style="color:#f92672">=</span>False)

dl_test <span style="color:#f92672">=</span> torchtext<span style="color:#f92672">.</span>data<span style="color:#f92672">.</span>Iterator(
    dataset_test, batch_size<span style="color:#f92672">=</span>batch_size, train<span style="color:#f92672">=</span>False, sort<span style="color:#f92672">=</span>False)

<span style="color:#75715e"># Put together in dictionary object</span>
dataloaders_dict <span style="color:#f92672">=</span> {<span style="color:#e6db74">&#34;train&#34;</span>: dl_train, <span style="color:#e6db74">&#34;val&#34;</span>: dl_eval}
</code></pre></div><h2 id="vectorizing-livedoor-news-articles-with-bert">Vectorizing livedoor news articles with BERT</h2>
<p>The Japanese version of BERT is used to vectorize the text of livedoor news articles.</p>
<p>It will be a vectorization of the document.</p>
<p>This time, let&rsquo;s simply treat the 768-dimensional embedded vector of the first word ([CLS]) of BERT as a document vector (there are various ways to create and validate document vectors).</p>
<p>Since it is not time-consuming to vectorize the document data every time when learning the neural network of clustering by IIC, create a DataLoader converted into a document vector with BERT.</p>
<p>The implementation is as follows.</p>
<p>First, prepare the BERT body. It is a parameter version of Tohoku University that has already learned Japanese.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">from</span> transformers.modeling_bert <span style="color:#f92672">import</span> BertModel

<span style="color:#75715e"># BERT&#39;s model of Japanese-learned parameters</span>
model <span style="color:#f92672">=</span> BertModel<span style="color:#f92672">.</span>from_pretrained(<span style="color:#e6db74">&#39;bert-base-japanese-whole-word-masking&#39;</span>)
model<span style="color:#f92672">.</span>eval()
<span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#39;Network setting completed&#39;</span>)
</code></pre></div><p>Next, define a function to be vectorized by BERT.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># Define a function to vectorize with BERT</span>


<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">vectorize_with_bert</span>(net, dataloader):

    <span style="color:#75715e"># Check if GPU can be used</span>
    device <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>device(<span style="color:#e6db74">&#34;cuda:0&#34;</span> <span style="color:#66d9ef">if</span> torch<span style="color:#f92672">.</span>cuda<span style="color:#f92672">.</span>is_available() <span style="color:#66d9ef">else</span> <span style="color:#e6db74">&#34;cpu&#34;</span>)
    <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;Device used:&#34;</span>, device)
    <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#39;-----start-------&#39;</span>)

    <span style="color:#75715e"># Network to GPU</span>
    net<span style="color:#f92672">.</span>to(device)

    <span style="color:#75715e"># If the network is fixed to some extent, speed up</span>
    torch<span style="color:#f92672">.</span>backends<span style="color:#f92672">.</span>cudnn<span style="color:#f92672">.</span>benchmark <span style="color:#f92672">=</span> True

    <span style="color:#75715e"># Mini batch size</span>
    batch_size <span style="color:#f92672">=</span> dataloader<span style="color:#f92672">.</span>batch_size

    <span style="color:#75715e"># Loop to retrieve mini-batch from data loader</span>
    <span style="color:#66d9ef">for</span> index, batch <span style="color:#f92672">in</span> enumerate(dataloader):
        <span style="color:#75715e"># batch is a Text and Lable dictionary object</span>
        device <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>device(<span style="color:#e6db74">&#34;cuda:0&#34;</span> <span style="color:#66d9ef">if</span> torch<span style="color:#f92672">.</span>cuda<span style="color:#f92672">.</span>is_available() <span style="color:#66d9ef">else</span> <span style="color:#e6db74">&#34;cpu&#34;</span>)
        inputs <span style="color:#f92672">=</span> batch<span style="color:#f92672">.</span>Text[<span style="color:#ae81ff">0</span>]<span style="color:#f92672">.</span>to(device) <span style="color:#75715e"># sentence</span>
        labels <span style="color:#f92672">=</span> batch<span style="color:#f92672">.</span>Label<span style="color:#f92672">.</span>to(device) <span style="color:#75715e"># labels</span>

        <span style="color:#75715e"># Forward propagation calculation</span>
        <span style="color:#66d9ef">with</span> torch<span style="color:#f92672">.</span>set_grad_enabled(False):

            Type <span style="color:#f92672">in</span> <span style="color:#75715e"># Ber</span>
            result <span style="color:#f92672">=</span> net(inputs)

            Extract the first word vector of <span style="color:#75715e"># sequence_output</span>
            vec_0 <span style="color:#f92672">=</span> result[<span style="color:#ae81ff">0</span>] <span style="color:#75715e"># The first 0 indicates sequence_output</span>
            vec_0 <span style="color:#f92672">=</span> vec_0[:, <span style="color:#ae81ff">0</span>, :] <span style="color:#75715e"># All batches. All 768 elements of the first 0th word</span>
            vec_0 <span style="color:#f92672">=</span> vec_0<span style="color:#f92672">.</span>view(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">768</span>) <span style="color:#75715e"># convert size to [batch_size, hidden_size]</span>

            <span style="color:#75715e"># Collect vectorized data in torch list</span>
            <span style="color:#66d9ef">if</span> index <span style="color:#f92672">==</span> <span style="color:#ae81ff">0</span>:
                list_text <span style="color:#f92672">=</span> vec_0
                list_label <span style="color:#f92672">=</span> labels
            <span style="color:#66d9ef">else</span>:
                list_text <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>cat([list_text, vec_0], dim<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)
                list_label <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>cat([list_label, labels], dim<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)

    <span style="color:#66d9ef">return</span> list_text, list_label
</code></pre></div><p>Vectorize the training, validation, and test DataLoader with the defined function.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># Convert Data Loader to vectorized version</span>
<span style="color:#75715e"># It takes a little longer than 5 minutes</span>

list_text_train, list_label_train <span style="color:#f92672">=</span> vectorize_with_bert(model, dl_train)
list_text_eval, list_label_eval <span style="color:#f92672">=</span> vectorize_with_bert(model, dl_eval)
list_text_test, list_label_test <span style="color:#f92672">=</span> vectorize_with_bert(model, dl_test)
</code></pre></div><p>Now let&rsquo;s turn this list into a PyToch Dataset.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">Convert list of <span style="color:#75715e"># torch to Dataset</span>

<span style="color:#f92672">from</span> torch.utils.data <span style="color:#f92672">import</span> TensorDataset

dataset_bert_train <span style="color:#f92672">=</span> TensorDataset(
    list_label_train<span style="color:#f92672">.</span>view(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>), list_text_train)
dataset_bert_eval <span style="color:#f92672">=</span> TensorDataset(list_label_eval<span style="color:#f92672">.</span>view(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>), list_text_eval)
dataset_bert_test <span style="color:#f92672">=</span> TensorDataset(list_label_test<span style="color:#f92672">.</span>view(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>), list_text_test)
</code></pre></div><p>Finally, make the Dataset a DataLoader. This Data Loader is used for IIC (Invariant Information Clustering) network learning.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># Make it Dataloader</span>
<span style="color:#f92672">from</span> torch.utils.data <span style="color:#f92672">import</span> DataLoader

batch_size <span style="color:#f92672">=</span> <span style="color:#ae81ff">1024</span>

dl_bert_train <span style="color:#f92672">=</span> DataLoader(dataset_bert_train, batch_size<span style="color:#f92672">=</span>batch_size, shuffle<span style="color:#f92672">=</span>True, drop_last<span style="color:#f92672">=</span>True)
<span style="color:#75715e"># drop_last will ignore if the last mini-batch is less than batch_size</span>

dl_bert_eval <span style="color:#f92672">=</span> DataLoader(
    dataset_bert_eval, batch_size<span style="color:#f92672">=</span>batch_size, shuffle<span style="color:#f92672">=</span>False)
dl_bert_test <span style="color:#f92672">=</span> DataLoader(
    dataset_bert_test, batch_size<span style="color:#f92672">=</span>batch_size, shuffle<span style="color:#f92672">=</span>False)
</code></pre></div><p>With the above, the text of livedoor news has been vectorized in Japanese BERT.</p>
<p>All we have to do now is cluster this vectorized data.</p>
<h2 id="preparation-3-prepare-iic-deep-learning-model">Preparation 3: Prepare IIC deep learning model</h2>
<p>Next, prepare the IIC deep learning model.
This part is basically</p>
<p><a href="https://qiita.com/sugulu/items/e3c01a0776a5df3ad552">[Implementation explanation] Brain science and unsupervised learning. Classify MNIST by information maximization clustering</a></p>
<p>It has the same configuration as.</p>
<p>The neural network model has contents corresponding to 768-dimensional vectors.
This time, the 1d convolution is repeated to convert the features.</p>
<p>Clustering 10 times the number of clusters actually estimated by overclustering is also performed at the same time to train a network that captures minute features.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> torch.nn <span style="color:#f92672">as</span> nn
<span style="color:#f92672">import</span> torch.nn.functional <span style="color:#f92672">as</span> F

OVER_CLUSTRING_RATE <span style="color:#f92672">=</span> <span style="color:#ae81ff">10</span>


<span style="color:#66d9ef">class</span> <span style="color:#a6e22e">NetIIC</span>(nn<span style="color:#f92672">.</span>Module):
    <span style="color:#66d9ef">def</span> __init__(self):
        super(NetIIC, self)<span style="color:#f92672">.</span>__init__()

        <span style="color:#75715e"># multi-head is not this time</span>
        self<span style="color:#f92672">.</span>conv1 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Conv1d(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">400</span>, kernel_size<span style="color:#f92672">=</span><span style="color:#ae81ff">768</span>, stride<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, padding<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)
        self<span style="color:#f92672">.</span>bn1 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>BatchNorm1d(<span style="color:#ae81ff">400</span>)
        self<span style="color:#f92672">.</span>conv2 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Conv1d(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">300</span>, kernel_size<span style="color:#f92672">=</span><span style="color:#ae81ff">400</span>, stride<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, padding<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)
        self<span style="color:#f92672">.</span>bn2 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>BatchNorm1d(<span style="color:#ae81ff">300</span>)
        self<span style="color:#f92672">.</span>conv3 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Conv1d(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">300</span>, kernel_size<span style="color:#f92672">=</span><span style="color:#ae81ff">300</span>, stride<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, padding<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)
        self<span style="color:#f92672">.</span>bn3 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>BatchNorm1d(<span style="color:#ae81ff">300</span>)

        self<span style="color:#f92672">.</span>fc1 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(<span style="color:#ae81ff">300</span>, <span style="color:#ae81ff">250</span>)
        self<span style="color:#f92672">.</span>bnfc1 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>BatchNorm1d(<span style="color:#ae81ff">250</span>)

        Does it correspond to <span style="color:#ae81ff">9</span> categories of <span style="color:#75715e">#livedoor news? Expected 9 categories</span>
        self<span style="color:#f92672">.</span>fc2 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(<span style="color:#ae81ff">250</span>, <span style="color:#ae81ff">9</span>)

        <span style="color:#75715e"># overclustering</span>
        <span style="color:#75715e"># Allow more minute changes in the network by capturing more clusters than expected</span>
        self<span style="color:#f92672">.</span>fc2_overclustering <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(<span style="color:#ae81ff">250</span>, <span style="color:#ae81ff">9</span><span style="color:#f92672">*</span>OVER_CLUSTRING_RATE)

    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x):
        x <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>view(x<span style="color:#f92672">.</span>size(<span style="color:#ae81ff">0</span>), <span style="color:#ae81ff">1</span>, <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>)
        x <span style="color:#f92672">=</span> F<span style="color:#f92672">.</span>relu(self<span style="color:#f92672">.</span>bn1(self<span style="color:#f92672">.</span>conv1(x)))

        x <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>view(x<span style="color:#f92672">.</span>size(<span style="color:#ae81ff">0</span>), <span style="color:#ae81ff">1</span>, <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>)
        x <span style="color:#f92672">=</span> F<span style="color:#f92672">.</span>relu(self<span style="color:#f92672">.</span>bn2(self<span style="color:#f92672">.</span>conv2(x)))

        x <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>view(x<span style="color:#f92672">.</span>size(<span style="color:#ae81ff">0</span>), <span style="color:#ae81ff">1</span>, <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>)
        x <span style="color:#f92672">=</span> F<span style="color:#f92672">.</span>relu(self<span style="color:#f92672">.</span>bn3(self<span style="color:#f92672">.</span>conv3(x)))

        x <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>view(x<span style="color:#f92672">.</span>size(<span style="color:#ae81ff">0</span>), <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>)
        x_prefinal <span style="color:#f92672">=</span> F<span style="color:#f92672">.</span>relu(self<span style="color:#f92672">.</span>bnfc1(self<span style="color:#f92672">.</span>fc1(x)))

        Don<span style="color:#e6db74">&#39;t use #multi-head</span>
        y <span style="color:#f92672">=</span> F<span style="color:#f92672">.</span>softmax(self<span style="color:#f92672">.</span>fc2(x_prefinal), dim<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
        y_overclustering <span style="color:#f92672">=</span> F<span style="color:#f92672">.</span>softmax(self<span style="color:#f92672">.</span>fc2_overclustering(
            x_prefinal), dim<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>) <span style="color:#75715e"># overclustering</span>

        <span style="color:#66d9ef">return</span> y, y_overclustering

</code></pre></div><p>Define an initialization function for the model weight parameters.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> torch.nn.init <span style="color:#f92672">as</span> init


<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">weight_init</span>(m):
    <span style="color:#e6db74">&#34;&#34;&#34;Weight initialization&#34;&#34;&#34;</span>
    <span style="color:#66d9ef">if</span> isinstance(m, nn<span style="color:#f92672">.</span>Conv1d):
        init<span style="color:#f92672">.</span>normal_(m<span style="color:#f92672">.</span>weight<span style="color:#f92672">.</span>data)
        <span style="color:#66d9ef">if</span> m<span style="color:#f92672">.</span>bias <span style="color:#f92672">is</span> <span style="color:#f92672">not</span> None:
            init<span style="color:#f92672">.</span>normal_(m<span style="color:#f92672">.</span>bias<span style="color:#f92672">.</span>data)
    <span style="color:#66d9ef">elif</span> isinstance(m, nn<span style="color:#f92672">.</span>BatchNorm1d):
        init<span style="color:#f92672">.</span>normal_(m<span style="color:#f92672">.</span>weight<span style="color:#f92672">.</span>data, mean<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, std<span style="color:#f92672">=</span><span style="color:#ae81ff">0.02</span>)
        init<span style="color:#f92672">.</span>constant_(m<span style="color:#f92672">.</span>bias<span style="color:#f92672">.</span>data, <span style="color:#ae81ff">0</span>)
    <span style="color:#66d9ef">elif</span> isinstance(m, nn<span style="color:#f92672">.</span>Linear):
        <span style="color:#75715e"># Xavier</span>
        <span style="color:#75715e"># init.xavier_normal_(m.weight.data)</span>

        <span style="color:#75715e"># He</span>
        init<span style="color:#f92672">.</span>kaiming_normal_(m<span style="color:#f92672">.</span>weight<span style="color:#f92672">.</span>data)

        <span style="color:#66d9ef">if</span> m<span style="color:#f92672">.</span>bias <span style="color:#f92672">is</span> <span style="color:#f92672">not</span> None:
            init<span style="color:#f92672">.</span>normal_(m<span style="color:#f92672">.</span>bias<span style="color:#f92672">.</span>data)
</code></pre></div><p>Defines how to calculate the loss function for the IID.</p>
<p>Calculate the mutual information between the output (x_out) when the target vectorized data is input to NetIID and the output (x_tf_out) when the converted vectorized data is input to NetIID.</p>
<p>See the previous article for more details on the implementation here.</p>
<p><a href="https://qiita.com/sugulu/items/e3c01a0776a5df3ad552">[Implementation explanation] Brain science and unsupervised learning. Classify MNIST by information maximization clustering</a></p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># IIS defines loss function</span>
<span style="color:#75715e"># Reference: https://github.com/RuABraun/phone-clustering/blob/master/mnist_basic.py</span>
<span style="color:#f92672">import</span> sys


<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">compute_joint</span>(x_out, x_tf_out):
    bn, k <span style="color:#f92672">=</span> x_out<span style="color:#f92672">.</span>size()
    <span style="color:#66d9ef">assert</span> (x_tf_out<span style="color:#f92672">.</span>size(<span style="color:#ae81ff">0</span>) <span style="color:#f92672">==</span> bn <span style="color:#f92672">and</span> x_tf_out<span style="color:#f92672">.</span>size(<span style="color:#ae81ff">1</span>) <span style="color:#f92672">==</span> k),<span style="color:#e6db74">&#39;{} {} {} {}&#39;</span><span style="color:#f92672">.</span>format(
        bn, k, x_tf_out<span style="color:#f92672">.</span>size(<span style="color:#ae81ff">0</span>), x_tf_out<span style="color:#f92672">.</span>size(<span style="color:#ae81ff">1</span>))

    p_i_j <span style="color:#f92672">=</span> x_out<span style="color:#f92672">.</span>unsqueeze(<span style="color:#ae81ff">2</span>) <span style="color:#f92672">*</span> x_tf_out<span style="color:#f92672">.</span>unsqueeze(<span style="color:#ae81ff">1</span>) <span style="color:#75715e"># bn, k, k</span>
    p_i_j <span style="color:#f92672">=</span> p_i_j<span style="color:#f92672">.</span>sum(dim<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>) <span style="color:#75715e"># k, k</span>
    p_i_j <span style="color:#f92672">=</span> (p_i_j <span style="color:#f92672">+</span> p_i_j<span style="color:#f92672">.</span>t()) <span style="color:#f92672">/</span> <span style="color:#ae81ff">2.</span> <span style="color:#75715e"># symmetrise</span>
    p_i_j <span style="color:#f92672">=</span> p_i_j <span style="color:#f92672">/</span> p_i_j<span style="color:#f92672">.</span>sum() <span style="color:#75715e"># normalise</span>
    <span style="color:#66d9ef">return</span> p_i_j


<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">IID_loss</span>(x_out, x_tf_out, EPS<span style="color:#f92672">=</span>sys<span style="color:#f92672">.</span>float_info<span style="color:#f92672">.</span>epsilon):
    <span style="color:#75715e"># has had softmax applied</span>
    bs, k <span style="color:#f92672">=</span> x_out<span style="color:#f92672">.</span>size()
    p_i_j <span style="color:#f92672">=</span> compute_joint(x_out, x_tf_out)
    <span style="color:#66d9ef">assert</span> (p_i_j<span style="color:#f92672">.</span>size() <span style="color:#f92672">==</span> (k, k))

    p_i <span style="color:#f92672">=</span> p_i_j<span style="color:#f92672">.</span>sum(dim<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)<span style="color:#f92672">.</span>view(k, <span style="color:#ae81ff">1</span>)<span style="color:#f92672">.</span>expand(k, k)
    p_j <span style="color:#f92672">=</span> p_i_j<span style="color:#f92672">.</span>sum(dim<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)<span style="color:#f92672">.</span>view(<span style="color:#ae81ff">1</span>, k)<span style="color:#f92672">.</span>expand(k, k)

    <span style="color:#75715e"># avoid NaN losses. Effect will get canceled out by p_i_j tiny anyway</span>
    <span style="color:#75715e"># This is an error with PyTorch version 1.3 and above</span>
    <span style="color:#75715e"># https://discuss.pytorch.org/t/pytorch-1-3-showing-an-error-perhaps-for-loss-computed-from-paired-outputs/68790/3</span>
    <span style="color:#75715e">#p_i_j[(p_i_j &lt;EPS).data] = EPS</span>
    <span style="color:#75715e">#p_j[(p_j &lt;EPS).data] = EPS</span>
    <span style="color:#75715e">#p_i[(p_i &lt;EPS).data] = EPS</span>

    p_i_j <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>where(p_i_j <span style="color:#f92672">&lt;</span>EPS, torch<span style="color:#f92672">.</span>tensor(
        [EPS], device<span style="color:#f92672">=</span>p_i_j<span style="color:#f92672">.</span>device), p_i_j)
    p_j <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>where(p_j <span style="color:#f92672">&lt;</span>EPS, torch<span style="color:#f92672">.</span>tensor([EPS], device<span style="color:#f92672">=</span>p_j<span style="color:#f92672">.</span>device), p_j)
    p_i <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>where(p_i <span style="color:#f92672">&lt;</span>EPS, torch<span style="color:#f92672">.</span>tensor([EPS], device<span style="color:#f92672">=</span>p_i<span style="color:#f92672">.</span>device), p_i)

    <span style="color:#75715e"># https://qiita.com/Amanokawa/items/0aa24bc396dd88fb7d2a</span>
    <span style="color:#75715e"># Add weight alpha for reference</span>

    alpha <span style="color:#f92672">=</span> <span style="color:#ae81ff">2.0</span>
    loss <span style="color:#f92672">=</span> (<span style="color:#f92672">-</span> p_i_j <span style="color:#f92672">*</span> (torch<span style="color:#f92672">.</span>log(p_i_j)<span style="color:#f92672">-</span>alpha <span style="color:#f92672">*</span>
                       torch<span style="color:#f92672">.</span>log(p_j)<span style="color:#f92672">-</span>alpha<span style="color:#f92672">*</span>torch<span style="color:#f92672">.</span>log(p_i)))<span style="color:#f92672">.</span>sum()

    <span style="color:#66d9ef">return</span> loss
</code></pre></div><p>Next, define the transformation to be applied to the vectorized data of interest.</p>
<p>This conversion function is the key to IIC.</p>
<p>In the case of image data, affine transformation (rotation/expansion), aspect ratio change, clipping (clipping), and noise addition are given according to normal data augmentation.</p>
<p>What about text data?</p>
<p>In data augmentation in competitions such as Kaggle, we sometimes translate it back into another language and back.</p>
<p>This time, let&rsquo;s simply add noise based on the standard deviation of all vector data.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e">#Define a function that adds noise to the datadevice =&#39;cuda&#39; if torch.cuda.is_available() else&#39;cpu&#39;</span>
tensor_std <span style="color:#f92672">=</span> list_text_train<span style="color:#f92672">.</span>std(dim<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)<span style="color:#f92672">.</span>to(device)


<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">perturb_data</span>(x):
    y <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>clone()
    noise <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>randn(len(tensor_std))<span style="color:#f92672">.</span>to(device)<span style="color:#f92672">*</span>tensor_std<span style="color:#f92672">*</span><span style="color:#ae81ff">2.0</span>
    noise <span style="color:#f92672">=</span> noise<span style="color:#f92672">.</span>expand(x<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>], <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>)
    y <span style="color:#f92672">+=</span> noise

    <span style="color:#66d9ef">return</span> y
</code></pre></div><h2 id="4-train-the-iic-network">4: Train the IIC network</h2>
<p>Now that the DataLoader and IIC models have been prepared, we will learn the weights of the IIC model.</p>
<p>First, define the training function.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># Learning function definition</span>


<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">train</span>(total_epoch, model, train_loader, optimizer, device):

    <span style="color:#75715e"># Put network into training mode</span>
    model<span style="color:#f92672">.</span>train()

    <span style="color:#75715e"># Learning rate scheduler CosAnnealing</span>
    scheduler <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>optim<span style="color:#f92672">.</span>lr_scheduler<span style="color:#f92672">.</span>CosineAnnealingWarmRestarts(
        optimizer, T_0<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>, T_mult<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>, eta_min<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)

    <span style="color:#66d9ef">for</span> epoch <span style="color:#f92672">in</span> range(total_epoch):
        <span style="color:#66d9ef">for</span> batch_idx, (target, data) <span style="color:#f92672">in</span> enumerate(train_loader):

            <span style="color:#75715e"># Change in learning rate</span>
            scheduler<span style="color:#f92672">.</span>step()

            data_perturb <span style="color:#f92672">=</span> perturb_data(data) <span style="color:#75715e"># give noise and make converted data</span>

            <span style="color:#75715e">#Send if you can send to GPU</span>
            data <span style="color:#f92672">=</span> data<span style="color:#f92672">.</span>to(device)
            data_perturb <span style="color:#f92672">=</span> data_perturb<span style="color:#f92672">.</span>to(device)

            <span style="color:#75715e"># Optimization function initialization</span>
            optimizer<span style="color:#f92672">.</span>zero_grad()

            <span style="color:#75715e"># Add to neural network</span>
            output, output_overclustering <span style="color:#f92672">=</span> model(data)
            output_perturb, output_perturb_overclustering <span style="color:#f92672">=</span> model(data_perturb)

            <span style="color:#75715e"># Loss calculation</span>
            loss1 <span style="color:#f92672">=</span> IID_loss(output, output_perturb)
            loss2 <span style="color:#f92672">=</span> IID_loss(output_overclustering,
                             output_perturb_overclustering)
            loss <span style="color:#f92672">=</span> loss1 <span style="color:#f92672">+</span> loss2

            <span style="color:#75715e">#Updated to reduce losses</span>
            loss<span style="color:#f92672">.</span>backward()
            optimizer<span style="color:#f92672">.</span>step()

        <span style="color:#75715e"># Log output</span>
        <span style="color:#66d9ef">if</span> epoch <span style="color:#f92672">%</span><span style="color:#ae81ff">50</span> <span style="color:#f92672">==</span> <span style="color:#ae81ff">0</span>:
            <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#39;Train Epoch {} </span><span style="color:#ae81ff">\t</span><span style="color:#e6db74">Loss1: {:.6f} </span><span style="color:#ae81ff">\t</span><span style="color:#e6db74">Loss2: {:.6f} </span><span style="color:#ae81ff">\t</span><span style="color:#e6db74">Loss_total: {:.6f}&#39;</span><span style="color:#f92672">.</span>format(
                epoch, loss1<span style="color:#f92672">.</span>item(), loss2<span style="color:#f92672">.</span>item(), loss1<span style="color:#f92672">.</span>item()<span style="color:#f92672">+</span>loss2<span style="color:#f92672">.</span>item()))

    <span style="color:#66d9ef">return</span> model, optimizer
</code></pre></div><p>In the above training, the learning rate is changed by preparing scheduler CosineAnnealingWarmRestarts.</p>
<p>This is a device to change the learning rate as shown below, to escape from the local solution when the value is rapidly increased from a small value, and to make it easy to learn the parameter to the global minimum solution.</p>
<p><img src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/191401/35665e4c-a555-def1-22d8-c9a09c7489c8.jpeg" alt="sgdr.jpg">
Figure: Quote
<a href="https://www.kaggle.com/c/imet-2019-fgvc6/discussion/94783">https://www.kaggle.com/c/imet-2019-fgvc6/discussion/94783</a></p>
<p>Conduct learning.
This time, the batch size is 1,024 and the data itself is about 4,000, so the number of epochs is increased.</p>
<p>It takes less than 5 minutes to study.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># Conduct learning (less than 5 minutes)</span>

total_epoch <span style="color:#f92672">=</span> <span style="color:#ae81ff">1000</span>
optimizer <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>optim<span style="color:#f92672">.</span>Adam(net<span style="color:#f92672">.</span>parameters(), lr<span style="color:#f92672">=</span><span style="color:#ae81ff">5e-4</span>) <span style="color:#75715e"># optimizer function</span>

model_trained, optimizer <span style="color:#f92672">=</span> train(
    total_epoch, net, dl_bert_train, optimizer, device)
</code></pre></div><p>Now that the learning is complete, let&rsquo;s make an inference with the test data.
To make it easier to understand the results, prepare the Data Loader for the test of mini batch size 1 again and infer one by one,
Store the result.</p>
<p>The function is defined below.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># Check the results of the model classification cluster</span>
<span style="color:#f92672">import</span> numpy <span style="color:#f92672">as</span> np

<span style="color:#75715e"># Prepare Data Loader for testing mini batch size 1</span>
dl_bert_test <span style="color:#f92672">=</span> DataLoader(
    dataset_bert_test, batch_size<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, shuffle<span style="color:#f92672">=</span>False)


<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">test</span>(model, device, test_loader):
    model<span style="color:#f92672">.</span>eval()

    out_targs <span style="color:#f92672">=</span> []
    ref_targs <span style="color:#f92672">=</span> []

    <span style="color:#75715e"># Prepare list for output</span>
    total_num <span style="color:#f92672">=</span> len(test_loader)
    <span style="color:#75715e"># index, (target_label, inferenced_label)</span>
    output_list <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>zeros((total_num, <span style="color:#ae81ff">2</span>))

    <span style="color:#66d9ef">with</span> torch<span style="color:#f92672">.</span>no_grad():
        <span style="color:#66d9ef">for</span> batch_idx, (target, data) <span style="color:#f92672">in</span> enumerate(test_loader):
            data <span style="color:#f92672">=</span> data<span style="color:#f92672">.</span>to(device)
            target <span style="color:#f92672">=</span> target<span style="color:#f92672">.</span>to(device)
            outputs, outputs_overclustering <span style="color:#f92672">=</span> model(data)

            <span style="color:#75715e"># Add classification result to list</span>
            out_targs<span style="color:#f92672">.</span>append(outputs<span style="color:#f92672">.</span>argmax(dim<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)<span style="color:#f92672">.</span>cpu())
            ref_targs<span style="color:#f92672">.</span>append(target<span style="color:#f92672">.</span>cpu())

            <span style="color:#75715e"># Put results in a list</span>
            output_list[batch_idx, <span style="color:#ae81ff">0</span>] <span style="color:#f92672">=</span> target<span style="color:#f92672">.</span>cpu() <span style="color:#75715e"># correct label</span>
            output_list[batch_idx, <span style="color:#ae81ff">1</span>] <span style="color:#f92672">=</span> outputs<span style="color:#f92672">.</span>argmax(dim<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)<span style="color:#f92672">.</span>cpu() <span style="color:#75715e"># prediction label</span>

    out_targs <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>cat(out_targs)
    ref_targs <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>cat(ref_targs)

    <span style="color:#66d9ef">return</span> out_targs<span style="color:#f92672">.</span>view(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>)<span style="color:#f92672">.</span>numpy(), ref_targs<span style="color:#f92672">.</span>numpy(), output_list
</code></pre></div><h2 id="5-infer-test-data">5. Infer test data</h2>
<p>Perform inference on test data.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># Perform inference with test data</span>

out_targs, ref_targs, output_list <span style="color:#f92672">=</span> test(model_trained, device, dl_bert_test)
</code></pre></div><p>Check the confusion matrix frequency table of inference results on the test data.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># Make a confusion matrix</span>
matrix <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>zeros((<span style="color:#ae81ff">9</span>, <span style="color:#ae81ff">9</span>))

<span style="color:#75715e"># Create vertical correct class of livedoor news and horizontal class frequency table</span>
<span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(len(out_targs)):
    row <span style="color:#f92672">=</span> ref_targs[i]
    col <span style="color:#f92672">=</span> out_targs[i]
    matrix[row][col] <span style="color:#f92672">+=</span> <span style="color:#ae81ff">1</span>

np<span style="color:#f92672">.</span>set_printoptions(suppress<span style="color:#f92672">=</span>True)
<span style="color:#66d9ef">print</span>(matrix)
</code></pre></div><p>The output result is as follows.</p>
<pre><code>[[ 55. 0. 1. 0. 4. 47. 2. 76. 0.]
 [3. 40. 4. 0. 14. 1. 1. 0.1. 116.]
 [7. 39. 21. 4. 16. 3. 6. 3. 1.]
 [11. 60. 16. 4. 13. 8. 27. 2. 17.]
 [8. 6. 20. 107. 1. 8. 16. 0. 1.]
 [11. 17. 15. 0. 40. 6. 78. 7. 0.]
 [18. 3. 65. 40. 13. 15. 14. 3. 1.]
 [63. 7. 45. 11. 2. 42. 7. 1. 1.]
 [27. 0. 6. 0. 4. 61. 1. 61. 1.]]
</code></pre><p>In addition, the vertical axis confirms the contents of <code>dic_id2cat</code>, and the order is as follows.</p>
<pre><code>{0:'sports-watch',
 1:'dokujo-tsushin',
 2:'livedoor-homme',
 3:'peachy',
 4:'smax',
 5:'movie-enter',
 6:'it-life-hack',
 7:'kaden-channel',
 8:'topic-news'}
</code></pre><p>First, as in MNIST, the clusters presumed to be classes do not match exactly.
I think this is something like that.</p>
<p>If you are instructed to classify a large number of documents into several types, I think that even humans will classify (cluster) in various patterns.
When I was asked to classify livedoor news into nine categories, this time I got this result.</p>
<p>The second estimated cluster is mostly &ldquo;it-life-hack&rdquo; and &ldquo;kaden-channel&rdquo;.
The third of the estimated clusters is mostly &ldquo;smax&rdquo; (smartphones and gadgets).
The last cluster inferred seems to be the cluster with the most &ldquo;single-girl communication&rdquo;.</p>
<p>The second from the end is &ldquo;sports-watch&rdquo; and &ldquo;topic-news&rdquo;.The &ldquo;sports-watch&rdquo; and &ldquo;topic-news&rdquo; classes are:
It is separated into 0th, 5th and 7th.</p>
<p>The sentence of the fifth cluster of &ldquo;sports-watch&rdquo;, the sentence of the seventh cluster
&ldquo;Topic-news&rdquo; 5th cluster sentence, 7th cluster sentence</p>
<p>Let&rsquo;s take a look at the characteristics of cluster 5 and cluster 7.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># Check cluster result</span>
<span style="color:#75715e">#&#39;Sports-watch&#39; 5th cluster sentence, 7th cluster sentence</span>
<span style="color:#75715e">#The text of the 5th cluster of &#34;topic-news&#34;, the text of the 7th cluster</span>
Check <span style="color:#75715e"># to see the characteristics of cluster 5 and cluster 7.</span>

<span style="color:#f92672">import</span> pandas <span style="color:#f92672">as</span> pd

df2 <span style="color:#f92672">=</span> pd<span style="color:#f92672">.</span>DataFrame(output_list)
df2<span style="color:#f92672">.</span>columns<span style="color:#f92672">=</span>[<span style="color:#e6db74">&#34;correct class&#34;</span>, <span style="color:#e6db74">&#34;estimated cluster&#34;</span>]
df2<span style="color:#f92672">.</span>head()
</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">df2[(df2[<span style="color:#e6db74">&#39;correct class&#39;</span>]<span style="color:#f92672">==</span><span style="color:#ae81ff">0</span>) <span style="color:#f92672">&amp;</span> (df2[<span style="color:#e6db74">&#39;estimated cluster&#39;</span>]<span style="color:#f92672">==</span><span style="color:#ae81ff">5</span>)]<span style="color:#f92672">.</span>head()
</code></pre></div><p>The output is as follows.</p>
<pre><code>Correct class Estimated cluster
21 0.0 5.0
59 0.0 5.0
126 0.0 5.0
142 0.0 5.0
153 0.0 5.0
</code></pre><p>Let&rsquo;s look at the 21st and 59th sentences in the test dataset.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">The original document <span style="color:#f92672">is</span> put <span style="color:#f92672">in</span> <span style="color:#75715e">#df. See about 300 characters.</span>
<span style="color:#66d9ef">print</span>(df<span style="color:#f92672">.</span>iloc[<span style="color:#ae81ff">21</span>, <span style="color:#ae81ff">0</span>][:<span style="color:#ae81ff">300</span>])
<span style="color:#66d9ef">print</span>(df<span style="color:#f92672">.</span>iloc[<span style="color:#ae81ff">59</span>, <span style="color:#ae81ff">0</span>][:<span style="color:#ae81ff">300</span>])

</code></pre></div><p>The first sentence of the fifth cluster of &ldquo;sports-watch&rdquo; is</p>
<blockquote>
<p>On the 29th of last month, after retiring as a professional wrestler, Kotetsu Yamamoto, who was loved by fans as a player training, commentator, and referee, died suddenly due to hypoxic encephalopathy and caused deep sorrow to fans and related parties. In such a case, &ldquo;Weekly Asahi Performing Arts&rdquo; released on the 7th this week, in the &ldquo;NEWS SHOT!&rdquo; corner, reported an amazing action that suggests Yamamoto&rsquo;s &ldquo;just before death&rdquo; masterpiece. Former weekly professional wrestling editor-in-chief Tarzan Yamamoto, who commented on the magazine, said, &ldquo;A body that is 170 cm tall and weighs 113 kg still maintains muscles that are reminiscent of the days in which he was active. I trained and had an appetite that I didn&rsquo;t think I was 68 years old, but Mr. Yamamoto was actually diabetic.&rdquo;</p>
</blockquote>
<p>have become.</p>
<p>Similarly,</p>
<p>The first sentence of the 7th cluster of &ldquo;sports-watch&rdquo; is</p>
<blockquote>
<p>An interview with Hiromitsu Ochiai, former director of the Chunichi Dragons, where shocking words such as &ldquo;everything went crazy&rdquo;, &ldquo;I have never touched the pitcher&rdquo;, &ldquo;nobody trusts&rdquo; .. On Nippon TV &ldquo;Going! Sports &amp; News&rdquo;, a baseball commentator, Mr. Taku Egawa, was the listener and the pattern was broadcast over two nights (17th and 18th). In the first broadcast, Mr. Ochiai revealed an unknown episode, saying, &ldquo;If I wear a uniform next year, I won&rsquo;t talk so far,&rdquo; Mr. Ochiai said, but the second part of the broadcast is more surprising than that. It should be the contents. Below is a summary of the interview. Egawa: Do you think you can be the best in Japan this year? I don&rsquo;t think</p>
</blockquote>
<p>is. The first sentence of the fifth cluster of &ldquo;topic-news&rdquo; is</p>
<blockquote>
<p>In Korea, it is generally a rule that monks are single, but the monk&rsquo;s Facebook, which says &ldquo;I decided to leave home without despair from the opposite sex,&rdquo; has become a hot topic on Korean bulletin boards. The monk named Hyobon said on Facebook on the 19th, I was in my twenties not popular with the opposite sex&hellip;. I decided to go home because I came to the conclusion that I couldn&rsquo;t help it. Go home. If you think about it now, you should have done rap and punk when you were young,&rdquo; said the background of becoming a monk. Regarding the monk&rsquo;s life, he said, &ldquo;The sooner you go home, the faster you become great. You don&rsquo;t have to worry about rice and washing when you get big.&rdquo;</p>
</blockquote>
<p>is. The first sentence of the 7th cluster of &ldquo;topic-news&rdquo; is</p>
<blockquote>
<p>On the 18th, the Nihon Keizai Shimbun reported on the distorted reality of the disaster-stricken area in an article titled &ldquo;Another abnormal situation in the disaster-stricken area: special demand for reconstruction/compensation for nuclear power generation&hellip; ing. The article pointed out that if the family to be compensated is a family of five, 800,000 yen per month will be in the pocket, ``I received money from TEPCO, I would like to pachinko, sushi, smartphones and bags new without working. The word of a local person saying &ldquo;I am doing&rdquo; is posted. The compensation paid to the victims and how to use it are positioned as distorted reproduction. On the online bulletin boards, &ldquo;I didn&rsquo;t have any blind spots for donations,&rdquo; &ldquo;I&rsquo;m sorry for the earthquake,&rdquo; &ldquo;I don&rsquo;t care because smartphones and sushi are consumed, but pachinko is useless&hellip;&rdquo;</p>
</blockquote>
<p>is.</p>
<p>You can&rsquo;t really understand the characteristics just by looking at the text. .. ..</p>
<p>However, I understand that sports and news have very similar moods in the articles.</p>
<p>More than this, in order to properly understand the characteristics of IIC&rsquo;ed cluster,</p>
<ul>
<li>See word frequency trends with wordcloud</li>
<li>Fiddle with all vectors of cluster document, create cluster vector of some kind, decide representative document and put out similar words</li>
</ul>
<p>Operations such as</p>
<p>That&rsquo;s too long for this article, so let&rsquo;s finish this time.</p>
<p>The implementation code of this post is placed in the following GitHub repository.</p>
<p><a href="https://github.com/YutaroOgawa/BERT_Japanese_Google_Colaboratory">GitHub: How to use Japanese version of BERT on Google Colaboratory: Implementation code</a>
This is 4_BERT_livedoor_news_IIC_on_Google_Colaboratory.ipynb.</p>
<p>The above is an implementation example and explanation for livedoor news as IIC clustering for unsupervised learning in Japanese BERT.</p>
<p>Thank you for reading this.</p>
<hr>
<p>[Remarks] <a href="https://www.isidgroup.com/u/job.phtml?job_code=430">The AI Technology Development Team, which I lead, is looking for members. Click here if you are interested</a></p>
<p>[Disclaimer] The content itself of this article is the opinion/dissemination of the author, not the official opinion of the company to which the author belongs.</p>
<hr>

    </article>
  </div>

  
  
  
  <script>
  window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
  ga('create', 'UA-169005401-1', 'auto');
  ga('send', 'pageview');
  </script>
  <script async src='https://www.google-analytics.com/analytics.js'></script>
  

  

  
<link rel="stylesheet" type="text/css" href="//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.0/cookieconsent.min.css" />
<script src="//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.0/cookieconsent.min.js"></script>
<script>
window.addEventListener("load", function(){
window.cookieconsent.initialise({
  "palette": {
    "popup": {
      "background": "#216942",
      "text": "#b2d192"
    },
    "button": {
      "background": "#afed71"
    }
  }
})});
</script>

</body>

</html>
