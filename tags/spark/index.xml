<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Spark on Some Title</title>
    <link>https://japan2018.github.io/tags/spark/</link>
    <description>Recent content in Spark on Some Title</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 19 Mar 2020 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://japan2018.github.io/tags/spark/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Calculate AUC by groupBy of PySpark DataFrame (Define aggregation function in pandas_udf)</title>
      <link>https://japan2018.github.io/calculate-auc-by-groupby-of-pyspark-dataframe-define-aggregation-function-in-pandas_udf-fd299/</link>
      <pubDate>Thu, 19 Mar 2020 00:00:00 +0000</pubDate>
      
      <guid>https://japan2018.github.io/calculate-auc-by-groupby-of-pyspark-dataframe-define-aggregation-function-in-pandas_udf-fd299/</guid>
      <description>#Introduction  When calculating AUC with PySpark, you can easily obtain it by using the BinaryClassificationEvaluator class. However, as it is, it is not possible to meet the need to calculate the AUC for each segment rather than the entire test data in order to understand the differences between the models.
As a workaround for this, I defined an aggregate function that calculates AUC using pandas_udf and calculated it using the agg method.</description>
    </item>
    
    <item>
      <title>Precautions when using string for TmeStampType of PySpark</title>
      <link>https://japan2018.github.io/precautions-when-using-string-for-tmestamptype-of-pyspark-fcecc/</link>
      <pubDate>Mon, 25 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>https://japan2018.github.io/precautions-when-using-string-for-tmestamptype-of-pyspark-fcecc/</guid>
      <description>#Introduction  It is a story that there is a pattern that behaves unintentionally when the character string type of date format is carelessly used when calculating TimeStampType of PySpark.
Therefore, it is possible to operate on TimeStampType with string, but it is safer to use datetime.
#Example
A specific example will be described.
The example shown here is from PySPark 2.4.4.
Verification data With the following code, create a Spark DataFrame with date data of 2000/1/1 to 2000/1/5 and try the conditional processing for this date data.</description>
    </item>
    
  </channel>
</rss>